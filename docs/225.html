<html>
<head>
<title>Training, Visualizing, and Understanding Word Embeddings: Deep Dive Into Custom Datasets </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>训练、可视化和理解单词嵌入:深入定制数据集</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/word-embeddings-deep-dive-into-custom-datasets#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/word-embeddings-deep-dive-into-custom-datasets#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>人工智能(AI)发展中最强大的趋势之一是自然语言处理(NLP)领域的快速发展。这一进步主要是由深度学习技术在神经网络架构中的应用推动的，这使得像<a href="https://web.archive.org/web/20221206214014/https://en.wikipedia.org/wiki/BERT_(language_model)" target="_blank" rel="noreferrer noopener nofollow">伯特</a>和<a href="https://web.archive.org/web/20221206214014/https://en.wikipedia.org/wiki/GPT-3" target="_blank" rel="noreferrer noopener nofollow"> GPT-3 </a>这样的模型在一系列语言任务中表现得更好，这些任务之前被教导为<a href="https://web.archive.org/web/20221206214014/https://thenewstack.io/openais-gpt-3-makes-big-leap-forward-for-natural-language-processing/" target="_blank" rel="noreferrer noopener nofollow">超出了大多数NLP模型</a>的范围。</p>



<p>虽然这些模型的全部潜力可能存在争议(更多内容见<a href="/web/20221206214014/https://neptune.ai/blog/ai-limits-can-deep-learning-models-like-bert-ever-understand-language" target="_blank" rel="noreferrer noopener">我们关于这些模型是否能真正理解语言的帖子</a>)，但毫无疑问，它们对学术界和商界的影响才刚刚开始被感受到。</p>



<p>因此，理解如何为您自己的用例测试和训练这些模型是很重要的。所有这些深度学习模型的一个关键部分是它们能够将语言信息编码到称为<a href="https://web.archive.org/web/20221206214014/https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526" target="_blank" rel="noreferrer noopener nofollow">嵌入</a>的向量中。</p>



<p>在这篇文章中，我们将看看你可以使用的不同技术，以更好地理解语言模型如何捕捉单词之间的上下文关系。我们将通过以下方式做到这一点:</p>



<ol><li>查看数据集，我们需要训练这些模型，看看我们是否可以找到一个简单的方法来帮助我们可视化这些模型如何“学习”不同单词之间的关系</li><li>看看您可以使用哪些工具和技术来跟踪这些模型的进度，并在它们处理我们的简化数据集时监控结果</li><li>在那之后，你应该有希望能够在一些真实的数据集上，为更复杂的模型重用那个模板。</li></ol>



<p>有许多因素使这种类型的项目变得困难。首先，语言本身是复杂的，所以很难知道一个词和另一个词的关系有多密切。这个模型说“猫”在意思上更接近“狗”或“狮子”是对的吗？其次，我们如何识别这种上下文关系？有没有我们可以用来理解相似性的分数，以及我们可以用来直观理解这种关系的方法？</p>



<p>我们将看看如何定制您的实验来控制其中的一些变量，以便更好地理解模型在处理文本和输出一些非常大的向量时实际上在做什么。说到矢量…</p>



<h2 id="h-the-worlds-of-vectors">向量的世界</h2>



<p>该信息可以包括特定单词的语义和句法方面。虽然每个词都可以被认为是一个范畴变量，即一个独立的实体，但它总是与其他词有某种联系。例如，都柏林是爱尔兰的首都。都柏林这个词因此与:</p>



<ul><li>作为城市的事物，</li><li>大写的东西，</li><li>爱尔兰的东西</li><li>诸如此类…</li></ul>



<p>如你所见，很难完全定义一个特定单词可能具有的所有关系。然而，这正是我们在创建嵌入时试图做的事情。</p>



<p>伯特和GPT-3等最新模型在这项任务上表现出色，因为它们可以处理大量文本，并对它们在这些嵌入中学习到的许多关系进行编码。他们的前辈是“不那么深”的神经网络，如<a href="https://web.archive.org/web/20221206214014/https://en.wikipedia.org/wiki/Word2vec" target="_blank" rel="noreferrer noopener nofollow"> Word2Vec </a>使用类似的技术来学习单词之间的关联，并将信息编码到嵌入中。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/71383c11553f93c06caff3d81938103f.png" alt="gpt3-embedding" class="wp-image-34505" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206214014im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/gpt3-embedding.gif?ssl=1"/><figcaption><em>From <a href="https://web.archive.org/web/20221206214014/http://jalammar.github.io/how-gpt3-works-visualizations-animations/" target="_blank" rel="noreferrer noopener nofollow">Jay Alammar’s amazing blog</a>: These models, like GPT-3 here, turn the word into a vector and then do their “magic” to learn the relationship between it and other words depending on the context. (Note you should checkout Jay’s blog for all things deep learning NLP)</em></figcaption></figure></div>



<p>虽然这些技术是相似的，但它们比这些语言模型(LMs)的最新实例中使用的一些更复杂和高级的方法更容易理解。因此，这是一个展示如何训练嵌入并理解它们在被训练时所包含的信息的好地方。</p>



<p>希望这将帮助你在你自己的ML项目中应用这些技术到更高级的LMs，跟踪你的进度，并可视化结果。</p>



<h2 id="h-project-description">项目描述</h2>



<p>您可能已经接触过大量关于如何创建自己的word嵌入的教程和示例。这些资源中的许多都是抓住主题的好方法。</p>



<p>从个人角度来看，我一直在努力理解这些嵌入在给定底层训练数据的情况下表现如何。例如，许多例子使用莎士比亚的作品来训练它们的嵌入。虽然我是“吟游诗人”的粉丝，但在读过他所有的作品后，很难知道“爱”、“家庭”和“战争”这些词应该有多么密切的联系。</p>



<p>同样，Word2Vec上的<a href="https://web.archive.org/web/20221206214014/https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noreferrer noopener nofollow">原论文</a>中最著名的例子就是<a href="https://web.archive.org/web/20221206214014/https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html" target="_blank" rel="noreferrer noopener nofollow">“国王”和“王后”</a>之间的关系。相反，我认为创建我们自己的简化语言数据集来训练我们的单词嵌入会更好。这样，我们可以控制词汇之间的关系，并更好地理解嵌入是否确实捕捉到了相关单词之间的关系。</p>



<p>希望这将是一个数据集，您可以使用并改进训练模式高级模型，如那些采用最新NLP模型使用的<a href="https://web.archive.org/web/20221206214014/https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noreferrer noopener nofollow">注意力架构</a>的模型，如BERT和GPT-3。</p>



<p>此外，我们将利用<a href="/web/20221206214014/https://neptune.ai/" target="_blank" rel="noreferrer noopener">海王星</a>来跟踪我们嵌入的进展，并可视化它们的关系。同样，您应该能够使用这些技术轻松地跟踪其他NLP项目的进展，在这些项目中，您正在训练自己的嵌入。你可以在这里查看<a href="https://web.archive.org/web/20221206214014/https://ui.neptune.ai/choran/sandbox/e/SAN-20/artifacts?path=charts%2F&amp;file=proximity-similarity-3d-plot-epoch900.html" target="_blank" rel="noreferrer noopener">我在Neptune </a>上的公共项目，看看你按照下面的代码运行时的一些结果。</p>


<p>该项目的结构如下:</p>



<ol><li><strong>数据集</strong>:为了能够理解我们“单词”之间的关系，我们将定义我们自己的简单语言数据集。</li><li><strong>模型</strong>:我们将使用<a href="https://web.archive.org/web/20221206214014/https://www.tensorflow.org/tutorials/text/word2vec" target="_blank" rel="noreferrer noopener nofollow"> TensorFlow教程系列</a>中的Word2Vec代码。有许多方法可以使用Word2Vec，例如在<a href="https://web.archive.org/web/20221206214014/https://radimrehurek.com/gensim/models/word2vec.html" target="_blank" rel="noreferrer noopener nofollow"> gensim库</a>中。TF教程系列的好处是，你可以看到所有血淋淋的内部，并可以改变任何你喜欢的修补和测试模型。</li><li><strong> <a href="/web/20221206214014/https://neptune.ai/experiment-tracking" target="_blank" rel="noreferrer noopener">实验跟踪</a> </strong>:我们会在训练时设置海王星来跟踪模型的进度。我们可以用它来跟踪损失和准确性，还可以显示“单词”之间的相似性在训练过程中是如何变化的。</li><li>可视化:我们还将展示如何通过观看Neptune中的二维和三维可视化来理解玩具数据集中“单词”之间的关系。</li></ol>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/403e95607a53713cd89c2b83ed54716a.png" alt="DL embeddings loss accuracy charts" class="wp-image-34468" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206214014im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/DL-embeddings-loss-accuracy-charts.png?resize=1024%2C412&amp;ssl=1"/><figcaption><em>Loss and accuracy charts available in Neptune</em></figcaption></figure></div>



<p class="has-text-align-center"/>



<p>让我们开始创建数据集吧。</p>



<h2 id="h-a-perfect-language">完美的语言</h2>



<p>为了更好地展示像Word2Vec这样的模型如何学习单词之间的关系，我们需要创建自己的数据集。我们的目标是创建一个数据集，在这个数据集内，单词的接近度被完美地定义。举个例子，想想你脑海中闪现的任何一个词。</p>



<p>例如，单词“computer”是一个常见的单词，与许多其他单词有关系或“接近”。“计算机”在通常情况下可以互换使用，或者在与PC、mac、IBM、笔记本电脑和台式机类似的上下文中使用。它通常与屏幕、程序、操作系统、软件、硬件等结合使用。当有人提到“计算机”科学家或“计算机”技术人员时，它与职业相关。</p>



<p>但是嘿！有些人的职业被称为“计算机”，他们负责复杂的计算。这个职业在17世纪被提及，当时科学家雇佣“计算机”来帮助他们进行计算，约翰尼斯·开普勒在推进他自己的科学项目之前就是这些计算机中的一员。随着ENIAC计算机的出现，这个职业一直繁荣到第二次世界大战结束，那时人类计算机成为了第一批程序员。</p>



<p>考虑到以上所有因素，人们可以期待计算机更人性化。</p>



<p>我们可以在这里继续下去，特定的关系通常是特定于领域的。如果你在医疗保健行业工作，那么“计算机”和IT或软件工程师之间的关系可能会有所不同。</p>



<p>这有什么意义？嗯，很难知道“计算机”与这些术语有多相似。是否应该更类似于“操作系统”而不是“笔记本电脑”或“科学家”。不知道这一点，我们将永远谈论Word2Vec及其“国王”和“王后”。相反，让我们创建自己的简单语言，这样我们就可以:</p>



<ol><li>缩小词汇量，以便快速训练</li><li>定义测试数据集中每个句子的长度，这样我们就不需要任何填充或者增加不同长度句子的复杂性。</li><li>在我们的数据集中定义“单词”之间的严格关系，这样我们就可以确切地知道层次结构应该是什么</li><li>不包括任何连词或例外或任何混乱不规则的语言魔法，使学习一门新语言如此困难！</li></ol>



<p>我能想到的最简单的方法是用一个简单的词汇，我们有26个基于字母表的三个字母的单词，例如“aaa”，“bbb”，“ccc”等等。</p>



<p>然后我们就可以定义这些“词”之间我们想要的“接近度”。“AAA”*这个简单的语言里没有大写！)例如，应该只出现在“bbb”或“ccc”旁边。“bbb”应该只出现在“ccc”或“ddd”等旁边。这样，当我们查看单词之间的相似性时，我们希望看到“aaa”在相似性方面总是接近“bbb”和“ccc ”,但应该远离“xxx”或“zzz”。</p>



<p>这样做的好处是你可以开始让你自己的语言变得简单或复杂。你可以开始包括更复杂的关系或连词等等。包括一个类似于“and”用法的单词，这样它就会出现在你的新语言中的任何单词旁边。然后看和不同的词有多相似。现在，我们将创建一些简单的示例来帮助您开始。</p>



<h3>随机语言</h3>



<p>让我们从一个随机的语言数据集开始，我们不期望你的新语言中的单词之间有任何关系。你可以在这里找到生成你自己语言<a href="https://web.archive.org/web/20221206214014/https://github.com/choran/word_embeddings/blob/main/custom_language_dataset.ipynb" target="_blank" rel="noreferrer noopener nofollow">的所有相关代码。</a></p>



<pre class="hljs"><span class="hljs-keyword">import</span> string
<span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> choice

alpha_d = dict.fromkeys(string.ascii_lowercase, <span class="hljs-number">0</span>)

vocab = {}
<span class="hljs-keyword">for</span> c, l <span class="hljs-keyword">in</span> enumerate(alpha_d, <span class="hljs-number">0</span>):
    vocab[c] = l*<span class="hljs-number">3</span>

sen_num = <span class="hljs-number">30000</span>
sen_len = <span class="hljs-number">7</span>
vocab_len = <span class="hljs-number">26</span>

text_file = open(<span class="hljs-string">"random_vocab.txt"</span>, <span class="hljs-string">"a"</span>)
text_file.truncate(<span class="hljs-number">0</span>)
<span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> range(sen_num):
    sentence = []
    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, sen_len):
        i = choice(range(<span class="hljs-number">0</span>, vocab_len))
        sentence.append(vocab[i])
    text_file.write(<span class="hljs-string">" "</span>.join(sentence))
    text_file.write(<span class="hljs-string">"n"</span>)
text_file.close()
</pre>



<h3>简单的语言</h3>



<p>现在让我们用一些关系创建一个简单的语言。正如我们上面提到的，我们的语言会有相同长度的句子，这些句子中的“单词”总是彼此靠近。这就像一个简单的字母表的扩展，其中“aaa”总是靠近“bbb”和“ccc”。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> string
<span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> choice

alpha_d = dict.fromkeys(string.ascii_lowercase, <span class="hljs-number">0</span>)

vocab = {}
<span class="hljs-keyword">for</span> c, l <span class="hljs-keyword">in</span> enumerate(alpha_d, <span class="hljs-number">0</span>):
    vocab[c] = l*<span class="hljs-number">3</span>

sen_num = <span class="hljs-number">30000</span>
sen_len = <span class="hljs-number">7</span>
vocab_len = <span class="hljs-number">26</span>

prev_word = <span class="hljs-keyword">None</span>
text_file = open(<span class="hljs-string">"proximity_vocab.txt"</span>, <span class="hljs-string">"a"</span>)
text_file.truncate(<span class="hljs-number">0</span>)
<span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> range(sen_num):
    sentence = []
    start_word = choice(range(<span class="hljs-number">0</span>, <span class="hljs-number">18</span>))
    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, sen_len):
        i = choice([x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(start_word+w+<span class="hljs-number">0</span>, start_word+w+<span class="hljs-number">3</span>) <span class="hljs-keyword">if</span> x <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [prev_word]])
        sentence.append(vocab[i])
        prev_word = i
    text_file.write(<span class="hljs-string">" "</span>.join(sentence))
    text_file.write(<span class="hljs-string">"n"</span>)
text_file.close()
</pre>



<p>现在让我们从TensorFlow教程系列中获取Word2Vec代码，用于训练我们的数据集，然后我们将能够通过Neptune跟踪和可视化结果。</p>



<h2 id="h-word2vec-model">Word2Vec模型</h2>



<p>TensorFlow教程系列有一个<a href="https://web.archive.org/web/20221206214014/https://www.tensorflow.org/tutorials/text/word_embeddings" target="_blank" rel="noreferrer noopener nofollow">文本部分</a>，其中包括许多例子，如BERT和Word2Vec。正如我们之前提到的，我们将使用Word2Vec模型来展示如何使用Neptune跟踪您的实验，以记录准确性和损失数据，以及单词之间的相似性如何变化和3D可视化。</p>



<p>我们将使用Word2Vec代码的精简版本，以便于运行。最初的包含了更多的信息，所以一定要检查一下。这很值得一读。这段代码的伟大之处在于，您几乎可以修改所有可用的参数。通过我们设置的跟踪功能，您将能够监控和比较这些变化。<br/>Word2Vec模型将读入我们创建的简单数据集，然后基于word 2 vec中使用的<a href="https://web.archive.org/web/20221206214014/https://www.tensorflow.org/tutorials/text/word2vec#skip-gram_and_negative_sampling" target="_blank" rel="noreferrer noopener nofollow"> Skip-Gram </a>方法创建训练示例。这种方法试图在给定单词本身的情况下预测单词的上下文。</p>







<p>跳格法试图在给定单词本身的情况下预测其周围的单词</p>



<p>您可以更改所有参数，例如<strong> <em> window_size </em> </strong>，这是您在每一步中尝试预测的上下文或相邻单词的数量。</p>



<p>下面的代码示例可以在一个笔记本<a href="https://web.archive.org/web/20221206214014/https://github.com/choran/word_embeddings/blob/main/train_track_embeddings.ipynb" target="_blank" rel="noreferrer noopener nofollow">这里</a>找到。</p>



<pre class="hljs">


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_training_data</span><span class="hljs-params">(sequences, window_size, num_ns, vocab_size, seed)</span>:</span>
  
  targets, contexts, labels = [], [], []

  
  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)

  
  <span class="hljs-keyword">for</span> sequence <span class="hljs-keyword">in</span> tqdm.tqdm(sequences):

    
    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(
          sequence,
          vocabulary_size=vocab_size,
          sampling_table=sampling_table,
          window_size=window_size,
          negative_samples=<span class="hljs-number">0</span>)

    
    
    <span class="hljs-keyword">for</span> target_word, context_word <span class="hljs-keyword">in</span> positive_skip_grams:
      context_class = tf.expand_dims(
          tf.constant([context_word], dtype=<span class="hljs-string">"int64"</span>), <span class="hljs-number">1</span>)
      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(
          true_classes=context_class,
          num_true=<span class="hljs-number">1</span>,
          num_sampled=num_ns,
          unique=<span class="hljs-keyword">True</span>,
          range_max=vocab_size,
          seed=SEED,
          name=<span class="hljs-string">"negative_sampling"</span>)

      
      negative_sampling_candidates = tf.expand_dims(
          negative_sampling_candidates, <span class="hljs-number">1</span>)

      context = tf.concat([context_class, negative_sampling_candidates], <span class="hljs-number">0</span>)
      label = tf.constant([<span class="hljs-number">1</span>] + [<span class="hljs-number">0</span>]*num_ns, dtype=<span class="hljs-string">"int64"</span>)

      
      targets.append(target_word)
      contexts.append(context)
      labels.append(label)

  <span class="hljs-keyword">return</span> targets, contexts, labels
</pre>



<p>您需要运行的代码在您可以使用的笔记本中，或者您可以直接从TensorFlow网站上获取这些代码，然后开始编码并复制您需要的代码。</p>



<h2 id="h-experiment-tracking">实验跟踪</h2>



<p>很难识别单词嵌入中发生的变化。理想情况下，我们希望能够看到随着训练的进行，我们的单词的相似度是如何变化的。我们可以跟踪每个时期的<strong> <em>损失</em> </strong>和<strong> <em>准确性</em> </strong>之类的事情，但跟踪我们单词之间的实际相似性会很棒。这是我们如何使用TensorFlow的<strong> <em>回调</em> </strong>功能的一个很好的例子。这将使我们能够准确记录我们想要的，然后在海王星实验中跟踪它。</p>



<p>要设置Neptune实验，您只需设置您的凭证并初始化您的Neptune模块。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune
<span class="hljs-keyword">from</span> neptunecontrib.api <span class="hljs-keyword">import</span> log_table
<span class="hljs-keyword">from</span> neptunecontrib.api <span class="hljs-keyword">import</span> log_chart

neptune.init(project_qualified_name=<span class="hljs-string">'choran/sandbox'</span>,
             api_token=<span class="hljs-string">'your token'</span>,)
</pre>



<p>然后你需要做的就是开始你的实验，你已经准备好了！</p>



<pre class="hljs">neptune.create_experiment()
</pre>



<p>这应该会输出您的实验的URL-&gt;<a href="https://web.archive.org/web/20221206214014/https://ui.neptune.ai/choran/sandbox/e/SAN-12" target="_blank" rel="noreferrer noopener">https://ui.neptune.ai/&lt;用户&gt;/沙盒/e/SAN-1 </a></p>



<p>为了检查我们的单词嵌入之间的相似性，我们可以比较一个示例单词的相似性，并跟踪它在不同时期之间如何变化。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_similarity</span><span class="hljs-params">(word, X, X_vocab, vocab)</span>:</span>
    
    vocab_d = {}
    <span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> enumerate(X_vocab):
        vocab_d[w] = i

    
    y = X[vocab_d[word]].reshape(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)
    res = cosine_similarity(X, y)
    df = pd.DataFrame(columns=[<span class="hljs-string">'word'</span>, <span class="hljs-string">'sim'</span>])
    df[<span class="hljs-string">'word'</span>], df[<span class="hljs-string">'sim'</span>]= vocab[<span class="hljs-number">1</span>:], res
    <span class="hljs-keyword">return</span>(df)
</pre>



<p>那么我们所需要做的就是调用这个函数时用我们自定义的<strong> <em>回调</em> </strong>函数。你可以阅读更多关于不同的<strong> <em>回调</em> </strong>特性，你可以在这里使用<a href="https://web.archive.org/web/20221206214014/https://www.tensorflow.org/api_docs/python/tf/keras/callbacks">。关于自定义<strong> <em>回调</em> </strong>的示例，请查看TensorFlow中关于编写自己的回调</a><a href="https://web.archive.org/web/20221206214014/https://www.tensorflow.org/guide/keras/custom_callback">的示例，此处为</a>。</p>



<p>你可以把你的结果以熊猫数据的形式记录下来，然后保存到你的实验中，并跟踪它是如何随时间变化的。</p>







<p>这显示了900个历元之后‘BBB’和其他单词之间的余弦相似性。正确地说，它看起来接近“aaa”和“ccc”。但是，随着时间的推移，这种情况发生了怎样的变化，它与随机数据集或更大的数据集相比又如何呢？</p>







<h2 id="h-3d-visualisation">3d视觉化</h2>



<p>对于我们的3D可视化，我们可以使用plotly和PCA来保存嵌入的3D图，并直观地比较它们之间的关系。例如，看看我们的“随机”数据集和更“结构化”的数据集之间的区别。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/a7954cf22e2960cc869dd12e3287eeef.png" alt="DL embeddings 3D visualization" class="wp-image-34461" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206214014im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/DL-embeddings-3D-visualization-1.png?ssl=1"/><figcaption><em>We can see here that the clustering of our “words” seems fairly chaotic?</em></figcaption></figure></div>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/5bf05fc95cb1921b12f0bb0a543560f2.png" alt="DL embeddings 3D visualization" class="wp-image-34462" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206214014im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/DL-embeddings-3D-visualization-2.png?ssl=1"/><figcaption><em>Whereas here you can see a “nicer” relationship between our “words”, it seems far more structured and there seems to be a pattern you can investigate by interacting with the visualisation.</em></figcaption></figure></div>



<p>您还可以看到，我们的回调函数是多么简单，可以将这些信息记录并跟踪到Neptune。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TrackLossAndSimilarity</span><span class="hljs-params">(tf.keras.callbacks.Callback)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">on_epoch_end</span><span class="hljs-params">(self, epoch, logs=None)</span>:</span>
        neptune.log_metric(<span class="hljs-string">'loss'</span>, logs[<span class="hljs-string">"loss"</span>])
        neptune.log_metric(<span class="hljs-string">'accuracy'</span>, logs[<span class="hljs-string">"accuracy"</span>])
        
        <span class="hljs-keyword">if</span> epoch%<span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            vocab = vectorize_layer.get_vocabulary()
            X, X_vocab = get_data(word2vec.get_layer(<span class="hljs-string">'w2v_embedding'</span>).get_weights()[<span class="hljs-number">0</span>],
                                  vocab)
            check_word = <span class="hljs-string">'bbb'</span>
            sim_df = get_similarity(check_word, X, X_vocab, vocab)
            sim_fig = get_3d_viz(X, X_vocab)
            log_chart(f<span class="hljs-string">'3d-plot-epoch{epoch}'</span>, sim_fig)
            log_table(f<span class="hljs-string">'similarity-{check_word}-epoch{epoch}'</span>, sim_df.sort_values(<span class="hljs-string">'sim'</span>, ascending=<span class="hljs-keyword">False</span>))
</pre>







<h2 id="h-summary-and-next-steps">总结和后续步骤</h2>



<p>最后，让我们回顾一下我们刚刚完成的内容，看看如何使用来训练和测试模式高级模型。</p>



<ol><li>我们创建了自己的“语言学”数据集:这里的目标是创建一个数据集，在这个数据集内我们可以知道(并设计)术语之间的关系。这样，当我们创建嵌入并检查它们的相似性时，我们将知道“aaa”是否更类似于“mmm”而不是“ccc ”,这是有问题的，因为嵌入没有正确地选取我们知道存在的关系。</li><li><strong>我们找到了一个创建嵌入的模型</strong>:我们使用Word2Vec模型的一些示例代码来帮助我们理解如何为输入文本创建标记，并使用skip-gram方法来学习单词嵌入，而不需要监督数据集。这个模型的输出是我们数据集中每个术语的嵌入。这些是“静态”嵌入，因为每个术语有一个嵌入。</li><li><strong>我们探索了嵌入</strong>:最终目标是理解模型如何将意义编码到嵌入中。因此，我们观察了在随机数据集上训练时嵌入所包含的信息，并将其与结构化数据集进行比较，在结构化数据集上，术语仅在与某些其他术语“接近”的情况下使用。我们通过跟踪示例术语之间的相似性来做到这一点，并显示这种相似性如何随时间变化以及两个实验之间的差异。我们还通过可视化所有嵌入之间的关系来展示这一点。在我们的简单示例中，我们能够看到随机数据集和邻近数据集之间的差异</li></ol>



<h4>我如何利用这一点来更好地理解像BERT这样的模型？</h4>



<p>请记住，不管像BERT这样的模型看起来多么先进和复杂，它们最终只是像我们在这里所做的那样将信息编码到一个嵌入中。不同之处在于，设计<a href="https://web.archive.org/web/20221206214014/https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" target="_blank" rel="noreferrer noopener nofollow"> Transformer </a>架构的人(这是大多数新的NLP模型，如BERT和GPT-3的基础)找到了一种“更好的”方法来将信息编码到嵌入中。我们使用的嵌入为每个术语生成一个嵌入。</p>



<p>但是，正如我们所知，真实的语言是混乱的，同样的词在不同的上下文中可能有不同的意思。这是一个博客帖子，但你可以张贴邮件，在篮球比赛中你可以张贴，在足球比赛中你可以击中门柱，你可以通过张贴在门上等方式在公共场合显示信息。对于我们刚刚创建的静态嵌入来说，这是一个问题，但是Transformer架构试图通过基于“上下文”创建嵌入来解决这个问题。所以它根据单词使用的上下文产生不同的嵌入。</p>



<p>例如，您可以通过创建一个新的数据集来测试这一点，在该数据集内，一个术语被用于非常不同的术语附近。术语“ccc”可以接近当前设计的“bbb”和“ddd ”,但是您也可以添加代码以确保它也接近“xxx”和“yyy”。在这种情况下，您认为“ccc”的相似性值将如何工作？例如，它们会显示出与“ddd”和“xxx”相同的相似性吗？</p>



<p>这里的关键是，你可以在这里应用同样的技术来尝试和理解其他模型是如何工作的。您可以在这个数据集上训练一个BERT模型，并尝试看看静态嵌入和基于上下文的嵌入之间有什么区别。如果你想做类似的事情，这里有一些你可以利用的资源:</p>



<ul><li>从零开始用世界语训练BERT:这个来自HugglingFace 的<a href="https://web.archive.org/web/20221206214014/https://huggingface.co/blog/how-to-train" target="_blank" rel="noreferrer noopener nofollow">指南向你展示如何用一种新的语言从零开始训练BERT模型。您可以尝试在您的自定义数据集或更复杂的数据集上进行训练。然后尝试跟踪嵌入，就像我们对Word2Vec所做的那样。记住，BERT嵌入是基于上下文的，所以您不需要像Word2Vec那样的查找字典。您将向它输入一个句子，并获得每个单词的嵌入或整个序列的“混合”嵌入。点击</a>了解更多<a href="https://web.archive.org/web/20221206214014/https://www.tensorflow.org/hub/tutorials/bert_experts" target="_blank" rel="noreferrer noopener nofollow">。</a></li><li>开始尝试句子嵌入:代替单词嵌入，你可以嵌入整个句子并比较它们的相似性。您可以使用一个简单的自定义数据集，然后获得输入序列的平均嵌入，并使用它来比较整个数据序列并确定它们的相似性。你可以在这里找到这些型号的一些例子<a href="https://web.archive.org/web/20221206214014/https://tfhub.dev/google/collections/universal-sentence-encoder/1" target="_blank" rel="noreferrer noopener nofollow">。</a></li><li>上下文与静态嵌入:我们真的需要上下文嵌入吗？上下文嵌入和静态嵌入有什么不同？我们能从上下文嵌入中生成更好的静态嵌入吗？如果你想找到这些问题的答案，请查看<a href="https://web.archive.org/web/20221206214014/http://ai.stanford.edu/blog/contextual/" target="_blank" rel="noreferrer noopener nofollow">这篇很棒的文章</a>。</li></ul>
        </div>
        
    </div>    
</body>
</html>