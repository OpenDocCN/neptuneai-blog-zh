# 如何解决 MLOps 堆栈的模型服务组件

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/model-serving-component-mlops-stack>

模型服务和部署是 MLOps 体系的支柱之一。在本文中，我将深入探讨这个问题，并讨论模型服务的基本、中级和高级设置。

让我们从一些基础知识开始。

## 什么是 ML 模型服务？

训练机器学习模型似乎是一项伟大的成就，但在实践中，它甚至还没有实现商业价值。为了让机器学习计划取得成功，我们需要部署该模型，并确保它满足我们的性能和可靠性要求。你可能会说，“但是我可以把它打包成一个 Docker 映像，然后就完事了”。在某些情况下，这可能已经足够了。但大多数时候，不会。当人们谈论生产 ML 模型时，他们使用术语**服务**而不是简单的部署。那么这意味着什么呢？

服务一个模型就是将它暴露给现实世界，并确保它满足您的所有生产需求，也就是说，您的延迟、准确性、容错和吞吐量都处于“业务愉快”的水平。仅仅将模型打包到 Docker 映像中并不是“解决方案”,因为您仍然需要考虑如何运行模型、缩放模型、部署新的模型更新等等。不要误会我的意思，有一个时间和地点的 Flask-server-in-Docker-image 服务风格；它只是针对有限数量的用例的有限工具，我将在后面概述。

现在我们知道了服务意味着什么，让我们开始吧。

## 模型服务场景

在决定如何为我们的 ML 模型服务时，我们必须问自己几个问题。回答这些问题应该有助于我们塑造服务于架构的模型。

### 我们的模型是面向用户的吗？

换句话说，用户是否通过某个动作触发它，并且需要实时查看依赖于我们的模型输出的效果？如果这听起来太抽象，举个例子怎么样？我们是否正在创建一个类似 Gmail 的电子邮件自动完成解决方案？我们的用户写了一些文本，并期待一个相关的完成。这种场景需要“交互式”部署。这可能是最常见的服务 ML 模型的方式。但这不是唯一的方法。

假设我们现在不需要模型的预测。为了得到我们需要的东西，我们甚至可以等上一个小时或更久。我们需要多久才能得到这些预测？我们是否需要像每周 excel 报告或每天标记一次库存项目描述这样的东西？如果这听起来差不多，我们可以运行一个“批处理”过程来服务我们的模型。这种设置可能是最容易维护和扩展的。但是还有第三条路。

### 延迟很重要吗？

你不需要“回应”用户，但仍然必须根据用户的动作来行动。类似于用户交易触发的欺诈检测模型。这个场景要求一个“流”设置。像这样的场景通常被认为是最复杂的。尽管听起来交互式设置更难构建，但流式传输通常更难推理，因此也更难正确实现。

让我们深入了解每种设置的细节、使用它们的最佳时间以及权衡。

## 模型部署设置

当涉及到向外界公开 ML 模型以供消费时，我们应该基于我们的业务需求考虑一些通用的“设置”。

### 批量模型服务

这是所有可能的设置中最容易实现和操作的。批处理不是交互式的，也就是说，它们不等待与另一个用户或进程的交互。他们只是跑，开始到结束。正因为如此，大多数情况下没有延迟需求；它只需要能够扩展到大型数据集。

由于这种延迟不敏感，您可以使用复杂的模型-类似 Kaggle 的集合，巨大的梯度增强树或神经网络，任何事情都可以，因为预计这些操作无论如何都不会在毫秒内完成。要处理甚至数百 GB 的数据集，您需要的只是 CRON 之类的东西，一个工作站/一个相对强大的云 VM，并知道如何开发核外数据处理脚本。不相信我？这里有一个用视频来证明我的观点的例子。

如果您需要处理数 TB 的数据，这将变得更具挑战性。您将需要处理多节点 Apache Spark、Apache Airflow 或类似的东西。您必须考虑潜在的节点故障，以及如何最大化这些节点的资源利用率。

最后，如果你在谷歌大小的数据集上操作，[检查这个链接](https://web.archive.org/web/20221125120121/https://sre.google/sre-book/data-processing-pipelines/)。以这样的规模运营会带来诸如“喋喋不休的邻居”、分散的任务/工作、“雷鸣般的牛群”和时区等问题。是啊，祝贺你的巨大规模。

### 流式模型服务

正如我们已经提到的，批处理不是唯一不需要等待用户交互的，也就是说，它们不是交互式的。我们也可以让我们的模型作用于数据流。这些场景比批处理对延迟更加敏感。

用于流模型服务的标准工具是 Apache Kafka、Apache Flink 和 Akka。但是如果您需要将您的模型作为一个流/事件驱动的基础设施组件来操作，这些并不是您唯一的选择。您可以创建一个组件，一方面作为事件的消费者，另一方面作为生产者。无论你做什么，都要注意反压力。流设置非常关心能够处理大量连续流动的数据，所以一定不要让您部署的 ML 模型成为该设置的瓶颈。

开发流式 ML 服务解决方案时要考虑的另一件事是模型序列化。大多数流式事件处理系统都是基于 JVM 的，要么是 Java，要么是 Scala 原生的。因此，您可能会发现您的模型结构受到了序列化程序能力的限制。关于模型序列化如何成为一个问题的故事，请查看[本文](/web/20221125120121/https://neptune.ai/blog/automl-solutions)的小节——生成的模型部署起来可能会很乏味。

以下是一些关于这个主题的有用链接:

[在分布式实时数据流应用中部署 ML 模型](https://web.archive.org/web/20221125120121/https://towardsdatascience.com/deploying-ml-models-in-distributed-real-time-data-streaming-applications-217954a0b423)

[使用 Akka 在模型服务中利用推测性执行](https://web.archive.org/web/20221125120121/https://www.lightbend.com/blog/akka-speculative-model-serving)

[使用流数据自动刷新模型](https://web.archive.org/web/20221125120121/https://aws.amazon.com/blogs/machine-learning/automated-model-refresh-with-streaming-data/)

### 交互式模型服务

服务 ML 模型最流行的方式——使用服务器！事实上，很多人在讨论 ML 服务时，指的是这个特定的设置，而不是这三个中的任何一个。交互式设置意味着用户以某种方式触发一个模型，并等待输出或由输出引起的某些东西。基本上，这是一种请求-响应交互模式。

在这种情况下，有许多方法可以为 ML 模型提供服务。从带有内存加载 ML 模型的 Flask 或 FastAPI 服务器到专门的解决方案，如 TF Serving 或 NVIDIA Triton，以及任何介于两者之间的解决方案。在本文中，我们将主要关注这种设置。

我见过有人开发批处理解决方案，其中 ML 组件实际上是由所述批处理程序调用的服务器。或者调用服务于 ML 模型的 HTTP 服务器的流式事件处理系统中的组件。作为一种灵活的、推理起来相当简单的、有良好文档记录的方法，许多人正在“滥用”交互模式。

### 关于云、边缘和客户端服务的说明

如果我们正在开发一个移动应用程序，并希望我们的 ML 功能在没有互联网的情况下也能工作，该怎么办？如果我们想为用户提供神奇的响应能力呢？让在网页上等待回应成为过去。输入客户端服务和边缘服务 ML。

#### 需要考虑的事项

当设计 ML 系统时，我们需要意识到这种可能性和这种部署场景的挑战。

*   使用 [TF.js](https://web.archive.org/web/20221125120121/https://github.com/tensorflow/tfjs) 、 [ONNX](https://web.archive.org/web/20221125120121/https://github.com/microsoft/onnxruntime/tree/master/js/web) 在浏览器客户端上部署特别简单，尽管有点复杂。
*   至于移动，我们有多个变种，包括苹果的 [CoreML](https://web.archive.org/web/20221125120121/https://developer.apple.com/documentation/coreml) ，谷歌的 [TFLite](https://web.archive.org/web/20221125120121/https://www.tensorflow.org/lite) ，以及 [ONNX](https://web.archive.org/web/20221125120121/https://onnx.ai/) 。
*   对于边缘设备，根据它们的计算性能，我们可以像在云中一样运行 ML 模型，也可以创建定制的 TinyML 解决方案。

注意，理论上，浏览器和智能手机都是边缘设备。实际上，由于编程模型大相径庭，它们受到不同的对待。通常情况下，边缘服务器是传统的计算机，要么运行在 ARM 上，要么运行在 x86 硬件上，使用传统的操作系统，只是在网络方面更接近用户。移动设备需要不同的编程，因为移动和更常见的操作系统之间有很大的差异。最近，移动设备拥有专门的 DSP 或协处理器，为人工智能推理进行了优化。

浏览器甚至更不同，因为浏览器代码通常是围绕沙箱环境和事件循环的思想构建的。最近，我们有了 web workers，这使得创建多进程应用程序变得更加容易。此外，当在浏览器中提供 ML 模型时，我们不能对模型将在其上运行的硬件做出任何假设，从而导致潜在的可怕的用户体验。很可能是用户在低端移动设备上用 ML 模型打开了我们的 web 应用程序。想象一下这个网站会有多滞后。

#### 权衡取舍

将 ML 服务移近边缘可能有多种原因。通常的动机是延迟敏感性、带宽控制、隐私问题和离线工作的能力。请记住，我们可以有各种分层部署目标，从用户的客户端设备到离用户最近的物联网集线器或路由器，再到城市或区域范围的数据中心。

在边缘设备或客户端设备上部署通常会牺牲模型大小和性能来降低网络延迟或大幅降低带宽。例如，在手机上部署一个自动人脸识别和分类的模型可能不是一个好主意，但一个微小而简单的模型可以检测场景中是否有人脸。这同样适用于自动电子邮件响应生成器和自动完成键盘模型。前者通常不需要安装在设备上，而后者必须安装在设备上。

在实践中，可以将边缘/设备上的模型与云部署的模型混合使用，以便在在线时获得最大的预测性能，但在离线时也有可能保留一些人工智能功能。这主要可以通过编写自定义代码来完成，但如果您的边缘设备能够运行 KubeEdge，也可以使用类似于 [Sedna](https://web.archive.org/web/20221125120121/https://github.com/kubeedge/sedna) 的东西来运行 [KubeEdge](https://web.archive.org/web/20221125120121/https://kubeedge.io/en/) 。

#### 真实世界的用例

在 edge 上部署的一个常见但较少讨论的场景是，一家零售商希望在其杂货店中使用视频分析。他们开发了一套强大的计算机视觉模型来分析来自店内摄像头的视频，但遇到了一个硬约束。互联网提供商无法确保上传延迟，并且他们所在位置的带宽无法支持多个视频流。

解决办法？他们在每家商店购买了一台游戏电脑，放在员工房间，在本地进行视频分析，而不需要从商店传输视频。是的，这是一个边缘 ML 场景。边缘计算不仅仅是物联网。

## 以正确的方式为机器学习模型服务

ML 模型服务与元数据存储、ML 模型注册、监控组件和特性存储有着紧密的关系。那是相当多的。另外，根据具体的组织需求，模型服务可能必须与 CI/CD 工具集成。可能需要确保一个试运行环境来测试新训练的模型，或者甚至持续部署到生产环境中，最有可能作为影子或金丝雀部署。

[![End-to-end MLOps architecture and workflow](img/5143a084bc0e7c5931e92f880b5b10c1.png)](https://web.archive.org/web/20221125120121/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/how-to-solve-the-model-serving-component-1-1.png?ssl=1)

*End-to-end MLOps architecture and workflow with functional components and roles | [Source](https://web.archive.org/web/20221125120121/https://arxiv.org/pdf/2205.02302.pdf)*

### 什么使模型部署变得好？

请记住，一个好的模型服务解决方案不仅要考虑成本效益和延迟，还要考虑它与堆栈其余部分的集成程度。如果我们有一个高性能的服务器，集成我们的可观察性、特征存储和模型注册是一场噩梦，那么我们就有一个糟糕的模型服务组件。

*   实现整个模型部署/服务工作流的一种常见方式是让模型服务组件基于来自 ML 模型注册中心和/或元数据存储的信息来获取具体的模型。
    *   比如使用 [Neptune.ai](/web/20221125120121/https://neptune.ai/) 这样的工具，我们可以追踪多个实验。在某种程度上，如果我们决定我们有一个好的候选模型，我们将它标记为一个准备就绪的模型/金丝雀。记住，我们还在和 Neptune.ai 交互，不需要使用任何其他工具。我们的 ML 服务组件定期向 ML 模型注册中心登记，如果有带有兼容标签的新模型，它将[更新部署](https://web.archive.org/web/20221125120121/https://docs.neptune.ai/how-to-guides/model-registry/querying-and-downloading-models-and-metadata/accessing-production-ready-models)。这种方法允许更易访问的模型更新，而不会触发映像构建或其他昂贵而复杂的工作流。
    *   另一种方法是[重新部署一个预构建的服务组件](https://web.archive.org/web/20221125120121/https://www.cloudskillsboost.google/focuses/17649?parent=catalog),并且只更改其配置来获取一个更新的模型。这种方法在云本地(Kubernetes)服务解决方案中更常见。

*   当然，如前所述，模型服务组件经常需要与特征库进行交互。为了与特性库进行交互，我们不仅需要能够服务于序列化的 ML 模型，还需要支持定制的支持 IO 的组件。在某些情况下，这可能是一场噩梦。一种解决方法是在应用服务器级别集成特性存储，而不是在 ML 服务组件级别。

*   最后，我们还需要记录和监控我们部署的 ML 模型。许多定制解决方案都集成了工具，如用于日志的 [ELK stack](https://web.archive.org/web/20221125120121/https://www.elastic.co/elastic-stack?ultron=B-Stack-Trials-EMEA-C-Exact&gambit=Stack-ELK&blade=adwords-s&hulk=paid&Device=c&thor=elk%20stack&gclid=Cj0KCQjwgO2XBhCaARIsANrW2X0_CHyfN9_prJ6xyx8wC21h1UTRIt9BMoywfi__ayIG5s0NNXsEpxMaAhS6EALw_wcB) ，用于跟踪的 [OpenTelemetry](https://web.archive.org/web/20221125120121/https://opentelemetry.io/docs/concepts/signals/traces/) ，以及用于度量的 [Prometheus](https://web.archive.org/web/20221125120121/https://prometheus.io/docs/concepts/metric_types/) 。不过，ML 确实带来了一些特殊的挑战。
    *   首先，我们需要能够为我们的数据集收集新数据。这主要是通过定制基础设施或 ELK 来完成的。
    *   然后，我们需要能够跟踪特定于 ML 的信号，比如输入值和输出的分布变化。对于像普罗米修斯这样的工具来说，这是一个高度非优化的场景。为了更好地理解这些挑战，[请看这篇博文](https://web.archive.org/web/20221125120121/https://www.shreya-shankar.com/rethinking-ml-monitoring-3/)。一些工具试图对此有所帮助，最突出的是 [WhyLabs](https://web.archive.org/web/20221125120121/https://whylabs.ai/) 和 [Arize](https://web.archive.org/web/20221125120121/https://arize.com/) 。

## 在服务 ML 模特的时候，我们真正在意的是什么？

除了常见的疑点——尾部延迟、每秒请求数和应用程序错误率，还建议跟踪模型性能。这是棘手的部分。几乎不可能实时或在短时间内获得真实标签。如果延迟显著，则需要更长时间来确定影响用户体验的问题。

正因为如此，跟踪输入和输出的分布，并在它们明显偏离模型预期时触发一些动作是很常见的。虽然这很有用，但它对跟踪我们的预测性能 SLO(服务级别目标)没有太大帮助。

### 跟踪性能的问题

让我解释一下，一方面，我们可以合理地假设输入和输出分布的差异会导致性能下降，但另一方面，我们实际上并不知道两者之间的确切关系。

我们可以有这样的场景，其中特征的分布偏离预期分布很多，但是对我们的 ML 模型性能没有显著影响。在这种情况下，我们会有一个错误的警报。但是这些关系会随着时间而改变。因此，下一次，当相同的特征再次漂移时，它会导致我们的 ML 模型的预测能力的显著损失。你可以想象，这是一场噩梦。那么能做些什么呢？

### 解决方案–检测和缓解

我们部署和更新 ML 模型以改善我们的业务。理想情况下，我们必须将我们的模型 SLO 与业务指标“联系”起来。例如，如果我们注意到点击我们推荐的用户比率下降，我们知道我们做得不好。对于文本自动校正解决方案，类似的商业衍生模型 SLO 可以是被接受建议的比率。如果它低于某个阈值，也许我们的模型并不比之前的好。遗憾的是，这并不总是这么容易做到。

因为这个问题非常棘手，我们通常将 ML 模型性能监控提取到一个单独的组件中，并且只跟踪 ML 服务组件级别的系统级度量、跟踪和日志。我们希望，随着 ML 模型监控的基础设施变得更好，ML 服务组件将提供与这些工具更好的集成，从而使部署模型的故障排除变得更加容易。

## 从基本到完全开发的模型服务设置

因为交互式服务设置是生产 ML 模型最流行的方式，我们将讨论基本、中级和高级设置是什么样子的。一个好的设置和一个普通的设置的区别在于成本效益、可伸缩性和延迟情况。当然，与 MLOps 堆栈的其余部分集成也很重要。

### 模型服务:基本设置

回想一下，在文章的开头，我提到过 ML-model-in-Flask-server-in-a-Docker-container 式的服务是有时间和地点的。关于这种服务已经说了很多，所以我就不赘述了。请注意，ML 模型既可以放在容器中，也可以作为卷附加。如果你只是创建一个演示 API，或者你确实知道你不会有很多流量(可能是一个内部应用，只有 3-5 个人会使用)，这可能是一个可以接受的解决方案。

或者，如果您可以为多个非常强大的云虚拟机配置强大的 GPU 和 CPU，并且不担心资源利用率低和尾部延迟次优，那么它也可以工作。我的意思是，[脸书为他们的软件](https://web.archive.org/web/20221125120121/https://www.zdnet.com/article/why-facebook-doesnt-have-or-need-testers/)做了很少的测试，并且仍然设法成为一个巨大的科技公司，所以遵循所有的软件工程最佳实践可能并不总是有意义的。

#### 赞成的意见

*   这种设置的优点是非常容易实现并且相对可伸缩(需要处理更多的请求= >运行多个副本)。

#### 骗局

*   最大的问题是资源利用率低，因为模型在每次请求单个输入条目时都会被触发，而且 web 服务器不需要与 ML 模型相同的硬件。
*   此外，对尾部延迟的控制严重不足，这意味着您无法通过这种设置实施几乎任何 SLO。控制尾部延迟的唯一希望是一个好的负载平衡器和足够强大的机器来运行 ML 服务组件的多个副本。

![Simple ML serving with a replicated container](img/874ed69377f4a268e16793fcef9124e4.png)

*Simple ML serving with a replicated container. The ML model can be either backed in or attached as a volume* | *Source: Author*

为了改进这种设置，我们必须转向中级配置。

### 模型服务:中间设置

如上所述，我们需要将 ML 推理从应用服务器组件中分离出来，以优化资源利用并更好地控制我们的延迟。一种方法是使用发布者-订阅者异步通信模式，例如用 [ZeroMQ](https://web.archive.org/web/20221125120121/https://hanxiao.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/) 或 [Redis](https://web.archive.org/web/20221125120121/https://pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/) PubSub 实现。

因此，在这种“分裂”之后，我们可以做许多很酷的技巧来完善我们的服务组件，使之成为一个高级组件。

*   首先，我们可以实施更精细的超时和重试。有了这样的设置，就有可能独立于应用服务器来扩展 ML 服务器。
*   然后，最神奇的方法是做[自适应批处理](https://web.archive.org/web/20221125120121/https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)。事实上，这是一项非常棒的技术，它将使解决方案在性能方面几乎达到高级水平。

一个好的模型服务解决方案不仅仅在于服务器性能有多好，还在于集成 ML 子系统的其他部分有多容易。机器学习服务组件将需要提供至少一些模型管理能力，以便容易地更新模型版本，而不需要重新构建整个系统。对于这种类型的设置，ML/MLOps 团队可以设计他们的 ML workers 来定期检查模型注册，如果有任何更新，获取新的模型，类似于 [this](https://web.archive.org/web/20221125120121/https://docs.neptune.ai/how-to-guides/model-registry/querying-and-downloading-models-and-metadata/accessing-production-ready-models) 。

![A medium ML serving blueprint](img/ef18fb7127058b11fbed1a4e43532aad.png)

*A medium ML serving blueprint, with both replicated application servers and ML servers. The solution also uses a feature store and a model registry* | *Source: Author*

我相信你已经注意到中等设置比基本设置要复杂得多。这种复杂性给这种方法带来了很大的缺点。在这个阶段，需要某种形式的容器编排，通常是 K8s，至少需要某种系统可观察性，例如，Prometheus 和 ELK。

### 模型服务:高级设置

公平地说，对于大多数 ML 服务场景，中等水平的设置就足够了。你不应该认为先进的 ML 服务设置是上一个设置的必要发展。高级设置更像“重炮”，只有在特殊情况下才需要。

在上面的解决方案中提出了所有的花里胡哨，一个问题出现了——“如果有现成的解决方案，我们为什么要为所有这些技巧费心呢？”。事实上，为什么呢？答案通常是——他们需要为他们的设置定制一些东西。

像 NVIDIA Triton、Tensorflow Serving 或 TorchServe 这样的专业解决方案有坚实的卖点，也有相当弱的卖点。

#### 赞成的意见

*   首先，这些服务解决方案经过了很好的优化，通常比“中等+花哨”的解决方案表现得更好。
*   其次，这些解决方案易于部署；大多数提供一个码头集装箱或舵图。
*   最后，这些解决方案通常包含对模型管理和 A/B 测试的相对基本的支持。

#### 骗局

*   现在是不利的一面。最大的一个问题是与 MLOps 生态系统的其他部分的尴尬集成。
*   其次，与第一点相关，这些解决方案很难扩展。解决这两个问题最方便的方法是创建定制的应用服务器，作为高性能预构建 ML 服务器的代理/装饰者/适配器。
*   第三，这可能是我个人不喜欢的一点，就是这些解决方案在可以部署什么模型方面非常有限。我想保留我的选择，拥有一个只接受 TF SavedModels 或 ONNX-serialized 的服务解决方案与我的价值观不一致。是的，甚至 ONNX 也有局限性，例如，[当您有一个定制模型](/web/20221125120121/https://neptune.ai/blog/automl-solutions)(参见小节——生成的模型部署起来可能很繁琐)时，它使用了 ONNX 不支持的操作。

您可能已经猜到了，我在大多数情况下并不使用这些解决方案。我更喜欢 PyTorch，所以 TF 发球对我来说是不可行的。注意，这只是我的上下文。如果用 TF，可以考虑用 TF 发球。几年前我在一个 TF 项目中尝试过。如果你问我的话，这对于服务来说很好，但是对于模型管理来说有点麻烦。

我说我主要用 PyTorch，所以也许 TorchServe？坦率地说，我甚至没有尝试过。看起来不错，但是恐怕它和 TF 上菜有同样的模型管理问题。海卫一呢？我可以说它的老版本，TensorRT 推理服务器。这是一个噩梦，配置，然后发现，因为一个定制的模型头，它不能得到适当的服务。再加上模型量化问题，再加上与前两个候选人相同的模型版本管理的困境…公平地说，我听说它变得更好了，但我仍然非常怀疑它。因此，除非我知道我的模型架构没有改变，并且我需要最大可能的性能，否则我不会使用它。

![Adaptive batching ](img/5a401aecdf26b5058b4a48dca9c2394a.png)

*Adaptive batching as a way to more efficiently use ML models* | [*Sourc*e](https://web.archive.org/web/20221125120121/https://mlserver.readthedocs.io/en/latest/user-guide/adaptive-batching.html)

总之，像 NVIDIA Triton 或 Tensorflow Serving 这样的专业解决方案是强大的工具，但如果您选择使用它们，您最好有严重的性能需求。否则，我建议不要这样做。但这还不是全部——

*   即使这些解决方案功能丰富、性能卓越，它们仍然需要大量的支持基础设施。这样的服务器最适合作为 ML 工作者，所以您仍然需要应用服务器。为了拥有一个真正先进的 ML 服务组件，你需要考虑与其他系统的紧密集成，ML 和数据的可观察性，定制或使用像 [Arize](https://web.archive.org/web/20221125120121/https://arize.com/) 和 [Montecarlo](https://web.archive.org/web/20221125120121/https://www.montecarlodata.com/) 这样的服务。

*   此外，您需要能够执行高级流量管理。上面提到的系统为 A/B 测试提供了一些有限的支持。尽管如此，在实践中，您必须以不同的方式实现它，要么在应用服务器级别实现更细粒度的控制，要么在基础设施级别使用像 [Istio](https://web.archive.org/web/20221125120121/https://istio.io/) 这样的工具。您通常需要能够支持新模型的逐步推出、金丝雀部署和流量阴影。没有现有的预建服务系统提供所有这些业务模式。如果你想支持这些，准备好弄脏你的手和白板。

### 关于 MLOps 云产品的说明

**TL；DR:** 云产品为您提供“全生命周期”解决方案，这意味着模型服务与数据集管理、培训、超参数调整、监控和模型注册的解决方案相集成。

云产品试图为您提供简单的基本设置，丰富的高级设置功能和中等设置的性能。对我们大多数人来说，这是一笔了不起的交易。

云产品的共同优势是无服务器和自动扩展推理，以及 GPU 和/或特殊芯片支持。

1.  以谷歌的 Vertex AI 为例。它们为您提供完整的 MLOps 体验和相对简单的模型部署，可以作为云功能或自动缩放容器，甚至作为批处理作业。因为是谷歌，他们有 TPU，这对于真正大规模的部署来说很方便。
2.  或者，如果有更完整的解决方案，可以使用 AWS。他们的 SageMaker 就像 Vertex AI 一样，在整个 MLOps 生命周期中为您提供帮助。尽管如此，它也增加了一种简单而具有成本效益的方式来运行模型，以便用弹性推理加速器进行推理，这些加速器似乎是分数 GPU，可能通过英伟达的安培代 MIGs，或使用一种称为推理的定制芯片。更好的是，SageMaker 允许对目标硬件进行训练后模型优化。

然而，两者都没有提供自适应批处理、某种形式的推测性执行/请求对冲或其他高级技术。根据您的 SLO，您可能仍然需要使用 NVIDIA Triton 等系统或开发内部解决方案。

## 结论

在生产中运行 ML 可能是一项艰巨的任务。要真正掌握这一点，必须针对许多目标进行优化——成本效率、延迟、吞吐量和可维护性，等等。如果你能从这篇文章中得到什么，那就让它成为这三个想法:

1.  为您的 ML 模型服务时，要有明确的目标和优先顺序
2.  让业务需求和约束驱动您的 ML 服务组件架构，而不是相反。
3.  将模型视为更广泛的 MLOps 堆栈中的一个组件。

有了这些想法，您应该能够从好的解决方案中筛选出不合格的 ML 服务解决方案，从而最大化对您的组织的影响。但是不要犯从一开始就试图把一切都做好的错误。尽早开始服务，迭代您的解决方案，让本文中的知识帮助您更好地完成最初的几次迭代。部署一般的东西总比什么都不部署好。

### 参考