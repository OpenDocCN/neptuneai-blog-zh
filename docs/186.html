<html>
<head>
<title>Gradient Boosted Decision Trees [Guide]: a Conceptual Explanation </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>梯度推进决策树[指南]:一个概念性的解释</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/gradient-boosted-decision-trees-guide#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/gradient-boosted-decision-trees-guide#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>梯度推进决策树已被证明优于其他模型。这是因为boosting涉及实现几个模型并聚合它们的结果。</p>



<p>由于在Kaggle上的机器学习比赛中的表现，梯度增强模型最近变得很受欢迎。</p>



<p>在这篇文章中，我们将看到梯度提升决策树是怎么回事。</p>



<h2 id="h-gradient-boosting">梯度推进</h2>



<p>在<a href="https://web.archive.org/web/20230304041944/https://en.wikipedia.org/wiki/Gradient_boosting#:~:text=Gradient%20boosting%20is%20a%20machine,prediction%20models%2C%20typically%20decision%20trees." target="_blank" rel="noreferrer noopener nofollow">梯度提升</a>中，弱学习器的集合用于提高机器学习模型的性能。弱学习者通常是决策树。结合起来，它们的输出会产生更好的模型。</p>



<p>在回归的情况下，从所有弱学习者的平均值产生最终结果。通过分类，最终结果可以被计算为来自弱学习者的大多数投票的类。</p>



<p>在梯度推进中，弱学习者顺序工作。每个模型都试图改进前一个模型的误差。这不同于bagging技术，在bagging技术中，几个模型以并行方式拟合数据子集。这些子集通常随机抽取并替换。装袋的一个很好的例子是在随机森林中。</p>



<p><strong>升压过程看起来是这样的</strong>:</p>



<ul>
<li>用数据建立一个初始模型，</li>



<li>对整个数据集进行预测，</li>



<li>使用预测值和实际值计算误差，</li>



<li>给不正确的预测分配更多的权重，</li>



<li>创建另一个模型，尝试修复上一个模型的错误，</li>



<li>使用新模型对整个数据集运行预测，</li>



<li>创建多个模型，每个模型旨在纠正前一个模型产生的错误，</li>



<li>通过加权所有模型的平均值获得最终模型。</li>
</ul>



<h2 id="h-boosting-algorithms-in-machine-learning">机器学习中的助推算法</h2>



<p>我们来看看机器学习中的boosting算法。</p>



<h3>adaboost算法</h3>



<p>AdaBoost将一系列弱学习者与数据进行拟合。然后，它给不正确的预测分配更多的权重，给正确的预测分配更少的权重。通过这种方式，算法更加关注难以预测的观察结果。最终结果是从分类中的多数票，或回归中的平均值获得的。</p>



<p>您可以使用Scikit-learn实现这个算法。“n_estimators”参数可以传递给它，以指示所需弱学习器的数量。您可以使用“learning_rate”参数来控制每个弱学习者的贡献。</p>



<p>默认情况下，该算法使用决策树作为基本估计器。可以调整基本估计器和决策树的参数来提高模型的性能。默认情况下，AdaBoost中的决策树只有一次拆分。</p>



<h4>使用AdaBoost分类</h4>



<p>您可以使用Scikit-learn中的“AdaBoostClassifier”来实现AdaBoost模型以解决分类问题。正如您在下面看到的，基本估计器的参数可以根据您的喜好进行调整。分类器也接受您想要的估计数。这是模型所需的决策树数量。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> AdaBoostClassifier
base_estimator=DecisionTreeClassifier(max_depth=<span class="hljs-number">1</span>,criterion=<span class="hljs-string">'gini'</span>, splitter=<span class="hljs-string">'best'</span>, min_samples_split=<span class="hljs-number">2</span>)
model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=<span class="hljs-number">100</span>)
model.fit(X_train, y_train)</pre>



<h4>使用AdaBoost进行回归</h4>



<p>将AdaBoost应用于回归问题类似于分类过程，只是做了一些表面上的改变。首先，您必须导入“AdaBoostRegressor”。然后，对于基本估计量，您可以使用“DecisionTreeRegressor”。就像上一个一样，你可以调整决策树回归器的参数。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> AdaBoostRegressor
base_estimator = DecisionTreeRegressor(max_depth=<span class="hljs-number">1</span>, splitter=<span class="hljs-string">'best'</span>, min_samples_split=<span class="hljs-number">2</span>)
model = AdaBoostRegressor(base_estimator=base_estimator,n_estimators=<span class="hljs-number">100</span>)
model.fit(X_train, y_train)</pre>



<h3>Scikit-learn梯度增强估计器</h3>



<p>梯度提升不同于AdaBoost，因为损失函数优化是通过梯度下降完成的。和AdaBoost一样，它也使用决策树作为弱学习器。它也依次适合这些树。添加后续树时，使用梯度下降将损失降至最低。</p>



<p>在Scikit-learn实现中，您可以指定树的数量。这是一个应该密切关注的参数，因为指定太多的树会导致过度拟合。另一方面，指定很少数量的树会导致拟合不足。</p>



<p>该算法允许您指定学习率。这决定了模型学习的速度。低学习率通常需要模型中有更多的树。这意味着更多的训练时间。</p>



<p>现在让我们看看Scikit-learn中梯度增强树的实现。</p>



<h4>使用Scikit-learn梯度增强估计器进行分类</h4>



<p>这是使用“GradientBoostingClassifier”实现的。该算法预期的一些参数包括:</p>



<ul>
<li>定义要优化的损失函数的“损失”</li>



<li>确定每棵树贡献的“学习率”</li>



<li>` n_estimatorst '表示决策树的数量</li>



<li>“最大深度”是每个估计器的最大深度</li>
</ul>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier
gbc = GradientBoostingClassifier(loss=<span class="hljs-string">'deviance'</span>, learning_rate=<span class="hljs-number">0.1</span>, n_estimators=<span class="hljs-number">100</span>, subsample=<span class="hljs-number">1.0</span>, criterion=<span class="hljs-string">'friedman_mse'</span>, min_samples_split=<span class="hljs-number">2</span>, min_samples_leaf=<span class="hljs-number">1</span>)
gbc.fit(X_train,y_train)</pre>



<p>拟合分类器后，您可以使用“feauture _ importances _”属性获得特征的重要性。这通常被称为基尼系数。</p>



<pre class="hljs">gbc.feature_importances_</pre>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/4d1de7a5006c73774586dd6a66cc1a93.png" alt="Gradient boosted decision tree feature import" class="wp-image-43419" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230304041944im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Gradient-boosted-decision-tree-feature-import.png?ssl=1"/></figure>



<p>值越高，特性越重要。获得的数组中的值总和将为1。</p>



<p>注意:基于杂质的重要性并不总是准确的，尤其是当有太多的特征时。在这种情况下，您应该考虑使用<a href="https://web.archive.org/web/20230304041944/https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance">基于排列的重要性</a>。</p>



<h4>使用Scikit-learn梯度增强估计器进行回归</h4>



<p>Scikit-learn梯度增强估计器可使用“GradientBoostingRegressor”实现回归。它采用类似于分类的参数:</p>



<ul>
<li>损失，</li>



<li>估计数，</li>



<li>树木的最大深度，</li>



<li>学习率…</li>
</ul>



<p>…仅举几个例子。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingRegressor
params = {<span class="hljs-string">'n_estimators'</span>: <span class="hljs-number">500</span>,
          <span class="hljs-string">'max_depth'</span>: <span class="hljs-number">4</span>,
          <span class="hljs-string">'min_samples_split'</span>: <span class="hljs-number">5</span>,
          <span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.01</span>,
          <span class="hljs-string">'loss'</span>: <span class="hljs-string">'ls'</span>}
gbc = GradientBoostingRegressor(**params)
gbc.fit(X_train,y_train)</pre>



<p>像分类模型一样，您也可以获得回归算法的特征重要性。</p>



<pre class="hljs">gbc.feature_importances_</pre>



<section id="blog-intext-cta-block_206e494616fba604c2fa99ce3ad1ee03" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>海王与<a href="/web/20230304041944/https://neptune.ai/blog/the-ultimate-guide-to-evaluation-and-selection-of-models-in-machine-learning" target="_blank" rel="noreferrer noopener">斯克利亚<br/>T2的融合</a></p>
    
    </section>



<h3>XGBoost</h3>



<p><a href="/web/20230304041944/https://neptune.ai/blog/how-to-organize-your-xgboost-machine-learning-ml-model-development-process" target="_blank" rel="noreferrer noopener"> XGBoost </a>是一个支持Java、Python、Java和C++、R、Julia的渐变增强库。它还使用弱决策树的集合。</p>



<p>这是一个通过并行计算进行树学习的线性模型。该算法还附带了用于执行交叉验证和显示特性重要性的特性。这种模式的主要特点是:</p>



<ul>
<li>接受树增强器和线性增强器的稀疏输入，</li>



<li>支持自定义评估和目标函数，</li>



<li>“Dmatrix ”,其优化的数据结构提高了性能。</li>
</ul>



<p>让我们看看如何在Python中应用XGBoost。该算法接受的参数包括:</p>



<ul>
<li>“目标”定义任务的类型，比如回归或分类；</li>



<li>` colsample_bytree `构造每个树时列的子采样比率。子采样在每次迭代中发生一次。这个数字通常是0到1之间的一个值；</li>



<li>“learning_rate ”,确定模型学习的快慢；</li>



<li>` max_depth '表示每棵树的最大深度。树越多，模型越复杂，过度拟合的机会就越大；</li>



<li>` alpha '是权重上的L1正则化;</li>



<li>“n_estimators”是要拟合的决策树的数量。</li>
</ul>



<h4>使用XGBoost分类</h4>



<p>导入算法后，您可以定义想要使用的参数。因为这是一个分类问题，所以使用“二元:逻辑”目标函数。下一步是使用“XGBClassifier”并解包定义的参数。您可以调整这些参数，直到获得最适合您的问题的参数。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb
params = {<span class="hljs-string">"objective"</span>:<span class="hljs-string">"binary:logistic"</span>,<span class="hljs-string">'colsample_bytree'</span>: <span class="hljs-number">0.3</span>,<span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.1</span>,
                <span class="hljs-string">'max_depth'</span>: <span class="hljs-number">5</span>, <span class="hljs-string">'alpha'</span>: <span class="hljs-number">10</span>}
classification = xgb.XGBClassifier(**params)
classification.fit(X_train, y_train)
</pre>



<h4>使用XGBoost进行回归</h4>



<p>在回归中，使用“XGBRegressor”。在这种情况下，目标函数将是“reg:squarederror”。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb
params = {<span class="hljs-string">"objective"</span>:<span class="hljs-string">"reg:squarederror"</span>,<span class="hljs-string">'colsample_bytree'</span>: <span class="hljs-number">0.3</span>,<span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.1</span>,
                <span class="hljs-string">'max_depth'</span>: <span class="hljs-number">5</span>, <span class="hljs-string">'alpha'</span>: <span class="hljs-number">10</span>}
regressor = xgb.XGBRegressor(**params)
regressor.fit(X_train, y_train)</pre>



<p>XGBoost模型还允许您通过“feature_importances_”属性获得特性的重要性。</p>



<pre class="hljs">regressor.feature_importances_</pre>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/818cbf602a5324b0a18fe41aecbd69df.png" alt="Regressor feature import" class="wp-image-43422" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230304041944im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Regressor-feature-import.png?ssl=1"/></figure>



<p>您可以使用Matplotlib轻松地将它们可视化。这是使用XGBoost中的“plot_importance”函数完成的。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
xgb.plot_importance(regressor)
plt.rcParams[<span class="hljs-string">'figure.figsize'</span>] = [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>]
plt.show()</pre>





<p>“保存模型”功能可用于保存您的模型。然后，您可以将这个模型发送到您的模型注册中心。</p>



<pre class="hljs">regressor.save_model(<span class="hljs-string">"model.pkl"</span>)</pre>



<section id="blog-intext-cta-block_d981ed924c3ea43e9bf96ffe3d968dbc" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>查看Neptune文档，了解与XGBoost和T2 matplotlib的集成</p>
    
    </section>



<h3>LightGBM</h3>



<p><a href="/web/20230304041944/https://neptune.ai/blog/how-to-organize-your-lightgbm-ml-model-development-process-examples-of-best-practices" target="_blank" rel="noreferrer noopener"> LightGBM </a>与其他梯度推进框架不同，因为它使用逐叶树生长算法。已知逐叶树生长算法比逐深度生长算法收敛得更快。然而，他们更容易过度适应。</p>





<p>该算法是基于<a href="https://web.archive.org/web/20230304041944/https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html" target="_blank" rel="noreferrer noopener nofollow">直方图的</a>，因此它将连续值放入离散的箱中。这导致更快的训练和有效的存储器利用。</p>



<p>该算法的其他显著特征包括:</p>



<ul>
<li>支持GPU训练，</li>



<li>对分类特性的本机支持，</li>



<li>处理大规模数据的能力，</li>



<li>默认情况下处理缺失值。</li>
</ul>



<p>让我们来看看这个算法的一些主要参数:</p>



<ul>
<li>` max_depth `每棵树的最大深度；</li>



<li>默认为回归的“目标”;</li>



<li>` learning_rate `提高学习率；</li>



<li>` n_estimators '要拟合的决策树的数量；</li>



<li>` device_type `无论你是在CPU上工作还是在GPU上工作。</li>
</ul>



<h4>LightGBM分类</h4>



<p>训练二元分类模型可以通过将“二元”设置为目标来完成。如果是多分类问题，则使用“多类”目标。</p>



<p>数据集也被转换成LightGBM的“数据集”格式。然后使用“训练”功能完成模型训练。您还可以使用“valid_sets”参数传递验证数据集。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb
lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)
params = {<span class="hljs-string">'boosting_type'</span>: <span class="hljs-string">'gbdt'</span>,
              <span class="hljs-string">'objective'</span>: <span class="hljs-string">'binary'</span>,
              <span class="hljs-string">'num_leaves'</span>: <span class="hljs-number">40</span>,
              <span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.1</span>,
              <span class="hljs-string">'feature_fraction'</span>: <span class="hljs-number">0.9</span>
              }
gbm = lgb.train(params,
    lgb_train,
    num_boost_round=<span class="hljs-number">200</span>,
    valid_sets=[lgb_train, lgb_eval],
    valid_names=[<span class="hljs-string">'train'</span>,<span class="hljs-string">'valid'</span>],
   )</pre>



<h4>用LightGBM回归</h4>



<p>对于使用LightGBM的回归，您只需要将目标改为“回归”。提升类型默认为梯度提升决策树。</p>



<p>如果您愿意，您可以将其更改为随机森林，“dart ”-漏失符合多重加法回归树，或“Goss ”-基于梯度的单侧采样。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb
lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)
params = {<span class="hljs-string">'boosting_type'</span>: <span class="hljs-string">'gbdt'</span>,
              <span class="hljs-string">'objective'</span>: <span class="hljs-string">'regression'</span>,
              <span class="hljs-string">'num_leaves'</span>: <span class="hljs-number">40</span>,
              <span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.1</span>,
              <span class="hljs-string">'feature_fraction'</span>: <span class="hljs-number">0.9</span>
              }
gbm = lgb.train(params,
    lgb_train,
    num_boost_round=<span class="hljs-number">200</span>,
    valid_sets=[lgb_train, lgb_eval],
    valid_names=[<span class="hljs-string">'train'</span>,<span class="hljs-string">'valid'</span>],
   )</pre>



<p>您还可以使用LightGBM来绘制模型的特征重要性。</p>



<pre class="hljs">lgb.plot_importance(gbm)</pre>





<p>LightGBM还有一个用于保存模型的内置函数。该功能是“保存模型”。</p>



<pre class="hljs">gbm.save_model(<span class="hljs-string">'mode.pkl'</span>)</pre>



<section id="blog-intext-cta-block_86a830c381e68401ed8ed8cd28af1f2e" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>海王星与<a href="https://web.archive.org/web/20230304041944/https://docs.neptune.ai/essentials/integrations/machine-learning-frameworks/lightgbm" target="_blank" rel="noreferrer noopener"> LightGBM </a>的融合</p>
    
    </section>



<h3>CatBoost</h3>



<p><a href="https://web.archive.org/web/20230304041944/https://github.com/catboost" target="_blank" rel="noreferrer noopener nofollow"> CatBoost </a>是Yandex开发的深度梯度推进库。该算法使用不经意决策树生成一棵平衡树。</p>



<p>它使用相同的特征在树的每一层进行左右分割。</p>



<p>例如，在下图中，您可以看到“297，值&gt; 0.5”被用于该级别。</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/54dedea82d9471a2a061b0563e0f7f0d.png" alt="Gradient-boosting-catboost" class="wp-image-43205" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230304041944im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Gradient-boosting-catboost.png?ssl=1"/></figure>



<p><a href="https://web.archive.org/web/20230304041944/https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus" target="_blank" rel="noreferrer noopener nofollow"> CatBoost </a>的其他显著特性包括:</p>



<ul>
<li>对分类特性的本机支持，</li>



<li>支持在多个GPU上训练，</li>



<li>使用默认参数会产生良好的性能，</li>



<li>通过CatBoost的模型应用程序进行快速预测，</li>



<li>本机处理丢失的值，</li>



<li>支持回归和分类问题。</li>
</ul>



<p>现在让我们来看看CatBoost的几个训练参数:</p>



<ul>
<li>` loss_function `用于分类或回归的损失；</li>



<li>` eval_metric `模型的评估度量；</li>



<li>` n_estimators '决策树的最大数量；</li>



<li>` learning_rate '决定模型学习的快慢；</li>



<li>“深度”,每棵树的最大深度；</li>



<li>` ignored_features '确定在训练期间应该忽略的特征；</li>



<li>` nan_mode '将用于处理缺失值的方法；</li>



<li>` cat_features `分类列的数组；</li>



<li>用于声明基于文本的列的“text_features”。</li>
</ul>



<h4>使用CatBoost分类</h4>



<p>对于分类问题，使用“CatBoostClassifier”。在训练过程中设置“plot=True”将使模型可视化。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> catboost <span class="hljs-keyword">import</span> CatBoostClassifier
model = CatBoostClassifier()
model.fit(X_train,y_train,verbose=<span class="hljs-keyword">False</span>, plot=<span class="hljs-keyword">True</span>)</pre>





<h4>使用CatBoost进行回归</h4>



<p>在回归的情况下，使用“CatBoostRegressor”。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> catboost <span class="hljs-keyword">import</span> CatBoostRegressor
model = CatBoostRegressor()
model.fit(X_train,y_train,verbose=<span class="hljs-keyword">False</span>, plot=<span class="hljs-keyword">True</span>)</pre>



<p>您还可以使用“feature_importances_”来获得特性的重要性排名。</p>



<pre class="hljs">model.feature_importances_</pre>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/57341c73797ed1748e7529a8c650b128.png" alt="model.feature_importances_" class="wp-image-43436" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230304041944im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/model.feature_importances_.png?ssl=1"/></figure>



<p>该算法还支持执行交叉验证。这是通过传递所需参数时使用“cv”函数来完成的。</p>



<p>通过“plot="True "”将使交叉验证过程可视化。“cv”函数要求数据集采用CatBoost的“Pool”格式。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> catboost <span class="hljs-keyword">import</span> Pool, cv
params = {<span class="hljs-string">"iterations"</span>: <span class="hljs-number">100</span>,
          <span class="hljs-string">"depth"</span>: <span class="hljs-number">2</span>,
          <span class="hljs-string">"loss_function"</span>: <span class="hljs-string">"RMSE"</span>,
          <span class="hljs-string">"verbose"</span>: <span class="hljs-keyword">False</span>}
cv_dataset = Pool(data=X_train,
                  label=y_train)
scores = cv(cv_dataset,
            params,
            fold_count=<span class="hljs-number">2</span>,
            plot=<span class="hljs-keyword">True</span>)</pre>



<p>您还可以使用CatBoost来执行网格搜索。这是使用“grid_search”功能完成的。搜索后，CatBoost在最佳参数上进行训练。</p>



<p>在此过程之前，您不应该拟合模型。传递“plot=True”参数将可视化网格搜索过程。</p>



<pre class="hljs">grid = {<span class="hljs-string">'learning_rate'</span>: [<span class="hljs-number">0.03</span>, <span class="hljs-number">0.1</span>],
        <span class="hljs-string">'depth'</span>: [<span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">10</span>],
        <span class="hljs-string">'l2_leaf_reg'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]}

grid_search_result = model.grid_search(grid, X=X_train, y=y_train, plot=<span class="hljs-keyword">True</span>)</pre>



<p>CatBoost还使您能够可视化模型中的单棵树。这是通过使用“plot_tree”函数并传递您想要可视化的树的索引来完成的。</p>



<pre class="hljs">model.plot_tree(tree_idx=<span class="hljs-number">0</span>)</pre>



<figure class="wp-block-image size-large is-resized"><img decoding="async" loading="lazy" src="../Images/a1cf67fba17f2672836f46935fd8c5ff.png" alt="Regression with catboost" class="wp-image-43441" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230304041944im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Regression-with-catboost-1.png?resize=1024%2C358&amp;ssl=1"/></figure>



<h2 id="h-advantages-of-gradient-boosting-trees">梯度推进树的优势</h2>



<p>有几个原因可以解释为什么您会考虑使用梯度推进树算法:</p>



<ul>
<li>通常比其他模式更精确，</li>



<li>训练速度更快，尤其是在大型数据集上，</li>



<li>它们中的大多数都支持处理分类特征，</li>



<li>它们中的一些本身就处理缺失值。</li>
</ul>



<h2 id="h-disadvantages-of-gradient-boosting-trees">梯度增强树的缺点</h2>



<p>现在，让我们来解决使用梯度增强树时面临的一些挑战:</p>



<ul>
<li>易于过度拟合:这可以通过应用L1和L2正则化惩罚来解决。你也可以尝试低学习率；</li>



<li>模型可能计算量很大，需要很长时间来训练，尤其是在CPU上；</li>



<li>很难解释最终的模型。</li>
</ul>



<h2 id="h-final-thoughts">最后的想法</h2>



<p>在本文中，我们探讨了如何在机器学习问题中实现梯度推进决策树。我们还介绍了各种基于boosting的算法，您可以立即开始使用。</p>



<p>具体来说，我们涵盖了:</p>



<ul>
<li>什么是梯度推进，</li>



<li>梯度推进是如何工作的，</li>



<li>各种类型的梯度增强算法，</li>



<li>如何对回归和分类问题使用梯度推进算法，</li>



<li>梯度推进树的优点，</li>



<li>梯度增强树的缺点，</li>
</ul>



<p>…还有更多。</p>



<p>你已经准备好开始提升你的机器学习模型了。</p>



<h3>资源</h3>




        </div>
        
    </div>    
</body>
</html>