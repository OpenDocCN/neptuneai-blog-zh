<html>
<head>
<title>Random Forest Regression: When Does It Fail and Why? </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>随机森林回归:何时失败，为什么？</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>在本文中，我们将关注使用随机森林进行回归的一个主要问题，即<strong>外推</strong>。</p>



<p>我们将讨论以下项目:</p>



<ul><li>随机森林回归与线性回归</li><li>随机森林回归外推问题</li><li>潜在的解决方案</li><li>应该使用随机森林进行回归吗？</li></ul>



<p>让我们开始吧。</p>



<h2 id="h-random-forest-regression-vs-linear-regression">随机森林回归与线性回归</h2>



<p>随机森林回归是一个相当健壮的算法，然而，问题是<strong>你应该把它用于回归</strong>？</p>



<p>为什么不用线性回归代替呢？线性回归中的函数可以很容易地写成y=mx + c，而复杂随机森林回归中的函数看起来就像一个黑盒，不能很容易地用函数来表示。</p>



<p>一般来说，随机森林会产生更好的结果，适用于大型数据集，并且能够通过为缺失数据创建估计值来处理缺失数据。然而，他们提出了一个主要的挑战，那就是他们不能在看不见的数据之外进行推断。我们稍后将深入探讨这些挑战</p>



<h3>决策树回归</h3>



<p>决策树对于获取输入特征和目标变量之间的非线性关系非常有用。</p>



<p>决策树的内部工作可以被认为是一堆if-else条件。</p>



<p>它从最顶端的一个节点开始。然后，该节点分为左右两个节点，即决策节点。这些节点然后分裂成它们各自的左右节点。</p>



<p>在叶节点的末端，计算该区域内发生的观测值的平均值。最底层的节点被称为叶节点或终端节点。</p>



<p>树叶中的值通常是该特定区域内观测值的平均值。例如，在下面最右边的叶节点中，552.889是5个样本的平均值。</p>







<p>这种分裂的程度就是我们所知的树的深度。这是可以调整的超参数之一。指定树的最大深度是为了防止树变得太深——导致过度拟合的情况。</p>



<h3>随机森林回归</h3>



<p>随机森林是决策树的集合。这就是说，许多树以某种“随机”的方式构成了一个随机森林。</p>



<ul><li>每棵树都是从不同的行样本中创建的，并且在每个节点处，选择不同的要素样本进行分割。</li><li>每棵树都有自己的预测。</li><li>然后对这些预测进行平均以产生一个结果。</li></ul>







<p>平均使得随机森林比单个决策树更好，因此提高了其准确性并减少了过度拟合。</p>



<p>来自随机森林回归变量的预测是森林中的树木产生的预测的平均值。</p>



<h3>训练线性回归和随机森林的示例</h3>



<p>为了进一步深入，让我们看一个线性回归和随机森林回归的例子。为此，我们将对同一数据集应用线性回归和随机森林回归，并比较结果。</p>



<p>让我们以这个数据集为例，您应该根据克拉、深度、表、x、y和z等其他特征来预测钻石的价格。如果我们看下面的价格分布:</p>











<p>我们可以看到<strong>价格从326到18823不等。</strong></p>



<p>让我们训练线性回归模型，并在验证集上运行预测。</p>



<p>预测价格的分布如下:</p>











<p><strong>预测价格明显在训练数据集中显示的“价格”值范围之外。</strong></p>



<p>线性回归模型，顾名思义，就是在数据上创建一个线性模型。一种简单的思考方式是采用y = mx+C的形式。因此，由于它符合线性模型，因此能够在预测过程中获得定型集以外的值。它能够根据数据进行推断。</p>



<p>现在让我们看看使用相同数据集从随机森林回归器获得的结果。</p>



<p class="has-text-align-center"><img decoding="async" loading="lazy" src="../Images/5844f1d3232988621ebb9aa848dc3a7f.png" data-original-src="https://web.archive.org/web/20221224114220im_/https://lh6.googleusercontent.com/93-amJ52_eGP2poE6wMoMG4RWkvH5Jw-w7fwSfq0unViy0TttQy8pUSHzzAKv9nSrqjU2LJqrht18N4tdRnylHQ5w_A4MGGkAiXzMmL1N_Jd1JuecRTH7d71oav74iv-Ca1Ysxt7"/></p>







<p>这些值显然在326和18823 的范围内<strong>——就像我们的训练集一样。没有超出该范围的值。<strong>随机森林不能举一反三。</strong></strong></p>







<p>正如您在上面看到的，当使用随机森林回归时，预测值永远不会超出目标变量的训练集值。</p>



<p>如果您查看预测值，它们将如下所示:</p>



<div class="wp-block-image"><figure class="aligncenter"><img decoding="async" src="../Images/f146a27c6738a660c029642360e2a0f5.png" alt="" data-original-src="https://web.archive.org/web/20221224114220im_/https://lh3.googleusercontent.com/aTc9sANC3x-luOlDmSVMy9rUkSp6K1JMYgODnWl_2iPaAfPgk-ee8Sm2orKIxl-LDnVss8u11_IxgpuLuFhBF_4yOcwl2LwsDXJ2xHHQZS_DUghDK-jU2kX1-tgX3s24WZz-euja"/><figcaption><em>Hengl, Tomislav et. al “Random forest as a generic framework for predictive modeling of spatial and spatio-temporal variables”. PeerJ. 6. e5518. 10.7717/peerj.5518. | <a href="https://web.archive.org/web/20221224114220/https://www.researchgate.net/publication/327298817_Random_forest_as_a_generic_framework_for_predictive_modeling_of_spatial_and_spatio-temporal_variables" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p>想知道为什么吗？</p>



<p>让我们在这里探索这一现象。上面使用的数据有以下几列克拉，深度，表，x，y，z预测价格。</p>



<p>下图显示了随机森林回归器中的一个决策树。</p>







<p>让我们放大到这棵树的一小部分。例如，有4个深度为&lt;= 62.75, x &lt;= 5.545, carat &lt;= 0.905, and z &lt;= 3.915. The price being predicted for these is 2775.75. This figure represents the mean of all these four samples. Therefore, <strong>的样本，测试集中落在该叶子中的任何值都将被预测为2775.75。</strong></p>







<p>也就是说，当随机森林回归器承担预测以前未见过的值的任务时，它将总是预测以前见过的值的平均值。显然，样本的平均值不能超出样本中的最高值和最低值。</p>



<p><strong>随机森林回归器无法发现使其能够外推超出训练集</strong>的值的趋势。当面对这种情况时，回归器假设预测将接近训练集中的最大值。上面的图1说明了这一点。</p>



<h3>潜在的解决方案</h3>



<p>好，那么你如何处理这个外推问题呢？</p>



<p>有几个选项:</p>



<ul><li>使用线性模型，如SVM回归、线性回归等</li><li>构建深度学习模型，因为神经网络能够进行外推(它们基本上是类固醇上的堆叠线性回归模型)</li><li>使用<a href="https://web.archive.org/web/20221224114220/https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html#sphx-glr-auto-examples-ensemble-plot-stack-predictors-py" target="_blank" rel="noreferrer noopener nofollow">堆叠</a>组合预测值。例如，您可以使用线性模型和随机森林回归量来创建堆叠回归量。</li><li><strong>使用随机森林的修改版本</strong></li></ul>



<p>其中一个扩展是<a href="https://web.archive.org/web/20221224114220/https://arxiv.org/pdf/1904.10416.pdf" rel="noreferrer noopener nofollow" target="_blank">回归增强随机森林</a> (RERFs)。本文作者提出了一种借鉴惩罚参数回归优点的技术，以在外推问题中给出更好的结果。</p>



<p>具体来说，该过程有两个步骤:</p>



<ul><li>在随机森林前跑套索，</li><li>根据Lasso的残差训练一个随机森林。</li></ul>



<p>由于随机森林是一个完全非参数的预测算法，它可能不能有效地结合反应和预测之间的已知关系。响应值是观察值Y1，.。。，Yn来自训练数据。RERFs能够整合响应和预测之间的已知关系，这是使用回归增强随机森林解决回归问题的另一个好处。</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/0d54fc4e1709980c22baa56f4b75f22b.png" alt="" data-original-src="https://web.archive.org/web/20221224114220im_/https://lh6.googleusercontent.com/dbxTPJM7BudrFSYY4lSDneKHYwxkt5XvXgRRU-3FMAzDflmOr96tVdcBW6idIjj556UqOwqLHOkPVF9Fm_pOPWPrxnjI4U8kAoOXFXB9nHWNepcK_ZjQtXfdIFAmRszn-0lF94MO"/><figcaption><em>Haozhe Zhang et. al 2019 “Regression-Enhanced Random Forests” | <a href="https://web.archive.org/web/20221224114220/https://arxiv.org/abs/1904.10416" target="_blank" rel="noreferrer noopener nofollow">Source</a></em> </figcaption></figure></div>







<h2 id="h-final-thoughts">最后的想法</h2>



<p>在这一点上，我相信您可能想知道是否应该使用随机森林来解决回归问题。</p>



<p>让我们看看那个。</p>



<h3>何时使用随机森林？</h3>



<ul><li>当数据具有非线性趋势时，训练数据之外的外推并不重要。</li></ul>



<h3>什么时候不用随机森林？</h3>



<ul><li>当您的数据是时间序列形式时。时间序列问题需要确定一个增长或下降的趋势，而这是一个随机的森林回归方程所不能表达的。</li></ul>



<p>希望这篇文章能给你一些随机森林回归内部工作的背景知识。</p>
        </div>
        
    </div>    
</body>
</html>