<html>
<head>
<title>Serving Machine Learning Models With Docker: 5 Mistakes You Should Avoid </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>用Docker服务机器学习模型:你应该避免的5个错误</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/serving-ml-models-with-docker-mistakes#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/serving-ml-models-with-docker-mistakes#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>正如您已经知道的，<a href="https://web.archive.org/web/20221201155629/https://www.docker.com/" target="_blank" rel="noreferrer noopener nofollow"> Docker </a>是一个工具，它允许您使用容器来创建和部署隔离的环境，以便运行您的应用程序及其依赖项。既然这样，在进入主题之前，让我们简单回顾一下Docker的一些基本概念。</p>



<h2 id="h-why-should-data-scientists-containerize-ml-models">数据科学家为什么要容器化ML模型？</h2>



<p>你是否曾经训练过一个机器学习模型，然后决定与同事分享你的代码，但后来发现你的代码不断出错，尽管它在你的笔记本电脑上工作得很好。大多数情况下，这可能是包兼容性问题或环境问题。解决这个问题的好办法是使用<strong> <a href="/web/20221201155629/https://neptune.ai/blog/data-science-machine-learning-in-containers" target="_blank" rel="noreferrer noopener">容器</a> </strong>。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img decoding="async" src="../Images/fcc904ac814a1162451969358e9a81a3.png" alt="Why should data scientists containerize ML models?" class="wp-image-66535" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221201155629im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Serving-Machine-Learning-Models-in-Docker-5-Mistakes-You-Should-Avoid6.png?resize=614%2C553&amp;ssl=1"/><figcaption><em>Source: Author</em></figcaption></figure></div>


<p>集装箱优惠:</p>



<ul><li><strong>再现性<em/></strong>——通过将你的机器学习模型容器化，你可以将你的代码运送到任何其他安装了Docker的系统，并期望你的应用程序能给你类似于你在本地测试时的结果。</li></ul>



<ul><li><strong>协作开发<em>–</em></strong>容器化的机器学习模型允许团队成员协作，这也使得版本控制更加容易。</li></ul>



<h2 id="h-using-docker-to-serve-your-machine-learning-models">使用Docker服务于您的机器学习模型</h2>



<p>既然你知道为什么你需要容器化你的机器学习模型，接下来的事情就是理解你如何容器化你的模型。</p>



<p>一些您可能已经知道并在本文中遇到的与Docker相关的术语:</p>



<ul><li>Dockerfile :你可以把Dockerfile想象成一个描述你想要如何设置你想要运行的系统的操作系统安装的文件。它包含了设置Docker容器所需的所有代码，从下载Docker映像到设置环境。</li></ul>



<ul><li><strong> Docker image </strong>:它是一个只读模板，包含创建Docker容器的指令列表。</li></ul>



<ul><li><strong> Docker容器</strong>:容器是Docker映像的一个可运行实例。</li></ul>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/0b119b7748c38a1c3139c5a938b24111.png" alt="Basic Docker commands" class="wp-image-66532" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221201155629im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Serving-Machine-Learning-Models-in-Docker-5-Mistakes-You-Should-Avoid3-1643958816-1653571155167.jpg?resize=707%2C559&amp;ssl=1"/><figcaption><em>Basic Docker commands | Source: Author</em></figcaption></figure></div>


<p>创建Docker文件时，可以考虑一些最佳实践，比如在构建Docker映像时避免安装不必要的库或包，减少Docker文件的层数等等。查看以下文章，了解使用Docker的最佳实践。</p>







<h2 id="h-how-to-serve-machine-learning-models">如何为机器学习模型服务？</h2>



<p>模型服务的重要概念是托管机器学习模型(内部或云中)，并通过API提供其功能，以便公司可以将人工智能集成到他们的系统中。</p>



<p>通常有两种模型服务:批处理和在线。</p>



<p><strong>批量预测</strong>表示模型的输入是大量的数据，通常是预定的操作，预测可以以表格的形式发布。</p>



<p><strong>在线部署<em> </em> </strong>需要部署带有端点的模型，以便应用程序可以向模型提交请求，并以最小的延迟获得快速响应。</p>



<h3>服务ML模型时需要考虑的重要要求</h3>



<h4>交通管理</h4>



<p>根据目标服务的不同，端点上的请求会采用不同的路径。为了同时处理请求，流量管理还可以部署负载平衡功能。</p>



<h4>监视</h4>



<p>监控在生产中部署的机器学习模型是很重要的。通过监控最大似然模型，我们可以检测模型的性能何时恶化以及何时重新训练模型。没有模型监控，机器学习生命周期是不完整的。</p>



<h4>数据预处理</h4>



<p>对于实时服务，机器学习模型要求模型的输入具有合适的格式。应该有一个专用的转换服务用于数据预处理。</p>



<p>您可以使用不同的工具来为生产中的机器学习模型提供服务。你可以查看<a href="/web/20221201155629/https://neptune.ai/blog/ml-model-serving-best-tools" target="_blank" rel="noreferrer noopener">这篇</a>文章，获得关于你可以用于模型服务的不同机器学习工具/平台的全面指导。</p>







<h2 id="h-mistakes-you-should-avoid-when-serving-your-machine-learning-models-with-docker">使用Docker服务机器学习模型时应该避免的错误</h2>



<p>现在您已经理解了模型服务的含义以及如何使用Docker来服务您的模型。在使用Docker为您的机器学习模型提供服务时，知道做什么和不做什么是很重要的。</p>



<p>操作错误是数据科学家在使用Docker部署他们的机器学习模型时最常见的错误。这种错误通常会导致应用程序的ML服务性能很差。一个ML应用程序是通过它的整体服务性能来衡量的——它应该具有低推理延迟、低服务延迟和良好的监控架构。</p>



<h3>错误一:用TensorFlow Serving和Docker服务机器学习模型时使用REST API而不是gRPC</h3>



<p>TensorFlow服务是由Google开发人员开发的，它提供了一种更简单的方法来部署您的算法和运行实验。</p>



<p>要了解更多关于如何使用TensorFlow服务Docker来服务您的ML模型，请查看这篇<a href="https://web.archive.org/web/20221201155629/https://neptune.ai/blog/how-to-serve-machine-learning-models-with-tensorflow-serving-and-docker">帖子</a>。</p>



<p>当使用TensorFlow服务为机器学习模型提供服务时，您需要了解Tensorflow服务提供的不同类型的端点以及何时使用它们。</p>



<h4>gRPC和REST API端点</h4>



<p><strong> gRPC </strong></p>



<p>是由谷歌发明的一种通讯协议。它使用一个协议缓冲区作为它的消息格式，它是高度打包的，对于序列化结构化数据是高效的。借助对负载平衡、跟踪、运行状况检查和身份验证的可插拔支持，它可以高效地连接数据中心内部和数据中心之间的服务。</p>



<p><strong>休息</strong></p>



<p>大多数web应用程序使用REST作为通信协议。它说明了客户端如何与web服务通信。尽管REST仍然是客户机和服务器之间交换数据的好方法，但它也有缺点，那就是速度和可伸缩性。</p>



<h4>gRPC和REST API的区别</h4>



<p>gRPC和REST API在操作方式上有不同的特点。下表比较了两种API的不同特征</p>



<div id="medium-table-block_a742211337ec3a79bea88007b502885a" class="block-medium-table c-table__outer-wrapper ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">特性</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">gRPC</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">休息</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>【协议缓冲区】</p> </div></td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>



<p id="separator-block_4d419b05e144d49f34f4f076df1cff58" class="block-separator block-separator--20"> </p>



<p>如下图所示，大多数服务API请求都是使用REST到达的。在使用<a href="https://web.archive.org/web/20221201155629/https://www.tensorflow.org/tfx/serving/api_rest" target="_blank" rel="noreferrer noopener nofollow">RESTful API</a>或<a href="https://web.archive.org/web/20221201155629/https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service.proto" target="_blank" rel="noreferrer noopener nofollow">gRPC API</a>进行预测，将预处理数据发送到Tensorflow服务器之前，预处理和后处理步骤在API内部进行。</p>


<div class="wp-block-image">
<figure class="aligncenter size-full"><img decoding="async" src="../Images/7974752a343d83fc3f22a3fda4a37a6e.png" alt="How to use gRPC for model serving " class="wp-image-66533" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221201155629im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Serving-Machine-Learning-Models-in-Docker-5-Mistakes-You-Should-Avoid4.png?ssl=1"/><figcaption><em>How to use gRPC for model serving | Source: Author</em></figcaption></figure></div>


<p>大多数数据科学家经常利用<strong> REST API </strong>进行模型服务，然而，它也有缺点。主要是速度和可伸缩性。你的模型在被输入后做出预测所花费的时间被称为ML推理延迟。为了改善应用程序的用户体验，ML服务快速返回预测是非常重要的。</p>



<p>对于较小的有效载荷，这两种API都可以产生类似的性能，同时<a href="https://web.archive.org/web/20221201155629/https://aws.amazon.com/blogs/machine-learning/reduce-compuer-vision-inference-latency-using-grpc-with-tensorflow-serving-on-amazon-sagemaker/" target="_blank" rel="noreferrer noopener nofollow"> AWS Sagemaker </a>证明，对于图像分类和对象检测等计算机视觉任务，在Docker端点中使用gRPC可以将整体延迟减少75%或更多。</p>



<h4>使用gRPC API和Docker部署您的机器学习模型</h4>



<p><strong>步骤1: </strong>确保您的电脑上安装了Docker</p>



<p><strong>步骤2: </strong>要使用Tensorflow服务，您需要从容器存储库中提取Tensorflow服务图像。</p>



<pre class="hljs">docker pull tensorflow/serving
</pre>



<p>第三步:建立并训练一个简单的模型</p>



<pre class="hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> asarray
<span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> unique
<span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> argmax
<span class="hljs-keyword">from</span> tensorflow.keras.datasets.mnist <span class="hljs-keyword">import</span> load_data
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Conv2D
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> MaxPool2D
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Flatten
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dropout


(x_train, y_train), (x_test, y_test) = load_data()
print(f<span class="hljs-string">'Train: X={x_train.shape}, y={y_train.shape}'</span>)
print(f<span class="hljs-string">'Test: X={x_test.shape}, y={y_test.shape}'</span>)


x_train = x_train.reshape((x_train.shape[<span class="hljs-number">0</span>], x_train.shape[<span class="hljs-number">1</span>], x_train.shape[<span class="hljs-number">2</span>], <span class="hljs-number">1</span>))
x_test = x_test.reshape((x_test.shape[<span class="hljs-number">0</span>], x_test.shape[<span class="hljs-number">1</span>], x_test.shape[<span class="hljs-number">2</span>], <span class="hljs-number">1</span>))


x_train = x_train.astype(<span class="hljs-string">'float32'</span>) / <span class="hljs-number">255.0</span>
x_test = x_test.astype(<span class="hljs-string">'float32'</span>) / <span class="hljs-number">255.0</span>


input_shape = x_train.shape[<span class="hljs-number">1</span>:]


n_classes = len(unique(y_train))


model = Sequential()
model.add(Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>, input_shape=input_shape))
model.add(MaxPool2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
model.add(Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
model.add(MaxPool2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
model.add(Flatten())
model.add(Dense(<span class="hljs-number">50</span>, activation=<span class="hljs-string">'relu'</span>))
model.add(Dropout(<span class="hljs-number">0.5</span>))
model.add(Dense(n_classes, activation=<span class="hljs-string">'softmax'</span>))


model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])


model.fit(x_train, y_train, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">128</span>, verbose=<span class="hljs-number">1</span>)


loss, acc = model.evaluate(x_test, y_test, verbose=<span class="hljs-number">0</span>)
print(<span class="hljs-string">'Accuracy: %.3f'</span> % acc)</pre>



<p><strong>第四步:</strong>保存模型</p>



<p>保存TensorFlow模型时，可以将其保存为协议缓冲文件，通过在save_format参数中传递“tf”将模型保存到协议缓冲文件中。</p>



<pre class="hljs">file_path = f<span class="hljs-string">"./img_classifier/{ts}/"</span>
model.save(filepath=file_path, save_format=<span class="hljs-string">'tf'</span>)</pre>



<p>可以使用<strong> <em> saved_model_cli </em> </strong>命令对保存的模型进行调查。</p>



<pre class="hljs">!saved_model_cli show --dir {export_path} --all</pre>



<p><strong>步骤5 </strong>:使用gRPC服务模型</p>



<p>您需要安装gRPC库。</p>



<pre class="hljs">Import grpc
<span class="hljs-keyword">from</span> tensorflow_serving.apis <span class="hljs-keyword">import</span> predict_pb2
<span class="hljs-keyword">from</span> tensorflow_serving.apis <span class="hljs-keyword">import</span> prediction_service_pb2_grpc
<span class="hljs-keyword">from</span> tensorboard.compat.proto <span class="hljs-keyword">import</span> types_pb2</pre>



<p>您需要使用端口8500在客户端和服务器之间建立一个通道。</p>



<pre class="hljs">channel = grpc.insecure_channel(<span class="hljs-string">'127.0.0.1:8500'</span>)
stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)</pre>



<p>服务器的请求有效负载需要通过指定模型的名称、存储模型的路径、预期的数据类型以及数据中的记录数来设置为协议缓冲区。</p>



<pre class="hljs">request = predict_pb2.PredictRequest()
request.model_spec.name = <span class="hljs-string">'mnist-model'</span>
request.inputs[<span class="hljs-string">'flatten_input'</span>].CopyFrom(tf.make_tensor_proto(X_test[<span class="hljs-number">0</span>],dtype=types_pb2.DT_FLOAT,  shape=[<span class="hljs-number">28</span>,<span class="hljs-number">28</span>,<span class="hljs-number">1</span>]))</pre>



<p>最后，要用Docker部署您的模型，您需要运行Docker容器。</p>



<pre class="hljs">docker run -p <span class="hljs-number">8500</span>:<span class="hljs-number">8500</span> --mount type=bind,source=&lt;absolute_path&gt;,target=/models/mnist-model/ -e MODEL_NAME=mnist -t tensorflow/serving</pre>



<p>现在服务器可以接受客户端请求了。从存根调用Predict方法来预测请求的结果。</p>



<pre class="hljs">stub.Predict(request, <span class="hljs-number">10.0</span>)</pre>



<p>按照上述步骤，您将能够使用gRPC API为TensorFlow服务模型提供服务。</p>



<h3>错误2:在使用Docker为机器学习模型提供服务时，对数据进行预处理</h3>



<p>开发人员在使用Docker服务于他们的机器学习模型时犯的另一个错误是在做出预测之前实时预处理他们的数据。在ML模型提供预测之前，它期望数据点必须包括在训练算法时使用的所有输入特征。</p>



<p>例如，如果您训练一个线性回归算法来根据房子的大小、位置、年龄、房间数量和朝向来估计房子的价格，则训练好的模型将需要这些要素的值作为推断过程中的输入，以便提供估计的价格。</p>



<p>在大多数情况下，需要对输入数据进行预处理和清理，甚至需要对某些要素进行工程设计。现在想象一下，每次触发模型端点时都要实时地做这件事，这意味着对一些特性进行重复的预处理，尤其是静态特性和高ML模型延迟。在这种情况下，<strong>特性存储库</strong>被证明是一个无价的资源。</p>



<h4>什么是功能商店？</h4>



<p>功能存储与存储相关，用于跨多个管道分支存储和服务功能，从而实现共享计算和优化。</p>



<h4>在Docker中为ml模型提供服务时使用特征库的重要性</h4>



<ul><li>数据科学家可以使用要素存储来简化要素的维护方式，为更高效的流程铺平道路，同时确保要素得到正确存储、记录和测试。</li></ul>



<ul><li>在整个公司的许多项目和研究任务中都使用了相同的功能。数据科学家可以使用要素存储来快速访问他们需要的要素，并避免重复工作。</li></ul>



<p>为机器学习模型提供服务时，为了调用模型进行预测，会实时获取两种类型的输入要素:</p>



<ol><li><strong>静态参考</strong>:这些特征值是需要预测的实体的静态或渐变属性。这包括描述性属性，如客户人口统计信息。它还包括客户的购买行为，如他们花了多少钱，多久消费一次等。</li></ol>



<ol start="2"><li><strong>实时动态特性:</strong>这些特性值是基于实时事件动态捕获和计算的。这些特征通常是在事件流处理管道中实时计算的。</li></ol>



<p>要素服务API使要素数据可用于生产中的模型。创建服务API时考虑到了对最新特性值的低延迟访问。要更好地理解特性存储，了解可用的不同特性存储，请查看本文:<a href="/web/20221201155629/https://neptune.ai/blog/feature-stores-components-of-a-data-science-factory-guide" target="_blank" rel="noreferrer noopener">特性存储:数据科学工厂的组件</a>。</p>



<h3>错误3:使用IP地址在Docker容器之间通信</h3>



<p>最后，您已经使用Docker部署了您的机器学习模型，并且您的应用程序正在生产环境中返回预测，但是由于某些原因，您需要对容器进行更新。在进行必要的更改并重启容器化的应用程序后，您会不断地得到<strong> <em>“错误:连接失败”。</em>T3】</strong></p>



<p>您的应用程序无法建立到数据库的连接，即使它以前工作得非常好。每个容器都有自己的内部IP地址，该地址在容器重新启动时会发生变化。数据科学家犯的错误是使用Docker的默认网络驱动程序bridge在容器之间进行通信。同一桥接网络中的所有容器可以通过IP地址相互通信。因为IP地址会波动，这显然不是最好的方法。</p>



<h4>不使用IP地址，如何在Docker容器之间进行通信？</h4>



<p>为了与容器通信，您应该使用环境变量来传递主机名，而不是IP地址。您可以通过创建用户定义的桥接网络来实现这一点。</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/03c19540138fbd1213e6be77953ec989.png" alt="How to create a user-defined bridge network" class="wp-image-66534" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221201155629im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Serving-Machine-Learning-Models-in-Docker-5-Mistakes-You-Should-Avoid5.png?resize=778%2C389&amp;ssl=1"/><figcaption><em>How to create a user-defined bridge network | <a href="https://web.archive.org/web/20221201155629/https://www.tutorialworks.com/container-networking/" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<ol><li>您需要创建自己的自定义桥接网络。您可以通过运行Docker network create命令来实现这一点。这里我们创建一个名为“虚拟网络”的网络。</li></ol>



<pre class="hljs">Docker network create dummy-network</pre>



<ol start="2"><li>用<strong> <em> docker run </em> </strong>命令正常运行你的容器。使用<strong><em>—网络选项</em> </strong>将其添加到您自定义的桥接网络中。您还可以使用–name选项添加别名。</li></ol>



<pre class="hljs">docker run --rm --net dummy-network --name tulipnginx -d nginx</pre>



<ol start="3"><li>将另一个容器连接到您创建的自定义桥接网络。</li></ol>



<pre class="hljs">docker run --net dummy-network -it busybox </pre>



<ol start="4"><li>现在，您可以使用容器主机名连接到任何容器，只要它们在同一个自定义桥接网络上，而不用担心重启。</li></ol>



<h3>错误4:作为根用户运行您的流程</h3>



<p>许多数据科学家在作为根用户运行他们的流程时犯了这样的错误，我将解释为什么这是错误的，并推荐解决方案。在设计系统时，坚持最小特权原则是很重要的。这意味着应用程序应该只能访问完成任务所需的资源。授予进程执行所需的最少特权是保护自己免受任何意外入侵的最佳策略之一。</p>



<p>因为大多数容器化的流程是应用程序服务，所以它们不需要root访问。容器不需要root才能运行，但是Docker需要。编写良好、安全且可重用的Docker映像不应该以root用户身份运行，而应该提供一种可预测且简单的方法来限制访问。</p>



<p>默认情况下当你运行你的容器时，它假定<strong> <em>根</em> </strong>用户。我也犯过这样的错误，总是以root用户身份运行我的进程，或者总是使用sudo来完成工作。但是我了解到，拥有不必要的权限会导致灾难性的问题。</p>



<p>让我通过一个例子来说明这一点。这是我过去用于一个项目的docker文件样本。</p>



<pre class="hljs">FROM tiangolo/uvicorn-gunicorn:python3<span class="hljs-number">.9</span>

RUN mkdir /fastapi

WORKDIR /fastapi

COPY requirements.txt /fastapi

RUN pip install -r /fastapi/requirements.txt

COPY . /fastapi

EXPOSE <span class="hljs-number">8000</span>

CMD [<span class="hljs-string">"uvicorn"</span>, <span class="hljs-string">"main:app"</span>, <span class="hljs-string">"--host"</span>, <span class="hljs-string">"0.0.0.0"</span>, <span class="hljs-string">"--port"</span>, <span class="hljs-string">"8000"</span>]</pre>



<p>第一件事是构建一个Docker映像并运行Docker容器，您可以用这个命令来完成</p>



<pre class="hljs">docker build -t getting-started .</pre>



<pre class="hljs">docker run -d p <span class="hljs-number">8000</span>:<span class="hljs-number">8000</span> getting-started</pre>



<p>接下来是获取containerID，你可以通过用<strong> <em> docker ps </em> </strong>检查你的Docker容器进程来做到这一点，然后你可以运行<strong> <em> whoami </em> </strong>命令来查看哪个用户可以访问容器。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/41104fe3949d2ad17b66292a7d2ae124.png" alt="Running your processes as root users" class="wp-image-66531" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221201155629im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Serving-Machine-Learning-Models-in-Docker-5-Mistakes-You-Should-Avoid2.png?ssl=1"/><figcaption><em>Source: Author</em></figcaption></figure></div>


<p>如果应用程序存在漏洞，攻击者就可以获得容器的超级用户访问权限。用户在容器中拥有root权限，可以做任何他们想做的事情。攻击者不仅可以利用这一点来干扰程序，还可以安装额外的工具来转到其他设备或容器。</p>



<h4>如何以非根用户身份运行Docker</h4>



<p>使用dockerfile文件:</p>



<pre class="hljs">




FROM debian:stretch


RUN useradd -u <span class="hljs-number">1099</span> user-tesla

USER user-tesla</pre>



<p>作为一个容器用户，对改变用户的支持程度取决于容器维护者。使用-user参数，Docker允许您更改用户(或docker-compose.yml中的用户密钥)。应将进程更改到的用户的用户id作为参数提供。这限制了任何不必要的访问。</p>



<h3>错误5:用Docker服务ML模型时没有监控模型版本</h3>



<p>数据科学家犯的一个操作错误是，在将ML系统部署到生产环境之前，没有跟踪对其进行的更改或更新。模型版本化帮助ML工程师了解模型中发生了什么变化，研究人员更新了哪些特性，以及特性是如何变化的。了解进行了哪些更改，以及在集成多个功能时，这些更改如何影响部署的速度和简易性。</p>



<h4>模型版本化的优势</h4>



<p>模型版本控制有助于跟踪您先前已经部署到生产环境中的不同模型文件，通过这样做，您可以实现:</p>



<ol><li><strong>模型谱系可追溯性:</strong>如果最近部署的模型在生产中表现不佳，您可以重新部署表现更好的模型的先前版本。</li></ol>



<ol start="2"><li><strong>模型注册表:</strong>像<a href="/web/20221201155629/https://neptune.ai/"> Neptune AI </a>和<a href="https://web.archive.org/web/20221201155629/https://mlflow.org/" target="_blank" rel="noreferrer noopener nofollow"> MLFlow </a>这样的工具可以作为模型注册表，方便你记录它们的模型文件。每当您需要服务的模型时，您可以获取模型和特定的版本。</li></ol>



<h4>使用Neptune.ai进行模型版本控制并使用Docker进行部署</h4>



<p>Neptune.ai允许您<a href="https://web.archive.org/web/20221201155629/https://docs.neptune.ai/how-to-guides/experiment-tracking" target="_blank" rel="noreferrer noopener">跟踪您的实验</a>，超参数值，用于特定实验运行的数据集，以及模型工件。Neptune.ai提供了一个python SDK，您可以在构建机器学习模型时使用它。</p>



<p>第一步是确保您已经安装了<a href="https://web.archive.org/web/20221201155629/https://docs.neptune.ai/integrations-and-supported-tools/languages/neptune-client-python" target="_blank" rel="noreferrer noopener"> neptune python客户端</a>。根据您的操作系统，打开您的终端并运行以下命令:</p>



<pre class="hljs">pip install neptune-client
</pre>



<p>在训练好你的模型之后，<a href="https://web.archive.org/web/20221201155629/https://docs.neptune.ai/how-to-guides/model-registry/registering-a-model" target="_blank" rel="noreferrer noopener">你可以在Neptune </a>中注册它来追踪任何相关的元数据。首先，需要初始化一个Neptune模型对象。模型对象适用于保存在训练过程中由所有模型版本共享的通用元数据。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune.new <span class="hljs-keyword">as</span> neptune
model = neptune.init_model(project=<span class="hljs-string">'&lt;project name&gt;’'</span>,
    name=<span class="hljs-string">"&lt;MODEL_NAME&gt;"</span>,
    key=<span class="hljs-string">"&lt;MODEL&gt;"</span>,
    api_token=<span class="hljs-string">"&lt;token&gt;"</span>
)</pre>



<p>这将生成一个到Neptune仪表板的URL，在这里您可以看到您已经创建的不同模型。查看<a href="https://web.archive.org/web/20221201155629/https://app.neptune.ai/akinwande/docker-demo/models" target="_blank" rel="noreferrer noopener">工作区</a>。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221201155629/https://neptune.ai/serving-machine-learning-models-in-docker-5-mistakes-you-should-avoid7"><img decoding="async" src="../Images/b7be3f3e4e593e7d4abcc173b6426621.png" alt="How to create a model version in neptune.ai" class="wp-image-66536" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221201155629im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Serving-Machine-Learning-Models-in-Docker-5-Mistakes-You-Should-Avoid7.png?ssl=1"/></a><figcaption><em>ML model logged in Neptune.ai | <a href="https://web.archive.org/web/20221201155629/https://app.neptune.ai/akinwande/docker-demo/models" target="_blank" rel="noreferrer noopener">Source</a></em></figcaption></figure></div>


<p>为了<a href="https://web.archive.org/web/20221201155629/https://docs.neptune.ai/how-to-guides/model-registry/creating-model-versions" target="_blank" rel="noreferrer noopener">在Neptune </a>中创建模型版本，您需要在同一个Neptune项目中注册您的模型，并且您可以在仪表板上的models选项卡下找到您的模型。</p>



<p>要在Neptune上创建模型版本，您需要运行以下命令:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune.new <span class="hljs-keyword">as</span> neptune
model_version = neptune.init_model_version(
    model=<span class="hljs-string">"MODEL_ID"</span>,
)
</pre>



<p>下一件事是存储任何相关的模型元数据和工件，您可以通过将它们分配给您创建的模型对象来完成。要了解如何记录模型元数据，请查看这个<a href="https://web.archive.org/web/20221201155629/https://docs.neptune.ai/how-to-guides/experiment-tracking/log-model-building-metadata" target="_blank" rel="noreferrer noopener">文档页面</a>。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221201155629/https://neptune.ai/serving-machine-learning-models-in-docker-5-mistakes-you-should-avoid1"><img decoding="async" src="../Images/2965ca9147c4793881fbb09bd12adb01.png" alt="How to create a model version in neptune.ai" class="wp-image-66530" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221201155629im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Serving-Machine-Learning-Models-in-Docker-5-Mistakes-You-Should-Avoid1.png?ssl=1"/></a><figcaption><em>Different versions of the model are visible in the Neptune’s UI | <a href="https://web.archive.org/web/20221201155629/https://app.neptune.ai/akinwande/docker-demo/m/DOC-MODEL/versions" target="_blank" rel="noreferrer noopener">Source</a></em></figcaption></figure></div>


<p>现在您可以看到您已经创建的模型的不同版本，每个模型版本的相关元数据，以及模型度量。您还可以管理每个模型版本的模型阶段。从上图来看，<strong> DOC-MODEL-1 </strong>已经部署到生产中。这样，您可以看到当前部署到生产环境中的模型版本以及该模型的相关元数据。</p>



<p>在构建您的机器学习模型时，您不应该将关联的元数据(如超参数、注释和配置数据)作为文件存储在Docker容器中。当容器被停止、销毁和替换时，您可能会丢失容器中的所有相关数据。使用Neptune-client，您可以记录和存储每次运行的所有相关元数据。</p>



<h4>在Docker与Neptune一起服务时如何监控模型版本</h4>



<p>因为Neptune通过创建模型、创建模型版本和管理模型阶段转换来管理您的数据，所以您可以使用Neptune作为<a href="https://web.archive.org/web/20221201155629/https://docs.neptune.ai/how-to-guides/model-registry">模型注册表</a>来查询和下载您存储的模型。</p>



<p>创建一个新脚本来提供和导入必要的依赖项。您所需要做的就是指定您需要在生产中使用的模型版本。您可以通过将您的<a href="https://web.archive.org/web/20221201155629/https://docs.neptune.ai/api-reference/environment-variables#neptune_api_token" target="_blank" rel="noreferrer noopener"> NEPTUNE_API_TOKEN </a>和您的<a href="https://web.archive.org/web/20221201155629/https://app.neptune.ai/common/showcase-model-registry/m/SHOW2-MOD21/v/SHOW2-MOD21-13/metadata" target="_blank" rel="noreferrer noopener"> MODEL_VERSION </a>作为Docker环境变量来运行您的Docker容器:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune.new <span class="hljs-keyword">as</span> neptune
<span class="hljs-keyword">import</span> pickle,requests

api_token = os.environ[<span class="hljs-string">'NEPTUNE_API_TOKEN'</span>]
model_version = os.environ[<span class="hljs-string">'MODEL_VERSION'</span>]


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_pickle</span><span class="hljs-params">(fp)</span>:</span>

   <span class="hljs-string">"""
   Load pickle file(data, model or pipeline object).
   Parameters:
       fp: the file path of the pickle files.

   Returns:
       Loaded pickle file
   """</span>
   <span class="hljs-keyword">with</span> open(fp, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
       <span class="hljs-keyword">return</span> pickle.load(f)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(data)</span>:</span>
   
   input_data = requests.get(data)
   
   model_version = neptune.init_model_version(project=<span class="hljs-string">'docker-demo'</span>,
   version=model_version,
   api_token=api_token
   )
   model_version[<span class="hljs-string">'classifier'</span>][<span class="hljs-string">'pickled_model'</span>].download()
   model = load_pickle(<span class="hljs-string">'xgb-model.pkl'</span>)
   predictions = model.predict(data)
   <span class="hljs-keyword">return</span> predictions</pre>



<p>通过创建Docker文件并提供requirements.txt文件上的依赖项列表，可以使用Docker将机器学习模型服务容器化。</p>



<pre class="hljs">neptune-client
sklearn==<span class="hljs-number">1.0</span><span class="hljs-number">.2</span></pre>



<pre class="hljs">
FROM python:<span class="hljs-number">3.8</span>-slim-buster

RUN apt-get update
RUN apt-get -y install gcc

COPY requirements.txt requirements.txt
RUN pip3 install -r requirements.txt

COPY . .
CMD [ <span class="hljs-string">"python3"</span>, <span class="hljs-string">"-W ignore"</span> ,<span class="hljs-string">"src/serving.py"</span>]</pre>



<p>要从上面的Docker文件构建Docker映像，您需要运行以下命令:</p>



<pre class="hljs">docker build --tag &lt;image-name&gt; . 

docker run -e NEPTUNE_API_TOKEN=<span class="hljs-string">"&lt;YOUR_API_TOKEN&gt;"</span>  -e MODEL_VERSION =”&lt;YOUR_MODEL_VERSION&gt;” &lt;image-name&gt;
</pre>



<p>在Docker容器上管理数据有几种替代方法，您可以在开发期间<a href="https://web.archive.org/web/20221201155629/https://docs.docker.com/storage/bind-mounts/" target="_blank" rel="noreferrer noopener nofollow">绑定挂载目录</a>。这是调试代码的一个很好的选择。您可以通过运行以下命令来实现这一点:</p>



<pre class="hljs">docker run -it &lt;image-name&gt;:&lt;image-version&gt; -v /home/&lt;user&gt;/my_code:/code
</pre>



<p>现在，您可以同时调试和执行容器中的代码，所做的更改将反映在主机上。<a href="https://web.archive.org/web/20221201155629/https://faun.pub/set-current-host-user-for-docker-container-4e521cef9ffc" target="_blank" rel="noreferrer noopener nofollow">这让我们回到了在容器中使用相同的主机用户ID和组ID的优势</a>。您所做的所有修改都将显示为来自主机用户。</p>



<p>要启动Docker容器，您需要运行以下命令:</p>



<pre class="hljs">docker run -d -e NEPTUNE_API_TOKEN=<span class="hljs-string">"&lt;YOUR_API_TOKEN&gt;"</span>  -e MODEL_VERSION =”&lt;YOUR_MODEL_VERSION&gt;” &lt;image-name&gt;
</pre>



<p><strong> <em> -d </em> </strong>选项指定容器应该以<a href="https://web.archive.org/web/20221201155629/https://docs.docker.com/get-started/overview/#the-docker-daemon" target="_blank" rel="noreferrer noopener nofollow">守护模式</a>启动。</p>



<h2 id="h-final-thoughts">最后的想法</h2>



<p>可再现性和协作开发是数据科学家应该用Docker容器部署他们的模型的最重要的原因。<a href="https://web.archive.org/web/20221201155629/https://www.tensorflow.org/tfx/guide/serving" target="_blank" rel="noreferrer noopener nofollow"> TensorFlow serving </a>是流行的模型服务工具之一，您可以扩展它来服务其他类型的模型和数据。此外，当使用TensorFlow服务机器学习模型时，您需要了解不同的<a href="https://web.archive.org/web/20221201155629/https://www.tensorflow.org/tfx/serving/api_rest" target="_blank" rel="noreferrer noopener nofollow">客户端API</a>，并选择最适合您的用例。</p>



<p>Docker 是在生产中部署和服务模型的好工具。尽管如此，找出许多数据科学家犯的错误并避免犯类似的错误是至关重要的。</p>



<p>数据科学家在用Docker服务机器学习模型时犯的错误围绕着模型延迟、应用程序安全和监控。模型延迟和<a href="/web/20221201155629/https://neptune.ai/blog/machine-learning-model-management" target="_blank" rel="noreferrer noopener">模型管理</a>是你的ML系统的重要部分。一个好的ML应用程序应该在收到请求时返回预测。通过避免这些错误，您应该能够使用Docker有效地部署一个工作的ML系统。</p>



<h3>参考</h3>



<ol><li><a href="https://web.archive.org/web/20221201155629/https://towardsdatascience.com/docker-best-practices-for-data-scientists-2ed7f6876dff" target="_blank" rel="noreferrer noopener nofollow"> Docker数据科学家最佳实践</a></li><li><a href="https://web.archive.org/web/20221201155629/https://aws.amazon.com/blogs/machine-learning/reduce-compuer-vision-inference-latency-using-grpc-with-tensorflow-serving-on-amazon-sagemaker/" target="_blank" rel="noreferrer noopener nofollow">使用gRPC和亚马逊SageMaker上的TensorFlow服务减少计算机视觉推理延迟</a></li><li><a href="https://web.archive.org/web/20221201155629/https://towardsdatascience.com/deploy-mlflow-with-docker-compose-8059f16b6039" target="_blank" rel="noreferrer noopener nofollow">使用docker编写部署ml flow</a></li><li><a href="https://web.archive.org/web/20221201155629/https://towardsdatascience.com/two-essentials-for-ml-service-level-performance-monitoring-2637bdabc0d2" target="_blank" rel="noreferrer noopener nofollow">ML服务水平性能监控的两大要素</a></li><li><a href="https://web.archive.org/web/20221201155629/https://cloud.google.com/architecture/minimizing-predictive-serving-latency-in-machine-learning#precomputing_and_caching_predictions" target="_blank" rel="noreferrer noopener nofollow">最小化机器学习中的实时预测服务延迟</a></li><li><a href="https://web.archive.org/web/20221201155629/https://towardsdatascience.com/serving-deep-learning-model-in-production-using-fast-and-efficient-grpc-6dfe94bf9234" target="_blank" rel="noreferrer noopener nofollow">如何使用gRPC API服务于深度学习模型？</a></li><li><a href="https://web.archive.org/web/20221201155629/https://docs.google.com/presentation/d/1yPxocBvpAM2dqdILfZtEwd13znC7l7ymx_y6BAD3Pg0/edit#slide=id.ge87238424d_0_47" target="_blank" rel="noreferrer noopener nofollow">将机器学习模型构建到Docker图像中</a></li></ol>
        </div>
        
    </div>    
</body>
</html>