<html>
<head>
<title>How to Organize Your ML Development in an Efficient Way </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>如何有效地组织你的ML开发</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/how-to-organize-your-ml-development#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/how-to-organize-your-ml-development#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>每个数据科学家和ML实践者最终都会遇到的一个主要问题是工作流管理。测试不同的场景和用例，记录信息和细节，共享和比较特定样本集的结果，可视化数据，跟踪洞察力。这些是数据科学工作流管理的关键组件。它们有助于业务，使您能够扩展任何数据科学项目。</p>



<p>数据科学家很清楚，测试一个版本的ML算法是不够的。我们的领域非常依赖经验主义，因此我们需要测试和比较具有不同超参数调整和特性选择的同一算法的多个版本。</p>



<p>所有这些都会产生元数据，需要正确存储。为了做到这一点，我使用了一个可以为我管理所有这些东西的平台——<strong>Neptune</strong>。它附带了一个完整的客户端库，可以无缝集成到您的代码中。它们还可以让您访问基于web的用户界面，在这里您的所有数据都被记录下来并可供使用。</p>



<p>为了让您了解Neptune所提供的功能，我用一个准备好的在线数据集模拟了一个真实的用例场景。我们将运行不同的分析和ML流程，看看海王星在日常工作中能为你提供多少支持。</p>


<h2 id="environment">设置Neptune环境</h2>



<p>为了使您能够快速开始将Neptune集成到项目的所有方面，了解如何安装软件包和库，以及如何将Jupyter笔记本连接到Neptune帐户可能会很有用。</p>



<p>首先，让我们创建一个conda虚拟环境，我们将在其中安装所有需要的neptune库:</p>



<pre class="hljs">conda create --name neptune python=<span class="hljs-number">3.6</span></pre>



<p>安装neptune客户端库:</p>



<pre class="hljs">pip  install neptune-client</pre>



<p>安装Neptune笔记本，将我们所有的工作保存到Neptune的web客户端:</p>



<pre class="hljs">pip install -U neptune-notebooks</pre>



<p>使用以下扩展启用jupyter集成:</p>



<pre class="hljs">jupyter nbextension enable --py neptune-notebooks</pre>



<p>获取您的API密钥，并将您的笔记本与您的Neptune会话连接起来:</p>







<p>成功连接笔记本后，您需要创建一个个人项目，您的所有实验都将保存在该项目中:</p>







<p>要完成设置，请在笔记本中导入neptune客户端库，并调用neptune.init()方法初始化连接:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune
neptune.init(project_qualified_name=<span class="hljs-string">'aymane.hachcham/CaseStudyOnlineRetail'</span>)</pre>



<p>你也可以去查一个海王星AI前数据科学家Kamil做的视频，里面把前面的细节解释的很透彻。</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><p class="wp-block-embed__wrapper"><iframe loading="lazy" title="Introduction to Neptune  – Your ML Experiments Tracking Tool" src="https://web.archive.org/web/20221206091240if_/https://www.youtube.com/embed/37X5iXiVop8?list=PLKePQLVx9tOd8TEGdG4PAKz0Owqdv1aaw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p></figure>



<p><em> <strong>注</strong>:我把代码放在最有启发性的地方，如果你想查看完整的代码版本和笔记本，请随意访问我的Github repo—<a href="https://web.archive.org/web/20221206091240/https://github.com/aymanehachcham/Neptune-Retail" target="_blank" rel="noreferrer noopener nofollow">Neptune-Retail</a></em></p>



<h2 id="dataset">探索数据集</h2>



<p>我们将看看在线零售数据集，在<a href="https://web.archive.org/web/20221206091240/https://www.kaggle.com/vijayuv/onlineretail" target="_blank" rel="noreferrer noopener nofollow"> Kaggle </a>公开发布。该数据集记录了世界各地使用在线销售平台的各种客户。每条记录都通知一个购买特定产品的订单。</p>



<p>数据集显示如下:</p>







<p>为了开始加载数据集，我创建了一个小的python<strong><em>data manager</em></strong>类来下载CSV文件，提取主要特征并将它们转换成可用的熊猫数据帧:</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DataETLManager</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, root_dir: str, csv_file: str)</span>:</span>
        <span class="hljs-keyword">if</span> os.path.exists(root_dir):
            <span class="hljs-keyword">if</span> csv_file.endswith(<span class="hljs-string">'.csv'</span>):
                self.csv_file = os.path.join(root_dir, csv_file)
            <span class="hljs-keyword">else</span>:
                logging.error(<span class="hljs-string">'The file is not in csv format'</span>)
                exit(<span class="hljs-number">1</span>)
        <span class="hljs-keyword">else</span>:
            logging.error(<span class="hljs-string">'The root dir path does not exist'</span>)
            exit(<span class="hljs-number">1</span>)

        self.retail_df = pd.read_csv(self.csv_file, sep=<span class="hljs-string">','</span>, encoding=<span class="hljs-string">'ISO-8859-1'</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extract_data</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.retail_df

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fetch_columns</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.retail_df.columns.tolist()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">data_description</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.retail_df.describe()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fetch_categorical</span><span class="hljs-params">(self, categorical=False)</span>:</span>
        <span class="hljs-keyword">if</span> categorical:
            categorical_columns = list(set(self.retail_df.columns) - set(self.retail_df._get_numerical_data().columns))
            categorical_df = self.retail_df[categorical_columns]
            <span class="hljs-keyword">return</span> categorical_df
        <span class="hljs-keyword">else</span>:
            non_categorical = list(set(self.retail_df._get_numerical_data().columns))
            <span class="hljs-keyword">return</span> self.retail_df[non_categorical]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transform_data</span><span class="hljs-params">(self)</span>:</span>
        data = self.retail_df

        
        data.drop_duplicates(keep=<span class="hljs-string">'last'</span>, inplace=<span class="hljs-keyword">True</span>)

        
        data[<span class="hljs-string">'InvoiceNo'</span>].fillna(value=<span class="hljs-number">0</span>, inplace=<span class="hljs-keyword">True</span>)
        data[<span class="hljs-string">'Description'</span>].fillna(value=<span class="hljs-string">'No Description'</span>, inplace=<span class="hljs-keyword">True</span>)
        data[<span class="hljs-string">'StockCode'</span>].fillna(value=<span class="hljs-string">'----'</span>, inplace=<span class="hljs-keyword">True</span>)
        data[<span class="hljs-string">'Quantity'</span>].fillna(value=<span class="hljs-number">0</span>, inplace=<span class="hljs-keyword">True</span>)
        data[<span class="hljs-string">'InvoiceDate'</span>].fillna(value=<span class="hljs-string">'00/00/0000 00:00'</span>, inplace=<span class="hljs-keyword">True</span>)
        data[<span class="hljs-string">'UnitPrice'</span>].fillna(value=<span class="hljs-number">0.00</span>, inplace=<span class="hljs-keyword">True</span>)

        data[<span class="hljs-string">'CustomerID'</span>].fillna(value=<span class="hljs-number">0</span>, inplace=<span class="hljs-keyword">True</span>)
        data[<span class="hljs-string">'Country'</span>].fillna(value=<span class="hljs-string">'None'</span>, inplace=<span class="hljs-keyword">True</span>)

        
        data[<span class="hljs-string">'InvoiceDate'</span>] = pd.to_datetime(data[<span class="hljs-string">'InvoiceDate'</span>])

        self.data_transfomed = data</pre>



<p>我们可以用来开始构建内部核心指标的重要栏目有:</p>



<ul><li><strong><em/></strong></li><li><strong> <em>数量</em> </strong></li><li><strong> <em>单价</em> </strong></li><li><strong> <em> CustomerID </em> </strong></li><li><strong> <em>国家</em> </strong></li></ul>



<p>首先使用DataETLManager加载数据集:</p>



<pre class="hljs">etl_manager = DataETLManager(root_dir=<span class="hljs-string">'./Data'</span>, csv_file=<span class="hljs-string">'OnlineRetail.csv'</span>)
etl_manager.extract_data()
etl_manager.transform_data()

dataset = etl_manager.data_transfomed</pre>



<p>对于零售企业来说，核心价值依赖于平台通过客户订单产生的收入。我们可以将单价和数量结合起来形成月收入，并按发票日期汇总:</p>



<pre class="hljs">dataset[<span class="hljs-string">'Profit'</span>] = dataset[<span class="hljs-string">'Quantity'</span>] * dataset[<span class="hljs-string">'UnitPrice'</span>]
revenue = dataset.groupby([<span class="hljs-string">'InvoiceDate'</span>])[<span class="hljs-string">'Profit'</span>].sum().reset_index()</pre>



<p>我们还可以通过绘制下图来直观显示收入在几个月内的变化情况:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> chart_studio.plotly <span class="hljs-keyword">as</span> py
<span class="hljs-keyword">import</span> plotly.graph_objects <span class="hljs-keyword">as</span> go
<span class="hljs-keyword">import</span> plotly.offline <span class="hljs-keyword">as</span> pyoff

pyoff.init_notebook_mode()

data = go.Scatter(
    x=revenuePerYear[<span class="hljs-string">'InvoiceDate'</span>],
    y=revenuePerYear[<span class="hljs-string">'Profit'</span>]
)

layout = go.Layout(
    xaxis={<span class="hljs-string">"type"</span>: <span class="hljs-string">"category"</span>},
    title=<span class="hljs-string">'Monthly Revenue'</span>
)

fig = go.Figure(data, layout)
pyoff.iplot(fig)</pre>







<p>由于我们的主要目标是客户，一个值得关注的指标是我们平台保留的活跃客户数量。我们将专门针对英国客户进行实验，因为他们构成了数据样本的大部分。</p>







<p>为了研究主动客户保持，我们需要检查每个月有多少客户订单:</p>



<pre class="hljs">uk_customers = dataset.query(<span class="hljs-string">"Country=='United Kingdom'"</span>).reset_index(drop=<span class="hljs-keyword">True</span>)
activeCustomers = dataset.groupby([<span class="hljs-string">'InvoiceDate'</span>])[<span class="hljs-string">'CustomerID'</span>].nunique().reset_index()</pre>



<p>分布似乎相当单调，在2011年11月达到峰值。</p>



<p>在我们的案例研究中，我们希望对这些客户进行适当的细分。通过这种方式，我们可以有效地管理投资组合，并剖析每个团队实际提供的不同价值水平。</p>



<p>我们还应该记住，随着业务规模的增长，不可能对每个客户都有一个直觉。在这个阶段，关于追求哪些客户的人类判断将不起作用，企业将不得不使用数据驱动的方法来建立适当的战略。</p>



<p>在下一部分，我们将更深入地挖掘不同的指标和分析，我们可以利用这些指标和分析对我们的客户群进行适当的细分。</p>











<h2 id="metrics-and-analytics">提取指标并对数据进行分析</h2>



<p>在这一部分，我们将彻底分析数据。我们希望根据财务标准对整个客户群进行细分。在本节结束时，我们应该能够描述和了解我们的客户购买行为。</p>



<p>由于我们已经在Neptune中初始化了我们的项目，我们将开始我们的第一个实验，记录我们将在本节中提取的统计数据。您可以将Neptune实验想象成一个命名空间，您可以在其中记录度量、预测、可视化以及您可能需要的任何其他内容。</p>



<p>首先初始化这个实验的参数，并调用create _ experiment()方法。</p>



<pre class="hljs">params = {
    <span class="hljs-string">'n_clusters'</span>:<span class="hljs-number">4</span>,
    <span class="hljs-string">'max_iterations'</span>: <span class="hljs-number">1000</span>,
    <span class="hljs-string">'first_metric'</span>: <span class="hljs-string">'Recency'</span>,
    <span class="hljs-string">'second_metric'</span>: <span class="hljs-string">'Frequency'</span>,
    <span class="hljs-string">'third_metric'</span>: <span class="hljs-string">'Monetary Value'</span>,
    <span class="hljs-string">'users'</span>: <span class="hljs-string">'UK'</span>
}

neptune.create_experiment(
    name=<span class="hljs-string">'KMeans-UK-Users'</span>,
    tags=[<span class="hljs-string">'KMeans-UK'</span>],
    params=params
)</pre>



<p>一旦你运行笔记本电池，你可以前往网站。如果您打开我们刚刚创建的实验，在<strong>参数</strong>下，您会发现值被正确记录，并准备好跟踪进一步的行动。</p>







<p>为了根据盈利能力和增长潜力对我们的客户群进行细分，我们将关注最终会影响我们客户金融行为的三个主要因素。该标准依赖于构成所谓的<strong> RFM分数</strong>的三个因素:</p>



<ul><li>使用频率:监控用户活动最近的一个指标</li><li>使用频率:用户多久在平台上购买一次产品</li><li>一元价值:从字面上看，它们有多有利可图</li></ul>



<p>首先，我们需要详细阐述数据集的指标。然后，我们将对这些数据点执行聚类，这样我们就可以根据相似性将它们从高价值客户到低价值客户分成不同的类别。从客户细分中获得的见解被用于开发量身定制的营销活动和设计营销策略。</p>



<p>对于这个任务，<strong> K-Means聚类</strong>算法仍然是一个非常强大的工具。它的易用性和性能为我们的用例提供了完美的平衡。</p>



<p>关于K-Means如何工作的详细解释，我推荐这篇完美完成这项工作的文章:<a href="https://web.archive.org/web/20221206091240/https://towardsdatascience.com/k-means-clustering-explained-4528df86a120" target="_blank" rel="noreferrer noopener nofollow"> K-Means聚类——解释</a></p>



<h3>RFM分数</h3>



<p>这个想法是测量自上次购买以来的天数，从而测量平台上记录的不活动天数。我们可以将其计算为所有客户的最大购买日期减去该范围内的总最大日期。<br/> <strong>创建我们将在</strong>工作的客户数据框架:</p>



<pre class="hljs">customers = pd.DataFrame(dataset[<span class="hljs-string">'CustomerID'</span>].unique())
customers.columns = [<span class="hljs-string">'CustomerID'</span>]</pre>



<p><strong>合计最大发票日期:</strong></p>



<pre class="hljs">
aggregatR = {<span class="hljs-string">'InvoiceDate'</span>: <span class="hljs-string">'max'</span>}
customers[<span class="hljs-string">'LastPurchaseDate'</span>]=dataset.groupby([<span class="hljs-string">'CustomerID'</span>],as_index=<span class="hljs-keyword">False</span>).agg(aggregatR)[<span class="hljs-string">'InvoiceDate'</span>]</pre>



<p><strong>生成最近得分:</strong></p>



<pre class="hljs">
customers[<span class="hljs-string">'Recency'</span>] = (customers[<span class="hljs-string">'LastPurchaseDate'</span>].max() - customers[<span class="hljs-string">'LastPurchaseDate'</span>]).dt.days</pre>



<p><strong>客户最近:</strong></p>







<p>因为我们有相应的表，所以将它记录到我们的Neptune实验中是一个好主意。</p>



<p>为此，我们可以调用neptune.log_table()方法，如下所示:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> neptunecontrib.api <span class="hljs-keyword">import</span> log_table
log_table(<span class="hljs-string">'Recency English Users'</span>, recency_UK)</pre>







<p>现在，您可以继续应用K-Means来对我们的最近分布进行聚类。在此之前，我们需要定义最适合我们需求的集群数量。一种方法是肘法。肘方法简单地告诉最佳惯性的最佳簇数。</p>



<pre class="hljs">K-means_metrics = {}

<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>):
    kmeans = KMeans(n_clusters=k, max_iter=<span class="hljs-number">1000</span>).fit(customers[<span class="hljs-string">'Recency'</span>])
    customers[<span class="hljs-string">"clusters"</span>] = kmeans.labels_
    k-means_metrics[k] = k means.inertia_</pre>



<p>让我们画出海王星的值，这样我们可以彻底检查曲线如何演变的细节:</p>



<pre class="hljs"><span class="hljs-keyword">for</span> val <span class="hljs-keyword">in</span> kmeans_metrics.values():
    neptune.log_metric(<span class="hljs-string">'Kmeans_Intertia_Values'</span>, val)</pre>



<p>Neptune自动记录日志部分的值，并相应地生成一个图表。</p>







<p>根据该图，最佳的最佳聚类数是4。因此，我们将对三个指标使用4个集群。</p>



<p><strong>K-表示最近:</strong></p>



<pre class="hljs">kmeans = KMeans(n_clusters=<span class="hljs-number">4</span>)
kmeans.fit(customers[[<span class="hljs-string">'Recency'</span>]])
customers[<span class="hljs-string">'RecencyCluster'</span>] = kmeans.predict(customers[[<span class="hljs-string">'Recency'</span>]])</pre>



<p>让我们记录最近的分布和预测的集群。</p>



<pre class="hljs">
<span class="hljs-keyword">for</span> cluster <span class="hljs-keyword">in</span> customers[<span class="hljs-string">'RecencyCluster'</span>]:
    neptune.log_metric(<span class="hljs-string">'UK Recency Clusters'</span>, cluster)


<span class="hljs-keyword">for</span> rec <span class="hljs-keyword">in</span> customers[<span class="hljs-string">'Recency'</span>]:
    neptune.log_metric(<span class="hljs-string">'Recency in days'</span>, rec)
</pre>







<p>如果您放大最近天数图表，您会注意到数值范围在50到280天之间。</p>







<p>我们可以通过查看一些常规统计数据来检查有关集群分布的更多信息:</p>







<p>我们可以注意到，聚类2中的客户比聚类1中的客户更新。</p>



<p>让我们通过分别计算频率和货币价值来推进我们的研究。我们将尝试在这三个指标之间进行更高层次的比较。</p>



<p><strong>汇总客户订单数:</strong></p>



<pre class="hljs">customers = pd.DataFrame(dataset[<span class="hljs-string">'CustomerID'</span>].unique())
customers.columns = [<span class="hljs-string">'CustomerID'</span>]


aggregatF = {<span class="hljs-string">'InvoiceDate'</span>: <span class="hljs-string">'count'</span>}
freq = dataset.groupby(<span class="hljs-string">'CustomerID'</span>, as_index=<span class="hljs-keyword">False</span>).agg(aggregatF)
customers = pd.merge(customers, freq, on=<span class="hljs-string">'CustomerID'</span>)</pre>



<p><strong>K-频率得分的均值:</strong></p>



<pre class="hljs">kmeans = KMeans(n_clusters=<span class="hljs-number">4</span>)
kmeans.fit(customers[[<span class="hljs-string">'Frequency'</span>]])
customers[<span class="hljs-string">'FrequencyCluster'</span>] = kmeans.predict(customers[[<span class="hljs-string">'Frequency'</span>]]</pre>



<p>当与前面的帧结合时，我们得到下表:</p>







<p><strong>合计每个客户产生的利润总和:</strong></p>



<pre class="hljs">
dataset[<span class="hljs-string">'Profit'</span>] = dataset[<span class="hljs-string">'UnitPrice'</span>] * dataset[<span class="hljs-string">'Quantity'</span>]
aggregatMV = {<span class="hljs-string">'Profit'</span>: <span class="hljs-string">'sum'</span>}
mv = dataset.groupby(<span class="hljs-string">'CustomerID'</span>, as_index=<span class="hljs-keyword">False</span>).agg(aggregatMV)
customers = pd.merge(customers, mv, on=<span class="hljs-string">'CustomerID'</span>)

customers.columns = [<span class="hljs-string">'CustomerID'</span>, <span class="hljs-string">'lastPurchase'</span>, <span class="hljs-string">'Recency'</span>, <span class="hljs-string">'Frequency'</span>, <span class="hljs-string">'MonetaryValue'</span>]
</pre>







<p>然后，我们将所有指标组合在一起，以便有一个总体概述。</p>



<p><strong>K-表示货币价值:</strong></p>



<pre class="hljs">kmeans = KMeans(n_clusters=<span class="hljs-number">4</span>)
kmeans.fit(customers[[<span class="hljs-string">'MonetaryValue'</span>]])
customers[<span class="hljs-string">'MonetaryCluster'</span>] = kmeans.predict(customers[[<span class="hljs-string">'MonetaryValue'</span>]])</pre>



<p>为了得到一个综合考虑了我们刚刚收集的所有值的RFM分数，我们需要在一个唯一的总分数中总结不同的聚类。然后，我们根据获得的范围值对每个客户部分进行细分。</p>



<p>三个部分:</p>



<ul><li><strong>高值:0-2分</strong></li><li><strong>中间值:3-6分</strong></li><li><strong>高值:6-9分</strong></li></ul>



<pre class="hljs">
customers[<span class="hljs-string">'RFMScore'</span>] = customers[<span class="hljs-string">'RecencyCluster'</span>] + customers[<span class="hljs-string">'FrequencyCluster'</span>] + customers[<span class="hljs-string">'MonetaryCluster'</span>]
customers[<span class="hljs-string">'UserSegment'</span>] = <span class="hljs-string">'Low'</span>


customers.loc[customers[<span class="hljs-string">'RFMScore'</span>] &lt;= <span class="hljs-number">2</span>, <span class="hljs-string">'UserSegment'</span>] = <span class="hljs-string">'Low'</span>
customers.loc[customers[<span class="hljs-string">'RFMScore'</span>] &gt; <span class="hljs-number">2</span>, <span class="hljs-string">'UserSegment'</span>] = <span class="hljs-string">'Mid'</span>
customers.loc[customers[<span class="hljs-string">'RFMScore'</span>] &gt; <span class="hljs-number">5</span>, <span class="hljs-string">'UserSegment'</span>] = <span class="hljs-string">'High'</span></pre>







<p>最精彩的部分是当我们绘制聚类图并可视化它们是如何分布的，将<strong>频率</strong>和<strong>新近度</strong>度量与它们产生的<strong>货币价值</strong>进行比较。</p>







<p>这两个指标都清楚地表明，最近的和频繁的用户更有利可图。因此，我们应该提高高价值用户(红色)的保留率，并根据该标准做出决策。此外，通过提高用户保留率，我们可以立即影响他们在平台上的频率和新近度。这意味着我们还应该在用户参与度上进行操作。</p>



<h2 id="organizing">在Neptune组织ML开发</h2>



<p>在这一节中，我们将利用Neptune提供的一个优秀特性，即<a href="https://web.archive.org/web/20221206091240/https://docs.neptune.ai/essentials/integrations" target="_blank" rel="noreferrer noopener"> ML集成</a>。在我们的例子中，我们将密切关注<strong> <a href="https://web.archive.org/web/20221206091240/https://docs.neptune.ai/essentials/integrations/machine-learning-frameworks/xgboost" target="_blank" rel="noreferrer noopener"> XGBoost </a> </strong>，因为海王星有助于所有的技术细节，比如:</p>



<ul><li>每次提升迭代后记录指标</li><li>训练后的模型记录</li><li>特征重要性</li><li>最后一次提升迭代后的树可视化</li></ul>



<p>e<strong>X</strong>treme<strong>G</strong>radient<strong>B</strong>oosting是梯度增强的优化和并行化开源实现，由华盛顿大学的博士生陈天琦创建。XGBoost使用决策树(像random forest)来解决分类(二元&amp;多类)、排序和回归问题。我们在监督学习算法领域。</p>



<p>本节的想法是预测客户终身价值，这是评估我们客户组合的另一个重要指标。平台对客户进行投资，进行获取成本、促销、折扣等。我们应该跟踪和密切关注当前的盈利客户，并预测他们在未来会如何发展。</p>



<p>在这个实验中，我们将在<strong> 9 </strong>个月的时间内锁定一组客户。我们将使用<strong> 3 </strong>个月的数据训练一个XGBoost模型，并尝试预测接下来的<strong> 6 </strong>个月。</p>



<h3>分离数据</h3>



<p><strong> 3个月用户:</strong></p>



<pre class="hljs"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime, date

uk = dataset.query(<span class="hljs-string">"Country=='United Kingdom'"</span>).reset_index(drop=<span class="hljs-keyword">True</span>)
uk[<span class="hljs-string">'InvoiceDate'</span>] = pd.to_datetime(uk[<span class="hljs-string">'InvoiceDate'</span>])

users_3m = uk[(uk[<span class="hljs-string">'InvoiceDate'</span>].dt.date &gt;= date(<span class="hljs-number">2010</span>, <span class="hljs-number">12</span>, <span class="hljs-number">1</span>)) &amp; (uk[<span class="hljs-string">'InvoiceDate'</span>].dt.date &lt; date(<span class="hljs-number">2011</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>))].reset_index(drop=<span class="hljs-keyword">True</span>)</pre>



<p><strong> 6个月用户:</strong></p>



<pre class="hljs">users_6m = uk[(uk[<span class="hljs-string">'InvoiceDate'</span>].dt.date &gt;= date(<span class="hljs-number">2011</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>)) &amp; (uk[<span class="hljs-string">'InvoiceDate'</span>].dt.date &lt; date(<span class="hljs-number">2011</span>, <span class="hljs-number">12</span>, <span class="hljs-number">1</span>))].reset_index(drop=<span class="hljs-keyword">True</span>)</pre>



<p>现在，在3个月的数据框架中，应用我们之前所做的相同汇总。关注频率、近期和货币价值。此外，计算与K-Means相同的聚类规则。</p>







<p>为了创建终身价值指标，我们将按6个月用户组每月产生的收入进行汇总:</p>



<pre class="hljs">users_6m[<span class="hljs-string">'Profit'</span>] = users_6m[<span class="hljs-string">'UnitPrice'</span>] * users_6m[<span class="hljs-string">'Quantity'</span>]
aggr = {<span class="hljs-string">'Profit'</span>: <span class="hljs-string">'sum'</span>}
customers_6 = users_6m.groupby(<span class="hljs-string">'CustomerID'</span>, as_index=<span class="hljs-keyword">False</span>).agg(aggr) customers_6.columns = [<span class="hljs-string">'CustomerID'</span>, <span class="hljs-string">'LTV'</span>]</pre>



<p>然后根据该度量生成K均值聚类:</p>



<pre class="hljs">kmeans = KMeans(n_clusters=<span class="hljs-number">3</span>)
kmeans.fit(customers_6[[<span class="hljs-string">'LTV'</span>]])
customers_6[<span class="hljs-string">'LTVCluster'</span>] = kmeans.predict(customers_6[[<span class="hljs-string">'LTV'</span>]])</pre>







<h3>开始培训过程</h3>



<p>将3个月的表与6个月的表合并，您将拥有相同的数据框，以及我们将在后续步骤中使用的训练集和验证集。</p>



<pre class="hljs">classification = pd.merge(customers_3, customers_6, on=<span class="hljs-string">'CustomerID'</span>, how=<span class="hljs-string">'left'</span>)
classification.fillna(<span class="hljs-number">0</span>, inplace=<span class="hljs-keyword">True</span>)</pre>



<p>我们的目标是根据核心预测功能为LTVCluster提供分类细分，例如:MVCluster、FrequencyCluster、RFMScore和Monetary Value。</p>



<p>然而，我们还不知道它们的相关性和预测能力。就此而言，我们需要运行一些属性相关性分析。</p>



<h3>属性相关性分析</h3>



<p>运行<strong>属性相关性分析</strong>，<strong> </strong>我们将考虑两个重要的功能:识别对目标变量影响最大的变量，以及理解最重要的预测因子和目标变量之间的关系。为了运行这种分析，可以使用<strong> <em>信息值</em> </strong>和<strong> <em>证据权重</em> </strong>的方法。</p>



<p><strong> <em>注</em> </strong> <em>:为了更深入地回顾WoE和IV，我强烈推荐这篇关于流失分析的中型文章:</em> <a href="https://web.archive.org/web/20221206091240/https://towardsdatascience.com/churn-analysis-information-value-and-weight-of-evidence-6a35db8b9ec5#9557" target="_blank" rel="noreferrer noopener nofollow"> <em>使用信息价值和证据权重的流失分析</em> </a> <em>，作者Klaudia Nazarko </em>。</p>



<p>在我们的例子中，我们将继续查看所有特性之间的相关性，并检查<strong> MVCluster </strong>和<strong>RFM store</strong>的信息值。</p>



<p><strong>相关矩阵:</strong></p>



<pre class="hljs">classification.corr()[<span class="hljs-string">'LTVCluster'</span>].sort_values(ascending=<span class="hljs-keyword">False</span>)</pre>



<pre class="hljs"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sn
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

corrMatrix = classification.corr()
sn.heatmap(corrMatrix, annot=<span class="hljs-keyword">False</span>)
plt.show()</pre>







<section id="blog-intext-cta-block_61b3599386f07" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>Neptune与<a href="https://web.archive.org/web/20221206091240/https://docs.neptune.ai/essentials/integrations/visualization-libraries" target="_blank" rel="noreferrer noopener">可视化库</a>(包括<a href="https://web.archive.org/web/20221206091240/https://docs.neptune.ai/essentials/integrations/visualization-libraries" target="_blank" rel="noreferrer noopener">熊猫</a>、<a href="https://web.archive.org/web/20221206091240/https://docs.neptune.ai/essentials/integrations/visualization-libraries/matplotlib" target="_blank" rel="noreferrer noopener"> matplotlib </a>等等)的集成。</p>
    
    </section>



<p>我们确实观察到与LTVCluster更相关的特征是频率、货币价值和新近性，这是有意义的。</p>



<p>同样，根据WoE和IV分析，MVCluster和RFMScore似乎比其他的预测能力更强。</p>







<p>最后，为了进行进一步的训练，我们需要将分类变量转换成数字。一种快速的方法是使用pd.get_dummies():</p>



<pre class="hljs">classification = pd.get_dummies(customers)</pre>







<p><em> UserSegment </em>列已经消失，但我们有新的数字来表示它。我们已经将它转换为3个不同的0和1列，并使其可用于我们的机器学习模型。</p>



<h3>列车XGBoost</h3>



<h4>创造实验</h4>



<p>首先在我们已经初始化的前一个项目中创建一个新的实验。在本节中，我们将使用XGBoost的多个版本来训练我们的数据。每个版本都将设置特定的超参数。</p>







<p>最后，我们将尝试比较不同的实验，以获得更多的见解。你可以随时查看Neptune docs，找到任何相关的资源和文档，以备不时之需。</p>



<pre class="hljs">params = {
    <span class="hljs-string">'max_depth'</span>:<span class="hljs-number">5</span>,
    <span class="hljs-string">'learning_rate'</span>:<span class="hljs-number">0.1</span>,
    <span class="hljs-string">'objective'</span>: <span class="hljs-string">'multi:softprob'</span>,
    <span class="hljs-string">'n_jobs'</span>:<span class="hljs-number">-1</span>,
    <span class="hljs-string">'num_class'</span>:<span class="hljs-number">3</span>
}

neptune.create_experiment(
    name=<span class="hljs-string">'XGBoost-V1'</span>,
    tags=[<span class="hljs-string">'XGBoost'</span>, <span class="hljs-string">'Version1'</span>],
    params=params
)</pre>



<p>根据hyper-parameters，我们需要一个能够进行多标签分类的XGBoost模型(因此我们使用了<strong> multi:softprob </strong>目标函数)。我们专门针对客户LTV集群范围内的三个类别。</p>



<h4>拆分数据</h4>



<p>将数据分成训练集和测试集:</p>



<pre class="hljs">X = classification.drop([<span class="hljs-string">'LTV'</span>, <span class="hljs-string">'LTVCluster'</span>, <span class="hljs-string">'lastPurchase'</span>], axis=<span class="hljs-number">1</span>)
Y = classification[<span class="hljs-string">'LTVCluster'</span>] </pre>



<pre class="hljs">
x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=<span class="hljs-number">0.05</span>, random_state=<span class="hljs-number">56</span>) </pre>



<p>实例化<strong> <em> XGB DMatrix </em> </strong>数据加载器，以便我们可以方便地将数据传递给模式:</p>



<pre class="hljs">dtrain = xgb.DMatrix(x_train, label=y_train)
dtest = xgb.DMatrix(x_test, label=y_test)</pre>



<h4>使用XGBClassifier Neptune回调并记录所有指标</h4>



<p>是时候让数据符合我们的模型了。我们将使用XGBClassifier，并在实验仪表板中实时记录所有指标。利用Neptune与所有不同种类的梯度增强算法的紧密集成，我们能够非常容易地监控性能和进度。</p>



<pre class="hljs">multi_class_XGB = xgb.XGBClassifier(**params3)
multi_class_XGB.fit(x_train, y_train, eval_set=[(x_test, y_test)], callbacks=[neptune_callback()])

neptune.stop()</pre>



<p>有一个非常好的视频解释了Neptune XGBoost集成是如何工作的:<a href="https://web.archive.org/web/20221206091240/https://www.youtube.com/watch?v=xc5gsJvf5Wo&amp;list=PLKePQLVx9tOd8TEGdG4PAKz0Owqdv1aaw&amp;index=13&amp;ab_channel=NeptuneAI" target="_blank" rel="noreferrer noopener nofollow">Integrations–XGBoost</a>。</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><p class="wp-block-embed__wrapper"><iframe loading="lazy" title="Integrations – XGBoost" src="https://web.archive.org/web/20221206091240if_/https://www.youtube.com/embed/xc5gsJvf5Wo?list=PLKePQLVx9tOd8TEGdG4PAKz0Owqdv1aaw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p></figure>



<p>如果我们回到Neptune并单击我们创建的实验，我们可以可视化损失图表、损失度量和特性重要性图表。</p>







<p>如果我们想看看我们的模型在测试集上的得分，我们可以使用sklearn.metrics包打印一个分类报告。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report,confusion_matrix
predict = multi_class_XGB.predict(x_test)
print(classification_report(y_test, predict))</pre>







<p>尽管我们对之前的结果非常满意，但我们仍然可以创建另一个实验，并调整或更改一些超参数以获得更好的结果。</p>



<pre class="hljs">params2 = {
    <span class="hljs-string">'max_depth'</span>:<span class="hljs-number">5</span>,
    <span class="hljs-string">'learning_rate'</span>:<span class="hljs-number">0.1</span>,
    <span class="hljs-string">'objective'</span>: <span class="hljs-string">'multi:softprob'</span>,
    <span class="hljs-string">'n_jobs'</span>:<span class="hljs-number">-1</span>,
    <span class="hljs-string">'num_class'</span>:<span class="hljs-number">3</span>,
    <span class="hljs-string">'eta'</span>:<span class="hljs-number">0.5</span>,
    <span class="hljs-string">'gamma'</span>: <span class="hljs-number">0.1</span>,
    <span class="hljs-string">'lambda'</span>:<span class="hljs-number">1</span>,
    <span class="hljs-string">'alpha'</span>:<span class="hljs-number">0.35</span>,
}

neptune.create_experiment(
    name=<span class="hljs-string">'XGBoost-V2'</span>,
    tags=[<span class="hljs-string">'XGBoost'</span>, <span class="hljs-string">'Version2'</span>],
    params=params2,
)</pre>



<p>让我们来训练:</p>



<pre class="hljs">multi_class_XGB = xgb.XGBClassifier(**params2)
multi_class_XGB.fit(
    x_train,
    y_train,
    eval_set=[(x_test, y_test)],
    callbacks=[neptune_callback()])

neptune.stop()</pre>



<p>检查训练集和测试集的准确性:</p>



<pre class="hljs">print(<span class="hljs-string">'Accuracy on Training Set: '</span>, multi_class_XGB.score(x_train, y_train))
print(<span class="hljs-string">'Accuracy on Testing Set: '</span>, multi_class_XGB.score(x_test[x_train.columns], y_test))</pre>



<p>总的来说，结果相当不错，几乎与之前的实验相同。</p>



<h4>比较两个实验</h4>



<p>Neptune允许我们选择多个实验，并在仪表板上进行比较:</p>







<p>我们可以并排观察这两个实验，并比较每个列实验中的参数实际上如何影响训练集、测试集和验证集的损失。</p>



<h4>版本化您的模型</h4>



<p>Neptune的一个有趣特性是能够对模型二进制文件进行版本控制，这样我们就可以在进行实验时跟踪不同的版本。</p>



<pre class="hljs">neptune.log_artifact(<span class="hljs-string">'xgb_classifier.pkl'</span>)</pre>



<p>但是，在训练过程中调用neptune_callback()时，最后一次boosting迭代会毫不费力地自动记录到neptune。</p>







<h2 id="conclusion">结论</h2>



<p>本教程的主要目标是帮助您快速开始使用Neptune。工具非常容易，很难在UI中迷失。</p>



<p>我希望这篇教程对你有用，因为我设计它是为了涵盖真实数据科学用例的不同方面。我给你留一些参考资料，看看你是否觉得自己的求知欲还需要淬火:</p>







<p>此外，不要忘了查看Neptune文档网站和他们的Youtube频道，那里有你开始更有效工作所需的所有工具的深度报道:</p>







<p>别忘了查看我的Github repo获取本教程的完整代码:<a href="https://web.archive.org/web/20221206091240/https://github.com/aymanehachcham/Neptune-Retail" target="_blank" rel="noreferrer noopener nofollow"> <em>海王星-零售</em> </a></p>
        </div>
        
    </div>    
</body>
</html>