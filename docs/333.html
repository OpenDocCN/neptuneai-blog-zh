<html>
<head>
<title>8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>8位创作者和核心贡献者谈论他们来自PyTorch生态系统的模型训练库</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">

<div id="author-box-new-format-block_605d8dd517661" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">雅各布·查肯</h3>
    
          <p class="article__authorContent-text">大部分是ML的人。构建MLOps工具，编写技术资料，在Neptune进行想法实验。</p>
    
          
    
  </div>
</div>


<p>我在2018年初使用py torch 0 . 3 . 1版本开始训练我的模型。我被Pythonic的手感、易用性和灵活性深深吸引。</p>



<p>在Pytorch中做事情要比在Tensorflow或Theano中容易得多。但是我错过的是PyTorch的类似Keras的高级接口，当时还没有太多。</p>



<p>快进到2020年，<strong>我们在<a href="https://web.archive.org/web/20220926085913/https://pytorch.org/ecosystem/" target="_blank" rel="noreferrer noopener nofollow"> PyTorch生态系统</a>中有6个高级培训API。</strong></p>


<div class="custom-point-list">
<ul><li>但是你应该选择哪一个呢？</li><li>每一种使用的利弊是什么？</li></ul>
</div>


<p>我想:谁能比作者自己更好地解释这些库之间的差异呢？</p>



<p>我拿起我的手机，请他们和我一起写一篇文章。他们都同意，这就是这个职位是如何创建的！</p>



<p>所以，<strong>我请作者们谈谈他们图书馆的以下几个方面:</strong></p>


<div class="custom-point-list">
<ul><li>项目理念</li><li>API结构</li><li>新用户的学习曲线</li><li>内置功能(开箱即用)</li><li>扩展能力(研究中集成的简单性)</li><li>再现性</li><li>分布式培训</li><li>生产化</li><li>流行</li></ul>
</div>


<p>…他们确实回答得很透彻🙂</p>






<h4 id="content">可以跳转到自己感兴趣的库或者直接到最后我的主观对比。</h4>









<a href="https://web.archive.org/web/20220926085913/https://www.linkedin.com/in/benjamin-bossan-3114a684/" class="author-box-another-format">
    <div class="left">
        
        <p class="author-data"><span class="name">本杰明·博桑</span> <span class="author-desc">核心撰稿人</span></p>
    </div>
    
</a>



<p><a href="https://web.archive.org/web/20220926085913/https://github.com/skorch-dev/skorch" target="_blank" rel="noreferrer noopener nofollow">斯科奇</a>发展背后的<strong>理念</strong>可以总结如下:</p>


<div class="custom-point-list">
<ul><li>遵循sklearn API</li><li>don’t hide PyTorch</li><li>不要多此一举</li><li>可以被黑客攻击</li></ul>
</div>


<p>这些原则规划了我们的设计空间。关于<strong> scikit-learn API </strong>，它最明显的表现在你<strong>如何训练和预测</strong>:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> skorch <span class="hljs-keyword">import</span> NeuralNetClassifier

net = NeuralNetClassifier(...)
net.fit(X_train, y_train)
net.predict(X_test)
</pre>



<p>因为skorch正在使用这个<strong>简单且完善的API </strong>,所以每个人都应该能够很快开始使用它。</p>



<p>但是sklearn的整合比“适应”和“预测”更深入。您可以将您的skorch模型无缝集成到sklearn的“Pipeline”中，使用sklearn的众多指标(无需重新实现F1、R等。)，并配合<code>GridSearchCV</code>使用。</p>



<p>说到<strong>参数扫描</strong>:你可以使用任何其他的超参数搜索策略，只要有一个sklearn兼容的实现。</p>



<p>我们特别自豪的是<strong>你可以搜索几乎任何超参数，而不需要额外的工作</strong>。例如，如果您的模块有一个名为<code>num_units</code>的初始化参数，您可以立即对该参数进行网格搜索。</p>



<p>这里有一个<strong>的列表，你可以用网格搜索现成的:</strong></p>


<div class="custom-point-list">
<ul><li>您的<code>Module</code>上的任何参数(单元和层数、非线性度、辍学率等)</li><li>优化器(学习率、动力……)</li><li>标准</li><li><code>DataLoader</code>(批量大小，洗牌，…)</li><li>回调(任何参数，甚至是自定义回调)</li></ul>
</div>


<p>这是它在代码中的样子:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV

params = {
    <span class="hljs-string">'lr'</span>: [<span class="hljs-number">0.01</span>, <span class="hljs-number">0.02</span>],
    <span class="hljs-string">'max_epochs'</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>],
    <span class="hljs-string">'module__num_units'</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>],
    <span class="hljs-string">'optimizer__momentum'</span>: [<span class="hljs-number">0.6</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">0.95</span>],
    <span class="hljs-string">'iterator_train__shuffle'</span>: [<span class="hljs-keyword">True</span>, <span class="hljs-keyword">False</span>],
    <span class="hljs-string">'callbacks__mycallback__someparam'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
}

net = NeuralNetClassifier(...)
gs = GridSearchCV(net, params, cv=<span class="hljs-number">3</span>, scoring=<span class="hljs-string">'accuracy'</span>)
gs.fit(X, y)

print(gs.best_score_, gs.best_params_)
</pre>



<p>据我所知，没有其他框架提供这种灵活性。最重要的是，通过使用dask并行后端，您可以<a href="https://web.archive.org/web/20220926085913/https://skorch.readthedocs.io/en/stable/user/parallelism.html" target="_blank" rel="noreferrer noopener nofollow"> <strong>将超参数搜索</strong> </a>分布到您的集群中，而不会有太多麻烦。</p>



<p>使用成熟的sklearn API，skorch用户可以<strong>避免在纯PyTorch中编写训练循环、验证循环和超参数搜索时常见的样板代码</strong>。</p>



<p>从PyTorch方面来说，我们决定不像keras那样将后端隐藏在抽象层之后。相反，<strong>我们公开了PyTorch </strong>的众多组件。作为用户，你可以使用PyTorch的<code>Dataset</code>(想想torchvision，包括TTA)<code>DataLoader</code>，和学习率调度器。最重要的是，你可以不受限制地使用PyTorch <code>Module</code> s。</p>



<p>因此，我们有意识地努力<strong>尽可能多地重用sklearn和PyTorch的现有功能</strong>，而不是重新发明轮子。这使得skorch <strong>易于在你现有的代码库</strong>上使用，或者在你最初的实验阶段后移除它，而没有任何锁定效应。</p>



<p>例如，您可以用任何sklearn模型替换神经网络，或者您可以提取PyTorch模块并在没有skorch的情况下使用它。</p>



<p>在重用现有功能的基础上，我们添加了一些自己的功能。最值得注意的是，skorch <strong>可以开箱即用地处理许多常见的数据类型</strong>。除了<code>Dataset</code> s，您还可以使用:</p>


<div class="custom-point-list">
<ul><li>numpy数组，</li><li>火炬张量，</li><li>熊猫，</li><li>保存异构数据的Python字典，</li><li>外部/自定义数据集，如torchvision的<a href="https://web.archive.org/web/20220926085913/https://nbviewer.jupyter.org/github/skorch-dev/skorch/blob/master/notebooks/Transfer_Learning.ipynb" target="_blank" rel="noreferrer noopener nofollow"> ImageFolder。</a></li></ul>
</div>


<p>我们已经付出了额外的努力来使这些与sklearn一起很好地工作。</p>



<p>此外，我们实现了一个简单而<strong>强大的回调系统</strong>，你可以用它来<strong>根据你的喜好</strong>调整skorch的大部分行为。我们提供的一些回调包括:</p>


<div class="custom-point-list">
<ul><li>学习率调度程序，</li><li>评分功能(使用自定义或sklearn指标)，</li><li>提前停车，</li><li>检查点，</li><li>参数冻结，</li><li>以及TensorBoard和Neptune集成。</li></ul>
</div>


<p>如果这还不足以满足您的定制需求，<strong>我们尽力帮助您实施自己的回访或您自己的模型培训师</strong>。我们的文档包含了如何实现<a href="https://web.archive.org/web/20220926085913/https://skorch.readthedocs.io/en/stable/user/callbacks.html#callback-base-class" target="_blank" rel="noreferrer noopener nofollow">定制回调</a>和<a href="https://web.archive.org/web/20220926085913/https://skorch.readthedocs.io/en/stable/user/neuralnet.html#subclassing-neuralnet" target="_blank" rel="noreferrer noopener nofollow">定制训练者</a>的例子，修改每一个可能的行为直到训练步骤。</p>



<p>对于任何熟悉sklearn和PyTorch的人来说，不重新发明轮子的哲学应该使skorch易于学习。由于我们围绕定制和灵活性设计了skorch，因此应该不难掌握。要了解更多关于skorch的信息，请查看这些<a href="https://web.archive.org/web/20220926085913/https://github.com/skorch-dev/skorch/tree/master/examples" target="_blank" rel="noreferrer noopener nofollow">示例</a>和<a href="https://web.archive.org/web/20220926085913/https://github.com/skorch-dev/skorch/tree/master/notebooks" target="_blank" rel="noreferrer noopener nofollow">笔记本</a>。</p>



<p>sko rch<strong>面向生产</strong>并用于生产。我们讨论了一些关于生产化的常见问题，特别是:</p>


<div class="custom-point-list">
<ul><li>我们确保<strong>是向后兼容的</strong>，并在必要时给出足够长的折旧期。</li><li>可以<strong>在GPU上训练，在CPU上服务，</strong></li><li>你可以<strong>腌制一整只包含skorch模型的sklearn <code>Pipeline</code> </strong>以备后用。</li><li>我们提供了一个助手函数来<strong> <a href="https://web.archive.org/web/20220926085913/https://github.com/skorch-dev/skorch/tree/master/examples/cli" target="_blank" rel="noreferrer noopener nofollow">将您的训练代码转换成命令行脚本</a> </strong>，它将您的所有模型参数，包括它们的文档，作为命令行参数公开，只需要三行额外的代码</li></ul>
</div>


<p>也就是说，我已经实现了，或者知道有人已经实现了，更多的<strong>研究</strong> -y的东西，像<strong> GANs </strong>和无数类型的<strong>半监督学习</strong>技术。不过，这确实需要对skorch有更深入的了解，所以您可能需要更深入地研究文档，或者向我们寻求github的指导。</p>



<p>就我个人而言，我还没有遇到任何人使用skorch进行强化学习，但我想听听人们对此有什么体验。</p>



<p>自从我们在2017年夏天首次发布skorch以来，该项目已经成熟了很多，并且围绕它已经发展了一个活跃的社区。在一个典型的星期里，github上会打开一些问题，或者在stackoverflow上提出一个问题。我们会在一天之内回答大多数问题，如果有好的功能需求或bug报告，我们会尝试引导报告者自己实现它。</p>



<p>通过这种方式，<strong>在项目的整个生命周期中，我们有20多个贡献者，其中3个是常客</strong>，这意味着项目的健康不依赖于一个人。</p>



<p>fastai说，skorch和其他一些高级框架的最大区别是skorch不“包含电池”。这意味着，实现他们自己的模块或者使用许多现有集合中的一个(比如torchvision)的模块取决于用户。斯科奇提供骨架，但你得带上肉。</p>



<h3><strong>何时不使用skorch </strong></h3>


<div class="custom-point-list">
<ul><li>超级自定义PyTorch代码，可能是强化学习</li><li>后端不可知代码(在PyTorch、tensorflow等之间切换)</li><li>根本不需要sklearn API</li><li>避免非常轻微的性能开销</li></ul>
</div>


<h3><strong>何时使用skorch </strong></h3>


<div class="custom-point-list">
<ul><li>获得sklearn API和所有相关的好处，如超参数搜索</li><li>大多数PyTorch工作流都能正常工作</li><li>避免样板文件，规范代码</li><li>使用上面讨论的许多实用程序中的一些</li></ul>
</div>








<a href="https://web.archive.org/web/20220926085913/https://www.linkedin.com/in/scitator/" class="author-box-another-format">
    <div class="left">
        
        <p class="author-data"><span class="name">谢尔盖·科列斯尼科夫</span> <span class="author-desc">创造者</span></p>
    </div>
    
</a>



<h3><strong>哲学</strong></h3>



<p>催化剂背后的想法很简单:</p>


<div class="custom-point-list">
<ul><li>在一个框架中收集所有技术、开发、深度学习的东西，</li><li>使重复使用枯燥的日常组件变得容易，</li><li>在我们的项目中关注研究和假设检验。</li></ul>
</div>


<p>为了实现这一点，我们研究了一个典型的深度学习项目，它通常具有以下结构:</p>



<pre class="hljs"><span class="hljs-keyword">for</span> stage <span class="hljs-keyword">in</span> stages:
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> epochs:
        <span class="hljs-keyword">for</span> dataloader <span class="hljs-keyword">in</span> dataloaders:
            <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataloader:
                handle(batch)</pre>



<p>想想看，大多数时候，您需要做的就是为新模型指定处理方法，以及如何将数据批量提供给该模型。那么，为什么我们把这么多时间花在实现流水线和调试训练循环上，而不是开发新东西或测试假设呢？</p>



<p>我们意识到有可能<strong>将工程与研究</strong>分开，这样我们就可以<strong>将我们的时间一次性</strong>投入到高质量、可重复使用的<strong>工程</strong>主干上<strong>在所有项目中使用它</strong>。</p>



<p>Catalyst就是这样诞生的:一个开源的PyTorch框架，它允许你编写紧凑但功能齐全的管道，<strong>抽象工程样板文件，</strong>让你专注于项目的主要部分。</p>



<blockquote class="wp-block-quote is-style-default"><p>我们在Catalyst的任务。团队将使用我们的软件工程和深度学习专业知识来标准化工作流，并实现深度学习和强化学习研究人员之间的跨领域交流。</p></blockquote>



<p>我们相信，开发摩擦的减少和思想的自由流动将导致未来数字图书馆的突破，这样的R&amp;D生态系统将有助于实现这一目标。</p>



<h3><strong>学习曲线</strong></h3>



<p>Catalyst可以被DL新手和经验丰富的专家轻松采用，这得益于两个API:</p>


<div class="custom-point-list">
<ul><li><strong>笔记本API </strong>，它的开发重点是<strong>简单的实验和Jupyter笔记本的使用</strong>-开始你的可重复DL研究之路。</li><li><strong> Config API </strong>，主要关注<strong>可伸缩性和CLI接口</strong>——即使在大型集群上也能发挥DL/RL的威力。</li></ul>
</div>


<p>说到PyTorch用户体验，我们真的希望它尽可能简单:</p>


<div class="custom-point-list">
<ul><li>您可以像平时一样定义加载器、模型、标准、优化器和调度器:</li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">import</span> torch


loaders = {<span class="hljs-string">"train"</span>: ..., <span class="hljs-string">"valid"</span>: ...}


model = Net()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)</pre>


<div class="custom-point-list">
<ul><li>你把这些PyTorch对象传递给Catalyst <code>Runner</code></li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">from</span> catalyst.dl <span class="hljs-keyword">import</span> SupervisedRunner


logdir = <span class="hljs-string">"./logdir"</span>
num_epochs = <span class="hljs-number">42</span>


runner = SupervisedRunner()


runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    logdir=logdir,
    num_epochs=num_epochs,
    verbose=<span class="hljs-keyword">True</span>,)</pre>



<p><strong>在几乎没有样板文件的情况下，将工程与深度学习明确分离</strong>。这是我们觉得深度学习代码应该有的样子。</p>



<p>要开始使用这两种API，你可以遵循我们的<a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/catalyst#docs-and-examples" target="_blank" rel="noreferrer noopener nofollow">教程和管道</a>或者如果你不想选择，只需检查最常见的:<a href="https://web.archive.org/web/20220926085913/https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/classification-tutorial.ipynb" target="_blank" rel="noreferrer noopener nofollow">分类</a>和<a href="https://web.archive.org/web/20220926085913/https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/segmentation-tutorial.ipynb" target="_blank" rel="noreferrer noopener nofollow">分割。</a></p>



<h3><strong>设计和架构</strong></h3>



<p>关于<strong> Notebook和Config API最有趣的部分是它们使用了相同的“后端”逻辑</strong>–<code>Experiment</code>、<code>Runner</code>、<code>State</code>和<code>Callback</code>抽象，这是Catalyst的核心特性。</p>


<div class="custom-point-list">
<ul><li><a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/catalyst/blob/master/catalyst/core/experiment.py" target="_blank" rel="noreferrer noopener nofollow"> <strong>实验</strong> : </a>包含实验信息的抽象——模型、标准、优化器、调度器以及它们的超参数。它还包含有关使用的数据和转换的信息。总的来说，实验知道<strong>你想要运行什么</strong>。</li><li><a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/catalyst/blob/master/catalyst/core/runner.py" target="_blank" rel="noreferrer noopener nofollow"> <strong> Runner </strong> : </a>知道如何进行实验的类。它包含了<strong>如何</strong>运行实验、阶段(催化剂的另一个显著特征)、时期和批次的所有逻辑。</li><li><a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/catalyst/blob/master/catalyst/core/state.py" target="_blank" rel="noreferrer noopener nofollow"> <strong>状态</strong> : </a>实验和运行程序之间的一些中间存储，保存实验的当前<strong>状态</strong>——模型、标准、优化器、调度器、度量、记录器、加载器等</li><li><a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/catalyst/blob/master/catalyst/core/callback.py" target="_blank" rel="noreferrer noopener nofollow"> <strong>回调</strong> : </a>一个强大的抽象，让你<strong>定制</strong>你的实验运行逻辑。为了给用户最大的灵活性和可扩展性，我们允许在训练循环的任何地方执行回调:</li></ul>
</div>


<pre class="hljs">on_stage_<a>start</a>
    on_epoch_<a>start</a>
       on_loader_<a>start</a>
           on_batch_<a>start</a>
           
       on_batch_<a>end</a>
    on_epoch_<a>end</a>
on_stage_<a>end</a>

on_<a>exception</a></pre>



<p>通过实现这些方法，您可以实现任何额外的逻辑。</p>



<p>因此，你可以在几行代码(以及Catalyst之后)中<strong>实现任何深度学习管道</strong> <strong>。RL 2.0版本-强化学习管道)，从可用的原语中组合它(感谢社区，他们的数量每天都在增长)。</strong></p>



<p>其他一切(模型、标准、优化器、调度器)都是纯PyTorch原语。Catalyst不会在顶层创建任何包装器或抽象，而是让在不同框架和领域之间重用这些构件变得容易。</p>



<h3><strong>扩展能力/研究中集成的简单性</strong></h3>



<p>由于灵活的框架设计和回调机制，Catalyst可以很容易地扩展到大量基于DL的项目。你可以在<a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/awesome-catalyst-list#repositories" target="_blank" rel="noreferrer noopener nofollow"> awesome-catalyst-list </a>上查看我们的Catalyst-powered知识库。</p>



<p>如果您对<strong>强化学习</strong>感兴趣，也有大量基于RL的回购和竞争解决方案。来比较催化剂。使用其他RL框架，你可以查看<a href="https://web.archive.org/web/20220926085913/https://docs.google.com/spreadsheets/d/1EeFPd-XIQ3mq_9snTlAZSsFY7Hbnmd7P5bbT8LPuMn0/edit?usp=sharing" target="_blank" rel="noreferrer noopener nofollow">开源RL列表</a>。</p>



<h3><strong>其他内置特性(开箱即用)</strong></h3>



<p>知道你可以很容易地扩展它会让你感觉很舒服，但是你有很多现成的特性。其中一些包括:</p>


<div class="custom-point-list">
<ul><li>基于灵活的回调系统，Catalyst已经<strong>轻松集成了</strong>如<strong>常见的深度学习最佳实践</strong>，如梯度累积、梯度裁剪、权重衰减校正、top-K最佳检查点保存、tensorboard集成以及许多其他有用的日常深度学习实用程序。</li><li>由于我们的贡献者和贡献模块，<strong> Catalyst可以访问所有最新的SOTA功能</strong>，如AdamW、OneCycle、SWA、Ranger、LookAhead和许多其他研究开发。</li><li>此外，<strong>我们整合了</strong>像Nvidia apex、<a href="https://web.archive.org/web/20220926085913/https://github.com/albu/albumentations" target="_blank" rel="noreferrer noopener nofollow">albuminations</a>、<a href="https://web.archive.org/web/20220926085913/https://github.com/qubvel/segmentation_models.pytorch" target="_blank" rel="noreferrer noopener nofollow"> SMP </a>、<a href="https://web.archive.org/web/20220926085913/https://github.com/huggingface/transformers" target="_blank" rel="noreferrer noopener nofollow"> transformers </a>、wandb和neptune.ai这样的<strong>流行库</strong>，让您的研究更加人性化。由于这样的集成，Catalyst完全支持测试时间扩充、混合精度和分布式训练。</li><li>为了满足行业需求，我们还提供了对<a href="https://web.archive.org/web/20220926085913/https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html" target="_blank" rel="noreferrer noopener nofollow"> PyTorch跟踪</a>的框架式支持，这使得将模型投入生产变得更加容易。此外，我们在每个版本中部署预定义的基于Catalyst的docker映像，以便于集成。</li><li>最后，我们支持针对模型服务—<a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/reaction" target="_blank" rel="noreferrer noopener">反应</a>(面向行业)和实验监控—<a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/alchemy" target="_blank" rel="noreferrer noopener nofollow">炼金术</a>(面向研究)的额外解决方案。</li></ul>
</div>


<p>一切都集成到库中，并涵盖了CI测试(我们有一个专用的gpu服务器)。感谢Catalyst脚本，您可以<strong>安排大量实验，并从命令行在所有可用的GPU上并行运行它们(查看catalyst-parallel-run了解更多信息)。</strong></p>



<h3><strong>再现性</strong></h3>



<p>我们做了大量的工作，使你用催化剂运行的实验具有可再现性。由于基于库的确定性<strong>，基于Catalyst的实验是可重复的</strong>不仅在一个服务器上的服务器运行之间<strong>，而且在不同服务器</strong>和不同硬件部件(当然，使用docker封装)上的几次运行之间也是如此。感兴趣的话可以在这里看到实验<a href="https://web.archive.org/web/20220926085913/https://app.wandb.ai/scitator/classification_reproducubility_check?workspace=user-scitator" target="_blank" rel="noreferrer noopener nofollow">。</a></p>



<p>而且，<strong>强化学习实验也是面向再现性的</strong>(就RL而言RL是可再现的)。例如，通过同步实验运行，由于采样轨迹的确定性，您可以获得非常接近的性能。这是众所周知的困难，据我所知<strong>催化剂有最可再生的RL管道。</strong></p>



<p>为了实现DL和RL再现性的新水平，我们必须创造几个额外的功能:</p>


<div class="custom-point-list">
<ul><li><strong>完整的源代码转储:</strong>由于实验、运行器和回调抽象，保存这些原语以备后用非常容易。</li><li><strong> Catalyst源代码包:</strong>有了这样的特性，即使使用Catalyst的开发版本，您也可以随时重现实验结果。</li><li><strong>环境版本化:</strong> Catalyst转储pip和conda包版本(稍后可用于定义您的docker映像)</li><li>最后，Catalyst支持几个<strong>监控工具</strong>，如Alchemy、Neptune.ai、Wandb，以存储您的所有实验指标和附加信息，从而更好地跟踪研究进度和再现性。</li></ul>
</div>


<p>由于这些基于库的解决方案，您可以确保在Catalyst中实现的管道是可重复的，并且保存了所有实验日志和检查点以供将来参考。</p>



<h3><strong>分布式培训</strong></h3>



<p>基于我们的集成，Catalyst已经有了对分布式培训的本地支持。此外，我们支持Slurm培训，并致力于更好地整合DL和RL管道。</p>



<h3>生产化</h3>



<p>既然我们知道Catalyst如何帮助深度学习研究，我们就可以谈论<strong>将训练好的模型部署到生产中</strong>。</p>



<p>正如已经提到的，Catalyst <strong>支持开箱即用的模型跟踪。</strong>它允许您将PyTorch模型(使用Python代码)转换为TorchScript模型(集成了所有内容)。TorchScript是一种从PyTorch代码创建可序列化和可优化模型的方法。任何TorchScript程序都可以从Python进程中保存，并在没有Python依赖的进程中加载。</p>



<p>此外，为了帮助Catalyst用户将其管道部署到生产系统中，Catalyst。团队有一个<strong> <a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/catalyst#docker" target="_blank" rel="noreferrer noopener nofollow"> Docker Hub </a>，带有预构建的基于Catalyst的映像</strong>(包括fp16支持)。</p>



<p>此外，为了帮助研究人员将他们的想法投入生产和现实世界的应用，我们创造了Catalyst。生态系统:</p>


<div class="custom-point-list">
<ul><li><a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/reaction" target="_blank" rel="noreferrer noopener nofollow"> <strong>反应</strong> : </a>我们自己的<strong> PyTorch服务解决方案</strong>，具有同步/异步API、批处理模式支持、quest，以及所有其他你可以从一个设计良好的生产系统中期待的典型后端。</li><li><a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/alchemy" target="_blank" rel="noreferrer noopener nofollow"> <strong>炼金</strong> : </a>我们的<strong>监控工具</strong>用于实验跟踪、模型对比、研究成果共享。</li></ul>
</div>


<h3><strong>人气</strong></h3>



<p>自从12个月前第一次发布pypi以来，Catalyst已经在Github上获得了1.5k颗星，超过<strong> 100k次下载</strong>。我们很自豪成为这样一个开源生态系统的一部分，非常感谢我们所有的用户和贡献者的不断支持和反馈。</p>



<p>其中一个特别有帮助的在线社区是<a href="https://web.archive.org/web/20220926085913/https://opendatascience.slack.com/messages/CGK4KQBHD" target="_blank" rel="noreferrer noopener nofollow"> ods.ai: </a>世界上最大的数据科学家和机器学习实践者的slack渠道之一(40k+用户)。没有他们的想法和反馈，Catalyst就不会有今天。</p>



<p>特别感谢我们的早期用户，</p>





<p>这一切都是值得的。</p>



<div class="wp-block-coblocks-accordion-item"><details><summary class="wp-block-coblocks-accordion-item__title"><a href="/web/20220926085913/https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem" target="_blank" rel="noreferrer noopener nofollow">Acknowledgments</a></summary><div class="wp-block-coblocks-accordion-item__content">
<p>自从сcatalyst开始发展以来，许多人以不同的方式影响了它。为了表达我的感激之情，我要向...表示我个人的谢意:</p>



</div></details></div>


<p id="block_5fff166729407" class="separator separator-5"/>



<p>感谢所有这些支持，Catalyst已经成为Kaggle docker image的一部分，被<strong>添加到<a href="https://web.archive.org/web/20220926085913/https://pytorch.org/ecosystem/" target="_blank" rel="noreferrer noopener nofollow"> PyTorch生态系统</a> </strong>中，现在我们正在<a href="https://web.archive.org/web/20220926085913/https://docs.google.com/presentation/d/1D-yhVOg6OXzjo9K_-IS5vSHLPIUxp1PEkFGnpRcNCNU/edit?usp=sharing" target="_blank" rel="noreferrer noopener nofollow">开发我们自己的DL R &amp; D生态系统</a>以加速您的研究和生产需求。</p>



<p>阅读更多关于<strong> Catalyst的信息。生态系统</strong>，请查看<a href="https://web.archive.org/web/20220926085913/https://docs.google.com/presentation/d/1D-yhVOg6OXzjo9K_-IS5vSHLPIUxp1PEkFGnpRcNCNU/edit?usp=sharing" target="_blank" rel="noreferrer noopener nofollow">我们的愿景</a>和<a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/catalyst/blob/master/MANIFEST.md" target="_blank" rel="noreferrer noopener nofollow">项目宣言。</a></p>



<p>最后，我们总是乐意帮助我们的<a href="https://web.archive.org/web/20220926085913/https://github.com/catalyst-team/awesome-catalyst-list#trusted-by" target="_blank" rel="noreferrer noopener nofollow">催化剂。朋友:</a>公司/初创公司/研究实验室，他们已经在使用Catalyst，或者正在考虑将它用于他们的下一个项目。</p>



<blockquote class="wp-block-quote is-style-default"><p>感谢阅读，并…打破循环-使用催化剂！</p></blockquote>



<h3><strong>何时使用催化剂</strong></h3>


<div class="custom-point-list">
<ul><li>拥有灵活和可重用的代码库，没有样板文件。你希望与来自不同深度学习领域的其他研究人员分享你的专业知识。</li><li>使用Catalyst.Ecosystem提高您的研究速度</li></ul>
</div>


<h3><strong>何时不使用催化剂</strong></h3>


<div class="custom-point-list">
<ul><li>你才刚刚开始你的深度学习之路——从这个角度来说，低级PyTorch是一个很好的入门。</li><li>你想用一堆不可复制的技巧创建非常具体的、定制的管道🙂</li></ul>
</div>








<a href="https://web.archive.org/web/20220926085913/https://twitter.com/jeremyphoward" class="author-box-another-format">
    
    
</a>



<a href="https://web.archive.org/web/20220926085913/https://twitter.com/guggersylvain" class="author-box-another-format">
    <div class="left">
        
        <p class="author-data"><span class="name">西尔万·古格</span> <span class="author-desc">核心撰稿人</span></p>
    </div>
    <p class="right">注意:</p>
</a>


<div class="note">
    <h3>下面是关于将于2020年7月发布的fastai的<strong>版本2。你可以在这里去回购<a href="https://web.archive.org/web/20220926085913/https://github.com/fastai/fastai" target="_blank" rel="noreferrer noopener">，在这里</a>查文件<a href="https://web.archive.org/web/20220926085913/https://docs.fast.ai/" target="_blank" rel="noreferrer noopener">。</a></strong></h3>
    <div class="content">
                    <div class="wysiwyg_editor">
                                    <p>What follows is about the <strong>version 2 of fastai that will be released in July 2020</strong>. You can go to repo <a href="https://web.archive.org/web/20220926085913/https://github.com/fastai/fastai" target="_blank" rel="noreferrer noopener">here</a> and check the documentation <a href="https://web.archive.org/web/20220926085913/https://docs.fast.ai/" target="_blank" rel="noreferrer noopener">here</a>.</p>
                            </div>
            </div>
</div>


<p id="block_5fff173b29408" class="separator separator-5">Fastai是一个深度学习库，它提供:</p>



<p><strong>从业者</strong>:有了可以快速便捷地提供标准深度学习领域最先进成果的高级组件，</p>


<div class="custom-point-list">
<ul><li><strong>研究人员</strong>:用可以混合搭配的低级组件来构建新的东西。</li><li>它的目标是在不牺牲易用性、灵活性或性能的情况下做到这两点。</li></ul>
</div>


<p>得益于精心分层的架构，这成为可能<strong>。它以<strong>解耦抽象</strong>的形式表达了许多深度学习和数据处理技术的通用底层模式。重要的是，这些抽象可以用<strong>清晰简洁地表达出来</strong>，这使得fastai变得平易近人<strong>快速高效，同时也是深度可黑客化和可配置的</strong>。</strong></p>



<p>一个高级API提供了<strong>可定制的模型和合理的默认值</strong>，它建立在一个由低级构建块构成的<strong>层级之上。</strong></p>



<p>本文涵盖了该库功能的一个代表性子集。有关详细信息，请参见我们的<a href="https://web.archive.org/web/20220926085913/https://arxiv.org/abs/2002.04688" target="_blank" rel="noreferrer noopener nofollow"> fastai论文</a>和文档。</p>



<p><strong> API </strong></p>



<h3>当谈到fastai API时，我们需要区分高级和中级/低级API。我们将在接下来的章节中讨论这两者。</h3>



<p><em> <strong>高级API </strong> </em></p>



<p>高级API对于初学者和<strong>主要对应用预先存在的深度学习方法感兴趣的从业者非常有用。</strong></p>



<p>它为主要应用领域提供了简明的API:</p>



<p>视觉，</p>


<div class="custom-point-list">
<ul><li>文字，</li><li>扁平的</li><li>时间序列分析，</li><li>推荐(协同过滤)</li><li>这些<strong>API基于所有可用信息选择智能默认值</strong>和行为。</li></ul>
</div>


<p>例如，fastai提供了一个<strong> <code>Learner</code>类</strong>，它集合了架构、优化器和数据，并且<strong>在可能的情况下自动选择一个合适的损失函数。</strong></p>



<p>再举一个例子，一般来说，训练集应该洗牌，验证集不应该洗牌。fastai提供了一个单独的<strong> <code>Dataloaders</code>类</strong>，该类自动<strong>构造验证和训练数据加载器，这些细节已经得到处理。</strong></p>



<p>为了了解这些“清晰简洁的代码”原则是如何发挥作用的，让我们在<a href="https://web.archive.org/web/20220926085913/https://www.robots.ox.ac.uk/~vgg/data/pets/" target="_blank" rel="noreferrer noopener nofollow">牛津IIT Pets数据集</a>上微调一个<a href="https://web.archive.org/web/20220926085913/http://www.image-net.org/" target="_blank" rel="noreferrer noopener nofollow"> imagenet </a>模型，并在单个GPU上几分钟的训练内实现接近最先进的精度:</p>



<p>这不是摘录。这是这项任务所需的所有代码行。每一行代码都执行一项重要的任务，让用户专注于他们需要做的事情，而不是次要的细节:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> fastai.vision.all <span class="hljs-keyword">import</span> *

path = untar_data(URLs.PETS)
dls = ImageDataloaders.from_name_re(path=path, bs=<span class="hljs-number">64</span>,
    fnames = get_image_files(path/<span class="hljs-string">"images"</span>), path = <span class="hljs-string">r'/([^/]+)_\d+.jpg$'</span>,
    item_tfms=RandomResizedCrop(<span class="hljs-number">450</span>, min_scale=<span class="hljs-number">0.75</span>), 
    batch_tfms=[*aug_transforms(size=<span class="hljs-number">224</span>, max_warp=<span class="hljs-number">0.</span>), 
                Normalize.from_stats(*imagenet_stats)])

learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(<span class="hljs-number">4</span>)</pre>



<p><strong>从库中导入所有必需的棋子</strong>。值得注意的是，这个库是经过精心设计的，以避免这些风格的导入搞乱名称空间。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> fastai.vision.all <span class="hljs-keyword">import</span> * 
</pre>



<p><strong>将标准数据集</strong>从fast.ai数据集集合(如果之前没有下载)下载到一个可配置的位置，提取它(如果之前没有提取)，并返回一个带有提取位置的<code>pathlib.Path</code>对象。</p>



<pre class="hljs">path = untar_data(URLs.PETS)
</pre>



<p>设置<code>Dataloaders</code>。注意<strong>项目级和批次级转换的分离</strong>:</p>



<pre class="hljs">dls = ImageDataloaders.from_name_re(path=path, bs=<span class="hljs-number">64</span>,
    fnames = get_image_files(path/<span class="hljs-string">"images"</span>), pat = <span class="hljs-string">r'/([^/]+)_\d+.jpg$'</span>,
    item_tfms=RandomResizedCrop(<span class="hljs-number">450</span>, min_scale=<span class="hljs-number">0.75</span>), 
    batch_tfms=[*aug_transforms(size=<span class="hljs-number">224</span>, max_warp=<span class="hljs-number">0.</span>), 
    Normalize.from_stats(*imagenet_stats)])
</pre>



<p><em> <strong>项</strong> </em>变换应用<strong>到CPU上的单个图像</strong></p>


<div class="custom-point-list">
<ul><li><em> <strong>批处理</strong> </em>变换应用<strong>到GPU </strong>(如果可用)上的一个小批处理。</li><li><code>aug_transforms()</code>选择一组数据扩充。与fastai中的一贯做法一样，我们选择了一个适用于各种视觉数据集的默认设置，但如果需要，也可以进行完全定制。</li></ul>
</div>


<p>创建一个<code>Learner</code>，这个<strong>结合了一个优化器、一个模型和一个用于训练的数据</strong>。<strong>每个应用程序(视觉、文本、表格)都有一个定制的函数，创建一个<code>Learner</code> </strong>，它能为用户自动处理任何细节。例如，在这个图像分类问题中，它将:</p>



<pre class="hljs">learn = cnn_learner(dls, resnet34, metrics=error_rate)
</pre>



<p>下载ImageNet预训练模型(如果还没有),</p>


<div class="custom-point-list">
<ul><li>去掉模型的分类头，</li><li>用适合于这个特定数据集的报头来替换它，</li><li>设置适当的优化器、权重衰减、学习率等等</li><li>微调模型。在这种情况下，它使用1周期策略，这是最近用于训练深度学习模型的最佳实践，但在其他库中并不广泛可用。很多事情发生在<code>.fine_tune()</code>的引擎盖下:</li></ul>
</div>


<pre class="hljs">learn.fine_tune(<span class="hljs-number">4</span>)
</pre>



<p>退火学习率和动量，</p>


<div class="custom-point-list">
<ul><li>在验证集上打印指标，</li><li>在HTML或控制台表格中显示结果</li><li>在每批之后记录损失和度量，等等。</li><li>如果有可用的GPU，将会使用它。</li><li>当模型的主体被冻结时，它将首先训练头部一个时期，然后使用区别学习率微调给定的许多时期(这里是4个)。</li><li>fastai库的<strong>优势之一是API跨应用程序的一致性。</strong></li></ul>
</div>


<p>例如，使用ULMFiT对IMDB数据集上的预训练模型进行微调(文本分类任务)只需6行代码:</p>



<p>用户在其他领域得到非常<strong>相似的体验</strong>，比如表格、时间序列或推荐系统。一旦一个<code>Learner</code>被训练，你可以用命令<code>learn.show_results()</code>来探索结果。这些结果如何呈现取决于应用，在视觉中你得到的是带标签的图片，在文本中你得到的是汇总样本、目标和预测的数据框架。在我们的宠物分类示例中，您会看到类似这样的内容:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> fastai2.text.all <span class="hljs-keyword">import</span> *

path = untar_data(URLs.IMDB)
dls = TextDataloaders.from_folder(path, valid=<span class="hljs-string">'test'</span>)
learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=<span class="hljs-number">0.5</span>, metrics=accuracy)
learn.fine_tune(<span class="hljs-number">4</span>, <span class="hljs-number">1e-2</span>)
</pre>



<p>在IMDb分类问题中，你会得到这样的结果:</p>







<p>另一个重要的高级API组件是<strong>数据块API，</strong>，它是一个用于数据加载的表达性API。这是我们所知的第一次尝试，系统地定义为深度学习模型准备数据所必需的所有步骤，并为用户提供一个混合搭配的食谱，用于组合这些片段(我们称之为数据块)。</p>







<p>下面是一个如何使用数据块API让<a href="https://web.archive.org/web/20220926085913/https://yann.lecun.com/exdb/mnist/" target="_blank" rel="noreferrer noopener"> MNIST </a>数据集为建模做好准备的示例:</p>



<p><em> <strong>中低档API </strong> </em></p>



<pre class="hljs">mnist = DataBlock(
    blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), 
    get_items=get_image_files, 
    splitter=GrandparentSplitter(),
    get_y=parent_label)
dls = mnist.databunch(untar_data(URLs.MNIST_TINY), batch_tfms=Normalize)
</pre>



<p>在上一节中，您看到了如何使用具有大量开箱即用功能的高级api快速完成大量工作。然而，有些情况下，你需要调整或扩展已经存在的东西。</p>



<p>这就是中级和低级API发挥作用的地方:</p>



<p><strong>中级API </strong>为这些应用中的每一个提供核心的深度学习和数据处理方法，</p>


<div class="custom-point-list">
<ul><li><strong>低级API </strong>提供了一个优化的原语库以及功能和面向对象的基础，允许中间层进行开发和定制。</li><li>可以使用<strong> <code>Learner</code>新型双向回调系统定制训练循环。</strong>它允许梯度、数据、损失、控制流和<strong>任何东西</strong>其他<strong>在训练期间的任何点被读取和改变。</strong></li></ul>
</div>


<p>使用回调来定制数值软件有着悠久的历史，今天几乎所有现代深度学习库都提供了这一功能。然而，fastai的回调系统是我们所知的第一个支持<strong>完成双向回调</strong>所必需的设计原则的系统:</p>



<p>在训练的每一点都应该有回叫<strong>，这给了用户充分的灵活性。每个回调都应该<strong>能够访问训练循环中该阶段可用的每条信息</strong>，包括超参数、损耗、梯度、输入和目标数据等等；</strong></p>


<div class="custom-point-list">
<ul><li>每次回调都应该能够在使用这些信息之前的任何时候修改所有这些信息，</li><li>训练循环的所有调整(不同的调度器、混合精度训练、在<a href="https://web.archive.org/web/20220926085913/https://www.tensorflow.org/tensorboard" target="_blank" rel="noreferrer noopener nofollow"> TensorBoard </a>、<a href="https://web.archive.org/web/20220926085913/https://www.wandb.com/" target="_blank" rel="noreferrer noopener nofollow"> wandb </a>、<a href="https://web.archive.org/web/20220926085913/https://neptune.ai/" target="_blank" rel="noreferrer noopener nofollow"> neptune </a>或等效物、<a href="https://web.archive.org/web/20220926085913/https://arxiv.org/abs/1710.09412" target="_blank" rel="noreferrer noopener nofollow">mix</a>、过采样策略、分布式训练、GAN训练……)都在回调中实现，最终用户<strong>可以将它们与自己的进行混合和匹配，从而更容易试验</strong>和进行消融研究。方便的方法可以为用户添加这些回调，使得混合精度的训练就像说的那样简单</li></ul>
</div>


<p>或者<strong>在分布式环境中培训</strong>一样简单</p>



<pre class="hljs">learn = learn.to_fp16()
</pre>



<p>fastai还提供了一个<strong>新的通用优化器抽象</strong>，允许用几行代码实现最近的优化技术，如LAMB、RAdam或AdamW。</p>



<pre class="hljs">learn = learn.to_distributed()
</pre>



<p>多亏了<strong>将优化器抽象</strong>重构为两个基本部分:</p>



<p><strong> <em> stats </em> </strong>，跟踪并汇总梯度移动平均线等统计数据</p>


<div class="custom-point-list">
<ul><li><strong> <em>步进器</em> </strong>，它结合了统计数据和超参数，使用一些函数来“步进”权重。</li><li>有了这个基础，我们可以用2-3行代码编写fastai的大部分优化器，而在其他流行的库中，这需要50多行代码。</li></ul>
</div>


<p>还有许多其他的中间层和低层APIs】使得研究人员和开发人员可以在快速灵活的基础上轻松构建新方法。</p>



<p>这个图书馆已经在研究、工业和教学中广泛使用。我们已经用它创建了一个完整的，非常受欢迎的深度学习课程:<a href="https://web.archive.org/web/20220926085913/https://course.fast.ai/" target="_blank" rel="noreferrer noopener nofollow">程序员实用深度学习</a>(最后一次迭代的第一个视频有256k的浏览量)。</p>



<p>在撰写本文时，<a href="https://web.archive.org/web/20220926085913/https://github.com/fastai/fastai" target="_blank" rel="noreferrer noopener nofollow">库</a>拥有<strong> 16.9k恒星，并在超过2000个项目</strong>中使用。社区在<a href="https://web.archive.org/web/20220926085913/https://forums.fast.ai/" target="_blank" rel="noreferrer noopener nofollow"> fast.ai论坛</a>上非常活跃，无论是澄清课程中不清楚的点，帮助调试还是合作解决新的深度学习项目。</p>



<p><strong>何时使用fastai </strong></p>



<h3>我们的目标是让一些东西对初学者来说足够简单，但对研究人员/从业者来说足够灵活。</h3>


<div class="custom-point-list">
<ul><li><strong>何时不使用fastai </strong></li></ul>
</div>


<h3>我能想到的唯一一件事是，你不会使用fastai在生产中服务于你在不同框架中训练的模型，因为我们不处理那个方面。</h3>


<div class="custom-point-list">
<ul><li><span class="name">维克多·福明</span> <span class="author-desc">核心撰稿人</span></li></ul>
</div>








<a href="https://web.archive.org/web/20220926085913/https://twitter.com/vfdev_5" class="author-box-another-format">
    <div class="left">
        
        <p class="author-data">Pytorch Ignite 是一个高级库，帮助在Pytorch中训练神经网络。自2018年初以来，我们的目标一直是:</p>
    </div>
    
</a>



<p>“让普通的事情变得容易，让困难的事情变得可能”。</p>



<blockquote class="wp-block-quote is-style-large"><p><strong>为什么要使用Ignite？</strong></p></blockquote>



<h3>Ignite的高抽象级别<strong>很少假设用户正在训练的模型类型或多个模型</strong>。我们只要求用户<strong>定义要在训练和可选验证循环</strong>中运行的闭包。它为用户提供了很大的灵活性，允许他们在任务中使用Ignite，例如共同训练多个模型(即gan)或在训练循环中跟踪多个损失和指标</h3>



<p><strong>点燃概念和API </strong></p>



<h3>您需要了解Ignite API中的一些核心对象:</h3>



<p><em> <strong>引擎</strong> </em>:精华库</p>


<div class="custom-point-list">
<ul><li><em> <strong>事件&amp;处理程序</strong> </em>:与<code>Engine</code>交互(如提前停止、检查点、日志记录)</li><li><em> <strong>指标</strong> </em>:各种任务的现成指标</li><li>我们将提供一些基础知识来理解主要思想，但可以随意深入挖掘存储库中的<a href="https://web.archive.org/web/20220926085913/https://pytorch.org/ignite" target="_blank" rel="noreferrer noopener nofollow">示例</a>。</li></ul>
</div>


<p><em> <strong>引擎</strong> </em></p>



<p>它只是遍历提供的数据，执行一个处理函数并返回一个结果。</p>



<p>一个<strong> <code>Trainer</code>是一个<code>Engine</code>，以模型的权重更新</strong>作为处理函数。</p>



<p>一个<strong> <code>Evaluator</code>(验证模型的对象)是一个以在线度量计算逻辑</strong>为处理功能的<code>Engine</code>。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> ignite.engine <span class="hljs-keyword">import</span> Engine

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_model</span><span class="hljs-params">(trainer, batch)</span>:</span>
    model.train()
    optimizer.zero_grad()
    x, y = prepare_batch(batch)
    y_pred = model(x)
    loss = criterion(y_pred, y)
    loss.backward()
    optimizer.step()
    <span class="hljs-keyword">return</span> loss.item()

trainer = Engine(update_model)
trainer.run(data, max_epochs=<span class="hljs-number">100</span>)</pre>



<p>这段代码可以悄悄地训练一个模型，并计算总损失。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> ignite.engine <span class="hljs-keyword">import</span> Engine

total_loss = []
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_metrics</span><span class="hljs-params">(_, batch)</span>:</span>
    x, y = batch
    model.eval()
    <span class="hljs-keyword">with</span> torch.no_grad():
        y_pred = model(x)
        loss = criterion(y_pred, y)
        total_loss.append(loss.item())

    <span class="hljs-keyword">return</span> loss.item()

evaluator = Engine(compute_metrics)
evaluator.run(data, max_epochs=<span class="hljs-number">1</span>)
print(f”Loss: {torch.tensor(total_loss).mean()}”)
</pre>



<p>在下一节中，我们将了解如何使培训和验证更加用户友好。</p>



<p><em> <strong>事件&amp;经手人</strong>T3】</em></p>



<p>为了<strong>提高<code>Engine</code>的灵活性</strong>，并允许用户在运行的每一步进行交互，<strong>我们引入了事件和处理程序</strong>。这个想法是，用户可以在训练循环内部执行一个自定义代码作为事件处理程序，类似于其他库中的回调。</p>



<p>在每次<em> fire_event </em>调用时，它的所有事件处理程序都会被执行。例如，用户可能希望在训练开始时设置一些运行相关变量(<code>Events.STARTED</code>)，并在每次迭代中更新学习率(<code>Events.ITERATION_COMPLETED</code>)。使用Ignite，代码将如下所示:</p>



<pre class="hljs">fire_event(Events.<a>STARTED</a>)

<span class="hljs-keyword">while</span> epoch &lt; max_epochs:
    fire_event(Events.<a>EPOCH_STARTED</a>)
    
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> data:
        fire_event(Events.<a>ITERATION_STARTED</a>)

        output = process_function(batch)

        fire_event(Events.<a>ITERATION_COMPLETED</a>)
    fire_event(Events.<a>EPOCH_COMPLETED</a>)
fire_event(Events.<a>COMPLETED</a>)
</pre>



<p><strong>处理程序(相对于“回调”接口)的酷之处在于，它可以是任何具有正确签名的函数</strong>(我们只要求第一个参数是engine)，例如lambda、简单函数、类方法等。我们不需要从一个接口继承，也不需要覆盖它的抽象方法。</p>



<pre class="hljs">train_loader = …
model = …
optimizer = …
criterion = ...
lr_scheduler = …

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_function</span><span class="hljs-params">(engine, batch)</span>:</span>
    

trainer = Engine(process_function)

<span class="hljs-meta">@trainer.on(Events.STARTED)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">setup_logging_folder</span><span class="hljs-params">(_)</span>:</span>
    
    

<span class="hljs-meta">@trainer.on(Events.ITERATION_COMPLETED)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_lr</span><span class="hljs-params">(engine)</span>:</span>
    lr_scheduler.step()

trainer.run(train_loader, max_epochs=<span class="hljs-number">50</span>)
</pre>



<p><strong>内置事件过滤</strong></p>



<pre class="hljs">trainer.add_event_handler(
    Events.STARTED, <span class="hljs-keyword">lambda</span> engine: print(<span class="hljs-string">"Start training"</span>))


mydata = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">on_training_ended</span><span class="hljs-params">(engine, data)</span>:</span>
    print(<span class="hljs-string">"Training is ended. mydata={}"</span>.format(data))


trainer.add_event_handler(
    Events.COMPLETED, on_training_ended, mydata)
</pre>



<h3>有些情况下，用户希望定期/一次性执行代码，或者使用自定义规则，如:</h3>



<p>每5个时期运行一次验证，</p>


<div class="custom-point-list">
<ul><li>每1000次迭代存储一个检查点，</li><li>在第20个时期改变一个变量，</li><li>在前10次迭代中记录梯度。</li><li>等等。</li><li>Ignite提供了这样的<strong>灵活性，将“要执行的代码”与逻辑“何时执行代码”分开。</strong></li></ul>
</div>


<p>例如，为了使<strong>每5个时期</strong>运行一次验证，只需简单编码:</p>



<p>类似地，为了<strong>在第20个时期</strong>改变一些训练变量一次:</p>



<pre class="hljs"><span class="hljs-meta">@trainer.on(Events.EPOCH_COMPLETED(every=5))</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_validation</span><span class="hljs-params">(_)</span>:</span>
    
</pre>



<p>更一般地，用户可以提供自己的事件过滤功能:</p>



<pre class="hljs"><span class="hljs-meta">@trainer.on(Events.EPOCH_STARTED(once=20))</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">change_training_variable</span><span class="hljs-params">(_)</span>:</span>
    
</pre>



<p><strong>现成的处理程序</strong></p>



<pre class="hljs"><span class="hljs-meta">@trainer.on(Events.EPOCH_STARTED(once=20))</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">change_training_variable</span><span class="hljs-params">(_)</span>:</span>
    
</pre>



<h3>Ignite提供了一系列处理程序和指标来简化用户代码:</h3>



<p><em> <strong>检查点</strong> </em>:保存训练检查点(由训练器、模型、优化器、lr调度器等组成)，保存最佳模型(按验证分数)</p>


<div class="custom-point-list">
<ul><li><strong> <em>提前停止</em> : </strong>如果没有进展(根据验证分数)则停止训练</li><li><em> <strong>终止南:</strong> </em>遇到南就停止训练</li><li><em> <strong>优化器参数调度:</strong> </em>串接，添加预热，设置线性或余弦退火，任意优化器参数的线性分段调度(lr，momentum，betas，…)</li><li>记录到通用平台:TensorBoard、Visdom、MLflow、Polyaxon或Neptune(批量损失、度量GPU内存/利用率、优化器参数等)。</li></ul>
</div>





<div class="custom-point-list">
<ul><li><em> <strong>指标</strong> </em></li></ul>
</div>






<p>Ignite还为各种任务提供了一个现成的指标列表<strong>:精度、召回率、准确度、混淆矩阵、IoU等，大约20个回归指标</strong></p>



<p>例如，下面我们计算验证数据集的验证准确度:</p>



<p>点击<a href="https://web.archive.org/web/20220926085913/https://pytorch.org/ignite/metrics.html#complete-list-of-metrics" target="_blank" rel="noreferrer noopener nofollow">此处</a>和<a href="https://web.archive.org/web/20220926085913/https://pytorch.org/ignite/contrib/metrics.html" target="_blank" rel="noreferrer noopener nofollow">此处</a>查看可用指标的完整列表。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> ignite.metrics <span class="hljs-keyword">import</span> Accuracy

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_predictions</span><span class="hljs-params">(_, batch)</span>:</span>
    
    <span class="hljs-keyword">return</span> y_pred, y_true

evaluator = Engine(compute_predictions)
metric = Accuracy()
metric.attach(evaluator, <span class="hljs-string">"val_accuracy"</span>)
evaluator.run(val_loader)
&gt; evaluator.state.metrics[“val_accuracy”] = <span class="hljs-number">0.98765</span>
</pre>



<p>Ignite指标有一个很酷的特性，即<strong>用户可以使用基本的算术运算</strong>或torch方法构建自己的指标:</p>



<p><strong>库结构</strong></p>



<pre class="hljs">precision = Precision(average=<span class="hljs-keyword">False</span>)
recall = Recall(average=<span class="hljs-keyword">False</span>)
F1_per_class = (precision * recall * <span class="hljs-number">2</span> / (precision + recall))
F1_mean = F1_per_class.mean()  
F1_mean.attach(engine, <span class="hljs-string">"F1"</span>)
</pre>



<h3>该库由两个主要模块组成:</h3>



<p><em> <strong>核心</strong> </em>模块包含像引擎、指标、一些必要的处理程序这样的基础。它把<strong> PyTorch作为唯一的附属国。</strong></p>


<div class="custom-point-list">
<ul><li><em> <strong> Contrib </strong> </em>模块可能依赖于其他库(如scikit-learn、tensorboardX、visdom、tqdm等)，并可能在版本之间有向后兼容性破坏更改。</li><li>单元测试涵盖了这两个模块。</li></ul>
</div>


<p><strong>扩展能力/研究中集成的简单性</strong></p>



<h3>我们相信，我们的事件/处理程序系统相当灵活，使人们能够与培训过程的每个部分进行交互。正因为如此，<strong>我们已经看到Ignite被用来训练GANs </strong>(我们提供了两个基本例子来训练<a href="https://web.archive.org/web/20220926085913/https://github.com/pytorch/ignite/tree/master/examples/gan" target="_blank" rel="noreferrer noopener nofollow"> DCGAN </a>和<a href="https://web.archive.org/web/20220926085913/https://github.com/pytorch/ignite/blob/master/examples/notebooks/CycleGAN.ipynb" target="_blank" rel="noreferrer noopener nofollow"> CycleGAN </a>)或<strong>强化学习模型。</strong></h3>



<p>根据Github的“被使用”，Ignite是被研究人员用于他们的论文的<strong>:</strong></p>



<p>BatchBALD:深度贝叶斯主动学习的高效多样批量获取，<a href="https://web.archive.org/web/20220926085913/https://github.com/BlackHC/BatchBALD" target="_blank" rel="noreferrer noopener nofollow"> github </a></p>


<div class="custom-point-list">
<ul><li>一个寻找可合成分子的模型，<a href="https://web.archive.org/web/20220926085913/https://github.com/john-bradshaw/molecule-chef" target="_blank" rel="noreferrer noopener nofollow"> github </a></li><li>本地化的生成流，<a href="https://web.archive.org/web/20220926085913/https://github.com/jrmcornish/lgf" target="_blank" rel="noreferrer noopener nofollow"> github </a></li><li>从生物医学文献中提取T细胞的功能和分化特征，<a href="https://web.archive.org/web/20220926085913/https://github.com/hammerlab/t-cell-relation-extraction" target="_blank" rel="noreferrer noopener nofollow"> github </a></li><li>由于这些(以及其他研究项目)，我们坚信<strong> Ignite为您提供了足够的灵活性来进行深度学习研究。</strong></li></ul>
</div>


<p><strong>与其他库/框架的集成</strong></p>



<h3>如果其他库或框架的特性不重叠的话，Ignite <strong>可以很好地与它们配合。我们拥有的一些很酷的集成包括:</strong></h3>



<p>用Ax ( <a href="https://web.archive.org/web/20220926085913/https://github.com/pytorch/ignite/blob/master/examples/notebooks/Cifar10_Ax_hyperparam_tuning.ipynb" target="_blank" rel="noreferrer noopener nofollow">点火示例</a>)调整超参数。</p>


<div class="custom-point-list">
<ul><li>使用Optuna进行超参数调整(<a href="https://web.archive.org/web/20220926085913/https://github.com/optuna/optuna/blob/master/examples/pytorch_ignite_simple.py" target="_blank" rel="noreferrer noopener nofollow"> Optuna示例</a>)。</li><li>登录TensorBoard，Visdom，MLflow，Polyaxon，Neptune (Ignite的代码)，Chainer UI (Chainer的代码)。</li><li>使用Nvidia Apex进行混合精度训练(<a href="https://web.archive.org/web/20220926085913/https://github.com/pytorch/ignite/tree/master/examples/references" target="_blank" rel="noreferrer noopener nofollow"> Ignite的例子</a>)。</li><li><strong>再现性</strong></li></ul>
</div>





<h3>我们为Ignite培训的可重复性付出了巨大努力:</h3>



<p>Ignite的<strong>引擎自动处理随机状态</strong>，并在可能的情况下强制数据加载器在不同的运行中提供相同的数据样本；</p>


<div class="custom-point-list">
<ul><li>Ignite <strong>集成了MLflow、Polyaxon、Neptune等实验跟踪系统</strong>。这有助于跟踪ML实验的软件、参数和数据依赖性；</li><li>我们提供了几个关于视觉任务的<strong>可重复训练的示例和<a href="https://web.archive.org/web/20220926085913/https://github.com/pytorch/ignite/tree/master/examples/references" target="_blank" rel="noreferrer noopener nofollow">【参考文献】</a>(灵感来自torchvision】(例如CIFAR10上的分类、ImageNet和Pascal VOC12上的分割)。</strong></li><li><strong>分布式培训</strong></li></ul>
</div>


<h3>Ignite也支持分布式培训<strong>，但是我们让用户来设置它的并行类型</strong>:模型或数据。</h3>



<p>例如，在数据分布式配置中，要求用户正确设置分布式过程组、包装模型、使用分布式采样器等。Ignite处理度量计算:减少所有进程的值。</p>



<p>我们<strong>提供了几个示例</strong>(例如<a href="https://web.archive.org/web/20220926085913/https://github.com/pytorch/ignite/tree/master/examples/contrib/cifar10#distributed-training" target="_blank" rel="noreferrer noopener nofollow">分布式CIFAR10 </a>)来展示如何在分布式配置中使用Ignite。</p>



<p><strong>人气</strong></p>



<h3>在撰写本文时，Ignite大约有<strong> 2.5k stars </strong>，根据Github的“用户”功能，有<strong>被205个存储库使用。</strong>一些荣誉奖包括:</h3>



<p>来自HuggingFace的Thomas Wolf也在他的一篇博客文章中为图书馆留下了一些令人敬畏的反馈(谢谢，Thomas！):</p>





<p>“使用令人敬畏的PyTorch ignite框架和NVIDIA的apex提供的自动混合精度(FP16/32)的新API，我们能够在不到<strong> 250行训练代码</strong>中提取+3k行比赛代码，并提供分布式和FP16选项！”</p>



<blockquote class="wp-block-quote is-style-default"><p>Max LapanThis是一本关于深度强化学习的书，其中第二版的例子是用Ignite编写的。</p></blockquote>


<div class="custom-point-list">
<ul><li><a href="https://web.archive.org/web/20220926085913/https://github.com/Project-MONAI/MONAI" target="_blank" rel="noreferrer noopener nofollow"> Project MONAI: </a>用于医疗保健成像的AI工具包。该项目主要关注医疗保健研究，旨在开发医学成像的DL模型，使用Ignite进行端到端培训。</li><li>关于其他用例，请看看<a href="https://web.archive.org/web/20220926085913/https://github.com/pytorch/ignite#they-use-ignite" target="_blank" rel="noreferrer noopener"> Ignite的github页面</a>和它的“使用者”。</li></ul>
</div>


<p><strong>何时使用Ignite </strong></p>



<h3>使用Ignite API的高度可定制模块，删除样板文件并标准化您的代码。</h3>


<div class="custom-point-list">
<ul><li>当您需要分解的代码，但不想牺牲灵活性来支持复杂的训练策略时</li><li>使用各种各样的实用工具，如度量、处理程序和记录器，轻松地评估/调试您的模型</li><li><strong>何时不使用Ignite </strong></li></ul>
</div>


<h3>当有一个超级自定义PyTorch代码，其中Ignite的API是开销。</h3>


<div class="custom-point-list">
<ul><li>当纯PyTorch API或另一个高级库完全满足时</li><li>感谢您的阅读！<strong> Pytorch-Ignite </strong>由<a href="https://web.archive.org/web/20220926085913/https://github.com/pytorch/ignite/graphs/contributors" target="_blank" rel="noreferrer noopener nofollow"> PyTorch社区</a>用爱献给您！</li></ul>
</div>


<p><strong>哲学</strong></p>









<a href="https://web.archive.org/web/20220926085913/https://www.linkedin.com/in/wfalcon/" class="author-box-another-format">
    
    
</a>



<h3>PyTorch Lightning 是PyTorch上的一个非常轻量级的包装器，它更像是一个<strong>编码标准，而不是一个框架</strong>。这种格式可以让你摆脱一大堆样板代码，同时<strong>保持易于理解</strong>。</h3>



<p>钩子的使用在训练的每个部分都是标准的，这意味着你可以忽略内部功能的任何部分，直到如何向后传球——这非常灵活。</p>



<p>结果是一个框架，<strong>给了研究人员、学生和制作团队终极的灵活性</strong>去尝试疯狂的想法，而不必学习另一个框架，同时自动化掉所有的工程细节。</p>



<p>Lightning还有另外两个更雄心勃勃的动机:<strong>深度学习社区中研究的可重复性和最佳实践的民主化</strong>。</p>



<p><strong>显著特征</strong></p>



<h3>在不改变你的代码的情况下，在CPU、GPU或TPUs上训练！</h3>


<div class="custom-point-list">
<ul><li>唯一支持TPU训练的库(训练者(数量_ TPU _核心=8))</li><li>琐碎的多节点训练</li><li>琐碎的多GPU训练</li><li>普通的16位精度支持</li><li>内置性能分析器(<code>Trainer(profile=True)</code>)</li><li>与tensorboard、comet.ml、neptune.ai等库的大量集成… ( <code>Trainer(logger=NeptuneLogger(...))</code>)</li><li><strong>团队</strong></li></ul>
</div>


<h3>Lightning有90多名贡献者和一个由8名贡献者组成的核心团队，他们确保项目以闪电般的速度向前推进。</h3>



<p><strong>文档</strong></p>



<h3><a href="https://web.archive.org/web/20220926085913/https://pytorch-lightning.readthedocs.io/en/latest/" target="_blank" rel="noreferrer noopener nofollow"> Lightning文档</a>非常全面，但简单易用。</h3>



<p><strong> API </strong></p>



<h3>在核心部分，Lightning有一个以两个对象<code>Trainer</code>和<code>LightningModule</code>T3<strong>为中心的API。</strong></h3>



<p>培训师抽象出所有的工程细节，照明模块捕获所有的科学/研究代码。这种分离使得研究代码更具可读性，并允许它在任意硬件上运行。</p>



<p><strong>照明模块</strong></p>







<h3>所有的<strong>研究逻辑</strong>进入<code>LightningModule</code>。</h3>



<p>例如，在癌症检测系统中，这部分将处理主要的事情，如对象检测模型、医学图像的数据加载器等。</p>



<p>它将构建深度学习系统所需的核心<strong>成分进行了分组</strong>:</p>



<p>计算(初始化，向前)。</p>


<div class="custom-point-list">
<ul><li>训练循环(training_step)中会发生什么。</li><li>验证循环中发生了什么(validation_step)。</li><li>测试循环中发生了什么(test_step)。</li><li>要使用的优化器(configure _ optimizers)。</li><li>要使用的数据(训练、测试、评估数据加载器)。</li><li>让我们看看文档中的例子，了解一下那里发生了什么。</li></ul>
</div>


<p>如你所见，LightningModule <strong>构建在纯PyTorch代码</strong>之上，并简单地将它们组织在<strong>的九个方法</strong>中:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> pytorch_lightning <span class="hljs-keyword">as</span> pl


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MNISTExample</span><span class="hljs-params">(pl.LightningModule)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(CoolSystem, self).__init__()
        
        self.l1 = torch.nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">10</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-keyword">return</span> torch.relu(self.l1(x.view(x.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">training_step</span><span class="hljs-params">(self, batch, batch_idx)</span>:</span>
        
        x, y = batch
        y_hat = self.forward(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {<span class="hljs-string">'train_loss'</span>: loss}
        <span class="hljs-keyword">return</span> {<span class="hljs-string">'loss'</span>: loss, <span class="hljs-string">'log'</span>: tensorboard_logs}

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validation_step</span><span class="hljs-params">(self, batch, batch_idx)</span>:</span>
        
        x, y = batch
        y_hat = self.forward(x)
        <span class="hljs-keyword">return</span> {<span class="hljs-string">'val_loss'</span>: F.cross_entropy(y_hat, y)}

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validation_end</span><span class="hljs-params">(self, outputs)</span>:</span>
        
        avg_loss = torch.stack([x[<span class="hljs-string">'val_loss'</span>]
                                <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> outputs]).mean()
        tensorboard_logs = {<span class="hljs-string">'val_loss'</span>: avg_loss}
        <span class="hljs-keyword">return</span> {<span class="hljs-string">'avg_val_loss'</span>: avg_loss, <span class="hljs-string">'log'</span>: tensorboard_logs}

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_step</span><span class="hljs-params">(self, batch, batch_idx)</span>:</span>
        
        x, y = batch
        y_hat = self.forward(x)
        <span class="hljs-keyword">return</span> {<span class="hljs-string">'test_loss'</span>: F.cross_entropy(y_hat, y)}

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_end</span><span class="hljs-params">(self, outputs)</span>:</span>
        
        avg_loss = torch.stack([x[<span class="hljs-string">'test_loss'</span>]
                                <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> outputs]).mean()
        tensorboard_logs = {<span class="hljs-string">'test_loss'</span>: avg_loss}
        <span class="hljs-keyword">return</span> {<span class="hljs-string">'avg_test_loss'</span>: avg_loss, <span class="hljs-string">'log'</span>: tensorboard_logs}

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">configure_optimizers</span><span class="hljs-params">(self)</span>:</span>
        
        
        
        
        
        <span class="hljs-keyword">return</span> torch.optim.Adam(self.parameters(), lr=<span class="hljs-number">0.02</span>)

<span class="hljs-meta">    @pl.data_loader</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_dataloader</span><span class="hljs-params">(self)</span>:</span>
        
        <span class="hljs-keyword">return</span> DataLoader(
            MNIST(os.getcwd(), train=<span class="hljs-keyword">True</span>, download=<span class="hljs-keyword">True</span>,
                  transform=transforms.ToTensor()), batch_size=<span class="hljs-number">32</span>)

<span class="hljs-meta">    @pl.data_loader</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">val_dataloader</span><span class="hljs-params">(self)</span>:</span>
        
        <span class="hljs-keyword">return</span> DataLoader(
            MNIST(os.getcwd(), train=<span class="hljs-keyword">True</span>, download=<span class="hljs-keyword">True</span>,
                  transform=transforms.ToTensor()), batch_size=<span class="hljs-number">32</span>)

<span class="hljs-meta">    @pl.data_loader</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_dataloader</span><span class="hljs-params">(self)</span>:</span>
        
        <span class="hljs-keyword">return</span> DataLoader(
            MNIST(os.getcwd(), train=<span class="hljs-keyword">False</span>, download=<span class="hljs-keyword">True</span>,
                  transform=transforms.ToTensor()), batch_size=<span class="hljs-number">32</span>)
</pre>



<p><em> <strong> __init__(): </strong> </em>定义本模型或多个模型，并初始化权重</p>


<div class="custom-point-list">
<ul><li><em> <strong> forward(): </strong> </em>您可以将它视为标准的PyTorch forward方法，但它具有额外的灵活性，可以在预测/推理级别定义您希望发生的事情。</li><li><strong> <em> training_step(): </em> </strong>定义了训练循环中发生的事情。它结合了一个向前传球，损失计算，以及任何其他你想在训练中执行的逻辑。</li><li><strong><em>validation _ step()</em>:</strong>定义了验证循环中发生的事情。例如，您可以计算每个批次的损失或准确度，并将它们存储在日志中。</li><li><strong><em>validation _ end()</em>:</strong>验证循环结束后，你希望发生的一切。例如，您可能想要计算认证批次的平均损失或准确度</li><li><strong> <em> test_step() </em> : </strong>你希望每一批在推断时发生什么。你可以把你的测试时间增强逻辑或者其他东西放在这里。</li><li><strong><em>【test _ end()</em>:</strong>与validation_end类似，您可以使用它来聚合test_step期间计算的批处理结果</li><li><strong><em>configure _ optimizer()</em>:</strong>初始化一个或多个优化器</li><li><strong><em>train/val/test _ data loader()</em>:</strong>返回训练、验证和测试集的PyTorch数据加载器。</li><li>因为每个PytorchLightning系统都需要实现这些方法，所以很容易看到研究中到底发生了什么。</li></ul>
</div>


<p>比如要了解一篇论文在做什么，你要做的就是看<code>LightningModule</code>的<code>training_step</code>！</p>



<p>这种可读性以及核心研究概念和实现之间的紧密映射是Lightning的核心。</p>



<blockquote class="wp-block-quote is-style-default"><p><strong>教练</strong></p></blockquote>



<h3>这是深度学习的工程部分发生的地方。</h3>



<p>在癌症检测系统中，这可能意味着你使用多少GPU，当你停止训练时你何时保存检查点，等等。这些细节构成了许多研究的“秘方”，这些研究是深度学习项目的标准最佳实践(即:与癌症检测没有太大的相关性)。</p>



<p>请注意，<code>LightningModule</code>没有任何关于GPU或16位精度或早期停止或日志记录之类的东西。<strong>所有这些都由教练自动处理。</strong></p>



<p>这就是训练这个模型的全部！培训师为您处理所有事情，包括:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> pytorch_lightning <span class="hljs-keyword">import</span> Trainer

model = MNISTExample()


trainer = Trainer()    
trainer.fit(model)
</pre>



<p>提前停止</p>


<div class="custom-point-list">
<ul><li>自动记录到Tensorboard(或comet、mlflow、neptune等)</li><li>自动检查点</li><li>更多(我们将在接下来的章节中讨论)</li><li>所有这些都是免费的！</li></ul>
</div>


<p><strong>学习曲线</strong></p>



<h3>因为<code>LightningModule</code>只是简单地重组纯Pytorch对象，一切都是“公开的”,所以<strong>将PyTorch代码重构为Lightning格式是微不足道的。</strong></h3>



<p>更多关于从纯PyTorch到Lightning转换的信息，请阅读<a href="https://web.archive.org/web/20220926085913/https://towardsdatascience.com/how-to-refactor-your-pytorch-code-to-get-these-42-benefits-of-pytorch-lighting-6fdd0dc97538" target="_blank" rel="noreferrer noopener nofollow">这篇文章</a>。</p>



<p><strong>内置特性(开箱即用)</strong></p>



<h3>Lightning提供了大量现成的高级功能。例如，一行程序可以使用以下内容:</h3>



<p>随时间截断反向传播</p>





<pre class="hljs">Trainer(gpus=<span class="hljs-number">8</span>)
</pre>





<pre class="hljs">Trainer(num_tpu_cores=<span class="hljs-number">8</span>)
</pre>





<pre class="hljs">Trainer(gpus=<span class="hljs-number">8</span>, num_nodes=<span class="hljs-number">8</span>, distributed_backend=’ddp’)
</pre>





<pre class="hljs">Trainer(gradient_clip_val=<span class="hljs-number">2.0</span>)
</pre>





<pre class="hljs">Trainer(accumulate_grad_batches=<span class="hljs-number">12</span>)
</pre>





<pre class="hljs">Trainer(use_amp=<span class="hljs-keyword">True</span>)
</pre>


<div class="custom-point-list">
<ul><li>如果你想看完整的免费魔法特性列表，请点击这里。</li></ul>
</div>


<pre class="hljs">Trainer(truncated_bptt_steps=<span class="hljs-number">3</span>)
</pre>





<p><strong>扩展能力/研究中集成的简单性</strong></p>



<h3>拥有大量内置功能固然很好，但对于研究人员来说，重要的是不必学习另一个库，直接控制研究的关键部分，如数据处理，而无需其他抽象操作。</h3>



<p>这种灵活的形式为培训和验证提供了最大的自由度。这个接口应该被认为是一个系统，而不是一个模型。该系统可能有多个模型(GANs、seq-2-seq等)或只有一个模型，如这个简单的MNIST例子。</p>



<p>因此，研究人员可以自由地尝试他们想做的疯狂的事情，，并且只需要担心结果。</p>



<p>但是也许你需要更多的灵活性。在这种情况下，您可以这样做:</p>



<p>更改后退步骤的完成方式。</p>


<div class="custom-point-list">
<ul><li>更改16位的初始化方式。</li><li>添加您自己的分布式培训方式。</li><li>添加学习率计划程序。</li><li>使用多个优化器。</li><li>更改优化器更新的频率。</li><li>还有很多很多东西。</li><li>在引擎盖下，Lightning中的所有东西都被实现为可以被用户覆盖的钩子。这使得培训的每个方面都具有高度的可配置性，而这正是研究或生产团队所需要的灵活性。</li></ul>
</div>


<p>但是等等，你会说…这对于你的用例来说太简单了？别担心，闪电是我在NYU和脸书人工智能研究所攻读博士学位时设计的，对研究人员来说尽可能灵活。</p>



<p>以下是一些例子:</p>



<p>需要<strong>自己的后传球</strong>？覆盖此挂钩:</p>


<div class="custom-point-list">
<ul><li>需要<strong>自己的放大器初始化</strong>？覆盖此挂钩:</li></ul>
</div>


<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, use_amp, loss, optimizer)</span>:</span>
    <span class="hljs-keyword">if</span> use_amp:
        <span class="hljs-keyword">with</span> amp.scale_loss(loss, optimizer) <span class="hljs-keyword">as</span> scaled_loss:
            scaled_loss.backward()
    <span class="hljs-keyword">else</span>:
        loss.backward()</pre>


<div class="custom-point-list">
<ul><li>想要深入到添加<strong>您自己的DDP实现</strong>？覆盖这两个挂钩:</li></ul>
</div>


<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">configure_apex</span><span class="hljs-params">(self, amp, model, optimizers, amp_level)</span>:</span>
    model, optimizers = amp.initialize(
        model, optimizers, opt_level=amp_level,
    )

    <span class="hljs-keyword">return</span> model, optimizers</pre>



<p>像这样的钩子有10个，我们会根据研究人员的要求增加更多。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">configure_ddp</span><span class="hljs-params">(self, model, device_ids)</span>:</span>
    
    model = LightningDistributedDataParallel(
        model,
        device_ids=device_ids,
        find_unused_parameters=<span class="hljs-keyword">True</span>
    )
    <span class="hljs-keyword">return</span> model

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_ddp_connection</span><span class="hljs-params">(self)</span>:</span>
    
    
    <span class="hljs-keyword">try</span>:
        
        default_port = os.environ[<span class="hljs-string">'SLURM_JOB_ID'</span>]
        default_port = default_port[<span class="hljs-number">-4</span>:]

        
        default_port = int(default_port) + <span class="hljs-number">15000</span>

    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        default_port = <span class="hljs-number">12910</span>

    
    <span class="hljs-keyword">try</span>:
        default_port = os.environ[<span class="hljs-string">'MASTER_PORT'</span>]
    <span class="hljs-keyword">except</span> Exception:
        os.environ[<span class="hljs-string">'MASTER_PORT'</span>] = str(default_port)

    
    <span class="hljs-keyword">try</span>:
        root_node = os.environ[<span class="hljs-string">'SLURM_NODELIST'</span>].split(<span class="hljs-string">' '</span>)[<span class="hljs-number">0</span>]
    <span class="hljs-keyword">except</span> Exception:
        root_node = <span class="hljs-string">'127.0.0.2'</span>

    root_node = self.trainer.resolve_root_node_address(root_node)
    os.environ[<span class="hljs-string">'MASTER_ADDR'</span>] = root_node
    dist.init_process_group(
        <span class="hljs-string">'nccl'</span>,
        rank=self.proc_rank,
        world_size=self.world_size
    )</pre>



<p>底线是，<strong> Lightning对于新用户来说使用起来很简单，如果你是一名研究人员或从事前沿人工智能研究的生产团队</strong>,它可以无限扩展。</p>



<p><strong>可读性和走向再现性</strong></p>



<h3>正如我提到的，Lightning的创建还有第二个更大的动机:可复制性。虽然真正的再现性需要标准代码、标准种子、标准硬件等，但Lightning以两种方式为可再现性研究做出了贡献:</h3>



<p>为了<strong>标准化ML代码</strong>的格式，</p>


<div class="custom-point-list">
<ul><li><strong>将工程与科学分离</strong>以便该方法可以在不同的系统中进行测试。</li><li>结果是一个用于研究的表达性强、功能强大的API。</li></ul>
</div>


<p>如果每个研究项目和论文都是使用LightningModule模板实现的，那么就很容易发现发生了什么(但是可能不容易理解哈哈)</p>



<blockquote class="wp-block-quote is-style-default"><p><strong>分布式培训</strong></p></blockquote>



<h3>闪电<strong>让多GPU甚至多GPU多节点训练变得琐碎。</strong></h3>



<p>例如，如果您想在多个GPU上训练上述示例，只需向训练器添加以下标志:</p>



<p>使用上述标志将在4个GPU上运行该模型。如果您想在16个GPU上运行，其中有4台机器，每台机器有4个GPU，请将教练标志更改为:</p>



<pre class="hljs">trainer = Trainer(gpus=<span class="hljs-number">4</span>, distributed_backend=<span class="hljs-string">'dp'</span>)    
trainer.fit(model)
</pre>



<p>并提交以下SLURM作业:</p>



<pre class="hljs">trainer = Trainer(gpus=<span class="hljs-number">4</span>, nb_gpu_nodes=<span class="hljs-number">4</span>, distributed_backend=<span class="hljs-string">'ddp'</span>)    
trainer.fit(model)
</pre>



<p>考虑到引擎盖下发生了多少事情，这简直太简单了。</p>



<pre class="hljs">









<span class="hljs-built_in">source</span> activate <span class="hljs-variable">$1</span>



 <span class="hljs-built_in">export</span> NCCL_DEBUG=INFO
 <span class="hljs-built_in">export</span> PYTHONFAULTHANDLER=1










srun python3 mnist_example.py
</pre>



<p>有关Pytorch lightning分布式训练的更多信息，请阅读这篇关于<a href="https://web.archive.org/web/20220926085913/https://towardsdatascience.com/how-to-train-a-gan-on-128-gpus-using-pytorch-9a5b27a52c73" target="_blank" rel="noreferrer noopener nofollow">“如何使用Pytorch在128 GPUs上训练GAN”的文章。</a></p>



<p>生产化</p>



<h3>Lightning模型很容易部署，因为它们仍然是简单的PyTorch模型。这意味着我们可以利用PyTorch社区在支持部署方面的所有工程进展。</h3>



<p><strong>人气</strong></p>



<h3>Pytorch Lightning在Github上拥有超过3800颗星星，最近的下载量达到了110万次。更重要的是，该社区正在快速发展，有超过90名贡献者，许多来自世界顶级人工智能实验室，每天都在添加新功能。你可以在<a href="https://web.archive.org/web/20220926085913/https://github.com/PyTorchLightning/pytorch-lightning/issues" target="_blank" rel="noreferrer noopener nofollow"> Github </a>或者<a href="https://web.archive.org/web/20220926085913/https://pytorch-lightning.slack.com/join/shared_invite/zt-f6bl2l0l-JYMK3tbAgAmGRrlNr00f1A#/" target="_blank" rel="noreferrer noopener nofollow"> Slack </a>上和我们交流。</h3>



<p><strong>何时使用PyTorch闪电</strong></p>



<h3>Lightning是为从事尖端研究的专业研究人员和生产团队而制造的。当你知道你需要做什么的时候，那是很棒的。这种关注意味着它为那些希望快速测试/构建东西而不陷入细节的人增加了高级特性。</h3>


<div class="custom-point-list">
<ul><li><strong>何时不使用PyTorch闪电</strong></li></ul>
</div>


<h3>虽然lightning是为专业研究人员和数据科学家设计的，但新来者仍然可以从中受益。对于新来者，我们建议他们使用纯PyTorch从头构建一个简单的MNIST系统。这将向他们展示如何建立训练循环等。一旦他们明白这是如何工作的，以及向前/向后传球是如何工作的，他们就可以进入闪电状态。</h3>


<div class="custom-point-list">
<ul><li>             </li></ul>
</div>











<a href="https://web.archive.org/web/20220926085913/https://www.linkedin.com/in/ethanwharris/" class="author-box-another-format">
    
    
</a>



<a href="https://web.archive.org/web/20220926085913/https://github.com/MattPainter01" class="author-box-another-format">
    
    <p class="right">我们的博客部分将与其他部分略有不同，因为<strong> <a href="https://web.archive.org/web/20220926085913/https://github.com/pytorchbearer/torchbearer" target="_blank" rel="noreferrer noopener nofollow">火炬手</a>即将结束</strong>(有点)。特别是，<strong>我们加入了PyTorch-Lightning </strong>团队。这一举动源于在NeurIPS 2019上与威廉·法尔肯的一次会面，并于最近在PyTorch博客上宣布。</p>
</a>



<p>因此，我们认为我们应该写我们做得好的地方，我们做错的地方，以及我们为什么要转向闪电，而不是试图向你推销火炬手。</p>



<p><strong>我们做得好的地方</strong></p>



<h3>lib变得非常受欢迎，在GitHub 上获得了500多颗星，这远远超出了我们的想象。</h3>


<div class="custom-point-list">
<ul><li>我们成为PyTorch生态系统的一部分。对我们来说，这是一次重要的经历，让我们觉得自己是更广泛的社区中有价值的一部分。</li><li>我们已经建立了一套全面的内置回调和指标。这是我们的主要成功之一；使用torchbearer，一行代码就可以实现许多强大的成果。</li><li>火炬手的一个重要特点是<strong>使极端的灵活性</strong>是<em>状态</em>对象。这是一个可变字典，包含核心训练循环使用的所有变量。通过在循环中不同点的回调中编辑这些变量，可以实现最复杂的结果。</li><li>火炬手拥有<strong>良好的文件</strong>对我们来说一直很重要。我们关注的是可以在你的浏览器中用Google Colab执行的示例文档。示例库非常成功，它提供了关于torchbearer更强大的用例的快速信息。</li><li>最后要注意的是，在过去的两年里，我们俩都在用火炬手进行我们的博士研究。我们认为这是一个成功，因为我们几乎<strong>从来不需要为了原型化我们的想法而改变火炬手API </strong>，即使是那些荒谬的想法！</li><li><strong>我们做错了什么</strong></li></ul>
</div>


<h3>使这个库如此灵活的<em>状态</em>对象也有问题。从任何其他地方访问库的任何部分的能力会像全局变量一样导致滥用。特别是，<strong>一旦不止一个对象作用于它，确定状态对象中的特定变量是如何以及何时被改变的是具有挑战性的</strong>。此外，为了使状态有效，你需要知道每个变量是什么，以及在哪个回调中可以访问它，所以<strong>学习曲线很陡。</strong></h3>


<div class="custom-point-list">
<ul><li>火炬手本质上不适合分布式训练，甚至在某种程度上不适合低精度训练。既然状态的每一部分在任何时候都是可用的，那么如何将它分块并在设备间分配呢？PyTorch可以以某种方式处理这个问题，因为火炬手可以在分发时使用，但目前还不清楚state在这些时候会发生什么。</li><li><strong>改变核心训练循环并非易事</strong>。Torchbearer提供了一种完全编写自己的核心循环的方法，但是您必须手动编写回调点，以确保所有内置的Torchbearer功能。与库的其他方面相比，再加上较低的文档标准，定制循环过于复杂，大多数用户可能完全不知道。</li><li>在攻读博士的同时管理一个开源项目变得比预期的更加困难。结果，该库的一些部分经过了彻底的测试并且是稳定的(因为它们对我们的博士工作很重要)，而其他部分则开发不足并且充满错误。</li><li>在我们最初的成长过程中，我们决定大幅改变核心API 。这极大地改进了火炬手，但也意味着从一个版本到下一个版本需要大量的努力。这感觉是合理的，因为我们仍然是1.0.0之前的稳定版本，但它肯定会促使一些用户选择其他库。</li><li><strong>为什么我们要加入PyTorch Lightning？</strong></li></ul>
</div>


<h3>我们愿意迁移到Lightning的第一个关键原因是它的受欢迎程度。有了Lightning，我们<strong>成为发展最快的PyTorch培训库</strong>的一部分，这已经让它的许多竞争对手黯然失色。</h3>


<div class="custom-point-list">
<ul><li>我们此举的第二个关键原因，也是Lightning成功的一个关键部分，是<strong>它是从头开始构建的，以支持分布式训练和低精度</strong>，这两者在火炬手中实现都具有挑战性。在Lightning开发的早期阶段做出的这些实际考虑对现代深度学习实践者来说是非常宝贵的，而<strong>在《火炬手》中进行改造将是一个挑战。</strong></li><li>此外，在Lightning <strong>我们将成为更大的核心开发团队的一部分。</strong>这将使我们能够确保更高的稳定性，支持更广泛的用例，而不是像现在这样只有两个开发人员。</li><li>最终，我们始终相信，推动事情向前发展的最佳方式是与另一个图书馆合作。这是我们实现这一目标并帮助闪电成为PyTorch最好的培训库的机会。</li></ul>
</div>


<p>可能有用</p>





<div id="blog-cta-intext-block_61768ec495b46" class="blog-cta-intext">
  <h3 class="blog-cta-intext__title">查看如何使用Neptune跟踪模型训练，这要归功于与以下软件的集成:<br/>➡️<a href="https://web.archive.org/web/20220926085913/https://docs.neptune.ai/integrations-and-supported-tools/model-training/catalyst" target="_blank" rel="noreferrer noopener">catalyst</a>T3】➡️<a href="https://web.archive.org/web/20220926085913/https://docs.neptune.ai/integrations-and-supported-tools/model-training/fastai" target="_blank" rel="noreferrer noopener">fastai<br/></a>➡️<a href="https://web.archive.org/web/20220926085913/https://docs.neptune.ai/integrations-and-supported-tools/model-training/pytorch-ignite" target="_blank" rel="noreferrer noopener">py torch ignite<br/></a>➡️<a href="https://web.archive.org/web/20220926085913/https://docs.neptune.ai/integrations-and-supported-tools/model-training/pytorch-lightning" target="_blank" rel="noreferrer noopener">py torch lightning</a><br/>➡️<a href="https://web.archive.org/web/20220926085913/https://docs.neptune.ai/integrations-and-supported-tools/model-training/skorch" target="_blank" rel="noreferrer noopener">sko rch</a></h3>
  <div class="blog-cta-intext__content"><p>(主观)比较和最终想法</p>
</div>
  </div>


<h2 id="7">雅各布·查肯</h2>




<div id="author-box-new-format-block_605d8dd517661" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">大部分是ML的人。构建MLOps工具，编写技术资料，在Neptune进行想法实验。</h3>
    
          <p class="article__authorContent-text">在这一点上，我想给一个…</p>
    
          
    
  </div>
</div>


<p>非常感谢所有作者！</p>



<blockquote class="wp-block-quote has-text-align-center is-style-default"><p>哇，这是很多第一手资料，我希望它能让你更容易选择适合你的图书馆。</p></blockquote>



<p>当我和他们一起写这篇文章并仔细观察他们的库所提供的东西(并创建一些拉请求)时，我得到了自己的观点，我想在这里和你们分享。</p>



<p><a href="https://web.archive.org/web/20220926085913/https://github.com/skorch-dev/skorch" target="_blank" rel="noreferrer noopener nofollow"> <em> <strong>斯科奇</strong></em>T5】</a></p>



<p>如果你想要类似sklearn的API，那么<strong> Skorch </strong>就是你的lib。它经过了很好的测试和记录。实际上<strong>给出了比我在撰写这篇文章之前所预期的</strong>更多的灵活性，这是一个很好的惊喜。也就是说，这个图书馆的<strong>重点不是尖端研究，而是生产应用。</strong>我觉得它真的兑现了他们的承诺，并且完全符合设计初衷。我真的很尊重这样的工具/库。</p>



<p><a href="https://web.archive.org/web/20220926085913/https://docs.fast.ai/" target="_blank" rel="noreferrer noopener nofollow"><strong><em/></strong></a></p>



<p>长期以来，Fastai<strong>一直是人们进入深度学习的绝佳选择。</strong>它可以用10行近乎神奇的代码为您提供最先进的结果。但是库还有<strong>的另一面，也许不太为人所知，它让你访问<strong>低级API</strong>并创建自定义构建块<strong>给研究人员和从业者实现非常复杂的系统的灵活性。也许是超级受欢迎的fastai深度学习课程在我的脑海中创造了这个库的错误形象，但我肯定会在未来使用它，特别是最近的v2预发布。</strong></strong></p>



<p><em> <strong> <a href="https://web.archive.org/web/20220926085913/https://pytorch.org/ignite/" target="_blank" rel="noreferrer noopener nofollow"> Pytorch点燃</a> </strong> </em></p>



<p><strong>点燃</strong>是一种有趣的动物。有了它，<strong>有点异国情调</strong>(对我个人来说)，<strong>引擎，事件和处理程序API </strong>，你可以<strong>做任何你想做的事情。它有大量开箱即用的功能，我完全理解为什么许多研究人员在日常工作中使用它。我花了一点时间来熟悉这个框架，但是你只需要停止用“回调术语”来思考，你就没事了。也就是说，API对我来说不像其他一些库那样清晰。不过你应该去看看，因为这对你来说可能是个不错的选择。</strong></p>



<p><a href="https://web.archive.org/web/20220926085913/https://catalyst-team.github.io/catalyst/" target="_blank" rel="noreferrer noopener nofollow"> <strong> <em>催化剂</em> </strong> </a></p>



<p>在研究<strong> Catalyst </strong>之前，我认为它是一个创建深度学习管道的沉重(有点)框架。现在我的看法完全不同了。<strong>它以一种美丽的方式将工程材料与研究分离开来</strong>。纯PyTorch对象进入处理训练的训练器。它非常灵活，有一个单独的模块来处理强化学习。它也<strong>给你很多现成的功能，当谈到可重复性，并在生产中服务模型。</strong>还有我跟你说过的那些多级管道？您可以用最少的开销轻松创建它们。总的来说，我认为这是一个很棒的项目，很多人可以从使用它中受益。</p>



<p><a href="https://web.archive.org/web/20220926085913/https://github.com/PyTorchLightning/pytorch-lightning" target="_blank" rel="noreferrer noopener nofollow"><em><strong>py torch</strong></em>T5】</a></p>



<p>闪电也想把科学和工程分开，我认为它在这方面做得很好。有大量的内置功能使它更具吸引力。但是这个库有一点不同的是，它通过使深度学习研究实现可读来实现<strong>的可重复性。</strong>遵循LightningModule内部的逻辑真的很容易，其中培训步骤(以及其他内容)没有被抽象掉。我认为以这种方式交流研究项目会非常有效。<strong>它很快变得非常受欢迎</strong>，随着<strong>火炬手的作者加入核心开发团队</strong>，我认为<strong>这个项目有一个光明的未来</strong>在它前面，甚至是闪电一样的光明🙂</p>



<p>那么你应该选择哪一个呢？像往常一样，这要视情况而定，但我认为你现在有足够的信息来做出一个好的决定！</p>



<p><strong>阅读下一篇</strong></p>



<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<p class="has-text-color">如何使用海王星跟踪PyTorch中的实验</p>



<h2>4分钟阅读| Aayush Bajaj |发布于2021年1月19日</h2>



<p class="has-small-font-size">4 mins read | Aayush Bajaj | Posted January 19, 2021</p>


<p id="block_5ffc75def9f8e" class="separator separator-10">机器学习开发看起来很像传统的软件开发，因为它们都需要我们编写大量的代码。但其实不是！让我们通过一些要点来更好地理解这一点。</p>



<p>机器学习代码不会抛出错误(当然我说的是语义)，原因是，即使你在神经网络中配置了错误的方程，它仍然会运行，但会与你的预期混淆。用<a rel="noreferrer noopener" href="https://web.archive.org/web/20220926085913/https://karpathy.github.io/2019/04/25/recipe/" target="_blank">安德烈·卡帕西</a>、<em>的话说，“神经网络无声无息地失败了”。</em></p>


<div class="custom-point-list">
<ul><li>机器学习代码/项目严重依赖结果的可重复性。这意味着，如果一个超参数被推动，或者训练数据发生变化，那么它会在许多方面影响模型的性能。这意味着你必须记下超参数和训练数据的每一个变化，以便能够重现你的工作。当网络很小时，这可以在一个文本文件中完成，但是如果是一个有几十或几百个超参数的大项目呢？文本文件现在不那么容易了吧！</li><li>机器学习项目复杂性的增加意味着复杂分支的增加，必须对其进行跟踪和存储以供将来分析。</li><li>机器学习也需要大量的计算，这是有代价的。你肯定不希望你的云成本暴涨。</li><li>有组织地跟踪实验有助于解决所有这些核心问题。海王星是一个完整的工具，可以帮助个人和团队顺利跟踪他们的实验。它提供了许多功能和演示选项，有助于更轻松地跟踪和协作。</li></ul>
</div>


<p>Tracking experiments in an organized way helps with all of these core issues. Neptune is a complete tool that helps individuals and teams to track their experiments smoothly. It presents a host of features and presentation options that helps in tracking and collaboration easier.</p>


<a class="button continous-post blue-filled" href="/web/20220926085913/https://neptune.ai/blog/how-to-keep-track-of-experiments-in-pytorch-using-neptune" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>