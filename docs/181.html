<html>
<head>
<title>Early Stopping With Neptune </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>海王星提前停止</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/early-stopping-with-neptune#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/early-stopping-with-neptune#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>最近对20，036名担任数据科学家的Kaggle成员进行的调查(发表在2020年机器学习和数据科学报告中)显示，大多数现实世界的问题都可以使用流行的机器学习算法轻松解决，如线性或逻辑回归，决策树或随机森林。但是，数据科学界发展最快的研究领域是深度学习——主要是神经网络[1]。</p>



<p>深度学习是机器学习的一个新兴领域。最近，它在研究人员和工程师中很受欢迎。深度学习在许多工业应用中表现出了一流的性能——自动驾驶、航空航天和国防、医学研究、工业自动化和电子。</p>



<p>深度学习实际上是人工智能子集的子集。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/1bc470d07c5c9f19fcbe4e0d4a6742ee.png" alt="AI, machine learning, and deep learning" class="wp-image-43856" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/AI-machine-learning-and-deep-learning.jpeg?ssl=1"/><figcaption><em>Figure 1: AI, machine learning, and deep learning</em></figcaption></figure></div>



<p>在过去的几年里，深度神经网络已经在计算机视觉、语音识别、自然语言处理、图像分类和对象检测等广泛的领域中显示出巨大的成功。</p>



<p>研究表明，在2006年之前，深度神经网络没有被成功训练。从那以后，已经实现了几种算法来提高神经网络的性能。</p>



<p>与浅层网络相比，深层神经网络更难训练。在本文中，我们的主要目标是强调训练神经网络中一个价值百万美元的问题:<strong>训练一个神经网络需要多长时间</strong>？</p>



<p>神经网络训练时间会导致训练和测试数据的欠拟合(过短)或过拟合(过长)。过度拟合是监督机器学习中的一个非常基本的问题，它阻止我们概括模型的性能，因为模型在训练集的观察数据上拟合得很好，但在测试集上表现很差。</p>



<h2 id="h-underfitting-vs-overfitting">欠拟合与过拟合</h2>



<p>一般来说，欠拟合和过拟合只不过是方差和偏差之间的权衡。</p>



<p>在统计学和机器学习中，偏差-方差权衡是模型的一个非常重要的性质。偏差是模型的平均预测值和我们试图预测的真实值之间的误差。高偏差会导致模型错过数据中的重要信息，并使模型过于简化(拟合不足)。</p>



<p>方差是对训练集的小波动的敏感性的误差。在具有高方差的模型中，训练集中的微小变化会导致精确度发生非常大的变化。该模型可以在观察到的数据上表现良好，而在以前没有观察到的数据上表现不佳(过拟合)。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/f9254a2ac7babf73610965e37de92d32.png" alt="bias variance tradeoff" class="wp-image-43858" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/bias-variance-tradeoff.png?ssl=1"/><figcaption><em>Figure 2: Bias – Variance Tradeoff [2]</em></figcaption></figure></div>



<h3>欠拟合</h3>



<p>无法捕捉数据潜在趋势的机器学习模型通常被定义为拟合不足。当模型拟合不足时，它不能很好地拟合数据，因此它可能会错过数据中的重要信息。</p>



<p>当模型与可用数据相比非常简单时，通常会出现拟合不足。可用数据较少，数据中有噪声，或者当我们试图用非线性数据建立线性模型时[3]。在欠拟合期间，模型不仅在测试数据上表现不佳，甚至在对训练数据集进行训练期间也表现不佳。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/e14e9b45f6076fb304cfdc8aef937e13.png" alt="underfitting" class="wp-image-43859" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/underfitting.png?ssl=1"/><figcaption><em>Figure 3: Underfitting [3]</em></figcaption></figure></div>



<p>欠拟合数据的模型往往具有:</p>



<ol><li>低方差和高偏差，</li><li>更少的特征(例如x)。</li></ol>



<p>减少欠拟合的技术:</p>



<ol><li>使模型更加复杂，</li><li>执行特征工程，</li><li>增加训练时间或历元数。</li></ol>



<h3>过度拟合</h3>



<p>当ML模型被允许训练更长的时间时，它将不仅从可用数据开始学习，而且从数据中的噪声开始学习，那么机器学习模型是过度拟合的。因此，由于存在噪声、训练集的有限大小以及分类器的复杂性，会发生过拟合[2]。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/37552a7dc235b5ad7cbbce6b116e9adb.png" alt="overfitting" class="wp-image-43860" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/overfitting-1.png?ssl=1"/><figcaption><em>Figure 4: Overfitting [3]</em></figcaption></figure></div>



<p>过度拟合的模型往往具有:</p>



<ol><li>高方差和低偏差。</li><li>高特征(例如x，x <sup> 2 </sup>，x <sup> 3 </sup>，x <sup> 4 </sup>，…)</li></ol>



<p>减少过度拟合的技术:</p>



<ol><li>增加训练集上的观察数据，</li><li>让模型更简单，</li><li>提前停车，</li><li>使用“网络缩减”策略减少训练集中的噪声，</li><li>数据增强，</li><li>正规化-L1，L2，辍学。</li></ol>



<p>捕捉数据总体趋势的模型不会趋向于欠拟合或过拟合。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/1c640848e957bec11f2e0dc0ff1b4f3d.png" alt="perfect fit" class="wp-image-43861" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/perfect-fit.png?ssl=1"/><figcaption><em>Figure 5: Perfect fit [3]</em></figcaption></figure></div>



<p>非常符合数据的模型往往具有:</p>



<ol><li>低方差和低偏差，</li><li>合理数量的特征，</li><li>即使在未观察到的数据上也表现很好。</li></ol>







<h2 id="h-early-stopping-in-neural-networks">神经网络中的早期停止</h2>



<p>训练神经网络的主要挑战之一是训练神经网络的时间长度。为有限数量的时期训练模型会导致拟合不足，而为大量时期训练模型会导致拟合过度。监控训练过程，在适当的时候停止训练是非常重要的。</p>



<p>在本文中，我们将发现在神经网络过度拟合之前尽早停止神经网络的训练，实际上可以减少过度拟合，并导致更好的模型性能——在训练和测试数据集上都是如此。</p>



<p>在训练神经网络时，在某个点上，模型将停止归纳并开始学习数据中的噪声，这使得模型在未观察到的数据上表现不佳。为了克服这个问题，我们将使用历元数作为超参数之一，并监控训练和测试损失。</p>



<p>训练神经网络时，我们将在每个时期后输入测试数据集，并监控测试损失。如果模型性能下降(测试损失开始增加)，或者测试精度降低，训练过程将停止。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/c48d5f7005fff474b0e6f9dbb8ce60ae.png" alt="early stopping" class="wp-image-43863" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/early-stopping-1.png?ssl=1"/><figcaption><em>Figure 6: Early stopping [4]</em></figcaption></figure></div>



<h2 id="h-monitoring-the-experiment-using-neptune">用海王星来监控实验</h2>



<p>使用Neptune可以轻松地监控训练过程和其他实验。这是一个非常棒的工具，可以帮助研究人员和机器学习工程师监控和组织他们的项目，与队友分享结果，并通过使用单一平台来改善团队合作。</p>



<hr class="wp-block-separator"/>



<p class="c-box">请注意，由于最近的<a href="/web/20221207132254/https://neptune.ai/blog/neptune-new" target="_blank" rel="noreferrer noopener"> API更新</a>，这篇文章也需要一些改变——我们正在努力！与此同时，请检查<a href="https://web.archive.org/web/20221207132254/https://docs.neptune.ai/" target="_blank" rel="noreferrer noopener">海王星文档</a>，那里的一切都是最新的！</p>



<hr class="wp-block-separator"/>



<p>在海王星设置实验的过程非常简单。第一步是注册一个帐户，这将为您的实验创建一个唯一的id和一个仪表板。</p>







<p>您将使用唯一的令牌ID将任何深度学习框架(如TensorFlow或PyTorch)链接到Neptune。在我们的实验中，我们将使用<strong> PyTorch </strong>。注册并按照以下步骤创建一个唯一的id。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/aa1dd094c0e6e1ca05f4ed4cf1bc9f82.png" alt="Neptune unique-id" class="wp-image-43865" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Neptune-unique-id.gif?ssl=1"/><figcaption><em>Figure 7: Neptune unique id [5]</em></figcaption></figure></div>



<p>为了使用Python直接从Pytorch访问Neptune仪表板，Neptune.ai的开发人员开发了一个易于访问的工具箱，可以使用pip安装:</p>



<pre class="hljs">pip install neptune-client
</pre>



<p>安装完成后，可以通过将Neptune导入到主Python脚本来轻松完成实验初始化:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune

NEPTUNE_API_TOKEN=<span class="hljs-string">"&lt;api-token-here&gt;"</span>
neptune.init(<span class="hljs-string">'&lt;username&gt;/sandbox'</span>,api_token=NEPTUNE_API_TOKEN)
neptune.create_experiment(<span class="hljs-string">'Pytorch-Neptune-Example'</span>)
</pre>



<section id="blog-intext-cta-block_607458a832c5c" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>海王星与<a href="https://web.archive.org/web/20221207132254/https://docs.neptune.ai/essentials/integrations/deep-learning-frameworks/pytorch" target="_blank" rel="noopener"> PyTorch </a>的融合</p>
    
    </section>



<h3>实验装置</h3>



<p>在我们的实验中，我们将使用CIFAR10影像分类数据集。CIFAR-10数据集由10类60000幅32×32彩色图像组成，每类6000幅图像。有50，000个训练图像和10，000个测试图像。</p>



<p>以下是数据集中的类，以及每个类中的10幅随机图像:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/970c0cd88802de52f7f250cee25589f8.png" alt="Experimental setup" class="wp-image-43866" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Experimental-setup.png?ssl=1"/><figcaption><em>Figure 8: CIFAR10 Dataset [6]</em></figcaption></figure></div>



<p>数据集可以在PyTorch中轻松下载。导入Torchvision并使用以下命令:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> datasets

cifar_trainset = datasets.CIFAR10(root=<span class="hljs-string">'./data'</span>, train=<span class="hljs-keyword">True</span>, download=<span class="hljs-keyword">True</span>, transform=transform)

cifar_testset = datasets.CIFAR10(root=<span class="hljs-string">'./data'</span>, train=<span class="hljs-keyword">False</span>, download=<span class="hljs-keyword">True</span>, transform=transform)
</pre>



<p>为了解释早期停止的重要性，我们将使用一个简单复杂的(例如，没有BatchNorm，或任何正则化技术，如dropout)卷积神经网络(CNN)架构来训练训练数据集上的模型，并故意过度拟合数据。</p>



<p>本实验使用的CNN架构如下:</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CNN</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
        self.conv2 = nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
        self.conv3 = nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
        self.conv4 = nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
        self.conv5 = nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
        self.conv6 = nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
        self.maxpool = nn.MaxPool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)
        self.fc1 = nn.Linear(<span class="hljs-number">128</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">1024</span>)
        self.fc2 = nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">512</span>)
        self.fc3 = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = F.relu(self.conv1(x))
        x = self.maxpool(F.relu(self.conv2(x)))
        x = F.relu(self.conv3(x))
        x = self.maxpool(F.relu(self.conv4(x)))
        x = F.relu(self.conv5(x))
        x = self.maxpool(F.relu(self.conv6(x)))
        x = x.view(<span class="hljs-number">-1</span>, <span class="hljs-number">128</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.log_softmax(self.fc3(x), dim=<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> x
</pre>



<h3>将Python脚本集成到Neptune</h3>



<p>在训练过程中，将监控训练/测试损失和训练/测试准确度，可以使用<strong> Neptune.log_metric() </strong>函数轻松调用。</p>



<p>PyTorch python脚本与Neptune的集成在fit()函数中类似如下:</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(model, train_loader, test_loader, epochs, optimizer, loss)</span>:</span>
    model.train()
    <span class="hljs-keyword">import</span> neptune
    neptune.init(project_qualified_name=<span class="hljs-string">'sanghvirajit/sandbox'</span>,    api_token=<span class="hljs-string">'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiN2Y4MzU5YTctZmJjZS00MmU5LTg4YmYtNDUwZWI5ZTQ3ZmJmIn0='</span>,)

    
    PARAMS = {<span class="hljs-string">'train_batch_size'</span>: <span class="hljs-number">5000</span>,
    <span class="hljs-string">'test_batch_size'</span>: <span class="hljs-number">1000</span>,
    <span class="hljs-string">'optimizer'</span>: <span class="hljs-string">'Adam'</span>}

    
    neptune.create_experiment(<span class="hljs-string">'Pytorch-Neptune-CIFAR10-Early     Stopping'</span>,params=PARAMS,tags=[<span class="hljs-string">'classification'</span>,<span class="hljs-string">'pytorch'</span>,<span class="hljs-string">'neptune'</span>])

    <span class="hljs-keyword">if</span> optimizer == <span class="hljs-string">'Adam'</span>:
    
    optimizer = torch.optim.Adam(model.parameters())
    

    <span class="hljs-keyword">if</span> loss == <span class="hljs-string">'CrossEntropy'</span>:
    
    error = nn.CrossEntropyLoss()

    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(epochs):

        correct = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> batch_idx, (X_batch, y_batch) <span class="hljs-keyword">in</span> enumerate(train_loader):

        
        
        var_X_batch = Variable(X_batch).float()
        var_y_batch = Variable(y_batch)

        
        optimizer.zero_grad()

        
        output = model.forward(var_X_batch)

        
        loss = error(output, var_y_batch)
        train_cost = loss.data

        
        loss.backward()

        
        
        optimizer.step()

        
        predicted = torch.max(output.data, <span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]
        correct += (predicted == var_y_batch).sum()
        Train_accuracy = float(correct*<span class="hljs-number">100</span>) / float(train_batch_size*(batch_idx+<span class="hljs-number">1</span>))

        
        test_accuracy, test_cost = evaluate(model, test_loader)

    print(<span class="hljs-string">'Epoch : {} [{}/{} ({:.0f}%)]tLoss: {:.6f}t Accuracy:{:.3f}%'</span>.format(
    epoch+<span class="hljs-number">1</span>,
    (batch_idx+<span class="hljs-number">1</span>)*(len(X_batch)),
    len(train_loader.dataset),
    <span class="hljs-number">100.</span>*(batch_idx+<span class="hljs-number">1</span>) / len(train_loader),
    train_cost,
    train_accuracy))

    neptune.log_metric(<span class="hljs-string">'training loss'</span>, train_cost)
    neptune.log_metric(<span class="hljs-string">'training accuracy'</span>, train_accuracy)

    neptune.log_metric(<span class="hljs-string">'testing loss'</span>, test_cost)
    neptune.log_metric(<span class="hljs-string">'testing accuracy'</span>, test_accuracy)


neptune.stop()
</pre>



<p>可以很容易地调用fit()函数，这将生成一个链接，该链接将我们重定向到Neptune仪表板:</p>



<pre class="hljs">fit(cnn, train_loader, test_loader, epochs=<span class="hljs-number">100</span>, optimizer=<span class="hljs-string">'Adam'</span>, loss=<span class="hljs-string">'CrossEntropy'</span>)
</pre>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/2bc2430ff72dea86eb8c09521522af20.png" alt="Early stopping fit function" class="wp-image-43868" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Early-stopping-fit-function.png?ssl=1"/></figure>



<p><strong>现在可以使用Neptune中的图表轻松监控测试损失:</strong></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/5e7f53a7e6d663da1202a26d97472cb8.png" alt="Neptune monitoring loss" class="wp-image-43869" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Neptune-monitoring-loss.png?ssl=1"/><figcaption><em>Figure 9: Monitoring testing loss in Neptune</em></figcaption></figure></div>



<p>日志指标可在日志下访问，通道数据可轻松下载。csv文件格式，用于结果的进一步后处理。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/b00ae626ae5acc91b29419cc28dea874.png" alt="Neptune log metrics" class="wp-image-43870" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Neptune-log-metrics.png?ssl=1"/><figcaption><em>Figure 10: Log metric in Neptune</em></figcaption></figure></div>



<p>让我们看看我们得到的结果:</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/d44bdb2519b25a98e107865082b4decc.png" alt="Early stopping results" class="wp-image-43871" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Early-stopping-results.png?resize=767%2C304&amp;ssl=1"/><figcaption><em>Figure 11: Results</em></figcaption></figure></div>



<p>图11显示了训练和测试数据集的分类结果。训练准确率达到98.84%，而测试准确率只能达到74.26%。</p>



<p>正如我们所看到的，测试损失在第55纪元左右开始分散。该模型已经学会对训练集进行很好的分类，以至于它失去了有效概括的能力，即正确分类测试集上未观察到的数据的能力。因此，该模型开始在测试数据集上表现不佳——它<strong>过度拟合</strong>。</p>



<p>在这种情况下，最好在第55世左右停止训练。</p>



<p><strong>现在，让我们在代码中引入提前停止:</strong></p>



<pre class="hljs">
valid_loss_array = np.array(valid_losses)
min_valid_loss = np.min(valid_loss_array)

<span class="hljs-keyword">if</span>(test_cost &gt; min_valid_loss):
patience_counter += <span class="hljs-number">1</span>
<span class="hljs-keyword">else</span>:
        
patience_counter = <span class="hljs-number">0</span>



<span class="hljs-keyword">if</span>(patience_counter &gt; patience):
        print(<span class="hljs-string">"Early stopping called at {} epochs"</span>.format(epoch+<span class="hljs-number">1</span>))
        <span class="hljs-keyword">break</span>
</pre>



<p>我们将使用耐心作为超参数之一来触发训练期间的提前停止。耐心是测试损失没有改善的次数，在此之后训练过程将停止。</p>



<p>让我们调用耐心值为10的fit()函数，并监控训练过程:</p>







<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/45fabc51076b63abcc991c2400edad3c.png" alt="Neptune monitoring patience" class="wp-image-43873" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Neptune-monitoring-patience.png?ssl=1"/><figcaption><em>Figure 12: Monitoring testing loss in Neptune</em></figcaption></figure></div>



<p><strong>让我们再来看看我们得到的结果:</strong></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/c04ff73f11064fe0085b221b5f6a0e1c.png" alt="Early stopping results" class="wp-image-43874" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221207132254im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Early-stopping-results-2.png?ssl=1"/><figcaption><em>Figure 13: Results</em></figcaption></figure></div>



<p>从结果中我们可以注意到，在第44个时期之后，测试损失没有进一步的改善，因此在第54个时期触发了早期停止，并且训练过程如我们所预期的那样停止了。</p>



<p>这消除了训练过程中过拟合的可能性，也有助于节省我们的计算资源和时间。</p>



<h2 id="h-summary">摘要</h2>



<p>在本文中，我们发现了早期停止在深度神经网络模型中的重要性。</p>



<p>具体来说，我们已经看到:</p>



<ul><li>早期停止减少了训练过程中的过度配合，</li><li>我们可以使用Neptune监控机器学习项目，以及如何将PyTorch python脚本集成到Neptune。</li></ul>



<p>如果你对实验的详细代码感兴趣，可以在<a href="https://web.archive.org/web/20221207132254/https://github.com/sanghvirajit/Early_Stopping_using_Neptune.ai" target="_blank" rel="noreferrer noopener nofollow"> my Github </a>上找到。</p>



<h3>参考</h3>



<p>[1] <a href="https://web.archive.org/web/20221207132254/https://www.kaggle.com/kaggle-survey-2020" target="_blank" rel="noreferrer noopener nofollow">数据科学与机器学习2020年状态</a>。</p>



<p>[2]了解偏差-方差权衡-<a href="https://web.archive.org/web/20221207132254/https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229" target="_blank" rel="noreferrer noopener nofollow">https://towards data science . com/Understanding-the-Bias-Variance-trade-165 e 6942 b229</a></p>



<p>[3]解决欠拟合和过拟合-<a href="https://web.archive.org/web/20221207132254/https://morioh.com/p/ebe9597eae3a" target="_blank" rel="noreferrer noopener nofollow">https://morioh.com/p/ebe9597eae3a</a></p>



<p>[4]使用PyTorch提前停止以防止模型过度拟合–<a href="https://web.archive.org/web/20221207132254/https://medium.com/analytics-vidhya/early-stopping-with-pytorch-to-restrain-your-model-from-overfitting-dce6de4081c5" target="_blank" rel="noreferrer noopener nofollow">https://medium . com/analytics-vid hya/Early-Stopping-with-py torch-to-inhibit-your-Model-from-over fitting-dce6de 4081 C5</a></p>



<p>[5] Neptune.ai文档—<a href="https://web.archive.org/web/20221207132254/https://docs.neptune.ai/getting-started/installation#authentication-neptune-api-token" target="_blank" rel="noreferrer noopener">设置Neptune API令牌</a></p>



<p>[6]CIFS ar 10和CIFAR100数据集—<a href="https://web.archive.org/web/20221207132254/https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noreferrer noopener nofollow">https://www.cs.toronto.edu/~kriz/cifar.html</a></p>



<p>[7]薛瑛。CISAT 2018。过度拟合及其解决方案概述-<a href="https://web.archive.org/web/20221207132254/https://iopscience.iop.org/article/10.1088/1742-6596/1168/2/022022/pdf" target="_blank" rel="noreferrer noopener nofollow">https://IOP science . IOP . org/article/10.1088/1742-6596/1168/2/022022/pdf</a></p>



<p>[8]Lutz pre helt。“早停——但是什么时候？."《神经网络:交易的诀窍》，第55-69页。施普林格，柏林，海德堡，1998—<a href="https://web.archive.org/web/20221207132254/https://docs.google.com/viewer?url=https%3A%2F%2Fpage.mi.fu-berlin.de%2Fprechelt%2FBiblio%2Fstop_tricks1997.pdf" target="_blank" rel="noreferrer noopener nofollow">https://docs.google.com/viewer?URL = https % 3A % 2F % 2f page . mi . fu-Berlin . de % 2f pre chelt % 2f iblio % 2f stop _ tricks 1997 . pdf</a></p>



<p/>



<p/>
        </div>
        
    </div>    
</body>
</html>