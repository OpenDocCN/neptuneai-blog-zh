<html>
<head>
<title>6 GAN Architectures You Really Should Know </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>你真的应该知道的6种GAN架构</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/6-gan-architectures#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/6-gan-architectures#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>生成对抗网络(GANs)是由Ian Goodfellow等人在2014年首次提出的。艾尔。从那时起，这个话题本身就开辟了一个新的研究领域。</p>



<p>几年之内，研究团体提出了大量关于这个主题的论文，其中一些有着非常有趣的名字:)。首先是CycleGAN，然后是BiCycleGAN，然后是ReCycleGAN，依此类推。</p>



<p>随着GANs的发明，生成模型开始在生成真实图像方面显示出有希望的结果。甘斯在计算机视觉领域取得了巨大的成功。最近，它开始在音频和文本方面显示出有希望的结果。</p>



<p>一些最受欢迎的GAN配方有:</p>


<div class="custom-point-list">
<ul class="is-style-default"><li>将图像从一个域变换到另一个域(CycleGAN)，</li><li>从文本描述生成图像(文本到图像)，</li><li>生成非常高分辨率的图像(ProgressiveGAN)等等。</li></ul>
</div>


<p>在本文中，我们将讨论一些最流行的GAN架构，特别是您应该了解的<em> <strong> 6架构</strong> </em>，以便对生成性对抗网络(GAN)进行多样化的报道。</p>



<p>即:</p>


<div class="custom-point-list">
<ul class="is-style-default"><li>CycleGAN</li><li>StyleGAN</li><li>像素网络</li><li>文本-2-图像</li><li>迪斯科根</li><li>伊斯甘</li></ul>
</div>


<h2>香草甘</h2>



<p>在监督学习的背景下有两种模型，生成模型和判别模型。判别模型主要用于解决分类任务，其中模型通常学习决策边界来预测数据点属于哪个类。另一方面，生成模型主要用于生成遵循与训练数据分布相同的概率分布的合成数据点。我们讨论的话题，<em> <strong>生成性对抗网络(GANs)就是生成性模型</strong> </em>的一个例子。</p>



<blockquote class="wp-block-quote"><p><em>生成模型的主要目标是学习对训练观测值进行采样的群体的未知概率分布。一旦模型训练成功，您就可以对遵循训练分布的新的“生成的”观察值进行采样。</em></p></blockquote>



<p>我们来讨论一下GAN公式的核心概念。</p>



<p>GAN由两个独立的网络、一个发生器和一个鉴别器组成。</p>



<h3>GAN发生器架构</h3>



<p><em> <strong>发生器在给定随机噪声[从潜在空间采样]的情况下生成合成样本，鉴别器是一个二元分类器，用于区分输入样本是真实的[输出标量值1]还是虚假的[输出标量值0]。</strong>T3】</em></p>



<p>由生成器生成的样本被称为假样本。正如您在图1和图2中看到的，当训练数据集中的一个数据点作为输入提供给鉴别器时，它会将其作为真实样本调用，而当它由生成器生成时，它会将另一个数据点作为假样本调用。</p>



<div class="wp-block-image"><figure class="aligncenter"><img data-attachment-id="16860" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig-1-generator-and-discriminator" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig.1-Generator-and-Discriminator.png?fit=556%2C292&amp;ssl=1" data-orig-size="556,292" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig.1 Generator and Discriminator" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig.1-Generator-and-Discriminator.png?fit=300%2C158&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig.1-Generator-and-Discriminator.png?fit=556%2C292&amp;ssl=1" src="../Images/1abae6e5304bdb604b3beb3fc678c4ad.png" alt="" class="wp-image-16860 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig.1-Generator-and-Discriminator.png?resize=556%2C292&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig.1-Generator-and-Discriminator.png?resize=556%2C292&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="16860" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig-1-generator-and-discriminator" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig.1-Generator-and-Discriminator.png?fit=556%2C292&amp;ssl=1" data-orig-size="556,292" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig.1 Generator and Discriminator" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig.1-Generator-and-Discriminator.png?fit=300%2C158&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig.1-Generator-and-Discriminator.png?fit=556%2C292&amp;ssl=1" src="../Images/1abae6e5304bdb604b3beb3fc678c4ad.png" alt="" class="wp-image-16860" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig.1-Generator-and-Discriminator.png?resize=556%2C292&amp;ssl=1"/></noscript><figcaption><em>Fig1: Generator and Discriminator as GAN building blocks</em></figcaption></figure></div>



<p>这个公式的美妙之处在于生成器和鉴别器之间的对立性质。</p>



<h3>GAN鉴频器架构</h3>



<p>鉴别者希望以最好的方式完成自己的工作。<em> <strong>当一个[由发生器生成的]假样本被提供给鉴别器时，它想称其为假样本，但发生器想以某种方式生成样本，使鉴别器错误地称其为真样本。在某种意义上，生成器试图欺骗鉴别器。</strong>T3】</em></p>



<div class="wp-block-image"><figure class="aligncenter"><img data-attachment-id="16861" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig2-generator-and-discriminator" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig2-Generator-and-Discriminator.png?fit=556%2C292&amp;ssl=1" data-orig-size="556,292" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig2 Generator and Discriminator" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig2-Generator-and-Discriminator.png?fit=300%2C158&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig2-Generator-and-Discriminator.png?fit=556%2C292&amp;ssl=1" src="../Images/9176591dc851949210e3ac6e1c39c039.png" alt="" class="wp-image-16861 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig2-Generator-and-Discriminator.png?resize=556%2C292&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig2-Generator-and-Discriminator.png?resize=556%2C292&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="16861" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig2-generator-and-discriminator" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig2-Generator-and-Discriminator.png?fit=556%2C292&amp;ssl=1" data-orig-size="556,292" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig2 Generator and Discriminator" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig2-Generator-and-Discriminator.png?fit=300%2C158&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig2-Generator-and-Discriminator.png?fit=556%2C292&amp;ssl=1" src="../Images/9176591dc851949210e3ac6e1c39c039.png" alt="" class="wp-image-16861" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig2-Generator-and-Discriminator.png?resize=556%2C292&amp;ssl=1"/></noscript><figcaption><em>Fig2: Generator and Discriminator as GAN building blocks</em></figcaption></figure></div>



<p>让我们快速看一下目标函数以及优化是如何完成的。<em> <strong>这是一个最小-最大优化公式，其中生成器想要最小化目标函数，而鉴别器想要最大化相同的目标函数。</strong>T3】</em></p>



<p>图3描述了被优化的目标函数。鉴别函数称为D，发生函数称为g。Pz是潜在空间的概率分布，通常为随机高斯分布。Pdata是训练数据集的概率分布。当x从Pdata中采样时，鉴别器希望将其归类为真实样本。G(z)是一个生成的样本，当G(z)作为输入被提供给鉴别器时，它想把它归类为一个伪样本。</p>



<div class="wp-block-image"><figure class="aligncenter"><img data-attachment-id="16862" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig3-objective-function" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig3-Objective-function.png?fit=620%2C254&amp;ssl=1" data-orig-size="620,254" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig3 Objective function" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig3-Objective-function.png?fit=300%2C123&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig3-Objective-function.png?fit=620%2C254&amp;ssl=1" src="../Images/c8b21469daa38d9b74877e2967ccf4b5.png" alt="" class="wp-image-16862 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig3-Objective-function.png?resize=620%2C254&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig3-Objective-function.png?resize=620%2C254&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="16862" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig3-objective-function" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig3-Objective-function.png?fit=620%2C254&amp;ssl=1" data-orig-size="620,254" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig3 Objective function" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig3-Objective-function.png?fit=300%2C123&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig3-Objective-function.png?fit=620%2C254&amp;ssl=1" src="../Images/c8b21469daa38d9b74877e2967ccf4b5.png" alt="" class="wp-image-16862" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig3-Objective-function.png?resize=620%2C254&amp;ssl=1"/></noscript><figcaption><em>Fig3: Objective function in GAN formulation</em></figcaption></figure></div>



<p><em> <strong>鉴别器想把D(G(z))的可能性驱动到0。因此，它希望最大化(1-D(G(z)))，而生成器希望将D(G(z))的似然性强制为1，以便鉴别器在调用生成的样本作为真实样本时出错。因此，生成器希望最小化(1-D(G(z))。</strong>T3】</em></p>



<div class="wp-block-image"><figure class="aligncenter"><img data-attachment-id="16863" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig4-objective-function-in-gan-formulation" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig4.-Objective-function-in-GAN-formulation.png?fit=636%2C254&amp;ssl=1" data-orig-size="636,254" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig4. Objective function in GAN formulation" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig4.-Objective-function-in-GAN-formulation.png?fit=300%2C120&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig4.-Objective-function-in-GAN-formulation.png?fit=636%2C254&amp;ssl=1" src="../Images/045c4497dc2e7b4f9b4022bb2dd26fd0.png" alt="" class="wp-image-16863 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig4.-Objective-function-in-GAN-formulation.png?resize=636%2C254&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig4.-Objective-function-in-GAN-formulation.png?resize=636%2C254&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="16863" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig4-objective-function-in-gan-formulation" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig4.-Objective-function-in-GAN-formulation.png?fit=636%2C254&amp;ssl=1" data-orig-size="636,254" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig4. Objective function in GAN formulation" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig4.-Objective-function-in-GAN-formulation.png?fit=300%2C120&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig4.-Objective-function-in-GAN-formulation.png?fit=636%2C254&amp;ssl=1" src="../Images/045c4497dc2e7b4f9b4022bb2dd26fd0.png" alt="" class="wp-image-16863" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig4.-Objective-function-in-GAN-formulation.png?resize=636%2C254&amp;ssl=1"/></noscript><figcaption>Fig4: Objective function in GAN formulation</figcaption></figure></div>



<p class="has-text-align-center">CycleGAN</p>



<h2>CycleGAN是一种非常流行的GAN架构，主要用于学习不同风格的图像之间的转换。</h2>



<p>作为一个例子，这种提法可以学到:</p>



<p>艺术图像和现实图像之间的映射，</p>


<div class="custom-point-list">
<ul class="is-style-default"><li>马和斑马图像之间的转换，</li><li>冬季意象与夏季意象的转换</li><li>等等</li><li>FaceApp是CycleGAN最受欢迎的例子之一，其中人脸被转换为不同的年龄组。</li></ul>
</div>


<p>举个例子，假设X是一组马的图像，Y是一组斑马的图像。</p>



<p>目标是学习一个映射函数G: X-&gt; Y，使得G(X)生成的图像与Y的图像不可区分。这个公式不仅学习G，而且还学习逆映射函数F: Y-&gt;X，并使用循环一致性损失来强制F(G(X)) = X，反之亦然。</p>



<blockquote class="wp-block-quote"><p>训练时，给出两种训练观察值作为输入。</p></blockquote>



<p>一组观察值有成对的图像{Xi，易}用于I，其中每个Xi都有其对应的易。</p>


<div class="custom-point-list">
<ul class="is-style-default"><li>另一组观察具有来自X的一组图像和来自Y的另一组图像，而在和易之间没有任何匹配。</li><li>正如我之前提到的，有两种函数正在学习，其中一种是G，将X转换为Y，另一种是F，将Y转换为X，它包括两个独立的GAN模型。所以，你会发现2个鉴别函数Dx，Dy。</li></ul>
</div>


<div class="wp-block-image"><figure class="aligncenter"><img data-attachment-id="16864" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig5-training-procedure-for-cyclegan" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig5-training-procedure-for-cyclegan.png?fit=596%2C145&amp;ssl=1" data-orig-size="596,145" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig5 training procedure for cyclegan" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig5-training-procedure-for-cyclegan.png?fit=300%2C73&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig5-training-procedure-for-cyclegan.png?fit=596%2C145&amp;ssl=1" src="../Images/c05268b70e4a98e0cb1af72bfda5008a.png" alt="" class="wp-image-16864 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig5-training-procedure-for-cyclegan.png?resize=596%2C145&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig5-training-procedure-for-cyclegan.png?resize=596%2C145&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="16864" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/fig5-training-procedure-for-cyclegan" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig5-training-procedure-for-cyclegan.png?fit=596%2C145&amp;ssl=1" data-orig-size="596,145" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig5 training procedure for cyclegan" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig5-training-procedure-for-cyclegan.png?fit=300%2C73&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/fig5-training-procedure-for-cyclegan.png?fit=596%2C145&amp;ssl=1" src="../Images/c05268b70e4a98e0cb1af72bfda5008a.png" alt="" class="wp-image-16864" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/fig5-training-procedure-for-cyclegan.png?resize=596%2C145&amp;ssl=1"/></noscript><figcaption><em>Fig5: The training procedure for CycleGAN.</em></figcaption></figure></div>



<p>作为对抗性公式的一部分，有一个鉴别器Dx，用于对转换后的Y是否与Y不可区分进行分类。类似地，还有一个鉴别器d Y，用于对是否与x不可区分进行分类。</p>



<p>除了对抗损失，CycleGAN还使用循环一致性损失来实现没有成对图像的训练，并且这种额外的损失帮助模型最小化重建损失F(G(x)) ≈ X和G(F(Y)) ≈ Y</p>



<p>因此，All-in-all CycleGAN配方包含如下3种损失:</p>



<p>作为优化的一部分，下面的损失函数被优化。</p>







<p>让我们来看看来自CycleGAN的一些结果。如您所见，该模型已经学会了将斑马图像转换为马图像，将夏季图像转换为冬季图像，反之亦然。</p>







<p>下面是不同损失函数的代码片段。完整的代码流程请参考以下参考。</p>







<p>CycleGAN</p>



<p>下面是一个例子，一个马的图像被转换成一个看起来像斑马的图像。</p>



<pre class="hljs">

fake_y = generator_g(real_x, training=<span class="hljs-keyword">True</span>)
cycled_x = generator_f(fake_y, training=<span class="hljs-keyword">True</span>)

fake_x = generator_f(real_y, training=<span class="hljs-keyword">True</span>)
cycled_y = generator_g(fake_x, training=<span class="hljs-keyword">True</span>)


same_x = generator_f(real_x, training=<span class="hljs-keyword">True</span>)
same_y = generator_g(real_y, training=<span class="hljs-keyword">True</span>)

disc_real_x = discriminator_x(real_x, training=<span class="hljs-keyword">True</span>)
disc_real_y = discriminator_y(real_y, training=<span class="hljs-keyword">True</span>)

disc_fake_x = discriminator_x(fake_x, training=<span class="hljs-keyword">True</span>)
disc_fake_y = discriminator_y(fake_y, training=<span class="hljs-keyword">True</span>)


gen_g_loss = generator_loss(disc_fake_y)
gen_f_loss = generator_loss(disc_fake_x)

total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + \
                   calc_cycle_loss(real_y,cycled_y)


total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)
total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)

disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)
disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)
</pre>



<p><strong>参考文献:</strong></p>







<p><strong>研究论文:</strong>【https://arxiv.org/pdf/1703.10593.pdf】T2</p>



<p>Tensorflow有一个关于CycleGAN的有据可查的教程。请参考以下网址作为参考:</p>



<p><a href="https://web.archive.org/web/20220928195313/https://www.tensorflow.org/tutorials/generative/cyclegan" target="_blank" rel="noreferrer noopener nofollow">https://www.tensorflow.org/tutorials/generative/cyclegan</a></p>



<p>stylenan</p>



<h2>你能猜出哪个图像(从下面的2个图像中)是真实的，哪个是GAN生成的吗？</h2>



<p>事实是，这两个图像都是由一种叫做StyleGAN的GAN公式想象出来的。</p>







<p>StyleGAN是一种GAN配方，能够生成非常高分辨率的图像，甚至是1024*1024的分辨率。<em> <strong>这个想法是建立一个层的堆栈，其中初始层能够生成低分辨率图像(从2*2开始),并且进一步的层逐渐增加分辨率。</strong>T3】</em></p>



<p>GAN生成高分辨率图像的最简单方法是记住来自训练数据集的图像，并且在生成新图像时，它可以向现有图像添加随机噪声。实际上，StyleGAN并不这样做，而是学习关于人脸的特征，并生成现实中不存在的人脸的新图像。如果这听起来很有趣，请访问<a href="https://web.archive.org/web/20220928195313/https://thispersondoesnotexist.com/" rel="nofollow">https://thispersondoesnotexist.com/</a>每次访问这个网址都会生成一张宇宙中不存在的人脸的新图像。</p>



<p>该图描述了StyleGAN的典型架构。潜在空间向量z通过由8个完全连接的层组成的映射变换，而合成网络由18层组成，其中每层产生从4×4到1024×1024的图像。输出层通过单独的卷积层输出RGB图像。这种架构有2620万个参数，由于可训练参数的数量非常大，因此该模型需要大量的训练图像来构建成功的模型。</p>



<div class="wp-block-image"><figure class="alignleft"><img data-attachment-id="16874" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/stylegan-architecture" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/StyleGan-architecture.png?fit=344%2C393&amp;ssl=1" data-orig-size="344,393" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="StyleGan architecture" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/StyleGan-architecture.png?fit=263%2C300&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/StyleGan-architecture.png?fit=344%2C393&amp;ssl=1" src="../Images/a6cada3ddc26e367e6a54af415139085.png" alt="" class="wp-image-16874 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/StyleGan-architecture.png?resize=344%2C393&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/StyleGan-architecture.png?resize=344%2C393&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="16874" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/stylegan-architecture" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/StyleGan-architecture.png?fit=344%2C393&amp;ssl=1" data-orig-size="344,393" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="StyleGan architecture" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/StyleGan-architecture.png?fit=263%2C300&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/StyleGan-architecture.png?fit=344%2C393&amp;ssl=1" src="../Images/a6cada3ddc26e367e6a54af415139085.png" alt="" class="wp-image-16874" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/StyleGan-architecture.png?resize=344%2C393&amp;ssl=1"/></noscript></figure></div>



<p>This figure depicts the typical architecture of StyleGAN. The latent space vector z is passed through a mapping transformation comprises of 8 fully connected layers whereas the synthesis network comprises of 18 layers, where each layer produces image from 4 x 4 to 1024 x 1024. The output layer output RGB image through a separate convolution layer. This architecture has 26.2 million parameters and because of this very high number of trainable parameters, this model requires a huge number of training images to build a successful model.</p>



<p>使用自适应实例规范化(AdaIN)函数对每个层进行规范化，如下所示:</p>



<p class="has-text-align-left">其中每个特征图xi被单独归一化，然后使用来自样式y的相应标量分量进行缩放和偏置。因此，y的维数是该层上特征图数量的两倍。</p>







<p><strong>参考文献:</strong></p>



<p>论文:<a href="https://web.archive.org/web/20220928195313/https://arxiv.org/pdf/1812.04948.pdf" target="_blank" rel="noreferrer noopener">https://arxiv.org/pdf/1812.04948.pdf</a></p>



<p>github:<a href="https://web.archive.org/web/20220928195313/https://github.com/NVlabs/stylegan" target="_blank" rel="noreferrer noopener">https://github.com/NVlabs/stylegan</a></p>



<p>像素网络</p>



<h2>PixelRNN是自回归生成模型的一个例子。</h2>



<p>在社交媒体时代，有大量的图片。但是在无人监管的环境下学习自然图像的分布是非常困难的。PixelRNN能够模拟图像的离散概率分布，并在两个空间维度上预测图像的像素。</p>



<p>我们都知道，rnn在学习条件分布方面很强大，尤其是LSTM擅长学习一系列像素中的长期依赖性。这个公式以渐进的方式工作，其中当提供了所有像素X <sub> 0 </sub>到X <sub> i </sub>时，模型预测下一个像素X <sub> i+1 </sub>。</p>







<p>We all know that RNNs are powerful in learning conditional distribution, especially  LSTM is good at learning the long-term dependency in a series of pixels. This formulation works in a progressive fashion where the model predicts the next pixel X<sub>i+1</sub> when all pixels X<sub>0 </sub>to X<sub>i</sub> are provided.</p>



<p>与gan相比，PixelRNN等自回归模型学习显式数据分布，而gan学习隐式概率分布。正因为如此，GAN并没有明确地揭示概率分布，而是允许我们从已知的概率分布中抽样观察。</p>



<p>该图描述了pixelRNN的各个剩余块。它被训练到几层深度。PixelRNN LSTM图层的输入地图具有2h要素。输入至状态组件通过在每个门产生h个特性来减少特性数量。应用递归图层后，通过1 × 1卷积将输出地图向上采样回每个位置2h个要素，并将输入地图添加到输出地图。</p>







<p>【来源:<a href="https://web.archive.org/web/20220928195313/https://arxiv.org/pdf/1601.06759.pdf#page=9&amp;zoom=100,0,0" target="_blank" rel="noreferrer noopener nofollow">https://arxiv.org/pdf/1601.06759.pdf#page=9&amp;zoom = 100，0，0 </a></p>



<p><strong>参考文献:</strong></p>



<p>论文:<a href="https://web.archive.org/web/20220928195313/https://arxiv.org/pdf/1601.06759.pdf" target="_blank" rel="noreferrer noopener nofollow">https://arxiv.org/pdf/1601.06759.pdf</a></p>



<p>github:<a href="https://web.archive.org/web/20220928195313/https://github.com/carpedm20/pixel-rnn-tensorflow" target="_blank" rel="noreferrer noopener nofollow">https://github.com/carpedm20/pixel-rnn-tensorflow</a></p>



<p>文本-2-图像</p>



<h2><strong> </strong>生成性对抗网络擅长生成随机图像。作为一个例子，在猫的图像上被训练的GAN可以生成具有两只眼睛、两只耳朵、胡须的猫的随机图像。但是猫身上的颜色图案可能是随机的。因此，随机图像通常对解决业务用例没有用处。现在，要求甘根据我们的期望生成一个图像，是一个极其困难的任务。</h2>



<p>在本节中，我们将讨论一种GAN架构，它在基于明确的文本描述生成有意义的图像方面取得了重大进展。该GAN公式采用文本描述作为输入，并生成文本描述中描述的RGB图像。</p>



<p>作为一个例子，给定<em>“这朵花有许多小的圆形粉红色花瓣”</em>作为输入，它将生成一朵有圆形粉红色花瓣的花的图像。</p>



<p>在这个公式中，不是只给噪声作为生成器的输入，而是首先将文本描述转换成文本嵌入，与噪声向量连接，然后给生成器作为输入。</p>







<blockquote class="wp-block-quote"><p>作为一个例子，文本描述已经被转换成256维的嵌入，并与100维的噪声向量(从通常为随机正态分布的潜在空间中采样)连接。</p></blockquote>



<p>这个公式将帮助生成器生成与输入描述一致的图像，而不是生成随机图像。</p>



<p>对于鉴别器，不是将唯一的图像作为输入，而是将一对图像和文本嵌入作为输入发送。输出信号为0或1。早先，鉴别者的职责只是预测一张给定的图像是真是假。</p>







<p><strong> <em>现在，鉴别者又多了一个额外的责任。除了识别给定图像是可读的还是伪造的，它还预测给定图像和文本是否相互对齐的可能性。</em>T3】</strong></p>



<p>这个公式迫使生成器不仅生成看起来真实的图像，而且生成与输入文本描述一致的图像。</p>



<p>为了实现鉴别器的双重责任的目的，在训练期间，一系列不同的(图像、文本)对被作为模型的输入给出，如下:</p>







<p>作为输入和目标变量的(真实图像、真实字幕)对被设置为1</p>


<div class="custom-point-list">
<ol><li>(错误图像，真实字幕)对作为输入，目标变量设置为0</li><li>作为输入和目标变量的(伪图像、真实字幕)对被设置为0</li><li>给定真实图像和真实字幕对，使得模型学习给定图像和文本对是否彼此对齐。错误的图像，阅读说明意味着图像不像说明中描述的那样。在这种情况下，目标变量被设置为0，以便模型知道给定的图像和标题没有对齐。这里假图像是指由生成器生成的图像，在这种情况下，目标变量被设置为0，以便鉴别器模型可以区分真实图像和假图像。</li></ol>
</div>


<p>用于训练的训练数据集具有图像以及描述图像属性的10种不同的文本描述。</p>



<p>以下是来自训练的文本-2-图像模型的一些结果。</p>







<p class="has-text-align-left"><strong>参考文献:</strong></p>







<p><strong>研究论文:</strong>【https://arxiv.org/pdf/1605.05396.pdf】T2</p>



<p><strong>Github:</strong>https://github.com/paarthneekhara/text-to-image<a href="https://web.archive.org/web/20220928195313/https://github.com/paarthneekhara/text-to-image" target="_blank" rel="noreferrer noopener nofollow">T3】</a></p>



<p>迪科曼</p>



<h2>最近，DiscoGAN变得非常受欢迎，因为它能够在无监督数据的情况下学习跨域关系。</h2>



<p>对于人类来说，跨域关系是非常自然的。给定两个不同领域的图像，人类可以找出它们之间的关系。例如，在下图中，我们有来自两个不同领域的图像，只需看一眼这些图像，我们就可以很容易地发现它们因其外观颜色的性质而相关联。</p>



<p>现在，在给定来自2个不同领域的不成对图像的情况下，建立一个机器学习模型来找出这种关系是一项极其困难的任务。</p>







<p>最近，DiscoGAN在跨两个不同领域学习这种关系方面显示出有希望的结果。</p>



<p>DiscoGAN的核心理念与CycleGAN非常相似:</p>



<p>两者都学习两个单独的变换函数，一个学习从域X到域Y的变换，而另一个学习反向映射，并且两者都使用重建损失作为在跨域的两次变换之后原始图像重建得有多好的度量。</p>


<div class="custom-point-list">
<ul><li>两者都遵循这样的原则:如果我们将一个图像从一个域1转换到域2，然后再转换回域1，那么它应该与原始图像匹配。</li><li>DiscoGAN和CycleGAN之间的主要区别在于，DiscoGAN使用两个重建损失，一个用于两个域，而CycleGAN使用单个循环一致性损失。T3】</li><li>和CycleGAN一样，DiscoGAN也是建立在重建损失的基础上。其思想是，当图像从一个域变换到另一个域，然后再变换回原始域时，生成的图像应该与原始图像一样接近。在这种情况下，定量差异被认为是重建损失，并且在训练期间，模型试图最小化这种损失。</li></ul>
</div>


<div class="wp-block-image"><figure class="aligncenter"><img data-attachment-id="16886" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/discogan-vs-cyclegan" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/DiscoGan-vs-CycleGAN.png?fit=593%2C258&amp;ssl=1" data-orig-size="593,258" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="DiscoGan vs CycleGAN" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/DiscoGan-vs-CycleGAN.png?fit=300%2C131&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/DiscoGan-vs-CycleGAN.png?fit=593%2C258&amp;ssl=1" src="../Images/46234ca991e320cbfa6d9701aaf1df2b.png" alt="" class="wp-image-16886 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/DiscoGan-vs-CycleGAN.png?resize=593%2C258&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/DiscoGan-vs-CycleGAN.png?resize=593%2C258&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="16886" data-permalink="https://web.archive.org/web/20220928195313/https://neptune.ai/discogan-vs-cyclegan" data-orig-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/DiscoGan-vs-CycleGAN.png?fit=593%2C258&amp;ssl=1" data-orig-size="593,258" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="DiscoGan vs CycleGAN" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/DiscoGan-vs-CycleGAN.png?fit=300%2C131&amp;ssl=1" data-large-file="https://web.archive.org/web/20220928195313/https://i0.wp.com/neptune.ai/wp-content/uploads/DiscoGan-vs-CycleGAN.png?fit=593%2C258&amp;ssl=1" src="../Images/46234ca991e320cbfa6d9701aaf1df2b.png" alt="" class="wp-image-16886" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220928195313im_/https://i0.wp.com/neptune.ai/wp-content/uploads/DiscoGan-vs-CycleGAN.png?resize=593%2C258&amp;ssl=1"/></noscript><figcaption><em>Figure: (a) Vanilla GAN (b) GAN with reconstruction loss (c) DiscoGAN architecture</em></figcaption></figure></div>



<p>因此，该模型由两个GAN网络组成，分别称为GAB和GBA。在上图中，模型正试图根据它们的方向来学习跨域关系。重建图像后，方向应该与原始方向相同。</p>







<p><strong>参考文献</strong>:</p>



<p><strong>研究</strong> <strong>论文</strong>:【https://arxiv.org/pdf/1703.05192.pdf】T4</p>



<p><strong>Github</strong>:【https://github.com/SKTBrain/DiscoGAN T2】T3】</p>



<p>伊斯甘</p>



<h2>最近，生成对抗网络在无监督任务中表现出了令人印象深刻的性能。</h2>



<p>在常规GAN中，鉴别器使用交叉熵损失函数，这有时会导致消失梯度问题。<em> <strong>取而代之，lsGAN提出对鉴别器使用最小二乘损失函数。</strong> </em>这种配方提供了更高质量的图像生成GAN。</p>



<p>早些时候，在vanilla GAN中，我们已经看到了以下最小-最大优化公式，其中鉴别器是二元分类器，并且在优化期间使用sigmoid交叉熵损失。</p>



<p>如前所述，对于位于决策边界正确一侧但远离密集区域的数据点，这种公式通常会导致消失梯度问题。最小平方公式解决了这个问题，并提供了模型的更稳定的学习和生成更好的图像。</p>







<p>以下是lsGAN的重新制定的优化公式，其中:</p>



<p>a是假样品的标签，</p>


<div class="custom-point-list">
<ul><li>b是真实样品的标签</li><li>c表示发生器希望鉴别器相信的假样本值。</li><li>现在，我们有两个单独的损失函数正在优化。一个相对于鉴别器最小化，另一个相对于发生器最小化。</li></ul>
</div>






<p>与香草GAN相比，lsGAN具有巨大的优势。在普通GAN中，由于鉴别器使用二进制交叉熵损失，只要观察值位于决策边界的正确一侧，其损失就是0。</p>



<p><em> <strong>但是在lsGAN的情况下，如果观察值远离决策边界，即使它在决策边界的正确一侧，该模型也会惩罚该观察值。</strong>T3】</em></p>



<p>这种惩罚迫使生成器向判定边界生成样本。除此之外，它还消除了渐变消失的问题，因为在更新生成器时，远点会生成更多的渐变。</p>



<p><strong>参考文献</strong>:</p>



<p><strong>研究</strong> <strong>论文</strong>:【https://arxiv.org/pdf/1611.04076.pdf】T4</p>



<p><strong>Github</strong>:【https://github.com/xudonmao/LSGAN T2】T3】</p>



<p>DiscoGAN与cycle gan–检查差异</p>



<h2>DiscoGAN和CycleGAN用于在给定不成对数据的情况下学习跨域关系。DiscoGAN的目标与CycleGAN非常相似。两者都在重建损失的基础上接受训练，并使用前向、后向循环一致性损失来实现双射映射。</h2>



<p>这两种模型几乎无法区分，除非损失函数有如下微小差异:</p>



<p><strong> <em>对于CycleGAN，L1距离</em> </strong>用于测量输入图像和重建图像之间的循环一致性损失，而<strong> <em> L2距离</em> </strong>用作<strong> <em> DiscoGAN </em> </strong>的距离测量。</p>



<p>CycleGAN使用一个额外的超参数来控制再生损失和循环一致性损失之间的相对重要性。</p>



<p>最后的想法</p>



<h2>我们讨论过的所有GAN架构都有一个共同点。它们都是建立在对抗性损失原则上的，它们都有生成器和鉴别器，它们遵循对抗性来愚弄对方。GANs在过去几年中取得了巨大的成功，成为机器学习研究领域最热门的研究课题之一。在未来，我们将看到这个领域的许多进展。</h2>



<p><em> <strong>以下Git知识库整理了一份独家的GAN论文列表。</strong>T3】</em></p>



<p><a href="https://web.archive.org/web/20220928195313/https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noreferrer noopener nofollow">https://github.com/hindupuravinash/the-gan-zoo</a></p>



<p>参考</p>



<h3>希布桑卡尔达斯</h3>






<div id="author-box-new-format-block_6095619503fc0" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">一位高级数据科学家@ WalmartLabs，在此之前，他曾在Envestnet | Yodlee、微软研究院和凯捷工作。他曾被《印度分析》杂志授予“40名40岁以下数据科学家”，因为他在基础机器学习和分析方面展示了专业知识，特别是在深度学习、生成模型和深度强化学习方面。</h3>
    
          <p class="article__authorContent-text"><strong>阅读下一篇</strong></p>
    
          
    
  </div>
</div>


<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>



<p class="has-text-color">Python中的图像处理:你应该知道的算法、工具和方法</p>



<h2>9分钟阅读|作者Neetika Khandelwal |更新于2021年5月27日</h2>



<p class="has-small-font-size">9 mins read | Author Neetika Khandelwal | Updated May 27th, 2021</p>


<p id="block_5ffc75def9f8e" class="separator separator-10">图像定义了世界，每张图像都有自己的故事，它包含了许多在许多方面都有用的重要信息。这些信息可以借助于被称为<strong>图像处理</strong>的技术来获得。</p>



<p>它是计算机视觉的核心部分，在机器人、自动驾驶汽车和物体检测等许多现实世界的例子中起着至关重要的作用。图像处理允许我们一次转换和操作数千幅图像，并从中提取有用的见解。它在几乎每个领域都有广泛的应用。</p>



<p>Python是为此目的广泛使用的编程语言之一。它惊人的库和工具有助于非常有效地完成图像处理任务。</p>



<p>通过本文，您将了解处理图像并获得所需输出的经典算法、技术和工具。</p>



<p>让我们开始吧！</p>



<p>Let’s get into it!</p>


<a class="button continous-post blue-filled" href="/web/20220928195313/https://neptune.ai/blog/image-processing-in-python-algorithms-tools-and-methods-you-should-know" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>