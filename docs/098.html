<html>
<head>
<title>Depth Estimation Models with Fully Convolutional Residual Networks (FCRN) </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>具有全卷积残差网络的深度估计模型(FCRN)</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/depth-estimation-models-with-fully-convolutional-residual-networks-fcrn#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/depth-estimation-models-with-fully-convolutional-residual-networks-fcrn#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>估计和评估2D场景的深度是一项困难的计算机视觉任务。首先，你需要昂贵的设备来绘制深度图。像用于视觉、运动和光投影的3D立体传感器这样的庞大系统是当今使用的最可靠的技术，它们依赖于需要额外评估以产生精确渲染的外部因素。</p>



<p>如果你不想为这一项任务携带和操作一大套设备，还有另一个解决方案。大量的工作已经投入到构建紧凑的系统中，这些系统统一并处理由设备的每一部分单独提供的所有相关功能。一个很好的例子是<a href="https://web.archive.org/web/20221206054745/https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-28-3-4156&amp;id=426432" target="_blank" rel="noreferrer noopener nofollow"> <em>考虑全光成像失真的光场相机</em> </a>。</p>



<p>深度估计具有广泛的用途，这就是在该领域进行大量研究的原因。深度估计的一些最公知的用途是:</p>



<ul>
<li>3D渲染和3D运动捕捉，</li>



<li>自动驾驶汽车，</li>



<li>机器人技术(包括机器人辅助手术)，</li>



<li>2D和3D电影转换，</li>



<li>计算机图形和游戏产业中的3D渲染和阴影映射。</li>
</ul>



<figure class="wp-block-video aligncenter"><video autoplay="" loop="" muted="" src="https://web.archive.org/web/20221206054745im_/https://neptune.ai/wp-content/uploads/2022/11/Depth-estimation-rendering-video.gif.mp4" playsinline=""/><figcaption class="wp-element-caption"><strong>Depth estimation rendering for a video | Source: <a href="https://web.archive.org/web/20221206054745/https://ai.googleblog.com/2019/05/moving-camera-moving-people-deep.html" target="_blank" rel="noreferrer noopener nofollow">Deep Learning approach to Depth prediction, Google AI</a></strong></figcaption></figure>



<h2 id="h-different-approaches-for-the-same-objective">同一目标的不同方法</h2>



<p>最近，为深度估计设计了几种方法。深度学习再次证明了它解决问题和应对各种挑战的能力。该方法集中于单个视点(2D图像),并优化参考深度图上的回归。</p>



<p>已经测试了多种神经架构，一些主要的实现为未来的研究铺平了道路，这使它们成为该领域中最先进的技术。让我们来看看其中的一些。</p>



<h3>利用全卷积残差网络进行更深的深度预测</h3>



<p>这种方法通过利用从RGB图像返回2D场景的深度图的完全卷积架构来解决这个问题。</p>



<p>所提出的架构包括全卷积层、转置卷积和有效的残差上采样块，有助于跟踪高维回归问题。</p>



<p>揭露该模型的研究论文解释了这种原始架构如何使用残差学习来处理<a href="https://web.archive.org/web/20221206054745/https://en.wikipedia.org/wiki/Monocular_vision"><em/></a>单目视觉和深度图之间的模糊映射。Huber损失函数用于优化，并且该模型可以以30 FPS的稳定帧速率在图像和视频上运行。</p>





<p>该网络由具有初始化的预训练权重的第一ResNet50块组成，并通过一系列卷积层和非卷积层前进，使网络学习其升级。然后，脱落图层被放置在生成预测结果的最终卷积图层旁边的末端。</p>





<p>关于整体架构的更多细节可以在官方研究论文中找到:<a href="https://web.archive.org/web/20221206054745/https://arxiv.org/pdf/1606.00373v2.pdf" target="_blank" rel="noreferrer noopener nofollow"> <em>使用全卷积残差网络的更深深度预测</em> </a> <em>。</em></p>



<h3>来自视频的深度和自我运动的无监督学习</h3>



<p>这种特定的方法完全基于无监督的学习策略，将单视图深度CNN(如之前所示)与在无标签视频序列上训练的相机姿势估计CNN相结合。</p>



<p>这种方法是独一无二的，也是同类方法中的首创。作者解释说，训练的整个监督管道是基于视图合成的。粗略地解释，网络被馈送以目标视图，并输出每像素深度图。给定每像素深度图以及来自原始图像的姿态和可见性附近视图，相应地合成目标视图。</p>



<p>因此，网络正确地管理使用CNN的图像合成和姿态估计模块之间的平衡。</p>





<p><em>注:对于这种特定架构背后的理论的详细解释可以在2017年发表的原始研究论文中查阅到:</em> <a href="https://web.archive.org/web/20221206054745/https://arxiv.org/pdf/1704.07813v2.pdf" target="_blank" rel="noreferrer noopener nofollow"> <em>来自视频的深度和自我运动的无监督学习</em> </a> <em>。</em></p>



<h3>具有左右一致性的无监督单目深度估计</h3>



<p>这种特定的架构是端到端的，并且在没有地面实况数据的情况下执行无监督的单目深度估计。如标题所示，该网络的训练损失加强了左右深度一致性，这意味着网络通过推断包裹左图像以匹配右图像的差异和差异来估计深度。</p>



<p>为网络提供动力的整个机制依赖于产生差异并在培训过程中不断纠正它们。通常，左输入图像用于推断从左到右和从右到左的差异。然后，网络使用双线性采样器通过反向映射生成预测的结果图像。</p>



<p>作者使用了与<a href="https://web.archive.org/web/20221206054745/https://arxiv.org/pdf/1512.02134.pdf" target="_blank" rel="noreferrer noopener nofollow"> DispNet非常相似的卷积架构。</a>它建立在编码器和解码器模式的基础上。解码器使用来自编码器激活模块的跳过连接来解析更高分辨率的细节。</p>





<p>最后，网络预测两个视差图:从左到右和从右到左。</p>



<h2 id="h-fcrn-fully-convolutional-residual-networks">FCRN:完全卷积剩余网络</h2>



<p><strong> FCRN </strong>是设备上深度预测最常用的模型之一。当苹果公司为其iPhone系列的前置摄像头实现并集成深度传感器时，这种架构变得很有名。由于该模型基于CNN (ResNet-50)，移动部署的压缩和量化过程相对简单明了。</p>



<p>我们将讨论<strong> FCRN </strong>架构的一些重要组件和构建模块，并一瞥Pytorch中的实际实现。</p>



<h3>实施和构建模块</h3>



<p>有趣的是，整个<strong> FCRN </strong>架构受到了U-Net方案的高度启发。两者都使用固定滤波器3×3的三个下采样和三个上采样卷积块。最初，U-Net在每个块中有两个卷积层，所有卷积层的滤波器数量保持不变。</p>



<p>相反，在FCRN实现中，作者增加了后续层的数量，以补偿由池化引起的高分辨率信息的损失。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/ef9c875431c30e7e2c5d75df0eaee808.png" alt="Convolutions FCRN" class="wp-image-52090" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206054745im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Convolutions-FCRN.png?ssl=1"/><figcaption class="wp-element-caption"><em> Faster up-Convolutions. Illustration of what we’ve explained above. The common up-convolutional steps: unpooling doubles a fea- ture map’s size, filling the holes with zeros, and a 5 × 5 convolution filters this map | Source: <a href="https://web.archive.org/web/20221206054745/https://arxiv.org/pdf/1606.00373.pdf" target="_blank" rel="noreferrer noopener nofollow">official research paper</a> </em></figcaption></figure></div>


<p>首先，让我们看一下<strong> FRCN </strong>网络的基本部分，即卷积块，由卷积层、批量归一化和激活函数组成。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv_block</span><span class="hljs-params">(channels: Tuple[int, int],
               size: Tuple[int, int],
               stride: Tuple[int, int]=<span class="hljs-params">(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)</span>,
               N: int=<span class="hljs-number">1</span>)</span>:</span>
    <span class="hljs-string">"""
    Create a block with N convolutional layers with ReLU activation function.
    The first layer is IN x OUT, and all others - OUT x OUT.
    Args:
        channels: (IN, OUT) - no. of input and output channels
        size: kernel size (fixed for all convolution in a block)
        stride: stride (fixed for all convolution in a block)
        N: no. of convolutional layers
    Returns:
        A sequential container of N convolutional layers.
    """</span>
    
    block = <span class="hljs-keyword">lambda</span> in_channels: nn.Sequential(
        nn.Conv2d(in_channels=in_channels,
                  out_channels=channels[<span class="hljs-number">1</span>],
                  kernel_size=size,
                  stride=stride,
                  bias=<span class="hljs-keyword">False</span>,
                  padding=(size[<span class="hljs-number">0</span>] // <span class="hljs-number">2</span>, size[<span class="hljs-number">1</span>] // <span class="hljs-number">2</span>)),
        nn.BatchNorm2d(num_features=channels[<span class="hljs-number">1</span>]),
        nn.ReLU()
    )
    
    
    <span class="hljs-keyword">return</span> nn.Sequential(*[block(channels[bool(i)]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(N)])</pre>



<p>conv _块函数具有<strong><em>【N】</em></strong>个卷积层，具有<strong> <em>个</em> </strong>个滤波器，遵循ReLU激活和批量归一化。</p>



<p>整个FCRN架构可以通过将专门定义的块堆叠在一起而获得，例如上采样块、解转换块、上转换解码器和FasterUpConv解码器，这些是pixelshuffle的特定技术。</p>



<p>我不能详细说明每个模块实际上是如何实现的，因为它的理论和实用性是如此的密集，以至于仅仅介绍和解释它们的功能就需要一整篇文章。但是，总体框架可以描述如下:</p>



<pre class="hljs">self.model = nn.Sequential(
    
    conv_block(channels=(input_filters, <span class="hljs-number">32</span>), size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), N=N),
    nn.MaxPool2d(<span class="hljs-number">2</span>),

    conv_block(channels=(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), N=N),
    nn.MaxPool2d(<span class="hljs-number">2</span>),

    conv_block(channels=(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>), size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), N=N),
    nn.MaxPool2d(<span class="hljs-number">2</span>),

    
    DeConv(channels=(<span class="hljs-number">128</span>, <span class="hljs-number">512</span>), size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), N=N),

    
    nn.Upsample(scale_factor=<span class="hljs-number">2</span>),
    UpConv(channels=(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>), size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), N=N),

    nn.Upsample(scale_factor=<span class="hljs-number">2</span>),
    FasterUpConv(channels=(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>), size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), N=N),

    nn.Upsample(scale_factor=<span class="hljs-number">2</span>),
    conv_block(channels=(<span class="hljs-number">64</span>, <span class="hljs-number">1</span>), size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), N=N),
)</pre>



<p>DeConv和UpConv由4个卷积块模块组成，通道数量逐渐减少，特征映射尺寸逐渐增大。它们在Pytorch中的实现如下所示:</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convt</span><span class="hljs-params">(in_channels)</span>:</span>
            stride = <span class="hljs-number">2</span>
            padding = (kernel_size - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span>
            output_padding = kernel_size % <span class="hljs-number">2</span>
            <span class="hljs-keyword">assert</span> <span class="hljs-number">-2</span> - <span class="hljs-number">2</span> * padding + kernel_size + output_padding == <span class="hljs-number">0</span>, <span class="hljs-string">"deconv parameters incorrect"</span>

            module_name = <span class="hljs-string">"deconv{}"</span>.format(kernel_size)
            <span class="hljs-keyword">return</span> nn.Sequential(collections.OrderedDict([
                (module_name, nn.ConvTranspose2d(in_channels, in_channels // <span class="hljs-number">2</span>, kernel_size,
                                                 stride, padding, output_padding, bias=<span class="hljs-keyword">False</span>)),
                (<span class="hljs-string">'batchnorm'</span>, nn.BatchNorm2d(in_channels // <span class="hljs-number">2</span>)),
                (<span class="hljs-string">'relu'</span>, nn.ReLU(inplace=<span class="hljs-keyword">True</span>)),
            ]))

        self.layer1 = convt(in_channels)
        self.layer2 = convt(in_channels // <span class="hljs-number">2</span>)
        self.layer3 = convt(in_channels // (<span class="hljs-number">2</span> ** <span class="hljs-number">2</span>))
        self.layer4 = convt(in_channels // (<span class="hljs-number">2</span> ** <span class="hljs-number">3</span>))</pre>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">UpConv</span><span class="hljs-params">(Decoder)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">upconv_module</span><span class="hljs-params">(self, in_channels)</span>:</span>
        
        upconv = nn.Sequential(collections.OrderedDict([
            (<span class="hljs-string">'unpool'</span>, Unpool(in_channels)),
            (<span class="hljs-string">'conv'</span>, nn.Conv2d(in_channels, in_channels // <span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>, bias=<span class="hljs-keyword">False</span>)),
            (<span class="hljs-string">'batchnorm'</span>, nn.BatchNorm2d(in_channels // <span class="hljs-number">2</span>)),
            (<span class="hljs-string">'relu'</span>, nn.ReLU()),
        ]))
        <span class="hljs-keyword">return</span> upconv

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_channels)</span>:</span>
        super(UpConv, self).__init__()
        self.layer1 = self.upconv_module(in_channels)
        self.layer2 = self.upconv_module(in_channels // <span class="hljs-number">2</span>)
        self.layer3 = self.upconv_module(in_channels // <span class="hljs-number">4</span>)
        self.layer4 = self.upconv_module(in_channels // <span class="hljs-number">8</span>)</pre>



<p>如果你对整个基础设施感兴趣和好奇，有一个很好的Github repo，它用ResNet-50实现了FCRN架构，并且完全遵循了研究论文的指示:<a href="https://web.archive.org/web/20221206054745/https://github.com/dontLoveBugs/FCRN_pytorch"> FCRN Pytorch实现</a>。</p>



<h3>NYU深度V2数据集上的训练</h3>



<p>该数据集由来自分割的元素组成，并支持来自RGBD ( <strong> <em>、RGB和深度</em> </strong>)图像的推断。它包含来自Microsoft Kinect的RGB和深度相机记录的各种3D场景的视频序列。</p>



<p>链接:<a href="https://web.archive.org/web/20221206054745/https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" target="_blank" rel="noreferrer noopener nofollow"> NYU深度数据集V2 </a></p>



<p>基本上，它的特点是:</p>



<ul>
<li>1449个密集标记的对齐RGB和深度图像对</li>



<li>来自3个城市的464个新场景</li>



<li>407024新的无标签帧</li>
</ul>



<p>数据集包含多种类型的数据:</p>



<ul>
<li>用伴随着密集多类标签的视频数据子集来标记。</li>



<li>由微软kinect摄像头提供的原始RGB、深度和加速度计数据。</li>
</ul>





<p>为了开始调试模型，我们将依赖一个使用Pytorch的开源Github实现。实施是由<a href="https://web.archive.org/web/20221206054745/https://github.com/dontLoveBugs" target="_blank" rel="noreferrer noopener nofollow">王鸽翔</a>(大喊着他出来的！)，他提供了一个完全用Pytorch制作的替代版本，仔细遵循了关于模型架构和训练过程的官方文件指示。</p>



<p><strong>用谢恩自己的话说</strong> : <em>这是PyTorch利用全卷积残差网络实现的更深深度预测。它可以使用全卷积残差网络来实现单目深度预测。目前，我们可以使用NYUDepthv2和Kitti里程计数据集训练FCRN。</em></p>



<h4>安装指南</h4>



<ul>
<li>克隆repo: git克隆git @ github . com:dontLoveBugs/FCRN _ pyo trch . git</li>



<li>安装所需的依赖项:pip安装matplotlib pillow tensorboard x torch torch vision</li>
</ul>



<h4>在dataloaders文件夹中配置数据集路径</h4>



<p>下载NYU深度V2数据集，标注版本约为2.8GB】此处下载。</p>



<p>让我们定义nyu_dataloader类，它从根目录加载数据集，并执行数据转换和数据扩充。</p>



<pre class="hljs">height, width = <span class="hljs-number">480</span>, <span class="hljs-number">640</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NYUDataset</span><span class="hljs-params">(Dataloader)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, root, type, sparsifier=None, modality=<span class="hljs-string">'rgb'</span>)</span>:</span>
        super(NYUDataset, self).__init__(root, type, sparsifier, modality)
        self.output_size = (<span class="hljs-number">228</span>, <span class="hljs-number">304</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_transform</span><span class="hljs-params">(self, rgb, depth)</span>:</span>
        s = np.random.uniform(<span class="hljs-number">1.0</span>, <span class="hljs-number">1.5</span>)  
        depth_np = depth / s
        angle = np.random.uniform(<span class="hljs-number">-5.0</span>, <span class="hljs-number">5.0</span>)  
        do_flip = np.random.uniform(<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>) &lt; <span class="hljs-number">0.5</span>  

        
        transform = transforms.Compose([
            transforms.Resize(<span class="hljs-number">250.0</span> / height),  
            transforms.Rotate(angle),
            transforms.Resize(s),
            transforms.CenterCrop(self.output_size),
            transforms.HorizontalFlip(do_flip)
        ])
        rgb_np = transform(rgb)
        rgb_np = self.color_jitter(rgb_np)  
        rgb_np = np.asfarray(rgb_np, dtype=<span class="hljs-string">'float'</span>) / <span class="hljs-number">255</span>
        depth_np = transform(depth_np)

        <span class="hljs-keyword">return</span> rgb_np, depth_np</pre>



<p>我们还在Neptune实验中记录了所有这些信息，以便跟踪数据集目录和应用的转换。</p>



<p>开始你的实验:</p>



<pre class="hljs">run = neptune.init(project=<span class="hljs-string">'aymane.hachcham/FCRN, api_token='</span>ANONYMOUS<span class="hljs-string">') # your credentials</span></pre>



<pre class="hljs">run[<span class="hljs-string">'config/dataset/path'</span>] = <span class="hljs-string">'Documents/FCRN/dataset'</span>
run[<span class="hljs-string">'config/dataset/size'</span>] = <span class="hljs-number">407024</span>
run[<span class="hljs-string">'config/dataset/transforms'</span>] = {
    <span class="hljs-string">'train'</span>: transforms.Compose([
            transforms.Rotate(angle),
            transforms.Resize(int(<span class="hljs-number">250</span> / height)),
            transforms.CenterCrop((<span class="hljs-number">228</span>, <span class="hljs-number">304</span>)),
            transforms.RandomHorizontalFlip(p=<span class="hljs-number">0.5</span>)
        ])
}</pre>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/5cca3caa62895cb0c6678353591be9b6.png" alt="Depth estimation - Neptune" class="wp-image-52095" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206054745im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Depth-estimation-Neptune.png?ssl=1"/><figcaption class="wp-element-caption"><em>Neptune dataset configurations</em></figcaption></figure></div>


<p>在开始培训之前，让我们记录我们想要使用的模型的超参数。</p>



<pre class="hljs">hparams = {
    <span class="hljs-string">'batch_size'</span>: <span class="hljs-number">128</span>,
    <span class="hljs-string">'decoder'</span>:<span class="hljs-string">'upproj'</span>,
    <span class="hljs-string">'epochs'</span>:<span class="hljs-number">10</span>,
    <span class="hljs-string">'lr'</span>:<span class="hljs-number">0.01</span>,
    <span class="hljs-string">'lr_patience'</span>:<span class="hljs-number">2</span>,
    <span class="hljs-string">'manual_seed'</span>:<span class="hljs-number">1</span>,
    <span class="hljs-string">'momentum'</span>:<span class="hljs-number">0.9</span>,
    <span class="hljs-string">'print_freq'</span>:<span class="hljs-number">10</span>,
    <span class="hljs-string">'resume'</span>:<span class="hljs-keyword">None</span>,
    <span class="hljs-string">'weight_decay'</span>:<span class="hljs-number">0.0005</span>,
    <span class="hljs-string">'workers'</span>:<span class="hljs-number">0</span>
}
run[<span class="hljs-string">"params"</span>] = hparams</pre>



<p>在记录了所有需要的超参数后，我们将启动epochs的培训课程。我们将把损失和所有指标记录到Neptune，以跟踪进度。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(train_loader, model, criterion, optimizer, epoch, logger)</span>:</span>
    average_meter = AverageMeter()
    model.train()  
    end = time.time()

    batch_num = len(train_loader)

    <span class="hljs-keyword">for</span> i, (input, target) <span class="hljs-keyword">in</span> enumerate(train_loader):

        
        input, target = input.cuda(), target.cuda()
        torch.cuda.synchronize()
        data_time = time.time() - end

        
        end = time.time()

        pred = model(input)
        loss = criterion(pred, target)
        optimizer.zero_grad()
        loss.backward()  
        optimizer.step()
        torch.cuda.synchronize()
        gpu_time = time.time() - end

        
        result = Result()
        result.evaluate(pred.data, target.data)
        acc = result.evaluate(pred.data/target.data)
        average_meter.update(result, gpu_time, data_time, input.size(<span class="hljs-number">0</span>))
        end = time.time()
        
        run[<span class="hljs-string">"training/batch/accuracy"</span>].log(acc)
        run[<span class="hljs-string">"training/batch/loss"</span>].log(loss)

        <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % args.print_freq == <span class="hljs-number">0</span>:
            current_step = epoch * batch_num + i
            logger.add_scalar(<span class="hljs-string">'Train/RMSE'</span>, result.rmse, current_step)
            logger.add_scalar(<span class="hljs-string">'Train/rml'</span>, result.absrel, current_step)
            logger.add_scalar(<span class="hljs-string">'Train/Log10'</span>, result.lg10, current_step)
            logger.add_scalar(<span class="hljs-string">'Train/Delta1'</span>, result.delta1, current_step)
            logger.add_scalar(<span class="hljs-string">'Train/Delta2'</span>, result.delta2, current_step)
            logger.add_scalar(<span class="hljs-string">'Train/Delta3'</span>, result.delta3, current_step)</pre>



<h3>Neptune中的精度和损耗设置</h3>





<p>对于训练会话，我们清楚地观察到这两个指标运行良好，除了由数据插值引起的精度曲线的线性下降，其中上卷积层丢失深度信息，并被连续实时呈现的索引矩阵的比例淹没。</p>



<p>即使我们执行新的训练课程，改变历元数或稍微调整训练数据，变化也不会很大。</p>





<p>通过增加历元的数量和减少数据转换的数量，我们在准确度分数上获得了轻微的提高，并且还消除了由数据插值引起的差异。</p>



<p>一个很酷的提示Shan建议在训练时使用较低的历元数(&lt; 100) is to decrease the <strong> <em>学习率</em> </strong>并增加<strong> <em>速率_优化器</em> </strong>)。这样，梯度计算得更慢，反向计算更同步，从而为模型留出相应的适应空间。</p>



<p>因此，新运行的新超参数如下所示:</p>



<pre class="hljs">hparams = {
    <span class="hljs-string">'batch_size'</span>: <span class="hljs-number">128</span>,
    <span class="hljs-string">'decoder'</span>:<span class="hljs-string">'upproj'</span>,
    <span class="hljs-string">'epochs'</span>:<span class="hljs-number">10</span>,
    <span class="hljs-string">'lr'</span>:<span class="hljs-number">0.002</span>,
    <span class="hljs-string">'lr_patience'</span>: <span class="hljs-number">2.5</span>,
    <span class="hljs-string">'manual_seed'</span>:<span class="hljs-number">1</span>,
    <span class="hljs-string">'momentum'</span>:<span class="hljs-number">0.9</span>,
    <span class="hljs-string">'print_freq'</span>:<span class="hljs-number">10</span>,
    <span class="hljs-string">'resume'</span>:<span class="hljs-keyword">None</span>,
    <span class="hljs-string">'weight_decay'</span>:<span class="hljs-number">0.0005</span>,
    <span class="hljs-string">'workers'</span>:<span class="hljs-number">0</span>
}</pre>



<p>这是两次运行之间的比较:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221206054745/https://app.neptune.ai/aymane.hachcham/FCRN/experiments?compare=EwGgjEA&amp;split=cmp&amp;dash=charts&amp;viewId=standard-view" target="_blank" rel="noopener"><img decoding="async" src="../Images/a94206dfcbce8877a67400c75d207068.png" alt="Depth estimation - Neptune comparison" class="wp-image-52098" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206054745im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Depth-estimation-Neptune-4.png?ssl=1"/></a><figcaption class="wp-element-caption"><em>Blue chart: previous accuracy, Red chart: the improved one | <a href="https://web.archive.org/web/20221206054745/https://app.neptune.ai/aymane.hachcham/FCRN/experiments?compare=EwGgjEA&amp;split=cmp&amp;dash=charts&amp;viewId=standard-view" target="_blank" rel="noreferrer noopener">See in the app</a></em></figcaption></figure></div>


<p>我们清楚地注意到，超参数调整和选择30个周期的长时间训练极大地提高了准确度标准。</p>



<p>我会把海王星项目的链接留给你。快来看看吧:<a href="https://web.archive.org/web/20221206054745/https://app.neptune.ai/aymane.hachcham/FCRN/experiments?split=tbl&amp;dash=charts&amp;viewId=standard-view" target="_blank" rel="noreferrer noopener"> <em> FCRN实验</em> </a></p>



<h2 id="h-final-output">最终输出</h2>



<p>一旦执行了训练，我们可以看看由模型基于来自验证集的看不见的室内图像生成的结果深度图。</p>



<p>为了加强和断言关于模型结果的确定性，Github repo展示了我们在这里使用的相同模型的预训练版本，其中包含一系列令人满意的断言模型推理质量的指标。</p>



<h3>结果</h3>



<p>在成功完成训练后，作者给出了一些与错误相关的度量，以便评估他的实现与以前的实现相比的性能。</p>



<p>显然，他用Pytorch的实现通过略微改进定义为基于地面真实深度图值归一化的绝对误差的<strong><em>【rel】</em></strong>(相对绝对误差)，以及<strong><em>log10</em></strong>(ABS(log10(gt)–log10(pred))(其中gt是地面真实深度图，pred是预测深度图)，击败了先前的尝试。</p>





<h3>定性结果</h3>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/4f649f0f7866498e310880c80fea9924.png" alt="Depth estimation results" class="wp-image-52102" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206054745im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Depth-estimation-results.png?resize=768%2C662&amp;ssl=1"/><figcaption class="wp-element-caption"><em>RGB images from validation set, their ground truth and final results generated by the model </em></figcaption></figure></div>


<p>正如在官方论文中所解释的那样，全卷积残差网络提高了精度，这要归功于其使用增强型上投影模块和上采样技术的特定架构，上采样技术涉及使用连续2×2内核的去卷积。定性地说，与相同数据集上的其他不同的完全卷积变体相比，这种方法在输出中保留了更多的结构。</p>



<h2 id="h-conclusion-and-perspectives">结论和展望</h2>



<p>我们浏览了深度估计领域中使用的各种技术和架构。我们实践和训练了FCRN的一个现有实现，以展示和观察这种方法在定性结果方面的能力。</p>



<p>您可能想尝试的一个有趣的项目是实现FCRN模型的设备版本，并创建一个实时执行深度估计的小型IOS应用程序。MLCore和Apple Vision已经提出了预训练FCRN模型的不同变体，这些模型可以快速用于涉及前置摄像头和深度传感器的任务。也许我可以在下一篇关于这个主题的文章中考虑这个问题。敬请期待！</p>



<p>一如既往，我将为您留下一些有用的资源，您可以查找关于该主题的深入知识:</p>




        </div>
        
    </div>    
</body>
</html>