<html>
<head>
<title>Model-Based and Model-Free Reinforcement Learning: Pytennis Case Study </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>基于模型和无模型的强化学习:Pytennis案例研究</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/model-based-and-model-free-reinforcement-learning-pytennis-case-study#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/model-based-and-model-free-reinforcement-learning-pytennis-case-study#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>强化学习是人工智能的一个领域，在这个领域中，你可以建立一个智能系统，通过交互从其环境中学习，并实时评估它所学习的内容。</p>



<p>自动驾驶汽车就是一个很好的例子，或者说DeepMind建立了我们今天所知的AlphaGo、AlphaStar和AlphaZero。</p>



<p>AlphaZero是一个旨在掌握国际象棋、日本兵棋和围棋的程序(AlphaGo是第一个击败人类围棋大师的程序)。阿尔法星玩电子游戏星际争霸2。</p>



<p>在本文中，我们将比较无模型和基于模型的强化学习。一路上，我们将探索:</p>



<ol><li>强化学习的基本概念<br/> a)马尔可夫决策过程/ Q值/ Q学习/深度Q网络</li><li>基于模型和无模型强化学习的区别。</li><li>打网球的离散数学方法-无模型强化学习。</li><li>使用深度Q网络的网球游戏——基于模型的强化学习。</li><li>比较/评估</li><li>参考资料了解更多信息</li></ol>







<h2 id="h-fundamental-concepts-of-reinforcement-learning">强化学习的基本概念</h2>



<p>任何强化学习问题都包括以下要素:</p>



<ol><li><strong>代理</strong>–控制相关对象的程序(例如，机器人)。</li><li><strong>环境</strong>–这以编程方式定义了外部世界。代理与之交互的一切都是环境的一部分。它是为代理人设计的，让它看起来像真实世界的案例。它需要证明一个代理的性能，意思是一旦在现实世界的应用程序中实现，它是否会做得很好。</li><li><strong>奖励</strong>–这给我们一个关于算法在环境中表现如何的分数。它被表示为1或0。‘1’表示策略网络做出了正确的举动，‘0’表示错误的举动。换句话说，奖励代表了得失。</li><li><strong>策略</strong>–代理用来决定其动作的算法。这是可以基于模型或无模型的部分。</li></ol>



<p>每个需要RL解决方案的问题都是从模拟代理的环境开始的。接下来，您将构建一个指导代理操作的策略网络。然后，代理可以评估该策略，看其相应的操作是否导致了收益或损失。</p>



<p>这项政策是我们这篇文章的主要讨论点。策略可以是基于模型的，也可以是无模型的。在构建时，我们关心的是如何通过政策梯度来优化政策网络。</p>



<p>PG算法直接尝试优化策略增加奖励。为了理解这些算法，我们必须看一看马尔可夫决策过程(MDP)。</p>



<h3>马尔可夫决策过程/ Q值/ Q学习/深度Q网络</h3>



<p>MDP是一个有固定数量状态的过程，它在每一步随机地从一个状态演化到另一个状态。它从状态A演化到状态B的概率是固定的。</p>



<p>许多具有离散动作的强化学习问题被建模为<strong>马尔可夫决策过程</strong>，代理对下一个转移状态没有初始线索。代理人也不知道奖励原则，所以它必须探索所有可能的状态，开始解码如何调整到一个完美的奖励系统。这将把我们引向我们所谓的Q学习。</p>



<p><strong> Q-Learning算法</strong>改编自<strong> Q-Value </strong>迭代算法，在这种情况下，智能体没有偏好状态和奖励原则的先验知识。q值可以定义为MDP中状态动作值的最优估计。</p>



<p>人们经常说，Q-Learning不能很好地扩展到具有许多状态和动作的大型(甚至中型)MDP。解决方法是近似任何状态-动作对(s，a)的Q值。这被称为近似Q学习。</p>



<p>DeepMind提出使用深度神经网络，这种网络的效果要好得多，特别是对于复杂的问题——不需要使用任何特征工程。用于估计Q值的深度神经网络被称为<strong>深度Q网络(DQN)。</strong>使用DQN进行近似Q学习被称为深度Q学习。</p>



<h2 id="h-difference-between-model-based-and-model-free-reinforcement-learning">基于模型和无模型强化学习的区别</h2>



<p>RL算法主要可以分为两类——<strong>基于模型和无模型</strong>。</p>



<p><strong>基于模型</strong>，顾名思义，有一个代理试图理解它的环境，并根据它与这个环境的交互为它创建一个模型。在这样的系统中，偏好优先于行动的结果，即贪婪的代理人总是试图执行一个行动，以获得最大的回报，而不管该行动可能导致什么。</p>



<p>另一方面，<strong>无模型算法</strong>通过诸如策略梯度、Q-Learning等算法，寻求通过经验学习其行为的结果。换句话说，这种算法将多次执行一个动作，并根据结果调整政策(动作背后的策略)以获得最佳回报。</p>



<p>可以这样想，如果代理可以在实际执行某个动作之前预测该动作的回报，从而计划它应该做什么，那么该算法就是基于模型的。而如果它真的需要执行动作来看看发生了什么并从中学习，那么它就是无模型的。</p>



<p>这导致了这两个类别的不同应用，例如，基于模型的方法可能非常适合下棋或产品装配线上的机器人手臂，其中环境是静态的，最有效地完成任务是我们的主要关注点。然而，在自动驾驶汽车等现实世界的应用中，基于模型的方法可能会促使汽车在更短的时间内碾过行人到达目的地(最大回报)，但无模型的方法会让汽车等到道路畅通无阻(最佳出路)。</p>



<p>为了更好地理解这一点，我们将用一个例子来解释一切。在示例中，<strong>我们将为网球游戏</strong>构建无模型和基于模型的RL。为了建立这个模型，我们需要一个环境来实施政策。然而，我们不会在本文中构建环境，我们将导入一个环境用于我们的程序。</p>



<h2 id="h-pytennis-environment">py网球环境</h2>



<p>我们将使用Pytennis环境来构建一个无模型和基于模型的RL系统。</p>



<p>网球比赛需要以下条件:</p>



<ol><li>两个玩家意味着两个代理。</li><li>网球场——主要环境。</li><li>一个网球。</li><li>代理从左向右(或从右向左)移动。</li></ol>



<p>Pytennis环境规范包括:</p>



<ol><li>有2名代理人(2名球员)带着一个球。</li><li>有一个维度为(x，y)–( 300，500)的网球场</li><li>球被设计成沿直线移动，使得代理A决定B侧(代理B侧)的x1 (0)和x2 (300)之间的目标点，因此它相对于20的FPS显示球50次不同的时间。这使得球从起点到终点直线运动。这也适用于代理b。</li><li>代理A和代理B的移动被限制在(x1= 100到x2 = 600)之间。</li><li>球的移动被限制在y轴上(y1 = 100到y2 = 600)。</li><li>球的移动被限制在x轴上(x1 = 100，到x2 = 600)。</li></ol>



<p>Pytennis是一个模拟真实网球环境的环境。如下图，左图是<a href="https://web.archive.org/web/20221203082542/https://youtu.be/iUYxZ2tYKHw" target="_blank" rel="noreferrer noopener nofollow">无模型</a>py tenness游戏，右图是<a href="https://web.archive.org/web/20221203082542/https://youtu.be/FCwGNRiq9SY" target="_blank" rel="noreferrer noopener nofollow">基于模型</a>。</p>







<h2 id="h-discrete-mathematical-approach-to-playing-tennis-model-free-reinforcement-learning">打网球的离散数学方法——无模型强化学习</h2>



<p>为什么是“打网球的离散数学方法”？因为这个方法是Pytennis环境的逻辑实现。</p>



<p>下面的代码向我们展示了球在草坪上运动的实现。你可以在这里找到源代码<a href="https://web.archive.org/web/20221203082542/https://github.com/elishatofunmi/pytennis-Discrete-Mathematics-Approach-" target="_blank" rel="noreferrer noopener nofollow">。</a></p>



<pre class="hljs"><span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pygame
<span class="hljs-keyword">import</span> sys


<span class="hljs-keyword">from</span> pygame.locals <span class="hljs-keyword">import</span> *
pygame.init()


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Network</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, xmin, xmax, ymin, ymax)</span>:</span>
       <span class="hljs-string">"""
       xmin: 150,
       xmax: 450,
       ymin: 100,
       ymax: 600
       """</span>

       self.StaticDiscipline = {
           <span class="hljs-string">'xmin'</span>: xmin,
           <span class="hljs-string">'xmax'</span>: xmax,
           <span class="hljs-string">'ymin'</span>: ymin,
           <span class="hljs-string">'ymax'</span>: ymax
       }

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">network</span><span class="hljs-params">(self, xsource, ysource=<span class="hljs-number">100</span>, Ynew=<span class="hljs-number">600</span>, divisor=<span class="hljs-number">50</span>)</span>:</span>  
       <span class="hljs-string">"""
       For Network A
       ysource: will always be 100
       xsource: will always be between xmin and xmax (static discipline)
       For Network B
       ysource: will always be 600
       xsource: will always be between xmin and xmax (static discipline)
       """</span>

       <span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:
           ListOfXsourceYSource = []
           Xnew = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(
               self.StaticDiscipline[<span class="hljs-string">'xmin'</span>], self.StaticDiscipline[<span class="hljs-string">'xmax'</span>])], <span class="hljs-number">1</span>)
           

           source = (xsource, ysource)
           target = (Xnew[<span class="hljs-number">0</span>], Ynew)

           
           slope = (ysource - Ynew)/(xsource - Xnew[<span class="hljs-number">0</span>])
           intercept = ysource - (slope*xsource)
           <span class="hljs-keyword">if</span> (slope != np.inf) <span class="hljs-keyword">and</span> (intercept != np.inf):
               <span class="hljs-keyword">break</span>
           <span class="hljs-keyword">else</span>:
               <span class="hljs-keyword">continue</span>

       
       
       XNewList = [xsource]

       <span class="hljs-keyword">if</span> xsource &lt; Xnew:
           differences = Xnew[<span class="hljs-number">0</span>] - xsource
           increment = differences / divisor
           newXval = xsource
           <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(divisor):

               newXval += increment
               XNewList.append(int(newXval))
       <span class="hljs-keyword">else</span>:
           differences = xsource - Xnew[<span class="hljs-number">0</span>]
           decrement = differences / divisor
           newXval = xsource
           <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(divisor):

               newXval -= decrement
               XNewList.append(int(newXval))

       
       yNewList = []
       <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> XNewList:
           findy = (slope * i) + intercept  
           yNewList.append(int(findy))

       ListOfXsourceYSource = [(x, y) <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(XNewList, yNewList)]

       <span class="hljs-keyword">return</span> XNewList, yNewList
</pre>



<p>以下是网络初始化后的工作方式(代理A的网络A和代理B的网络B):</p>



<pre class="hljs">
net = Network(<span class="hljs-number">150</span>, <span class="hljs-number">450</span>, <span class="hljs-number">100</span>, <span class="hljs-number">600</span>)
NetworkA = net.network(<span class="hljs-number">300</span>, ysource=<span class="hljs-number">100</span>, Ynew=<span class="hljs-number">600</span>)  
NetworkB = net.network(<span class="hljs-number">200</span>, ysource=<span class="hljs-number">600</span>, Ynew=<span class="hljs-number">100</span>)  
</pre>



<p>每个网络都以球的运动方向为界。网络A代表代理A，它定义了球从代理A到代理B处沿x轴100到300之间的任何位置的移动。这也适用于网络B(代理B)。</p>



<p>当网络启动时。网络方法离散地为网络A生成50个y点(在y1 = 100和y2 = 600之间)和相应的x点(在x1和代理B侧随机选择的点x2之间)，这也适用于网络B(代理B)。</p>



<p>为了使每个代理的移动自动化，对方代理必须相对于球在相应的方向上移动。这只能通过将球的x位置设置为对方代理的x位置来实现，如下面的代码所示。</p>



<pre class="hljs">playerax = ballx 

playerbx = ballx </pre>



<p>同时，源代理必须从其当前位置移回其默认位置。下面的代码说明了这一点。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DefaultToPosition</span><span class="hljs-params">(x1, x2=<span class="hljs-number">300</span>, divisor=<span class="hljs-number">50</span>)</span>:</span>
   XNewList = []
   <span class="hljs-keyword">if</span> x1 &lt; x2:
       differences = x2 - x1
       increment = differences / divisor
       newXval = x1
       <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(divisor):
           newXval += increment
           XNewList.append(int(np.floor(newXval)))

   <span class="hljs-keyword">else</span>:
       differences = x1 - x2
       decrement = differences / divisor
       newXval = x1
       <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(divisor):
           newXval -= decrement
           XNewList.append(int(np.floor(newXval)))
   <span class="hljs-keyword">return</span> XNewList
</pre>



<p>现在，为了让代理递归地互相玩，这必须在一个循环中运行。每50次计数(球的50帧显示)后，对方球员成为下一名球员。下面的代码将所有这些放在一个循环中。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
   <span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:
       display()
       <span class="hljs-keyword">if</span> nextplayer == <span class="hljs-string">'A'</span>:
           
           <span class="hljs-keyword">if</span> count == <span class="hljs-number">0</span>:
               
               NetworkA = net.network(
                   lastxcoordinate, ysource=<span class="hljs-number">100</span>, Ynew=<span class="hljs-number">600</span>)  
               out = DefaultToPosition(lastxcoordinate)

               

               bally = NetworkA[<span class="hljs-number">1</span>][count]
               playerax = ballx 
               count += <span class="hljs-number">1</span>




           <span class="hljs-keyword">else</span>:
               ballx = NetworkA[<span class="hljs-number">0</span>][count]
               bally = NetworkA[<span class="hljs-number">1</span>][count]
               playerbx = ballx
               playerax = out[count]
               count += <span class="hljs-number">1</span>

           
           <span class="hljs-keyword">if</span> count == <span class="hljs-number">49</span>:
               count = <span class="hljs-number">0</span>
               nextplayer = <span class="hljs-string">'B'</span>
           <span class="hljs-keyword">else</span>:
               nextplayer = <span class="hljs-string">'A'</span>

       <span class="hljs-keyword">else</span>:
           
           <span class="hljs-keyword">if</span> count == <span class="hljs-number">0</span>:
               
               NetworkB = net.network(
                   lastxcoordinate, ysource=<span class="hljs-number">600</span>, Ynew=<span class="hljs-number">100</span>)  
               out = DefaultToPosition(lastxcoordinate)

               
               bally = NetworkB[<span class="hljs-number">1</span>][count]
               playerbx = ballx
               count += <span class="hljs-number">1</span>





           <span class="hljs-keyword">else</span>:
               ballx = NetworkB[<span class="hljs-number">0</span>][count]
               bally = NetworkB[<span class="hljs-number">1</span>][count]
               playerbx = out[count]
               playerax = ballx
               count += <span class="hljs-number">1</span>
           

           
           <span class="hljs-keyword">if</span> count == <span class="hljs-number">49</span>:
               count = <span class="hljs-number">0</span>
               nextplayer = <span class="hljs-string">'A'</span>
           <span class="hljs-keyword">else</span>:
               nextplayer = <span class="hljs-string">'B'</span>

       
       DISPLAYSURF.blit(PLAYERA, (playerax, <span class="hljs-number">50</span>))
       DISPLAYSURF.blit(PLAYERB, (playerbx, <span class="hljs-number">600</span>))
       DISPLAYSURF.blit(ball, (ballx, bally))

       
       lastxcoordinate = ballx

       pygame.display.update()
       fpsClock.tick(FPS)

       <span class="hljs-keyword">for</span> event <span class="hljs-keyword">in</span> pygame.event.get():

           <span class="hljs-keyword">if</span> event.type == QUIT:
               pygame.quit()
               sys.exit()
       <span class="hljs-keyword">return</span></pre>



<p>这是基本的无模型强化学习。它是无模型的，因为你不需要任何形式的学习或建模来让两个代理同时准确地进行游戏。</p>



<h2 id="h-tennis-game-using-deep-q-network-model-based-reinforcement-learning">使用深度Q网络的网球游戏——基于模型的强化学习</h2>



<p>基于模型的强化学习的一个典型例子是深度Q网络。这项工作的源代码可在<a href="https://web.archive.org/web/20221203082542/https://github.com/elishatofunmi/pytennis-Deep-Q-Network-DQN-">这里</a>获得。</p>



<p>下面的代码说明了Deep Q网络，这是这项工作的模型架构。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> Sequential, layers
<span class="hljs-keyword">from</span> keras.optimizers <span class="hljs-keyword">import</span> Adam
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np



<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DQN</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
       self.learning_rate = <span class="hljs-number">0.001</span>
       self.momentum = <span class="hljs-number">0.95</span>
       self.eps_min = <span class="hljs-number">0.1</span>
       self.eps_max = <span class="hljs-number">1.0</span>
       self.eps_decay_steps = <span class="hljs-number">2000000</span>
       self.replay_memory_size = <span class="hljs-number">500</span>
       self.replay_memory = deque([], maxlen=self.replay_memory_size)
       n_steps = <span class="hljs-number">4000000</span> 
       self.training_start = <span class="hljs-number">10000</span> 
       self.training_interval = <span class="hljs-number">4</span> 
       self.save_steps = <span class="hljs-number">1000</span> 
       self.copy_steps = <span class="hljs-number">10000</span> 
       self.discount_rate = <span class="hljs-number">0.99</span>
       self.skip_start = <span class="hljs-number">90</span> 
       self.batch_size = <span class="hljs-number">100</span>
       self.iteration = <span class="hljs-number">0</span> 
       self.done = <span class="hljs-keyword">True</span> 




       self.model = self.DQNmodel()

       <span class="hljs-keyword">return</span>



   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DQNmodel</span><span class="hljs-params">(self)</span>:</span>
       model = Sequential()
       model.add(Dense(<span class="hljs-number">64</span>, input_shape=(<span class="hljs-number">1</span>,), activation=<span class="hljs-string">'relu'</span>))
       model.add(Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>))
       model.add(Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>))
       model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>, optimizer=Adam(lr=self.learning_rate))
       <span class="hljs-keyword">return</span> model


   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample_memories</span><span class="hljs-params">(self, batch_size)</span>:</span>
       indices = np.random.permutation(len(self.replay_memory))[:batch_size]
       cols = [[], [], [], [], []] 
       <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> indices:
           memory = self.replay_memory[idx]
           <span class="hljs-keyword">for</span> col, value <span class="hljs-keyword">in</span> zip(cols, memory):
               col.append(value)
       cols = [np.array(col) <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> cols]
       <span class="hljs-keyword">return</span> (cols[<span class="hljs-number">0</span>], cols[<span class="hljs-number">1</span>], cols[<span class="hljs-number">2</span>].reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>), cols[<span class="hljs-number">3</span>],cols[<span class="hljs-number">4</span>].reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>))


   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">epsilon_greedy</span><span class="hljs-params">(self, q_values, step)</span>:</span>
       self.epsilon = max(self.eps_min, self.eps_max - (self.eps_max-self.eps_min) * step/self.eps_decay_steps)
       <span class="hljs-keyword">if</span> np.random.rand() &lt; self.epsilon:
           <span class="hljs-keyword">return</span> np.random.randint(<span class="hljs-number">10</span>) 
       <span class="hljs-keyword">else</span>:
           <span class="hljs-keyword">return</span> np.argmax(q_values) 
</pre>



<p>在这种情况下，我们需要一个策略网络来控制每个代理沿着x轴的移动。由于这些值是连续的，也就是从(x1 = 100到x2 = 300)，我们不可能有一个预测或处理200个状态的模型。</p>



<p>为了简化这个问题，我们可以把x1和x2拆分成10个状态/ 10个动作，为每个状态定义一个上下限。</p>



<p>注意，我们有10个动作，因为一个状态有10种可能性。</p>



<p>下面的代码说明了每个状态的上限和下限的定义。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_state_from_last_coordinate</span><span class="hljs-params">(self, c)</span>:</span>
       <span class="hljs-string">"""
       cmax: 450
       cmin: 150

       c definately will be between 150 and 450.
       state0 - (150 - 179)
       state1 - (180 - 209)
       state2 - (210 - 239)
       state3 - (240 - 269)
       state4 - (270 - 299)
       state5 - (300 - 329)
       state6 - (330 - 359)
       state7 - (360 - 389)
       state8 - (390 - 419)
       state9 - (420 - 450)
       """</span>
       <span class="hljs-keyword">if</span> c &gt;= <span class="hljs-number">150</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">179</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>
       <span class="hljs-keyword">elif</span> c &gt;= <span class="hljs-number">180</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">209</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>
       <span class="hljs-keyword">elif</span> c &gt;= <span class="hljs-number">210</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">239</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">2</span>
       <span class="hljs-keyword">elif</span> c &gt;= <span class="hljs-number">240</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">269</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">3</span>
       <span class="hljs-keyword">elif</span> c &gt;= <span class="hljs-number">270</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">299</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">4</span>
       <span class="hljs-keyword">elif</span> c &gt;= <span class="hljs-number">300</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">329</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">5</span>
       <span class="hljs-keyword">elif</span> c &gt;= <span class="hljs-number">330</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">359</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">6</span>
       <span class="hljs-keyword">elif</span> c &gt;= <span class="hljs-number">360</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">389</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">7</span>
       <span class="hljs-keyword">elif</span> c &gt;= <span class="hljs-number">390</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">419</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">8</span>
       <span class="hljs-keyword">elif</span> c &gt;= <span class="hljs-number">420</span> <span class="hljs-keyword">and</span> c &lt;= <span class="hljs-number">450</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-number">9</span>
</pre>



<p>实验性地用于这项工作的深度神经网络(DNN)是1个输入(其代表前一状态)、2个各64个神经元的隐藏层和10个神经元的输出层(从10个不同状态的二进制选择)的网络。如下所示:</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DQNmodel</span><span class="hljs-params">(self)</span>:</span>
       model = Sequential()
       model.add(Dense(<span class="hljs-number">64</span>, input_shape=(<span class="hljs-number">1</span>,), activation=<span class="hljs-string">'relu'</span>))
       model.add(Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>))
       model.add(Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>))
       model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>, optimizer=Adam(lr=self.learning_rate))
       <span class="hljs-keyword">return</span> model
</pre>



<p>现在，我们有了一个预测模型下一个状态/动作的DQN模型，Pytennis环境也已经整理出了球的直线运动，让我们继续编写一个函数，根据DQN模型对其下一个状态的预测，由代理执行一个动作。</p>



<p>下面的详细代码说明了代理A如何决定球的方向(代理B这边，反之亦然)。如果代理B能够接到球，这个代码也会评估它。</p>



<pre class="hljs">   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">randomVal</span><span class="hljs-params">(self, action)</span>:</span>
       <span class="hljs-string">"""
       cmax: 450
       cmin: 150

       c definately will be between 150 and 450.
       state0 - (150 - 179)
       state1 - (180 - 209)
       state2 - (210 - 239)
       state3 - (240 - 269)
       state4 - (270 - 299)
       state5 - (300 - 329)
       state6 - (330 - 359)
       state7 - (360 - 389)
       state8 - (390 - 419)
       state9 - (420 - 450)
       """</span>
       <span class="hljs-keyword">if</span> action == <span class="hljs-number">0</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">150</span>, <span class="hljs-number">180</span>)])
       <span class="hljs-keyword">elif</span> action == <span class="hljs-number">1</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">180</span>, <span class="hljs-number">210</span>)])
       <span class="hljs-keyword">elif</span> action == <span class="hljs-number">2</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">210</span>, <span class="hljs-number">240</span>)])
       <span class="hljs-keyword">elif</span> action == <span class="hljs-number">3</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">240</span>, <span class="hljs-number">270</span>)])
       <span class="hljs-keyword">elif</span> action == <span class="hljs-number">4</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">270</span>, <span class="hljs-number">300</span>)])
       <span class="hljs-keyword">elif</span> action == <span class="hljs-number">5</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">300</span>, <span class="hljs-number">330</span>)])
       <span class="hljs-keyword">elif</span> action == <span class="hljs-number">6</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">330</span>, <span class="hljs-number">360</span>)])
       <span class="hljs-keyword">elif</span> action == <span class="hljs-number">7</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">360</span>, <span class="hljs-number">390</span>)])
       <span class="hljs-keyword">elif</span> action == <span class="hljs-number">8</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">390</span>, <span class="hljs-number">420</span>)])
       <span class="hljs-keyword">else</span>:
           val = np.random.choice([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">420</span>, <span class="hljs-number">450</span>)])
       <span class="hljs-keyword">return</span> val

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">stepA</span><span class="hljs-params">(self, action, count=<span class="hljs-number">0</span>)</span>:</span>
       
       <span class="hljs-keyword">if</span> count == <span class="hljs-number">0</span>:
           self.NetworkA = self.net.network(
               self.ballx, ysource=<span class="hljs-number">100</span>, Ynew=<span class="hljs-number">600</span>)  
           self.bally = self.NetworkA[<span class="hljs-number">1</span>][count]
           self.ballx = self.NetworkA[<span class="hljs-number">0</span>][count]

           <span class="hljs-keyword">if</span> self.GeneralReward == <span class="hljs-keyword">True</span>:
               self.playerax = self.randomVal(action)
           <span class="hljs-keyword">else</span>:
               self.playerax = self.ballx







       <span class="hljs-keyword">else</span>:
           self.ballx = self.NetworkA[<span class="hljs-number">0</span>][count]
           self.bally = self.NetworkA[<span class="hljs-number">1</span>][count]

       obsOne = self.evaluate_state_from_last_coordinate(
           int(self.ballx))  
       obsTwo = self.evaluate_state_from_last_coordinate(
           int(self.playerbx))  
       diff = np.abs(self.ballx - self.playerbx)
       obs = obsTwo
       reward = self.evaluate_action(diff)
       done = <span class="hljs-keyword">True</span>
       info = str(diff)

       <span class="hljs-keyword">return</span> obs, reward, done, info


   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_action</span><span class="hljs-params">(self, diff)</span>:</span>

       <span class="hljs-keyword">if</span> (int(diff) &lt;= <span class="hljs-number">30</span>):
           <span class="hljs-keyword">return</span> <span class="hljs-keyword">True</span>
       <span class="hljs-keyword">else</span>:
           <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span>
</pre>



<p>从上面的代码中，当AgentA必须播放时，函数stepA被执行。在比赛时，AgentA使用DQN预测的下一个动作来估计目标(从球的当前位置x1开始，在Agent B处的x2位置，在自己一侧)，通过使用Pytennis环境开发的球轨迹网络来进行自己的移动。</p>



<p>例如，代理A能够通过使用函数<strong> randomVal </strong>在代理B侧获得一个精确的点x2，如上所示，随机选择一个由DQN给出的动作所限定的坐标x2。</p>



<p>最后，函数stepA通过使用函数<strong> evaluate_action </strong>来评估AgentB对目标点x2的响应。函数<strong> evaluate_action </strong>定义了AgentB应该受到惩罚还是奖励。正如对AgentA到AgentB的描述一样，它也适用于AgentB到AgentA(不同变量名的相同代码)。</p>



<p>现在我们已经正确定义了策略、奖励、环境、状态和动作，我们可以继续递归地让两个代理相互玩游戏。</p>



<p>下面的代码显示了在50个球显示后每个代理如何轮流。注意，对于每一个球的展示，DQN都要决定把球扔给下一个代理去哪里玩。</p>



<pre class="hljs"><span class="hljs-keyword">while</span> iteration &lt; iterations:

           self.display()
           self.randNumLabelA = self.myFontA.render(
               <span class="hljs-string">'A (Win): '</span>+str(self.updateRewardA) + <span class="hljs-string">', A(loss): '</span>+str(self.lossA), <span class="hljs-number">1</span>, self.BLACK)
           self.randNumLabelB = self.myFontB.render(
               <span class="hljs-string">'B (Win): '</span>+str(self.updateRewardB) + <span class="hljs-string">', B(loss): '</span> + str(self.lossB), <span class="hljs-number">1</span>, self.BLACK)
           self.randNumLabelIter = self.myFontIter.render(
               <span class="hljs-string">'Iterations: '</span>+str(self.updateIter), <span class="hljs-number">1</span>, self.BLACK)

           <span class="hljs-keyword">if</span> nextplayer == <span class="hljs-string">'A'</span>:

               <span class="hljs-keyword">if</span> count == <span class="hljs-number">0</span>:
                   
                   q_valueA = self.AgentA.model.predict([stateA])
                   actionA = self.AgentA.epsilon_greedy(q_valueA, iteration)

                   
                   obsA, rewardA, doneA, infoA = self.stepA(
                       action=actionA, count=count)
                   next_stateA = actionA

                   
                   self.AgentA.replay_memory.append(
                       (stateA, actionA, rewardA, next_stateA, <span class="hljs-number">1.0</span> - doneA))
                   stateA = next_stateA

               <span class="hljs-keyword">elif</span> count == <span class="hljs-number">49</span>:

                   
                   q_valueA = self.AgentA.model.predict([stateA])
                   actionA = self.AgentA.epsilon_greedy(q_valueA, iteration)
                   obsA, rewardA, doneA, infoA = self.stepA(
                       action=actionA, count=count)
                   next_stateA = actionA

                   self.updateRewardA += rewardA
                   self.computeLossA(rewardA)

                   
                   self.AgentA.replay_memory.append(
                       (stateA, actionA, rewardA, next_stateA, <span class="hljs-number">1.0</span> - doneA))

                   
                   <span class="hljs-keyword">if</span> rewardA == <span class="hljs-number">0</span>:
                       self.restart = <span class="hljs-keyword">True</span>
                       time.sleep(<span class="hljs-number">0.5</span>)
                       nextplayer = <span class="hljs-string">'B'</span>
                       self.GeneralReward = <span class="hljs-keyword">False</span>
                   <span class="hljs-keyword">else</span>:
                       self.restart = <span class="hljs-keyword">False</span>
                       self.GeneralReward = <span class="hljs-keyword">True</span>

                   
                   X_state_val, X_action_val, rewards, X_next_state_val, continues = (
                       self.AgentA.sample_memories(self.AgentA.batch_size))
                   next_q_values = self.AgentA.model.predict(
                       [X_next_state_val])
                   max_next_q_values = np.max(
                       next_q_values, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-keyword">True</span>)
                   y_val = rewards + continues * self.AgentA.discount_rate * max_next_q_values

                   
                   self.AgentA.model.fit(X_state_val, tf.keras.utils.to_categorical(
                       X_next_state_val, num_classes=<span class="hljs-number">10</span>), verbose=<span class="hljs-number">0</span>)

                   nextplayer = <span class="hljs-string">'B'</span>
                   self.updateIter += <span class="hljs-number">1</span>

                   count = <span class="hljs-number">0</span>
                   

               <span class="hljs-keyword">else</span>:
                   
                   q_valueA = self.AgentA.model.predict([stateA])
                   actionA = self.AgentA.epsilon_greedy(q_valueA, iteration)

                   
                   obsA, rewardA, doneA, infoA = self.stepA(
                       action=actionA, count=count)
                   next_stateA = actionA

                   
                   self.AgentA.replay_memory.append(
                       (stateA, actionA, rewardA, next_stateA, <span class="hljs-number">1.0</span> - doneA))
                   stateA = next_stateA

               <span class="hljs-keyword">if</span> nextplayer == <span class="hljs-string">'A'</span>:
                   count += <span class="hljs-number">1</span>
               <span class="hljs-keyword">else</span>:
                   count = <span class="hljs-number">0</span>

           <span class="hljs-keyword">else</span>:
               <span class="hljs-keyword">if</span> count == <span class="hljs-number">0</span>:
                   
                   q_valueB = self.AgentB.model.predict([stateB])
                   actionB = self.AgentB.epsilon_greedy(q_valueB, iteration)

                   
                   obsB, rewardB, doneB, infoB = self.stepB(
                       action=actionB, count=count)
                   next_stateB = actionB

                   
                   self.AgentB.replay_memory.append(
                       (stateB, actionB, rewardB, next_stateB, <span class="hljs-number">1.0</span> - doneB))
                   stateB = next_stateB

               <span class="hljs-keyword">elif</span> count == <span class="hljs-number">49</span>:

                   
                   q_valueB = self.AgentB.model.predict([stateB])
                   actionB = self.AgentB.epsilon_greedy(q_valueB, iteration)

                   
                   obs, reward, done, info = self.stepB(
                       action=actionB, count=count)
                   next_stateB = actionB

                   
                   self.AgentB.replay_memory.append(
                       (stateB, actionB, rewardB, next_stateB, <span class="hljs-number">1.0</span> - doneB))

                   stateB = next_stateB
                   self.updateRewardB += rewardB
                   self.computeLossB(rewardB)

                   
                   <span class="hljs-keyword">if</span> rewardB == <span class="hljs-number">0</span>:
                       self.restart = <span class="hljs-keyword">True</span>
                       time.sleep(<span class="hljs-number">0.5</span>)
                       self.GeneralReward = <span class="hljs-keyword">False</span>
                       nextplayer = <span class="hljs-string">'A'</span>
                   <span class="hljs-keyword">else</span>:
                       self.restart = <span class="hljs-keyword">False</span>
                       self.GeneralReward = <span class="hljs-keyword">True</span>

                   
                   X_state_val, X_action_val, rewards, X_next_state_val, continues = (
                       self.AgentB.sample_memories(self.AgentB.batch_size))
                   next_q_values = self.AgentB.model.predict(
                       [X_next_state_val])
                   max_next_q_values = np.max(
                       next_q_values, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-keyword">True</span>)
                   y_val = rewards + continues * self.AgentB.discount_rate * max_next_q_values

                   
                   self.AgentB.model.fit(X_state_val, tf.keras.utils.to_categorical(
                       X_next_state_val, num_classes=<span class="hljs-number">10</span>), verbose=<span class="hljs-number">0</span>)

                   nextplayer = <span class="hljs-string">'A'</span>
                   self.updateIter += <span class="hljs-number">1</span>
                   

               <span class="hljs-keyword">else</span>:
                   
                   q_valueB = self.AgentB.model.predict([stateB])
                   actionB = self.AgentB.epsilon_greedy(q_valueB, iteration)

                   
                   obsB, rewardB, doneB, infoB = self.stepB(
                       action=actionB, count=count)
                   next_stateB = actionB

                   
                   self.AgentB.replay_memory.append(
                       (stateB, actionB, rewardB, next_stateB, <span class="hljs-number">1.0</span> - doneB))
                   tateB = next_stateB

               <span class="hljs-keyword">if</span> nextplayer == <span class="hljs-string">'B'</span>:
                   count += <span class="hljs-number">1</span>
               <span class="hljs-keyword">else</span>:
                   count = <span class="hljs-number">0</span>

           iteration += <span class="hljs-number">1</span>
</pre>



<h2 id="h-comparison-evaluation">比较/评估</h2>



<p>玩了这个无模型和基于模型的游戏后，我们需要注意以下几点差异:</p>



<p id="separator-block_61adffc912727" class="block-separator block-separator--10"> </p>



<div id="medium-table-block_61adfbe612713" class="block-medium-table c-table__outer-wrapper ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">序列号</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">无模型</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">基于模型的</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>奖励不入账(因为这是自动化的，奖励= 1) </p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>奖励占</p> </div></td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>【无建模(不需要决策策略)</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>【造型要求】(政策网)</p> </div></td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>这不需要利用初始状态来预测下一个状态</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>这就需要使用策略网络</p> </div>使用初始状态来预测下一个状态</td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>失球率相对于时间为零</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>失球率相对于时间趋近于零</p> </div></td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>



<p id="separator-block_600ab90635544" class="block-separator block-separator--25"> </p>



<p>如果你感兴趣，下面的视频展示了这两种打网球的技巧:</p>



<p>1.无模型</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><p class="wp-block-embed__wrapper"><iframe loading="lazy" title="A reinforcement learning based tennis game - Discrete mathematics approach" src="https://web.archive.org/web/20221203082542if_/https://www.youtube.com/embed/iUYxZ2tYKHw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p></figure>



<p>2.基于模型的</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><p class="wp-block-embed__wrapper"><iframe loading="lazy" title="An RL implementation of pytennis using Deep Q Network (DQN) - early stage of Learning" src="https://web.archive.org/web/20221203082542if_/https://www.youtube.com/embed/FCwGNRiq9SY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p></figure>



<h2 id="h-conclusion">结论</h2>



<p>与自动驾驶汽车相比，网球可能很简单，但希望这个例子向你展示了一些你不知道的关于RL的事情。</p>



<p>无模型RL和基于模型RL的主要区别是策略网络，基于模型RL需要策略网络，无模型不需要策略网络。</p>



<p>值得注意的是，通常情况下，基于模型的RL需要大量的时间让DNN在不出错的情况下完美地学习状态。</p>



<p>但是每种技术都有它的缺点和优点，选择正确的技术取决于你到底需要你的程序做什么。</p>



<p>感谢阅读，我留下了一些额外的参考资料，如果你想进一步探讨这个话题，可以参考。</p>



<h3>参考</h3>



<ol><li>AlphaGo纪录片:<a href="https://web.archive.org/web/20221203082542/https://www.youtube.com/watch?v=WXuK6gekU1Y" target="_blank" rel="noreferrer noopener nofollow">https://www.youtube.com/watch?v=WXuK6gekU1Y</a></li><li>强化学习环境列表:<a href="https://web.archive.org/web/20221203082542/https://medium.com/@mauriciofadelargerich/reinforcement-learning-environments-cff767bc241f" target="_blank" rel="noreferrer noopener nofollow">https://medium . com/@ mauriciofadelargerich/reinforcement-learning-environments-CFF 767 BC 241 f</a></li><li>创建自己的强化学习环境:<a href="https://web.archive.org/web/20221203082542/https://towardsdatascience.com/create-your-own-reinforcement-learning-environment-beb12f4151ef" target="_blank" rel="noreferrer noopener nofollow">https://towards data science . com/create-your-own-reinforcement-learning-environment-beb 12 f 4151 ef</a></li><li>RL环境的类型:<a href="https://web.archive.org/web/20221203082542/https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781838649777/1/ch01lvl1sec14/types-of-rl-environment" target="_blank" rel="noreferrer noopener nofollow">https://subscription . packtpub . com/book/big _ data _ and _ business _ intelligence/9781838649777/1/ch 01 LV 1 sec 14/types-of-RL-environment</a></li><li>基于模型的深度Q网:<a href="https://web.archive.org/web/20221203082542/https://github.com/elishatofunmi/pytennis-Deep-Q-Network-DQN-" target="_blank" rel="noreferrer noopener nofollow">https://github . com/elishatofunmi/py tennis-Deep-Q-Network-DQN</a></li><li>离散数学方法youtube视频:<a href="https://web.archive.org/web/20221203082542/https://youtu.be/iUYxZ2tYKHw" target="_blank" rel="noreferrer noopener nofollow">https://youtu.be/iUYxZ2tYKHw</a></li><li>深Q网途径YouTube视频:<a href="https://web.archive.org/web/20221203082542/https://youtu.be/FCwGNRiq9SY" target="_blank" rel="noreferrer noopener nofollow">https://youtu.be/FCwGNRiq9SY</a></li><li>无模型离散数学实现:<a href="https://web.archive.org/web/20221203082542/https://github.com/elishatofunmi/pytennis-Discrete-Mathematics-Approach-" target="_blank" rel="noreferrer noopener nofollow">https://github . com/elishatofunmi/py tennis-离散-数学-方法- </a></li><li>用scikit-learn和TensorFlow动手进行机器学习:<a href="https://web.archive.org/web/20221203082542/https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291" target="_blank" rel="noreferrer noopener nofollow">https://www . Amazon . com/Hands-Machine-Learning-Scikit-Learn-tensor flow/DP/1491962291</a></li></ol>
        </div>
        
    </div>    
</body>
</html>