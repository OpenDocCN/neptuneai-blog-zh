<html>
<head>
<title>Building and Managing Data Science Pipelines with Kedro </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>使用Kedro构建和管理数据科学管道</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/data-science-pipelines-with-kedro#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/data-science-pipelines-with-kedro#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>通过机器学习的强大功能，数据科学有望在所有行业产生巨大的商业价值。然而，<a href="https://web.archive.org/web/20220926103904/https://www.gartner.com/en/newsroom/press-releases/2021-11-22-gartner-forecasts-worldwide-artificial-intelligence-software-market-to-reach-62-billion-in-2022" target="_blank" rel="noreferrer noopener nofollow"> Gartner </a>最近的一份报告显示，尽管有足够的数据和意图，但大多数数据科学项目都无法超越实验。</p>



<p>为了释放数据科学的全部潜力，机器学习模型需要作为可扩展的端到端系统部署在现实世界中，并自动管理。最大化数据科学影响的愿望导致了机器学习操作(<a href="/web/20220926103904/https://neptune.ai/blog/mlops" target="_blank" rel="noreferrer noopener"> MLOps </a>)的兴起，这是一套用于构建、维护和监控机器学习生产系统的最佳实践。</p>






<p>作为MLOps的一部分，数据科学管道形成了有效部署和使用机器学习模型的基础。本文探讨了数据科学管道(专注于机器学习)背后的<strong>概念，以及<strong>如何利用开源框架Kedro来创建一个</strong>。</strong></p>


<p id="block_6247099198c65" class="separator separator-15"/>





<h2>什么是数据科学管道？</h2>



<p>顾名思义，数据科学管道包括各种组件的无缝链接，以促进数据按预期顺利移动。</p>



<p>如果我们在网上搜索数据科学管道，我们会看到令人眼花缭乱的管道设计。好消息是，我们可以将这些管道归结为六个核心要素:</p>


<div class="case-study-numbered-list">
    <h2/>
    <ul>
                    <li><span> 1 </span>数据检索和摄取</li>
                    <li><span> 2 </span>数据准备</li>
                    <li><span> 3 </span>模特培训</li>
                    <li><span> 4 </span>模型评估和调整</li>
                    <li><span> 5 </span>模型部署</li>
                    <li><span> 6 </span>监控</li>
            </ul>
</div>


<p id="block_6245f5bb486ac" class="separator separator-20">上图说明了这六个组件是如何连接起来形成一个管道的，在这个管道中，机器学习模型可以在生产环境中提供最佳结果。</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img data-attachment-id="64153" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro2" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro2.png?fit=1476%2C576&amp;ssl=1" data-orig-size="1476,576" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro2" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro2.png?fit=300%2C117&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro2.png?fit=1024%2C400&amp;ssl=1" src="../Images/163e6ef25189539e55f88db18fe345e2.png" alt="Data science pipeline" class="wp-image-64153 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro2.png?resize=817%2C319&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro2.png?resize=817%2C319&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="64153" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro2" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro2.png?fit=1476%2C576&amp;ssl=1" data-orig-size="1476,576" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro2" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro2.png?fit=300%2C117&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro2.png?fit=1024%2C400&amp;ssl=1" src="../Images/163e6ef25189539e55f88db18fe345e2.png" alt="Data science pipeline" class="wp-image-64153" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro2.png?resize=817%2C319&amp;ssl=1"/></noscript><figcaption><em>Data science pipeline | Source: Author</em></figcaption></figure></div>



<p>让我们仔细看看这些组件:</p>






<p>(1)数据检索和摄取</p>



<h3>数据是所有数据科学项目的生命线，因此第一步是从各种数据源中识别相关的原始数据。</h3>


<div class="custom-point-list">
<ul><li>这一步比听起来更具挑战性，因为数据通常以不同的格式存储在不同的孤岛中(例如，第三方来源、内部数据库)。</li><li>一旦所需的数据集被正确识别，它们就被提取并整合到一个中心位置，以供下游处理。</li><li>(2)数据准备</li></ul>
</div>


<h3>来自数据的洞察力的质量取决于数据质量本身。因此，数据准备花费最多的时间和精力也就不足为奇了。</h3>


<div class="custom-point-list">
<ul><li>用于数据准备的技术基于手边的任务(例如，分类、回归等)。)并包括<a href="/web/20220926103904/https://neptune.ai/blog/data-cleaning-process" target="_blank" rel="noreferrer noopener">数据清理</a>、数据转换、特征选择和<a href="/web/20220926103904/https://neptune.ai/blog/feature-engineering-in-machine-learning" target="_blank" rel="noreferrer noopener">特征工程</a>等步骤。</li><li>(3)模型训练</li></ul>
</div>


<h3>现在，我们已经准备好使用准备好的数据在训练数据集上运行机器学习。</h3>



<p><a href="/web/20220926103904/https://neptune.ai/blog/top-10-best-machine-learning-tools-for-model-training" target="_blank" rel="noreferrer noopener">模型训练</a>是模型在数据中潜行，学习潜在的模式。经过训练的模型将被表示为从数据中捕获模式信息的统计函数。</p>


<div class="custom-point-list">
<ul><li>要实现的机器学习模型的选择取决于实际任务、数据的性质和业务需求。</li><li>(4)模型评估和调整</li></ul>
</div>


<h3>一旦模型训练完成，评估其表现是至关重要的。评估是通过让模型对它以前从未见过的数据集运行预测来完成的。它代表了它在现实世界中的表现。</h3>


<div class="custom-point-list">
<ul><li>评估指标有助于指导优化模型性能所需的更改(例如，选择不同的模型、调整超参数配置等)。).</li><li>机器学习开发周期是高度迭代的，因为有许多方法可以基于度量和错误分析来调整模型。</li><li>(5)部署</li></ul>
</div>

<p id="block_6245f5f0486ad" class="separator separator-10">一旦我们确信我们的模型可以提供出色的预测，我们就通过将它部署到生产环境中来将模型暴露给实际的操作。</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><a href="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro3" target="_blank" rel="noopener"><img data-attachment-id="64154" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro3" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro3.png?fit=1476%2C680&amp;ssl=1" data-orig-size="1476,680" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro3" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro3.png?fit=300%2C138&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro3.png?fit=1024%2C472&amp;ssl=1" src="../Images/1f911eb1be6da8a461292a52af81372d.png" alt="Example of a model management system using Neptune.ai" class="wp-image-64154 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro3.png?resize=805%2C371&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro3.png?resize=805%2C371&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="64154" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro3" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro3.png?fit=1476%2C680&amp;ssl=1" data-orig-size="1476,680" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro3" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro3.png?fit=300%2C138&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro3.png?fit=1024%2C472&amp;ssl=1" src="../Images/1f911eb1be6da8a461292a52af81372d.png" alt="Example of a model management system using Neptune.ai" class="wp-image-64154" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro3.png?resize=805%2C371&amp;ssl=1"/></noscript></a><figcaption><em>Example of a model management system using Neptune.ai | </em><a href="https://web.archive.org/web/20220926103904/https://app.neptune.ai/common/example-project-tensorflow-keras/experiments?compare=JwGgzEA&amp;split=tbl&amp;dash=artifacts&amp;viewId=44675986-88f9-4182-843f-49b9cfa48599&amp;base=TFKERAS-9&amp;to=TFKERAS-12" target="_blank" rel="noreferrer noopener"><em>Source</em></a></figcaption></figure></div>



<h3>模型部署(又名服务)是将模型集成到生产环境中的关键步骤，在生产环境中，模型接收实际数据并为数据驱动的业务决策生成输出。</h3>


<div class="custom-point-list">
<ul><li>(6)监测</li><li>虽然看起来我们已经成功地完成了模型部署，但是我们离完成还有一段距离。</li></ul>
</div>





<h3>为了保持一个强大且持续运行的数据科学管道，最重要的是我们要监控它在部署后的表现。</h3>



<p>除了模型性能和数据质量，监控指标还可以包括操作方面，如<a href="https://web.archive.org/web/20220926103904/https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#hardware-consumption" target="_blank" rel="noreferrer noopener">资源利用率和模型延迟</a>。</p>


<div class="custom-point-list">
<ul><li>在成熟的MLOps设置中，我们可以根据预测性能或新数据的可用性触发新的模型训练迭代。</li><li>Beyond model performance and data quality, the monitoring metrics can also include operational aspects such as <a href="https://web.archive.org/web/20220926103904/https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#hardware-consumption" target="_blank" rel="noreferrer noopener">resource utilization and model latency</a>.</li><li>需要强调的重要一点是，数据科学管道是动态的，需要迭代改进，以确保持续的稳健性和准确性。</li></ul>
</div>

<p id="block_6245f5f8486ae" class="separator separator-10">在前面的六个核心管道组件的图表中，我们看到了几个虚线箭头连接到管道的前面的元素。这些箭头反映了我们更新组件以响应监控指标变化时所需的各种迭代。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://web.archive.org/web/20220926103904/https://neptune.ai/blog/ml-model-monitoring-best-tools/arize-drift-monitor" target="_blank" rel="noopener"><img data-attachment-id="53676" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/blog/ml-model-monitoring-best-tools/attachment/arize-drift-monitor" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Arize-drift-monitor.jpg?fit=3580%2C1894&amp;ssl=1" data-orig-size="3580,1894" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Arize-drift-monitor" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Arize-drift-monitor.jpg?fit=300%2C159&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Arize-drift-monitor.jpg?fit=1024%2C542&amp;ssl=1" src="../Images/9fe25bb62d6b0b831ec7bafa46517b21.png" alt="Arize-drift-monitor" class="wp-image-53676 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Arize-drift-monitor.jpg?resize=1024%2C542&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Arize-drift-monitor.jpg?resize=1024%2C542&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="53676" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/blog/ml-model-monitoring-best-tools/attachment/arize-drift-monitor" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Arize-drift-monitor.jpg?fit=3580%2C1894&amp;ssl=1" data-orig-size="3580,1894" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Arize-drift-monitor" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Arize-drift-monitor.jpg?fit=300%2C159&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Arize-drift-monitor.jpg?fit=1024%2C542&amp;ssl=1" src="../Images/9fe25bb62d6b0b831ec7bafa46517b21.png" alt="Arize-drift-monitor" class="wp-image-53676" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Arize-drift-monitor.jpg?resize=1024%2C542&amp;ssl=1"/></noscript></a><figcaption><em><em>Example drift monitor in Arize | <a href="https://web.archive.org/web/20220926103904/https://arize.com/" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></em></figcaption></figure></div>



<p>例如，假设输入客户数据的年龄分布在最近几周越来越不均匀。在这种情况下，团队应该考虑检查数据准备步骤，或者根据新数据重新训练模型。</p>



<p>数据科学管道的重要性</p>



<p>首先理解数据科学管道的重要性和好处至关重要，因为它们需要努力构建。</p>



<h2>这些管道的目标是创建一个系统化的工作流程，将原始数据以自动化和可重复的方式转化为可操作的业务洞察。</h2>



<p>它的重要性来自于自动化数据科学开发周期中的手动步骤，这些步骤是重复的、劳动密集型的、容易出错的和耗时的。</p>



<p>这种简化的数据移动允许数据科学家专注于他们最擅长的事情，即改进数据和模型，而不用担心数据流、算法更新和模型部署的工程方面。</p>



<p>以下是数据科学管道可以带来的商业优势:</p>



<p>加快数据驱动的决策制定，以响应不断变化的业务需求和客户偏好。</p>



<p>通过将数据从孤岛整合到单个目的地，释放新的机会和全面的洞察力。</p>


<div class="custom-point-list">
<ul><li>确保数据洞察力的一致性、可再现性和质量。</li><li>简化了向机器学习系统中引入新的商业想法和需求。</li><li>数据科学管道用例</li><li>数据科学管道是行业不可知的，因此我们可以预计它们可能会在不同的领域带来巨大的好处。</li></ul>
</div>


<h2>以下是行业中实施的数据科学管道的一些真实示例:</h2>



<p><strong>软件行业</strong> — <a href="https://web.archive.org/web/20220926103904/https://dropbox.tech/machine-learning/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning" target="_blank" rel="noreferrer noopener nofollow"> Dropbox </a>建立了一个现代光学字符识别管道，以创建移动文档扫描功能。</p>



<p><strong>运输行业</strong> — <a href="https://web.archive.org/web/20220926103904/https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59" target="_blank" rel="noreferrer noopener nofollow"> Lyft </a>利用内部管道框架来加速机器学习和数据协调，以实现定价、位置和到达时间估计等核心乘车功能。<br/></p>


<div class="custom-point-list">
<ul><li><strong>保险行业</strong> — <a href="https://web.archive.org/web/20220926103904/https://cloud.google.com/blog/products/ai-machine-learning/usaa-and-google-cloud-transform-insurance-operations" target="_blank" rel="noreferrer noopener nofollow"> USAA </a>利用机器学习管道，通过创建一项服务来绘制应该修理或更换的受损车辆零件的照片，从而改善汽车索赔处理。<br/></li></ul>
</div>


<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img data-attachment-id="64161" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro9" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro9.gif?fit=650%2C325&amp;ssl=1" data-orig-size="650,325" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro9" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro9.gif?fit=300%2C150&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro9.gif?fit=650%2C325&amp;ssl=1" src="../Images/dd82f421d8fa890e70181e5924c56bc7.png" alt="Creating a modern ocr pipeline using computer vision and deep learning" class="wp-image-64161 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro9.gif?resize=774%2C387&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro9.gif?resize=774%2C387&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="64161" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro9" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro9.gif?fit=650%2C325&amp;ssl=1" data-orig-size="650,325" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro9" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro9.gif?fit=300%2C150&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro9.gif?fit=650%2C325&amp;ssl=1" src="../Images/dd82f421d8fa890e70181e5924c56bc7.png" alt="Creating a modern ocr pipeline using computer vision and deep learning" class="wp-image-64161" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro9.gif?resize=774%2C387&amp;ssl=1"/></noscript><figcaption><em>Creating a modern OCR pipeline using computer vision and deep learning | <a href="https://web.archive.org/web/20220926103904/https://dropbox.tech/machine-learning/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<div class="custom-point-list">
<ul><li><strong>医疗保健行业</strong> —英国<a href="https://web.archive.org/web/20220926103904/https://link.springer.com/article/10.1007/s10994-020-05921-4#Ack1" target="_blank" rel="noreferrer noopener nofollow">国家卫生系统(NHS) </a>开发并部署了一套机器学习管道，作为新冠肺炎国家医院容量规划系统的一部分。</li><li>凯卓是什么？</li><li>看到数据科学管道可以带来的巨大价值后，让我们探索如何实现它们，并将这些理论优势转化为系统现实。</li></ul>
</div>


<h2>对管道重要性的认识促进了一些有效建设和管理管道的框架的发展。Kedro就是这样一个框架，这是本文的重点。</h2>



<p><a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/stable/01_introduction/01_introduction.html" target="_blank" rel="noreferrer noopener nofollow"> Kedro </a> <strong> </strong>是一个开源的Python框架，用于创建可复制、可维护和模块化的<strong> </strong>数据科学代码。该框架有助于加速数据流水线，增强数据科学原型，并促进流水线的可再现性。</p>



<p>Kedro将软件工程概念应用于开发生产就绪的机器学习代码，以减少成功部署模型所需的时间和精力。</p>



<p>它的影响是通过消除低质量代码的重新设计工作和无缝协作的项目模板的标准化来实现的。</p>



<p>让我们看看Kedro中的应用概念:</p>



<p><strong>再现性</strong>:准确一致地跨不同管道运行和环境重新创建工作流程步骤的能力。<br/></p>



<p>模块化:将大的代码块分解成更小的、独立的、易于理解的单元，这些单元易于测试和修改。<br/></p>


<div class="custom-point-list">
<ul><li><strong>可维护性</strong>:使用标准代码模板，允许团队成员容易理解和维护任何项目的设置，从而促进协作开发的标准化方法<br/></li><li><strong>版本控制</strong>:精确跟踪每个项目的每个管道运行中使用的数据、配置和机器学习模型。<br/></li><li><strong>文档</strong>:清晰结构化的代码信息，便于阅读和理解。<br/></li><li><strong>无缝打包</strong>:允许数据科学项目被记录并有效地运送到生产中(使用诸如Airflow或Docker之类的工具)。</li><li>为什么是凯卓？</li><li>将数据科学项目从试点开发带入生产的过程充满了挑战。</li></ul>
</div>


<h2>一些重大困难包括:</h2>



<p>需要为生产环境重写代码，导致项目严重延迟。</p>



<p>项目结构可能是杂乱无章和不连贯的，这给协作带来了挑战。</p>


<div class="custom-point-list">
<ul><li>难以追踪的数据流。</li><li>过于冗长且难以测试或重用的功能。</li><li>难以理解的函数之间的关系。</li><li>QuantumBlack团队开发了Kedro来应对上述挑战。它诞生于一个共同的信念，即数据科学代码应该从一开始就为生产做好准备，因为它是成功管道部署的严格起点。</li><li>真实世界中的凯卓</li></ul>
</div>


<p>和所有项目一样，证据就在布丁里。以下是Kedro在现实应用中成功使用的一些例子:</p>



<h2><a href="https://web.archive.org/web/20220926103904/https://github.com/nasa/ML-airport-taxi-out" target="_blank" rel="noreferrer noopener nofollow">美国宇航局</a>利用Kedro作为其基于云的预测引擎的一部分，预测空域内受阻和无障碍滑行的持续时间。<br/></h2>



<p>在Kedro的帮助下，JungleScout 加快了18次销售估算模型的培训和评审。<br/></p>


<div class="custom-point-list">
<ul><li><a href="https://web.archive.org/web/20220926103904/https://medium.com/life-at-telkomsel/how-we-build-a-production-grade-data-pipeline-7004e56c8c98" target="_blank" rel="noreferrer noopener nofollow"> Telkomsel </a>使用Kedro运行数百个功能工程任务，并在其生产环境中服务数十个机器学习模型。<br/></li><li><a href="https://web.archive.org/web/20220926103904/https://medium.com/quantumblack/element-ai-uses-kedro-to-apply-research-and-develop-enterprise-ai-models-bbbf2e3ff722" target="_blank" rel="noreferrer noopener nofollow">elemental</a>通过在他们的调度软件中利用Kedro来测量历史性能和创建回放场景，提高了他们的工作效率。</li><li>为异常检测构建数据科学管道</li><li>现在我们已经了解了Kedro，让我们进入激动人心的部分，通过一个实际动手的例子来工作。</li></ul>
</div>


<h2>该项目用例围绕财务欺诈检测展开。<strong> </strong>我们将使用<strong>隔离森林</strong>作为我们的机器学习模型，构建一个<strong>异常检测管道</strong>，来识别信用卡交易中的异常。</h2>



<p><a href="https://web.archive.org/web/20220926103904/https://github.com/Fraud-Detection-Handbook" target="_blank" rel="noreferrer noopener nofollow">信用卡交易数据</a>从Worldline和<a href="https://web.archive.org/web/20220926103904/http://mlg.ulb.ac.be/" target="_blank" rel="noreferrer noopener nofollow">机器学习小组</a>的合作中获得。这是对真实世界信用卡交易的真实模拟，旨在包括复杂的欺诈检测问题。</p>



<p>下面的可视化展示了我们最终的异常检测管道将会是什么样子，并作为我们将在以下部分构建的<strong>蓝图</strong>。</p>



<p>The<a href="https://web.archive.org/web/20220926103904/https://github.com/Fraud-Detection-Handbook" target="_blank" rel="noreferrer noopener nofollow"> credit card transaction data</a> is obtained from the collaboration between Worldline and<a href="https://web.archive.org/web/20220926103904/http://mlg.ulb.ac.be/" target="_blank" rel="noreferrer noopener nofollow"> Machine Learning Group</a>. It is a realistic simulation of real-world credit card transactions and has been designed to include complicated fraud detection issues.</p>



<p>随意查看这个项目的<a href="https://web.archive.org/web/20220926103904/https://github.com/kennethleungty/Anomaly-Detection-Pipeline-Kedro" target="_blank" rel="noreferrer noopener nofollow"> <strong> GitHub repo </strong> </a>来跟随这个演练。</p>


<p id="block_6245f626486af" class="separator separator-10">Kedro管道</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img data-attachment-id="64155" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro4" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro4.png?fit=1586%2C397&amp;ssl=1" data-orig-size="1586,397" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro4" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro4.png?fit=300%2C75&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro4.png?fit=1024%2C256&amp;ssl=1" src="../Images/41be1ebfe14fa1afd961efc3800a82b2.png" alt="Anomaly detection pipeline" class="wp-image-64155 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro4.png?resize=824%2C206&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro4.png?resize=824%2C206&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="64155" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro4" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro4.png?fit=1586%2C397&amp;ssl=1" data-orig-size="1586,397" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro4" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro4.png?fit=300%2C75&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro4.png?fit=1024%2C256&amp;ssl=1" src="../Images/41be1ebfe14fa1afd961efc3800a82b2.png" alt="Anomaly detection pipeline" class="wp-image-64155" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro4.png?resize=824%2C206&amp;ssl=1"/></noscript><figcaption><em> Anomaly detection pipeline | Source: Author</em></figcaption></figure></div>



<p>步骤1:安装Kedro和Kedro-Viz</p>






<h3>建议创建一个虚拟环境，以便每个项目都有其独立的环境和相关的依赖项。</h3>



<h4>要使用Kedro，Kedro官方文档建议用户<a href="https://web.archive.org/web/20220926103904/https://www.anaconda.com/products/individual#Downloads" target="_blank" rel="noreferrer noopener nofollow">下载并安装Anaconda </a>。</h4>



<p>因为我的Python版本是3.10以上，Anaconda使得在兼容Kedro需求的版本(即，在撰写本文时Python 3.6-3.8)上创建环境(使用<strong> conda </strong>而不是<strong> venv </strong>)变得很容易。</p>



<p>特别是，这是生成我们的Kedro环境的命令(在Anaconda Powershell提示符中):</p>



<p>一旦建立了虚拟环境并用conda activate kedro-env激活，我们就可以使用<em> pip </em>来安装kedro和Kedro-Viz插件:</p>



<p>我们可以通过将目录更改到我们的项目文件夹，然后输入kedro info来检查Kedro是否正确安装。如果安装正确，我们应该会看到以下输出:</p>



<pre class="hljs">conda create --name kedro-env python=<span class="hljs-number">3.7</span> -y</pre>



<p>此时，我们可以安装项目所需的其他包:</p>



<pre class="hljs">pip install kedro kedro-viz</pre>



<p>如果我们希望将这个项目初始化为Git存储库，我们可以使用:</p>







<p>步骤2:项目设置</p>



<pre class="hljs">pip install scikit-learn matplotlib</pre>



<p>Kedro的一个关键特性是创建标准的、可修改的和易于使用的项目模板。我们可以很容易地初始化一个新的Kedro项目:</p>



<pre class="hljs">git init</pre>



<h4>在为一系列提示提供相关名称之后，我们将得到一个高度组织化的项目目录，我们可以在这个目录上进行构建:</h4>



<p>项目结构可以分为六个主文件夹:</p>



<pre class="hljs">kedro new</pre>



<p>/ <strong> conf </strong> <strong> : </strong>包含指定重要细节的配置文件，如数据源(即数据目录)、模型参数、凭证和日志信息。<br/></p>







<p>/ <strong>数据</strong> <strong> : </strong>包含输入、中间和输出数据。它被组织成一个八层的数据工程约定，以清楚地分类和组织数据的处理方式。<br/></p>


<div class="custom-point-list">
<ul><li>/<strong/><strong>:</strong>包含与项目文档相关的文件。<br/></li><li>/ <strong> logs </strong> <strong> : </strong>包含管道运行时生成的日志文件。<br/></li><li>/ <strong>笔记本</strong> <strong> : </strong>包含项目中使用的Jupyter笔记本，例如用于实验或初步探索性数据分析。<br/></li><li>/ <strong> src </strong> <strong> : </strong>包含项目的源代码，例如用于流水线步骤、数据处理和模型训练的Python脚本。</li><li>步骤3:数据设置</li><li>数据先于科学，所以让我们从数据设置开始。在这个项目中，原始数据(每日信用卡交易的70个CSV文件)首先放在<strong> <em> data/01_raw </em> </strong>文件夹中。</li></ul>
</div>


<h4>根据前面描述的项目蓝图，我们已经知道在整个过程中将会生成和利用哪些数据。因此，我们可以将这些信息翻译成<a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html" target="_blank" rel="noreferrer noopener nofollow"> <strong>数据目录</strong> </a>，这是一个项目可用的数据源注册表。</h4>



<p>数据目录提供了一种定义如何存储和解析数据的一致方式，使得从管道中的任何位置加载和保存数据集变得容易。</p>



<p>我们可以在<em>中找到数据目录。yml </em>文件—<strong><em>conf/base/catalog . yml</em></strong><em>。</em></p>



<p>上图是数据目录中定义的数据源的一个片段。例如，我们首先期望我们的原始CSV文件被读取并合并到一个名为merged_data.csv的中间CSV数据集。</p>



<p>Kedro自带众多<a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.html" target="_blank" rel="noreferrer noopener nofollow">内置数据连接器</a>(比如熊猫。CSVDataSet，matplotlib。MatplotlibWriter)来适应不同的数据类型。</p>







<p>步骤4:创建管道</p>



<p>一旦我们的数据目录被适当地定义，我们就可以构建我们的管道。首先要理解两个关键概念:<a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/stable/06_nodes_and_pipelines/01_nodes.html" target="_blank" rel="noreferrer noopener nofollow"> <strong>节点</strong> </a> <strong> </strong>和<a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/stable/06_nodes_and_pipelines/02_pipeline_introduction.html" target="_blank" rel="noreferrer noopener nofollow"> <strong>管道</strong> </a>。</p>



<h4><strong>节点</strong>是管道的构建块。它们本质上是Python函数，表示要执行的数据转换，例如数据预处理、建模。<br/></h4>



<p><strong>管道</strong>是被连接以交付工作流的节点序列。它组织节点的依赖关系和执行顺序，连接输入和输出，同时保持代码模块化。</p>


<div class="custom-point-list">
<ul><li>用于异常检测的完整管线可分为<strong>三个较小的模块化管线，</strong>我们将最终连接它们:</li><li><strong>Pipelines </strong>are sequences of nodes that are connected to deliver a workflow. It organizes the nodes’ dependencies and execution order and connects inputs and outputs while keeping the code modular.</li></ul>
</div>


<p><span> 1 </span>数据工程管道</p>


<div class="case-study-numbered-list">
    <h2><span> 2 </span>数据科学管道<br/></h2>
    <ul>
                    <li><span> 3 </span>模型评估管道</li>
                    <li>我们可以根据指定的名称，使用以下命令实例化这些模块化管道:</li>
                    <li>虽然管道在这个阶段是空的，但是它们的结构已经在<strong> <em> /src </em> </strong>文件夹中很好地生成了:</li>
            </ul>
</div>



<p>每个管道文件夹都有相同的文件，包括<strong> nodes.py </strong>(节点代码)<strong> </strong>和<strong> pipeline.py </strong>(管道代码)。</p>



<pre class="hljs">kedro pipeline create data_engineering
kedro pipeline create data_science
kedro pipeline create model_evaluation</pre>



<p>步骤5:构建数据工程管道</p>







<p>我们先来看看数据工程管道，在这里我们对数据进行处理，使其适合下游的机器学习。更具体地说，有三个预处理任务要执行:</p>



<h4>Step 5:  Build a data engineering pipeline</h4>



<p><span> 1 </span>将原始数据集合并成一个中间合并数据集。</p>


<div class="case-study-numbered-list">
    <h2><span> 2 </span>通过仅保留预测值列并为后续训练测试分割创建新的日期列来处理合并的数据集。</h2>
    <ul>
                    <li><span> 3 </span>按时间顺序执行80:20列车测试，拆分并删除不必要的列。</li>
                    <li>我们首先将任务脚本化为<strong>三个独立的</strong> <strong>节点函数</strong>在<strong>节点. py、</strong>中，如下所示:</li>
                    <li>然后，我们将这些节点函数导入到<strong> pipeline.py </strong>中，以正确的顺序将它们链接起来。</li>
            </ul>
</div>



<p>注意，在每个节点包装器的<strong> <em>节点(..)</em> </strong>上面，我们指定了一个节点名，函数(从node.py导入)，以及在数据目录中定义的输入输出数据集。</p>



<pre class="hljs">

<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> Any, Callable, Dict
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> timedelta, datetime <span class="hljs-keyword">as</span> dt

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge_data</span><span class="hljs-params">(partitioned_input: Dict[str, Callable[[], Any]])</span> -&gt; pd.DataFrame:</span>
    merged_df = pd.DataFrame()
    <span class="hljs-keyword">for</span> partition_id, partition_load_func <span class="hljs-keyword">in</span> sorted(partitioned_input.items()):
        partition_data = partition_load_func()  
        merged_df = pd.concat([merged_df, partition_data], ignore_index=<span class="hljs-keyword">True</span>, sort=<span class="hljs-keyword">True</span>) 
    <span class="hljs-keyword">return</span> merged_df

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_data</span><span class="hljs-params">(merged_df: pd.DataFrame, predictor_cols: list)</span> -&gt; pd.DataFrame:</span>
    merged_df[<span class="hljs-string">'TX_DATETIME'</span>] =  pd.to_datetime(merged_df[<span class="hljs-string">'TX_DATETIME'</span>], infer_datetime_format=<span class="hljs-keyword">True</span>)
    merged_df[<span class="hljs-string">'TX_DATE'</span>] = merged_df[<span class="hljs-string">'TX_DATETIME'</span>].dt.date
    processed_df = merged_df[predictor_cols]
    <span class="hljs-keyword">return</span> processed_df

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_test_split</span><span class="hljs-params">(processed_df: pd.DataFrame)</span> -&gt; pd.DataFrame:</span>
    processed_df[<span class="hljs-string">'TX_DATE'</span>] =  pd.to_datetime(processed_df[<span class="hljs-string">'TX_DATE'</span>], infer_datetime_format=<span class="hljs-keyword">True</span>)
    split_date = processed_df[<span class="hljs-string">'TX_DATE'</span>].min() + timedelta(days=(<span class="hljs-number">8</span>*<span class="hljs-number">7</span>))
    train_df = processed_df.loc[processed_df[<span class="hljs-string">'TX_DATE'</span>] &lt;= split_date]
    test_df = processed_df.loc[processed_df[<span class="hljs-string">'TX_DATE'</span>] &gt; split_date]
    train_df.drop(columns=[<span class="hljs-string">'TX_DATE'</span>], inplace=<span class="hljs-keyword">True</span>)
    test_df.drop(columns=[<span class="hljs-string">'TX_DATE'</span>], inplace=<span class="hljs-keyword">True</span>)

    
    <span class="hljs-keyword">if</span> <span class="hljs-string">'TX_FRAUD'</span> <span class="hljs-keyword">in</span> train_df.columns:
        train_df = train_df.drop(columns=[<span class="hljs-string">'TX_FRAUD'</span>])
    <span class="hljs-keyword">if</span> <span class="hljs-string">'TX_FRAUD'</span> <span class="hljs-keyword">in</span> test_df.columns:
        test_labels = test_df[[<span class="hljs-string">'TX_FRAUD'</span>]] 
        test_df = test_df.drop(columns=[<span class="hljs-string">'TX_FRAUD'</span>])
    <span class="hljs-keyword">else</span>:
        test_labels = pd.DataFrame() 
    <span class="hljs-keyword">return</span> train_df, test_df, test_labels</pre>



<p>节点包装器中的参数应该与数据目录中的数据集名称和节点函数的参数相匹配。</p>



<pre class="hljs">

<span class="hljs-keyword">from</span> kedro.pipeline <span class="hljs-keyword">import</span> Pipeline, node, pipeline
<span class="hljs-keyword">from</span> .nodes <span class="hljs-keyword">import</span> merge_data, process_data, train_test_split

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_pipeline</span><span class="hljs-params">(**kwargs)</span> -&gt; Pipeline:</span>
    <span class="hljs-keyword">return</span> pipeline([
        node(
            func=merge_data,
            inputs=<span class="hljs-string">"raw_daily_data"</span>,
            outputs=<span class="hljs-string">"merged_data"</span>,
            name=<span class="hljs-string">"node_merge_raw_daily_data"</span>
            ),
        node(
            func=process_data,
            inputs=[<span class="hljs-string">"merged_data"</span>, <span class="hljs-string">"params:predictor_cols"</span>],
            outputs=<span class="hljs-string">"processed_data"</span>,
            name=<span class="hljs-string">"node_process_data"</span>
            ),
        node(
            func=train_test_split,
            inputs=<span class="hljs-string">"processed_data"</span>,
            outputs=[<span class="hljs-string">"train_data"</span>, <span class="hljs-string">"test_data"</span>, <span class="hljs-string">"test_labels"</span>],
            name=<span class="hljs-string">"node_train_test_split"</span>
            ),
    ])</pre>



<p>对于节点node_process_data，预测器列的列表存储在位于<strong><em>conf/base/parameters . yml</em></strong>中的参数文件中。</p>



<p>我们的数据工程管道设置已经完成，但是它还没有准备好，因为它还没有在T2注册。我们将在稍后的<strong>步骤8 </strong>中探讨这个问题，所以让我们继续构建剩下的两条管道。</p>



<p>步骤6:构建数据科学管道</p>



<p>我们的数据科学管道的异常检测模型是一个<a href="https://web.archive.org/web/20220926103904/https://en.wikipedia.org/wiki/Isolation_forest" target="_blank" rel="noreferrer noopener nofollow">隔离林</a>。<strong>隔离森林</strong>是一种使用决策树构建的无监督算法。</p>



<h4>它通过随机选择一个特征，然后在其最大值和最小值之间选择一个分割值来“隔离”观察值。由于异常现象很少且不同，因此预计它们比正常观察更容易分离。</h4>



<p>我们将使用scikit-learn <a href="https://web.archive.org/web/20220926103904/https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" target="_blank" rel="noreferrer noopener nofollow">隔离林实现</a>进行建模。有两个任务(和节点)要被创建——<strong>(I)</strong>模型训练和<strong> (ii) </strong>模型预测(又名推理)。</p>



<p>隔离森林模型的<strong>污染值</strong>被设置为<strong> 0.009 </strong>，反映了在原始数据集中观察到的欺诈案例的比例(即0.9%)。</p>



<p><br/>像以前一样，我们在<strong> pipeline.py </strong>中的管道函数内将节点链接在一起。</p>



<p>正如前面在数据目录中看到的，我们将把我们训练的隔离森林模型作为<strong> pickle </strong>文件保存在<strong> <em> data/06_models </em> </strong>中。</p>



<pre class="hljs">

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> IsolationForest

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_model</span><span class="hljs-params">(train_df: pd.DataFrame, contamination_value: float)</span>:</span>
    clf = IsolationForest(random_state=<span class="hljs-number">42</span>, bootstrap=<span class="hljs-keyword">True</span>, contamination=contamination_value)
    clf.fit(train_df.values) 
    <span class="hljs-keyword">return</span> clf

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(ml_model, test_df: pd.DataFrame)</span>:</span>
    preds = ml_model.predict(test_df.values) 
    preds_mod = np.array(list(map(<span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span>*(x==<span class="hljs-number">-1</span>), preds)))
    
    anomaly_scores = ml_model.score_samples(test_df) 
    anomaly_scores_mod = np.array([-x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> anomaly_scores])

    test_df[<span class="hljs-string">'ANOMALY_SCORE'</span>] = anomaly_scores_mod
    test_df[<span class="hljs-string">'ANOMALY'</span>] = preds_mod
    <span class="hljs-keyword">return</span> test_df</pre>



<p>步骤7:构建模型评估管道</p>



<pre class="hljs">

<span class="hljs-keyword">from</span> kedro.pipeline <span class="hljs-keyword">import</span> Pipeline, node, pipeline
<span class="hljs-keyword">from</span> .nodes <span class="hljs-keyword">import</span> train_model, predict

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_pipeline</span><span class="hljs-params">(**kwargs)</span> -&gt; Pipeline:</span>
    <span class="hljs-keyword">return</span> pipeline([
        node(
            func=train_model,
            inputs=[<span class="hljs-string">"train_data"</span>, <span class="hljs-string">"params:contamination_value"</span>],
            outputs=<span class="hljs-string">"ml_model"</span>,
            name=<span class="hljs-string">"node_train_model"</span>
            ),
        node(
            func=predict,
            inputs=[<span class="hljs-string">"ml_model"</span>, <span class="hljs-string">"test_data"</span>],
            outputs=<span class="hljs-string">"predictions"</span>,
            name=<span class="hljs-string">"node_predict"</span>
            ),
    ])</pre>



<p>虽然隔离森林是一个无监督的模型，但如果我们有地面真实标签，我们仍然可以评估它的性能。</p>



<h3>在原始数据集中，有一个用作欺诈交易指示器的<strong> TX_FRAUD </strong>变量。</h3>



<p>利用模型预测的基本事实标签和异常分数，我们可以很容易地获得评估指标，这些指标将表示为<a href="https://web.archive.org/web/20220926103904/https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noreferrer noopener nofollow"> AUC和AUCPR </a>图。</p>



<p>下面是运行模型评估节点的<strong> pipeline.py </strong>脚本。</p>



<p>这个模型评估步骤与步骤6中的数据科学管道是分开的。这种分离是因为我们正在使用一种<strong>无监督的</strong>异常检测算法，并且我们并不期望总是得到真实的数据。</p>



<pre class="hljs">

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve, precision_recall_curve, auc

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_model</span><span class="hljs-params">(predictions: pd.DataFrame, test_labels: pd.DataFrame)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_auc</span><span class="hljs-params">(labels, scores)</span>:</span>
        fpr, tpr, thr = roc_curve(labels, scores)
        auc_score = auc(fpr, tpr)
        <span class="hljs-keyword">return</span> fpr, tpr, auc_score

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_aucpr</span><span class="hljs-params">(labels, scores)</span>:</span>
        precision, recall, th = precision_recall_curve(labels, scores)
        aucpr_score = np.trapz(recall, precision)
        <span class="hljs-keyword">return</span> precision, recall, aucpr_score

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_metric</span><span class="hljs-params">(ax, x, y, x_label, y_label, plot_label, style=<span class="hljs-string">"-"</span>)</span>:</span>
        ax.plot(x, y, style, label=plot_label)
        ax.legend()
        ax.set_ylabel(x_label)
        ax.set_xlabel(y_label)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prediction_summary</span><span class="hljs-params">(labels, predicted_score, info, plot_baseline=True, axes=None)</span>:</span>
        <span class="hljs-keyword">if</span> axes <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>: axes = [plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>), plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)]
        fpr, tpr, auc_score = get_auc(labels, predicted_score)
        plot_metric(axes[<span class="hljs-number">0</span>], fpr, tpr, <span class="hljs-string">"False positive rate"</span>, <span class="hljs-string">"True positive rate"</span>, <span class="hljs-string">"{} AUC={:.4f}"</span>.format(info, auc_score))
        <span class="hljs-keyword">if</span> plot_baseline:
            plot_metric(axes[<span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-string">"False positive rate"</span>, <span class="hljs-string">"True positive rate"</span>, <span class="hljs-string">"Baseline AUC=0.5"</span>, <span class="hljs-string">"r--"</span>)
        precision, recall, aucpr_score = get_aucpr(labels, predicted_score)
        plot_metric(axes[<span class="hljs-number">1</span>], recall, precision, <span class="hljs-string">"Recall"</span>, <span class="hljs-string">"Precision"</span>, <span class="hljs-string">"{} AUCPR={:.4f}"</span>.format(info, aucpr_score))
        <span class="hljs-keyword">if</span> plot_baseline:
            thr = sum(labels)/len(labels)
            plot_metric(axes[<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [thr, thr], <span class="hljs-string">"Recall"</span>, <span class="hljs-string">"Precision"</span>, <span class="hljs-string">"Baseline AUCPR={:.4f}"</span>.format(thr), <span class="hljs-string">"r--"</span>)
        plt.show()
        <span class="hljs-keyword">return</span> axes
    
    fig = plt.figure()
    fig.set_figheight(<span class="hljs-number">4.5</span>)
    fig.set_figwidth(<span class="hljs-number">4.5</span>*<span class="hljs-number">2</span>)
    axes = prediction_summary(test_labels[<span class="hljs-string">'TX_FRAUD'</span>].values, predictions[<span class="hljs-string">'ANOMALY_SCORE'</span>].values, <span class="hljs-string">"Isolation Forest"</span>)
    <span class="hljs-keyword">return</span> fig</pre>



<p>步骤8:在管道注册表中注册所有管道</p>



<pre class="hljs"><span class="hljs-keyword">from</span> kedro.pipeline <span class="hljs-keyword">import</span> Pipeline, node, pipeline
<span class="hljs-keyword">from</span> .nodes <span class="hljs-keyword">import</span> evaluate_model

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_pipeline</span><span class="hljs-params">(**kwargs)</span> -&gt; Pipeline:</span>
    <span class="hljs-keyword">return</span> pipeline([
        node(
            func=evaluate_model,
            inputs=[<span class="hljs-string">"predictions"</span>, <span class="hljs-string">"test_labels"</span>],
            outputs=<span class="hljs-string">"evaluation_plot"</span>,
            name=<span class="hljs-string">"node_model_evaluation"</span>
            )
    ])</pre>



<p>至此，创建管道的所有艰苦工作都已完成。我们现在需要通过在<strong>管道注册表</strong>中导入和注册所有三个模块化管道来结束。</p>



<h3>return语句中的__default__行指示模块化管道的默认运行顺序，在我们的示例中，这是所有三个模块化管道——data _ engineering、data_science和model_evaluation。</h3>



<p>Kedro的美妙之处在于，它的模块化结构让我们能够灵活地构建我们的管道。例如，如果我们没有基础事实标签，我们可以从缺省管线运行中排除model_evaluation。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> Dict
<span class="hljs-keyword">from</span> kedro.pipeline <span class="hljs-keyword">import</span> Pipeline, pipeline

<span class="hljs-keyword">from</span> anomaly_detection_pipeline_kedro.pipelines <span class="hljs-keyword">import</span> (
    data_engineering <span class="hljs-keyword">as</span> de,
    data_science <span class="hljs-keyword">as</span> ds,
    model_evaluation <span class="hljs-keyword">as</span> me
)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">register_pipelines</span><span class="hljs-params">()</span> -&gt; Dict[str, Pipeline]:</span>
    data_engineering_pipeline = de.create_pipeline()
    data_science_pipeline = ds.create_pipeline()
    model_evaluation_pipeline = me.create_pipeline()

    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">"de"</span>: data_engineering_pipeline,
        <span class="hljs-string">"ds"</span>: data_science_pipeline,
        <span class="hljs-string">"me"</span>: model_evaluation_pipeline,
        <span class="hljs-string">"__default__"</span>: data_engineering_pipeline + data_science_pipeline + model_evaluation_pipeline
    }</pre>



<p>步骤9:可视化管道</p>



<p>在运行管道之前，检查一下我们到目前为止已经构建了什么是一个好主意。奇妙的<strong> Kedro-Viz </strong>插件让我们可以很容易地可视化整个管道结构和依赖关系。</p>



<h3>鉴于其易用性、清晰度和美观的显示，许多QuantumBlack客户<a href="https://web.archive.org/web/20220926103904/https://www.mckinsey.com/about-us/new-at-mckinsey-blog/meet-kedro-mckinseys-first-open-source-software-tool" target="_blank" rel="noreferrer noopener nofollow">对这一功能表达了他们的喜悦</a>也就不足为奇了。</h3>



<p>我们可以使用以下命令轻松生成可视化效果:</p>



<p>我们的浏览器中将会打开一个新的选项卡，我们将会看到一个漂亮的可视化工具来探索我们的管道结构。这种可视化也可以很容易地导出为一个.<strong> png </strong>图像文件。</p>



<p>步骤10:运行管道</p>



<pre class="hljs">kedro viz</pre>



<p>我们终于准备好运行我们的管道。下面的命令将执行我们之前注册的默认管道。</p>







<h3>运行时，管道将使用生成的数据填充各自的目录，包括异常预测和模型评估图。</h3>



<p>我们还可以运行已经在管道注册表中注册的特定管道。例如，如果我们只想运行数据工程模块化管道(de)，我们可以在命令中添加<em>–管道&lt;名称&gt; </em>:</p>



<pre class="hljs">kedro run</pre>



<p>步骤11:评估管道输出</p>



<p>最后，是时候评估我们异常检测管道的输出了。特别是，让我们回顾一下matplotlib评估图(保存在<strong> <em>数据/08 _报告</em> </strong>)来看看该模型的性能如何。</p>



<pre class="hljs">kedro run --pipeline de</pre>



<h3>从图中，我们可以看到隔离森林模型AUC是<strong> 0.8486 </strong>，这是一个相当好的基线机器学习模型性能。</h3>



<p>附加功能</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img data-attachment-id="64157" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro6" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro6.png?fit=766%2C400&amp;ssl=1" data-orig-size="766,400" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro6" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro6.png?fit=300%2C157&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro6.png?fit=766%2C400&amp;ssl=1" src="../Images/136818fb1301bb24f2f0083d0e03728a.png" alt="Evaluating pipeline output" class="wp-image-64157 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro6.png?resize=742%2C387&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro6.png?resize=742%2C387&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="64157" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro6" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro6.png?fit=766%2C400&amp;ssl=1" data-orig-size="766,400" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro6" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro6.png?fit=300%2C157&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro6.png?fit=766%2C400&amp;ssl=1" src="../Images/136818fb1301bb24f2f0083d0e03728a.png" alt="Evaluating pipeline output" class="wp-image-64157" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro6.png?resize=742%2C387&amp;ssl=1"/></noscript><figcaption><em>Evaluating pipeline output | Source: Author</em></figcaption></figure></div>



<p>恭喜你走到这一步，并成功地用Kedro创建了一个异常检测管道！</p>



<h2>除了前面详细介绍的基本功能之外，Kedro还提供了其他用于管理数据科学项目的有用特性。这里有几个值得一提的功能:</h2>



<p>(1)实验跟踪</p>



<p>Kedro使得设置<a href="/web/20220926103904/https://neptune.ai/experiment-tracking" target="_blank" rel="noreferrer noopener">实验跟踪</a>和访问每个管道运行的记录指标变得容易。除了它的<a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/stable/03_tutorial/07_set_up_experiment_tracking.html" target="_blank" rel="noreferrer noopener nofollow">内部实验跟踪能力</a>，Kedro与其他MLOps服务整合得很好。</p>



<h3>例如，<a href="https://web.archive.org/web/20220926103904/https://docs.neptune.ai/integrations-and-supported-tools/automation-pipelines/kedro" target="_blank" rel="noreferrer noopener"> <strong> Kedro-Neptune插件</strong> </a>让用户享受到组织良好的管道以及为元数据管理而构建的强大Neptune用户界面的好处。</h3>



<p>(2)管道切片</p>



<p>Kedro的<a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/stable/06_nodes_and_pipelines/06_slice_a_pipeline.html" target="_blank" rel="noreferrer noopener nofollow">管道切片功能</a>允许我们根据需要执行管道的特定部分。例如，我们可以在希望运行的管道切片中定义开始和结束节点:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro10" target="_blank" rel="noopener"><img data-attachment-id="64162" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro10" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro10.png?fit=1999%2C650&amp;ssl=1" data-orig-size="1999,650" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro10" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro10.png?fit=300%2C98&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro10.png?fit=1024%2C333&amp;ssl=1" src="../Images/44491de5391bf1000a07d06b13212ae0.png" alt="Kedro pipeline metadata in a custom dashboard in the Neptune UI" class="wp-image-64162 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro10.png?resize=1024%2C333&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro10.png?resize=1024%2C333&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="64162" data-permalink="https://web.archive.org/web/20220926103904/https://neptune.ai/building-and-managing-data-science-pipelines-with-kedro10" data-orig-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro10.png?fit=1999%2C650&amp;ssl=1" data-orig-size="1999,650" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Building and Managing Data Science Pipelines with Kedro10" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro10.png?fit=300%2C98&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926103904/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro10.png?fit=1024%2C333&amp;ssl=1" src="../Images/44491de5391bf1000a07d06b13212ae0.png" alt="Kedro pipeline metadata in a custom dashboard in the Neptune UI" class="wp-image-64162" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926103904im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Building-and-Managing-Data-Science-Pipelines-with-Kedro10.png?resize=1024%2C333&amp;ssl=1"/></noscript></a><figcaption><em><em>Kedro pipeline metadata in a custom dashboard in the Neptune UI | </em><a href="https://web.archive.org/web/20220926103904/https://app.neptune.ai/kennethleung.ty/Anomaly-Detection-Pipeline-Kedro/e/AN-7/dashboard/Baseline-Metadata-95f30634-5136-43aa-8d6f-a27d65609396" target="_blank" rel="noreferrer noopener"><em>Source</em></a></em></figcaption></figure></div>



<h3>(3)项目文档、打包和部署</h3>



<p>我们可以通过在项目的根目录下运行这个命令来生成特定于项目的文档(构建在Sphinx框架上)。</p>



<pre class="hljs">kedro run --<span class="hljs-keyword">from</span>-nodes train-test-split --to-nodes train_model</pre>



<h3>接下来，为了将项目的<a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/stable/03_tutorial/08_package_a_project.html" target="_blank" rel="noreferrer noopener nofollow">打包</a>为Python库，我们运行以下命令:</h3>



<p>最后，我们可以通过第三方插件<a href="https://web.archive.org/web/20220926103904/https://github.com/kedro-org/kedro-plugins/tree/main/kedro-docker" target="_blank" rel="noreferrer noopener nofollow"> Docker </a>和<a href="https://web.archive.org/web/20220926103904/https://github.com/kedro-org/kedro-plugins/tree/main/kedro-airflow" target="_blank" rel="noreferrer noopener nofollow"> Airflow </a>来<a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/latest/10_deployment/01_deployment_guide.html" target="_blank" rel="noreferrer noopener nofollow">部署</a>这些打包的数据科学管道。<br/></p>



<pre class="hljs">kedro build-docs</pre>



<p>(4)通过Jupyter笔记本访问Kedro</p>



<pre class="hljs">kedro package</pre>



<p>我们还可以使用Jupyter笔记本来探索项目数据或试验代码，为管道创建新节点。这些任务可以通过使用以下命令启动<a href="https://web.archive.org/web/20220926103904/https://kedro.readthedocs.io/en/latest/11_tools_integration/02_ipython.html" target="_blank" rel="noreferrer noopener nofollow"> Kedro Jupyter会话</a>来完成:</p>



<h3>结论</h3>



<p>下面是我们在本文中经历的内容:</p>



<pre class="hljs">kedro jupyter notebook</pre>



<h2>数据科学管道的概念、优势和用例是什么。</h2>



<p>什么是Kedro，以及如何使用它来构建和管理数据科学管道。</p>


<div class="custom-point-list">
<ul><li>如何使用隔离林模型和Kedro框架为信用卡交易数据创建异常检测管道。</li><li>还有许多其他有趣的特性和教程可用，所以请查看官方Kedro文档做进一步的探索。另外，可以随意探索包含这个项目所有代码的<a href="https://web.archive.org/web/20220926103904/https://github.com/kennethleungty/Anomaly-Detection-Pipeline-Kedro" target="_blank" rel="noreferrer noopener nofollow"> GitHub repo </a>。</li><li>参考</li></ul>
</div>


<p>梁振英</p>



<h3>数据科学家、技术作家、开发人员、药剂师。热衷于学习和推动实干家，喜欢用技术和数据寻找和解决有意义的问题。</h3>






<div id="author-box-new-format-block_6246c41e966fa" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name"><strong>阅读下一篇</strong></h3>
    
          <p class="article__authorContent-text">最佳MLOps工具以及如何评估它们</p>
    
          
    
  </div>
</div>


<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>



<p class="has-text-color">12分钟阅读| Jakub Czakon |年8月25日更新</p>



<h2>The Best MLOps Tools and How to Evaluate Them</h2>



<p class="has-small-font-size">在我们的一篇文章中——<a href="https://web.archive.org/web/20220926103904/https://neptune.ai/blog/tools-libraries-frameworks-methodologies-ml-startups-roundup" target="_blank" rel="noreferrer noopener">机器学习团队实际使用的最好的工具、库、框架和方法——我们从41家ML初创公司学到的东西</a>——Acerta的CTO Jean-Christophe Petkovich解释了他们的ML团队如何接近MLOps。</p>


<p id="block_5ffc75def9f8e" class="separator separator-10"><strong>据他所说，一个完整的MLOps系统有几个要素:</strong></p>



<p>您需要能够构建包含预处理数据和生成结果所需的所有信息的模型工件。</p>



<p>一旦您能够构建模型工件，您必须能够跟踪构建它们的代码，以及它们被训练和测试的数据。</p>


<div class="custom-point-list">
<ul><li>您需要跟踪所有这三样东西，模型、它们的代码和它们的数据，是如何关联的。</li><li>一旦您可以跟踪所有这些内容，您还可以将它们标记为准备就绪，进行生产，并通过CI/CD流程运行它们。</li><li>最后，为了在该过程的最后实际部署它们，您需要某种方法来基于该模型工件旋转服务。</li><li>这是对如何在公司中成功实施MLOps的高度概括。但是理解高层需要什么只是拼图的一部分。另一个是采用或创建适当的工具来完成工作。</li><li>这就是为什么我们编制了一份<strong>最佳MLOps工具</strong>的清单。我们将它们分为六类，以便您可以为您的团队和业务选择合适的工具。让我们开始吧！</li></ul>
</div>


<p>It’s a great high-level summary of how to successfully implement MLOps in a company. But understanding what is needed in high-level is just a part of the puzzle. The other one is adopting or creating proper tooling that gets things done. </p>



<p>That’s why we’ve compiled a list of the <strong>best MLOps tools</strong>. We’ve divided them into six categories so you can choose the right tools for your team and for your business. Let’s dig in!</p>


<a class="button continous-post blue-filled" href="/web/20220926103904/https://neptune.ai/blog/best-mlops-tools" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>