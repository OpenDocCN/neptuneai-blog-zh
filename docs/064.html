<html>
<head>
<title>Vanishing and Exploding Gradients in Neural Network Models: Debugging, Monitoring, and Fixing </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>神经网络模型中的消失和爆炸梯度:调试、监控和修复</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>神经网络模型采用<strong>梯度下降</strong>的优化算法进行训练。输入训练数据有助于这些模型学习，损失函数衡量参数更新时每次迭代的预测性能有多准确。随着训练的进行，目标是通过迭代调整参数来减少损失函数/预测误差。具体来说，梯度下降算法有一个前进步骤和一个后退步骤，这让它做到这一点。</p>



<ul><li>在前向传播中，输入向量/数据通过网络向前移动，使用一个公式来计算下一层中的每个神经元。该公式由输入/输出、激活函数<em> f </em>、权重<em> W </em>和偏差<em> b </em>组成:</li></ul>







<p id="separator-block_623b1a9c5bb6f" class="block-separator block-separator--10"> </p>



<p>该计算向前迭代，直到达到输出或预测。然后，我们计算由目标变量y(在输出层中)和每个预测y cap之间的损失函数(例如，均方误差MSE)定义的差:</p>



<p id="separator-block_623b1ab75bb70" class="block-separator block-separator--5"> </p>







<p id="separator-block_623b1ac15bb71" class="block-separator block-separator--5"> </p>



<ul><li>有了这个初始评估，我们通过反向传递(也称为反向传播)来调整每层中每个神经元的权重和偏差。为了更新我们的神经网络，我们首先计算梯度，这只不过是损失函数<em> w.r.t. </em>权重和偏差的导数。然后，我们推动我们的算法采取梯度下降步骤来最小化损失函数(其中α是学习速率):</li></ul>







<p id="separator-block_623b1ad75bb72" class="block-separator block-separator--10"> </p>



<p>在这种情况下，可能发生两种相反的情况:导数项变得非常小，即接近零，而该项变得非常大并溢出。这些问题分别被称为消失和爆炸梯度。</p>



<p>当你训练你的模型一段时间后，性能似乎没有变得更好，很可能你的模型正在遭受<strong>消失或爆炸梯度</strong>。</p>



<p>本文针对这些问题，具体来说，我们将涵盖:</p>



<ul><li>消失和爆炸渐变问题背后的直觉</li><li>为什么会出现这些梯度问题</li><li>如何在模型训练过程中识别梯度问题</li><li>解决消失和爆炸渐变的案例演示和解决方案<ul><li>消失渐变<ul><li>ReLU作为激活函数</li><li>降低模型复杂性</li><li>具有方差的权重初始值设定项</li><li>更好的优化器，具有良好的学习率</li></ul></li><li>爆炸渐变<ul><li>渐变剪辑</li><li>合适的权重初始化器</li><li>L2范数正则化</li></ul></li></ul></li></ul>







<h2 id="h-vanishing-or-exploding-gradients-intuition-behind-the-problem">消失或爆发的渐变——问题背后的直觉</h2>



<h3>消失</h3>



<p>在反向传播期间，<em> </em>权重更新公式中的(部分)<em>导数/梯度</em>的计算遵循链式法则，其中早期层中的梯度是后期层的梯度的乘积:</p>



<p id="separator-block_623b22605bb73" class="block-separator block-separator--5"> </p>







<p id="separator-block_623b22735bb74" class="block-separator block-separator--5"> </p>



<p>由于梯度经常变小，直到它们接近零，新的模型权重(初始层的)将几乎与旧的权重相同，而没有任何更新。因此，梯度下降算法永远不会收敛到最优解。这就是所谓的消失梯度问题，这是神经网络不稳定行为的一个例子。</p>



<h3>爆炸</h3>



<p>相反，如果随着反向传播的进行，梯度变得越来越大，甚至越来越小，我们将会以具有大的权重更新的爆炸梯度结束，导致梯度下降算法的发散。</p>



<h2 id="h-why-vanishing-or-exploding-gradients-problem-happens">为什么会出现渐变消失或爆炸的问题？</h2>



<p>有了对什么是消失/爆炸梯度的直观理解，您一定想知道-为什么梯度首先会消失或爆炸，也就是说，为什么这些梯度值在通过网络返回时会减小或爆炸？</p>



<h3>消失</h3>



<p>简而言之，当我们在隐藏层中使用<a href="https://web.archive.org/web/20230103154738/https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="noreferrer noopener nofollow"> Sigmoid </a> <strong> </strong>或<a href="https://web.archive.org/web/20230103154738/https://en.wikipedia.org/wiki/Hyperbolic_functions#Hyperbolic_tangent" target="_blank" rel="noreferrer noopener nofollow"> Tanh </a> <strong>激活函数</strong>时，就会出现渐变消失的问题；这些函数将一个大的输入空间压缩成一个小的空间。以乙状结肠为例，我们有以下概率密度函数:</p>







<p id="separator-block_623b24da5bb75" class="block-separator block-separator--10"> </p>



<p>对参数x求导，我们得到:</p>



<p id="separator-block_623b24e65bb76" class="block-separator block-separator--10"> </p>







<p id="separator-block_623b24ed5bb77" class="block-separator block-separator--10"> </p>



<p>如果我们想象Sigmoid函数及其导数:</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/1fe4be22ae67cfc13057633b0d12d8ad.png" alt="Sigmoid function and it's derivative" class="wp-image-63709" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-Exploding-Gradients-in-Neural-Network-Models-Debugging-Monitoring-and-Fixing-Practical-Guide_7.png?resize=636%2C497&amp;ssl=1"/><figcaption><em>Sigmoid function and its derivative | Source: Author </em></figcaption></figure></div>



<p>我们可以看到，Sigmoid函数将我们的输入空间压缩到[0，1]之间的范围内，当输入变得相当小或相当大时，这个函数<strong> <em>在0或1处饱和</em> </strong>。这些区域被称为“饱和区域”，其导数变得非常接近于零。同样适用于在-1和1处<strong> <em>饱和</em> </strong>的Tanh函数。</p>



<p>假设我们有位于任何饱和区域中的输入，我们将基本上没有梯度值传播回来，导致早期层权重的零更新。通常，对于只有几层的浅层网络来说，这不是什么大问题，但是，当我们添加更多层时，初始层中梯度的消失将导致模型训练或收敛失败。</p>



<p>这是由于将这些小数字的<em> n </em>相乘以计算<em> n </em>层网络中早期层的梯度的效果，这意味着梯度随着<em> n </em>呈指数下降，而早期层训练非常缓慢，因此整个网络的性能下降。</p>



<h3>爆炸</h3>



<p>转到爆炸梯度，简而言之，这个问题是由于分配给神经网络的<strong>初始权重</strong>造成了巨大的损失。大的梯度值可以累积到观察到大的参数更新的点，导致梯度下降振荡而不会达到全局最小值。</p>



<p>更糟糕的是，这些参数可能太大，以至于溢出并返回无法再更新的NaN值。</p>



<h2 id="h-how-to-identify-a-vanishing-or-exploding-gradients-problem">如何识别渐层消失或爆炸的问题？</h2>



<p>承认梯度的问题是我们需要避免或解决的事情，当它们发生时，我们应该如何知道一个模型正在遭受消失或爆炸的梯度问题？以下是一些迹象。</p>



<h3>消失</h3>



<ul><li>在后面的层的参数中观察到大的变化，而前面的层的参数变化很小或保持不变</li><li>在某些情况下，随着训练的进行，早期层的权重可以变成0</li><li>该模型学习缓慢，并且经常多次，在几次迭代之后训练停止</li><li>模型性能很差</li></ul>



<h3>爆炸</h3>



<ul><li>与消失的情况相反，爆炸梯度显示出它本身是不稳定的，批量/迭代与批量/迭代之间的大参数变化</li><li>模型权重可以很快变为NaN</li><li>南也失去了模特</li></ul>



<h2 id="h-practices-to-fix-a-vanishing-or-exploding-gradients-problem">修复消失或爆炸渐变问题的实践</h2>



<p>记住这些梯度问题的指标，让我们探索潜在的补救措施来解决它们。</p>



<ul><li>首先，我们将关注于<strong>消失场景</strong>:模拟一个遭受此问题的<strong>二进制分类</strong>网络模型，然后演示各种解决方案</li><li>出于同样的原因，我们将在稍后用一个<strong>回归</strong>网络模型来解决<strong>爆炸场景</strong></li></ul>



<p>通过解决不同类型的深度学习任务，我的目标是演示不同的场景供您带走。还请注意，本文致力于为您提供实用的方法和技巧，因此我们将只讨论每种方法背后的一些直觉，而跳过数学或理论证明。</p>



<p>如上所述，由于观察是确定这些问题的关键部分，我们将使用<a href="https://web.archive.org/web/20230103154738/https://docs.neptune.ai/getting-started/how-to-add-neptune-to-your-code"> Neptune.ai </a>来跟踪我们的建模管道:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune.new <span class="hljs-keyword">as</span> neptune
<span class="hljs-keyword">import</span> os
myProject = <span class="hljs-string">'YourUserName/YourProjectName'</span>
project = neptune.init(api_token=os.getenv(<span class="hljs-string">'NEPTUNE_API_TOKEN'</span>),
                        project=myProject)
project.stop()</pre>



<h3>渐变消失时的解决方案</h3>



<p>首先我们定义几个helper函数来训练并<a href="https://web.archive.org/web/20230103154738/https://docs.neptune.ai/you-should-know/what-can-you-log-and-display" target="_blank" rel="noreferrer noopener">在Neptune.ai </a>中登录我们的模型。</p>



<ul><li>记录梯度和重量:</li></ul>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getBatchGradWgts</span><span class="hljs-params">(grads, wgts, lossVal,
                      gradHist, lossHist, wgtsHist,
                      recordWeight=True, npt_exp=None)</span>:</span>
    dataGrad, dataWeight = {}, {}
    
    <span class="hljs-keyword">for</span> wgt, grad <span class="hljs-keyword">in</span> zip(wgts, grads):
        <span class="hljs-keyword">if</span> <span class="hljs-string">'/kernel:'</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> wgt.name:
            <span class="hljs-keyword">continue</span>
        layerName = wgt.name.split(<span class="hljs-string">"/"</span>)[<span class="hljs-number">0</span>]
        dataGrad[layerName] = grad.numpy()
        dataWeight[layerName] = wgt.numpy()
        
        <span class="hljs-keyword">if</span> npt_exp:

npt_exp[f<span class="hljs-string">'MeanGrads{layerName.upper()}'</span>].log(np.mean(grad.numpy()))
 npt_exp[f<span class="hljs-string">'MeanWgtBatch{layerName.upper()}'</span>].log(np.mean(wgt.numpy()))


  gradHist.append(dataGrad)
  lossHist.append(lossVal.numpy())
  <span class="hljs-keyword">if</span> recordWeight:
      wgtsHist.append(dataWeight)</pre>







<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fitModel</span><span class="hljs-params">(X, y, model, optimizer,
              n_epochs=n_epochs, curBatch_size=batch_size, npt_exp=None)</span>:</span>

    lossFunc = tf.keras.losses.BinaryCrossentropy()
    subData = tf.data.Dataset.from_tensor_slices((X, y))
    subData = subData.shuffle(buffer_size=<span class="hljs-number">42</span>).batch(curBatch_size)

    gradHist, lossHist, wgtsHist = [], [], []

    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(n_epochs):
        print(f<span class="hljs-string">'== Starting epoch {epoch} =='</span>)
        <span class="hljs-keyword">for</span> step, (x_batch, y_batch) <span class="hljs-keyword">in</span> enumerate(subData):
            <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
                
                yPred = model(x_batch, training=<span class="hljs-keyword">True</span>)
                lossVal = lossFunc(y_batch, yPred)

            
            grads = tape.gradient(lossVal, model.trainable_weights)
            wgts = model.trainable_weights
            optimizer.apply_gradients(zip(grads, model.trainable_weights))

            
            <span class="hljs-keyword">if</span> step == <span class="hljs-number">5</span>:
                getBatchGradWgts(gradHist=gradHist, lossHist=lossHist, wgtsHist=wgtsHist,
                                 grads=grads, wgts=wgts, lossVal=lossVal, npt_exp=npt_exp)
                <span class="hljs-keyword">if</span> npt_exp:
                    npt_exp[<span class="hljs-string">'BatchLoss'</span>].log(lossVal)

    getBatchGradWgts(gradHist=gradHist, lossHist=lossHist, wgtsHist=wgtsHist,
                     grads=grads, wgts=wgts, lossVal=lossVal, npt_exp=npt_exp)
    <span class="hljs-keyword">return</span> gradHist, lossHist, wgtsHist</pre>



<ul><li>可视化各层的平均梯度:</li></ul>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gradientsVis</span><span class="hljs-params">(curGradHist, curLossHist, modelName)</span>:</span>
    fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, sharex=<span class="hljs-keyword">True</span>, constrained_layout=<span class="hljs-keyword">True</span>, figsize=(<span class="hljs-number">7</span>,<span class="hljs-number">5</span>))
    ax.set_title(f<span class="hljs-string">"Mean gradient {modelName}"</span>)
    <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> curGradHist[<span class="hljs-number">0</span>]:
        ax.plot(range(len(curGradHist)), [gradList[layer].mean() <span class="hljs-keyword">for</span> gradList <span class="hljs-keyword">in</span> curGradHist], label=f<span class="hljs-string">'Layer_{layer.upper()}'</span>)
    ax.legend()
    <span class="hljs-keyword">return</span> fig</pre>



<h4>渐变消失模型</h4>



<p>现在，我们将模拟一个数据集，并构建我们的基线<strong>二元</strong>分类神经网络:</p>



<pre class="hljs">
X, y = make_moons(n_samples=<span class="hljs-number">3000</span>, shuffle=<span class="hljs-keyword">True</span> , noise=<span class="hljs-number">0.25</span>, random_state=<span class="hljs-number">1234</span>)</pre>



<pre class="hljs">batch_size, n_epochs = <span class="hljs-number">32</span>, <span class="hljs-number">100</span></pre>



<pre class="hljs">npt_exp = neptune.init(
        api_token=os.getenv(<span class="hljs-string">'NEPTUNE_API_TOKEN'</span>),
        project=myProject,
        name=<span class="hljs-string">'VanishingGradSigmoid'</span>,
        description=<span class="hljs-string">'Vanishing Gradients with Sigmoid Activation Function'</span>,
        tags=[<span class="hljs-string">'vanishingGradients'</span>, <span class="hljs-string">'sigmoid'</span>, <span class="hljs-string">'neptune'</span>])

    
    neptune_cbk = NeptuneCallback(run=npt_exp, base_namespace=<span class="hljs-string">'metrics'</span>)
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">binaryModel</span><span class="hljs-params">(curName, curInitializer, curActivation, x_tr=None)</span>:</span>
        model = Sequential()
        model.add(InputLayer(input_shape=(<span class="hljs-number">2</span>, ), name=curName+<span class="hljs-string">"0"</span>))
        model.add(Dense(<span class="hljs-number">10</span>, kernel_initializer=curInitializer, activation=curActivation, name=curName+<span class="hljs-string">"1"</span>))
        model.add(Dense(<span class="hljs-number">10</span>, kernel_initializer=curInitializer, activation=curActivation, name=curName+<span class="hljs-string">"2"</span>))
        model.add(Dense(<span class="hljs-number">5</span>, kernel_initializer=curInitializer, activation=curActivation,  name=curName+<span class="hljs-string">"3"</span>))
        model.add(Dense(<span class="hljs-number">1</span>, kernel_initializer=curInitializer, activation=<span class="hljs-string">'sigmoid'</span>, name=curName+<span class="hljs-string">"4"</span>))
        <span class="hljs-keyword">return</span> model

    curOptimizer = tf.keras.optimizers.RMSprop()
    optimizer = curOptimizer
    curInitializer = RandomUniform(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)
    
    model = binaryModel(curName=<span class="hljs-string">"SIGMOID"</span>, curInitializer=curInitializer, curActivation=<span class="hljs-string">"sigmoid"</span>)
    model.compile(optimizer=curOptimizer, loss=<span class="hljs-string">'binary_crossentropy'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
    
    curGradHist, curLossHist, curWgtHist = fitModel(X, y, model, optimizer=curOptimizer, npt_exp=npt_exp)
    
    npt_exp[<span class="hljs-string">'Comparing All Layers'</span>].upload(neptune.types.File.as_image(gradientsVis(curGradHist, curLossHist, modelName=<span class="hljs-string">'Sigmoid_Raw'</span>)))
    npt_exp.stop()</pre>



<p>几个注意事项:</p>



<ul><li>我们目前的香草/基线模型由3个隐藏层组成，每一层都有一个sigmoid激活</li><li>我们使用RMSprop作为优化器，Uniform [-1，1]作为权重初始化器</li></ul>



<p>运行此模型将返回Neptune.ai中所有时期每个层的(平均)梯度，下面显示了层1和层4之间的比较:</p>



<figure class="wp-block-image size-full"><a href="https://web.archive.org/web/20230103154738/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing/attachment/vanishing-and-exploding-gradients-1"><img decoding="async" src="../Images/6b993f074e7a771406b91212daf77226.png" alt="Comparison between Layer 1 and Layer 4 from the baseline Sigmoid model generated using Neptune.ai" class="wp-image-64139" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-exploding-gradients-1.png?ssl=1"/></a><figcaption><em><em>Comparison between Layer 1 and Layer 4 from the baseline Sigmoid model generated using Neptune.ai</em></em> <em>|<a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/e/GRAD1-6/charts" target="_blank" rel="noreferrer noopener"> Source</a></em></figcaption></figure>



<p>对于第4层，随着训练的进行，我们看到平均梯度的明显波动，然而，对于第1层，梯度实际上为零，即，值大约小于0.006。消失渐变发生了！现在我们来谈谈如何解决这个问题。</p>



<h4>使用ReLU作为激活功能</h4>



<p>如前所述，消失梯度问题是由于Sigmoid或Tanh函数的饱和性质。因此，一个有效的补救办法是切换到其他激活函数，这些激活函数的导数<strong>不饱和</strong>，例如ReLU(整流线性单位):</p>



<p id="separator-block_623b36605bb7b" class="block-separator block-separator--10"> </p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/ff13c0afffb05a102d7bf8a36e0c9933.png" alt="ReLU as the activation function" class="wp-image-63708" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-Exploding-Gradients-in-Neural-Network-Models-Debugging-Monitoring-and-Fixing-Practical-Guide_8.png?resize=635%2C123&amp;ssl=1"/><figcaption><em>ReLU as the activation function | <a href="https://web.archive.org/web/20230103154738/https://en.wikipedia.org/wiki/Activation_function" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p>如此图所示，对于正输入x，ReLU不饱和。当x &lt;= 0, ReLU has derivative/gradient = 0, and when x &gt; 0时，导数/梯度= 1。因此，乘以ReLU导数会返回0或1；因此，不会有消失的梯度。</p>



<p>为了实现ReLU激活，我们可以简单地在如下所示的模型函数中指定`<em> relu </em>':</p>



<pre class="hljs">
model = binaryModel(curName=<span class="hljs-string">"Relu"</span>, curInitializer=curInitializer, curActivation=<span class="hljs-string">"relu"</span>)

model.compile(optimizer=curOptimizer, loss=<span class="hljs-string">'binary_crossentropy'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])</pre>



<p>运行该模型并比较从该模型计算的梯度，我们观察到跨时期的梯度变化，甚至对于标记为RELU1:</p>



<figure class="wp-block-image size-full"><a href="https://web.archive.org/web/20230103154738/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing/attachment/vanishing-and-exploding-gradients-2"><img decoding="async" src="../Images/c68d37d9397deb89591ef47a79f36df1.png" alt="Comparison between Layer 1 and Layer 4 from the ReLu model generated using Neptune.ai " class="wp-image-64141" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-exploding-gradients-2.png?ssl=1"/></a><figcaption><em><em><em>Comparison between Layer 1 and Layer 4 from the ReLu model generated using Neptune.ai</em> |<a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/e/GRAD1-8/charts" target="_blank" rel="noreferrer noopener"> Source</a></em></em></figcaption></figure>



<p id="separator-block_623b335a5bb79" class="block-separator block-separator--15"> </p>



<p>在大多数情况下，类似ReLU的激活函数本身应该足以处理渐变消失的问题。但是，这是否意味着我们要一直使用ReLU，彻底抛弃乙状结肠？嗯，消失梯度的存在不应该阻止你使用Sigmoid，它有许多可取的属性，如单调性和易于微分。即使使用Sigmoid激活函数，也有一些方法可以解决这个问题，这些方法是我们将在接下来的会议中尝试的。</p>



<h4>降低模型的复杂性</h4>



<p>由于消失梯度的根本原因在于一堆小梯度的相乘，直观上，通过减少梯度的数量，即减少我们网络中的层数来解决这个问题是有意义的。例如，与基线模型中指定3个隐藏层不同，我们可以只保留1个隐藏层，以使我们的模型更简单:</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">binaryModel</span><span class="hljs-params">(curName, curInitializer, curActivation, x_tr=None)</span>:</span>
        model = Sequential()
        model.add(InputLayer(input_shape=(<span class="hljs-number">2</span>, ), name=curName+<span class="hljs-string">"0"</span>))
        model.add(Dense(<span class="hljs-number">3</span>, kernel_initializer=curInitializer, activation=curActivation,  name=curName+<span class="hljs-string">"3"</span>))
        model.add(Dense(<span class="hljs-number">1</span>, kernel_initializer=curInitializer, activation=<span class="hljs-string">'sigmoid'</span>, name=curName+<span class="hljs-string">"4"</span>))
        <span class="hljs-keyword">return</span> model</pre>



<p>该模型为我们提供了清晰的梯度更新，显示在该图中:</p>



<p id="separator-block_623b34015bb7a" class="block-separator block-separator--15"> </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://web.archive.org/web/20230103154738/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing/attachment/vanishing-and-exploding-gradients-3" target="_blank" rel="noopener"><img decoding="async" src="../Images/8d1e8628623d494a37d963fd348c9557.png" alt="Comparison between Layer 1 and Layer 4 from the reduced Sigmoid model generated using Neptune.ai " class="wp-image-64705" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-exploding-gradients-3.png?ssl=1"/></a><figcaption><em>Comparison between Layer 1 and Layer 4 from the reduced Sigmoid model generated using Neptune.ai | <em><em><a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/e/GRAD1-30/charts" target="_blank" rel="noreferrer noopener">Source</a></em></em></em></figcaption></figure></div>



<p>这种方法的一个警告是，我们的模型性能可能不如更复杂的模型(具有更多隐藏层)。</p>



<h4>使用带有方差的权重初始值设定项</h4>



<p>当我们的初始权重设置得太小或缺少方差时，通常会导致梯度消失。回想一下，在我们的基线模型中，我们将权重初始化为均匀的[-1，1]分布，这可能会陷入这些权重太小的陷阱！</p>



<p>在他们2010年<a href="https://web.archive.org/web/20230103154738/http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noreferrer noopener nofollow">的论文</a>中，Xavier Glorot和Yoshua Bengio提供了从<strong>特定方差</strong>的均匀或正态分布中采样初始权重的理论依据，并保持所有层的激活方差相同。</p>



<p>在<a href="https://web.archive.org/web/20230103154738/https://keras.io/api/layers/initializers/" target="_blank" rel="noreferrer noopener nofollow"> Keras/Tensorflow </a>中，这种方法被实现为Glorot正态`<em> glorot_normal </em>和Glorot Uniform`<em>Glorot _ Uniform</em>`，顾名思义，这两种方法分别从(截尾)正态和均匀分布中采样初始权重。两者都考虑了输入和输出单元的数量。</p>



<p>对于我们的模型，让我们用<em> glorot_uniform </em>进行实验，根据<a href="https://web.archive.org/web/20230103154738/https://keras.io/api/layers/initializers/#glorotuniform-class" target="_blank" rel="noreferrer noopener nofollow"> Keras文档</a>:</p>







<p>回到具有3个隐藏层的原始模型，我们将模型权重初始化为<em> glorot_uniform </em>:</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">binaryModel</span><span class="hljs-params">(curName, curInitializer, curActivation, x_tr=None)</span>:</span>
        model = Sequential()
        model.add(InputLayer(input_shape=(<span class="hljs-number">2</span>, ), name=curName+<span class="hljs-string">"0"</span>))
        model.add(Dense(<span class="hljs-number">10</span>, kernel_initializer=curInitializer, activation=curActivation, name=curName+<span class="hljs-string">"1"</span>))
        model.add(Dense(<span class="hljs-number">10</span>, kernel_initializer=curInitializer, activation=curActivation, name=curName+<span class="hljs-string">"2"</span>))
        model.add(Dense(<span class="hljs-number">5</span>, kernel_initializer=curInitializer, activation=curActivation,  name=curName+<span class="hljs-string">"3"</span>))
        model.add(Dense(<span class="hljs-number">1</span>, kernel_initializer=curInitializer, activation=<span class="hljs-string">'sigmoid'</span>, name=curName+<span class="hljs-string">"4"</span>))
        <span class="hljs-keyword">return</span> model

    curOptimizer = tf.keras.optimizers.RMSprop()
    optimizer = curOptimizer
    
    curInitializer = <span class="hljs-string">'glorot_uniform'</span>
    
    npt_exp[<span class="hljs-string">'Comparing All Layers'</span>].upload(neptune.types.File.as_image(gradientsVis(curGradHist, curLossHist,
                                                                                    modelName=<span class="hljs-string">'Sigmoid_NormalWeightInit'</span>)))
    npt_exp.stop()</pre>



<p>检查我们的Neptune.ai跟踪器，我们看到梯度随着这个权重初始化而变化，尽管第1层(在左边)与最后一层相比显示出较少的波动:</p>



<p id="separator-block_623b367d5bb7c" class="block-separator block-separator--15"> </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://web.archive.org/web/20230103154738/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing/attachment/vanishing-and-exploding-gradients-4" target="_blank" rel="noopener"><img decoding="async" src="../Images/c52667748d565624f9d62eb2be00a421.png" alt="Comparison between Layer 1 and Layer 4 from the Glorot weight initializer model generated using Neptune.ai" class="wp-image-64707" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-exploding-gradients-4.png?ssl=1"/></a><figcaption><em><em><em>Comparison between Layer 1 and Layer 4 from the Glorot weight initializer model generated using Neptune.ai</em></em></em> <em><em><em>| <a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/e/GRAD1-48/charts" target="_blank" rel="noreferrer noopener">Source</a></em></em></em></figcaption></figure></div>



<h4>选择更好的优化器并调整学习率</h4>



<p>现在，我们已经解决了导数和初始权重的选择，公式的最后一部分是学习率。随着梯度接近零，优化器陷入次优局部最小值或鞍点。为了克服这一挑战，我们可以使用一个优化器，它具有将累积的先前梯度考虑在内的动力。例如，<a href="https://web.archive.org/web/20230103154738/https://arxiv.org/abs/1412.6980" target="_blank" rel="noreferrer noopener nofollow"> Adam </a>有一个动量项，计算为过去梯度的指数衰减平均值。</p>



<p>此外，作为一个高效的优化器，Adam可以快速收敛或发散。因此，稍微降低学习率将有助于防止你的网络太容易发散，从而降低梯度接近零的可能性。要使用Adam优化器，我们需要修改的只是<em> `curOptimizer` </em> arg。：</p>



<pre class="hljs">curOptimizer = keras.optimizers.Adam(learning_rate=<span class="hljs-number">0.008</span>) 

curInitializer = RandomUniform(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)

model = binaryModel(curName=<span class="hljs-string">"SIGMOID"</span>, curInitializer=curInitializer, curActivation=<span class="hljs-string">"sigmoid"</span>)
model.compile(optimizer=curOptimizer, loss=<span class="hljs-string">'binary_crossentropy'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])</pre>



<p>在上面的代码中，我们将Adam指定为模型优化器，学习率相对较小，为0.008，激活函数设置为sigmoid。下面是第1层和第4层渐变的比较:</p>



<p id="separator-block_623b36885bb7d" class="block-separator block-separator--15"> </p>



<div class="wp-block-image"><figure class="aligncenter size-full"><a href="https://web.archive.org/web/20230103154738/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing/attachment/vanishing-and-exploding-gradients-5" target="_blank" rel="noopener"><img decoding="async" src="../Images/e8e3c3862a0113682e8ee25acb69d6d3.png" alt="Comparison between Layer 1 and Layer 4 from the Adam model generated using Neptune.ai" class="wp-image-64138" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-exploding-gradients-5.png?ssl=1"/></a><figcaption><em><em><em><em>Comparison between Layer 1 and Layer 4 from the Adam model generated using Neptune.ai</em></em></em></em> <em><em><em>|<a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/e/GRAD1-50/charts" target="_blank" rel="noreferrer noopener"> Source</a></em></em></em></figcaption></figure></div>



<p>正如我们所看到的，通过Adam和调整良好的小学习率，我们看到梯度的变化使它们的值远离零，并且我们的模型也根据下面所示的损失图收敛到局部最小值:</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><a href="https://web.archive.org/web/20230103154738/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing/attachment/vanishing-and-exploding-gradients-6" target="_blank" rel="noopener"><img decoding="async" loading="lazy" src="../Images/5e8b89fcd169452e977779defc172c82.png" alt="Selecting better optimizer and adjusting learning rate " class="wp-image-64144" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-exploding-gradients-6.png?resize=430%2C358&amp;ssl=1"/></a><figcaption><em>Selecting better optimizer and adjusting learning rate <em><em><em>|<a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/e/GRAD1-50/charts" target="_blank" rel="noreferrer noopener"> Source</a></em></em></em></em></figcaption></figure></div>



<p>到目前为止，我们已经讨论了渐变消失的解决方案，让我们继续讨论渐变爆炸的问题。</p>



<h3>渐变爆炸时的解决方案</h3>



<p>对于爆炸梯度问题，让我们看看这个回归模型。</p>



<pre class="hljs">
nfeatures = <span class="hljs-number">15</span>
X, y = make_regression(n_samples=<span class="hljs-number">1500</span>, n_features=nfeatures, noise=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">42</span>)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">regressionModel</span><span class="hljs-params">(X, y, curInitializer, USE_L2REG, secondLayerAct=<span class="hljs-string">'relu'</span>)</span>:</span>
    
    inp = Input(shape = (X.shape[<span class="hljs-number">1</span>],))
    <span class="hljs-keyword">if</span> USE_L2REG:
        
        x = Dense(<span class="hljs-number">35</span>, activation=<span class="hljs-string">'tanh'</span>, kernel_initializer=curInitializer,
                  kernel_regularizer=regularizers.l2(<span class="hljs-number">0.01</span>),
                  activity_regularizer=regularizers.l2(<span class="hljs-number">0.01</span>))(inp)
    <span class="hljs-keyword">else</span>:
        x = Dense(<span class="hljs-number">35</span>, activation=secondLayerAct, kernel_initializer=curInitializer)(inp)

    out = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'linear'</span>)(x)
    model = Model(inp, out)
    <span class="hljs-keyword">return</span> model</pre>



<p>为了编译该模型，我们将使用统一的[4，5]权重初始化器以及ReLu激活，目的是创建爆炸梯度情况:</p>



<pre class="hljs">sgd = tf.keras.optimizers.SGD()
curOptimizer = sgd


curInitializer = RandomUniform(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)

model = regressionModel(X, y, curInitializer, USE_L2REG=<span class="hljs-keyword">False</span>)
model.compile(loss=<span class="hljs-string">'mean_squared_error'</span>, optimizer=curOptimizer, metrics=[<span class="hljs-string">'mse'</span>])

curModelName = <span class="hljs-string">'Relu_Raw'</span></pre>



<pre class="hljs">
curGradHist, curLossHist, curWgtHist = fitModel(X, y, model, optimizer=curOptimizer,                                                modelType = <span class="hljs-string">'regression'</span>,                                                npt_exp=npt_exp)

npt_exp[<span class="hljs-string">'Comparing All Layers'</span>].upload(neptune.types.File.as_image(gradientsVis(curGradHist, curLossHist,
                                                                                modelName=curModelName)))
npt_exp.stop()</pre>



<p>有了这么大的权重初始化，随着训练的进行，下面的错误消息出现在我们的Neptune.ai跟踪器中就不足为奇了，正如前面讨论的那样，这清楚地表明我们的梯度爆炸了:</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/6660d725a67eadf15ef93c4b9f675120.png" alt="Error message in neptune.ai" class="wp-image-63703" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-Exploding-Gradients-in-Neural-Network-Models-Debugging-Monitoring-and-Fixing-Practical-Guide_13.png?resize=846%2C148&amp;ssl=1"/><figcaption><em>Error message in Neptune.ai | Source</em></figcaption></figure></div>



<h4>渐变剪辑</h4>



<p>为了防止渐变爆炸，最有效的方法之一是渐变裁剪。简而言之，梯度裁剪将导数限定在一个阈值，并使用限定的梯度来更新权重。如果您对该方法的详细解释感兴趣，请参考<a href="/web/20230103154738/https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem" target="_blank" rel="noreferrer noopener">文章“了解渐变裁剪(以及它如何修复爆炸渐变问题)”</a>。</p>



<p>可以通过`<em> clipvalue </em>'参数指定将渐变限制为某个值。如下图所示:</p>



<pre class="hljs">
sgd = tf.keras.optimizers.SGD(clipvalue=<span class="hljs-number">50</span>)
curOptimizer = sgd
curInitializer = <span class="hljs-string">'glorot_normal'</span>

model = regressionModel(X, y, curInitializer, USE_L2REG=<span class="hljs-keyword">False</span>)
model.compile(loss=<span class="hljs-string">'mean_squared_error'</span>, optimizer=curOptimizer, metrics=[<span class="hljs-string">'mse'</span>])
curModelName = <span class="hljs-string">'GradClipping'</span> </pre>



<p>通过裁剪运行该模型，我们能够将梯度保持在定义的范围内:</p>



<p id="separator-block_623b36b85bb7e" class="block-separator block-separator--15">合适的权重初始化器</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://web.archive.org/web/20230103154738/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing/attachment/vanishing-and-exploding-gradients-8" target="_blank" rel="noopener"><img decoding="async" src="../Images/dfc8f1ef31f973da0a3bc6c17640deb9.png" alt="Layer gradients from the gradients clipping model generated using Neptune.ai" class="wp-image-64711" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-exploding-gradients-8.png?ssl=1"/></a><figcaption><em><em>Layer gradients from the gradients clipping model generated using Neptune.ai</em> | <em><em><em><a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/e/GRAD1-56/charts">Source</a></em></em></em></em></figcaption></figure></div>



<h4>如前所述，梯度爆炸的一个主要原因在于太大的权重初始化和更新，这是我们的回归模型中梯度爆炸的原因。因此，正确初始化模型权重是解决这个爆炸梯度问题的关键。</h4>



<p>与消失梯度相同，我们将使用正态分布实现Glorot初始化。</p>



<p>由于Glorot初始化与Tanh或Sigmoid配合使用效果最佳，我们将在本实验中将Tanh指定为激活函数:</p>



<p>这是该模型的渐变图，修复了渐变爆炸问题:</p>



<pre class="hljs">curOptimizer = tf.keras.optimizers.SGD()

curInitializer = <span class="hljs-string">'glorot_normal'</span>


model = regressionModel(X, y, curInitializer, USE_L2REG=<span class="hljs-keyword">False</span>, secondLayerAct=<span class="hljs-string">'tanh'</span>)

model.compile(loss=<span class="hljs-string">'mean_squared_error'</span>, optimizer=curOptimizer, metrics=[<span class="hljs-string">'mse'</span>])

curModelName = <span class="hljs-string">'GlorotInit'</span></pre>



<p>L2范数正则化</p>



<p id="separator-block_623b36c45bb7f" class="block-separator block-separator--15">除了权重初始化之外，另一个优秀的方法是采用<a href="https://web.archive.org/web/20230103154738/https://ml-cheatsheet.readthedocs.io/en/latest/regularization.html" target="_blank" rel="noreferrer noopener nofollow"> L2正则化</a>，它通过将模型权重的平方项强加到损失函数来惩罚大的权重值:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><a href="https://web.archive.org/web/20230103154738/https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing/attachment/vanishing-and-exploding-gradients-7" target="_blank" rel="noopener"><img decoding="async" src="../Images/ecf4d5abec6974a4365ca084768e8020.png" alt="Layer gradients from the Glorot initialization model generated using Neptune.ai" class="wp-image-64710" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-exploding-gradients-7.png?ssl=1"/></a><figcaption><em>Layer gradients from the Glorot initialization model generated using Neptune.ai | <em><em><a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/e/GRAD1-54/charts">Source</a></em></em></em></figcaption></figure></div>



<h4>添加L2范数经常会导致整个网络中较小的权重更新，并且在Keras中使用args实现这种正则化相当简单。`<em>内核_正则化子</em>`和`<em>活动_正则化子</em>`:</h4>



<p>根据Razvan等人的建议，我们将这个模型的初始权重设置为glorot normal。，2013 初始化小值和方差的参数。这里显示了每层的损耗曲线和梯度:</p>







<p> </p>



<pre class="hljs">curInitializer = <span class="hljs-string">'glorot_normal'</span>
x = Dense(<span class="hljs-number">35</span>, activation=<span class="hljs-string">'tanh'</span>, kernel_initializer=curInitializer,
          kernel_regularizer=regularizers.l2(<span class="hljs-number">0.01</span>),
          activity_regularizer=regularizers.l2(<span class="hljs-number">0.01</span>))(inp)


curInitializer = <span class="hljs-string">'glorot_normal'</span>
model = regressionModel(X, y, curInitializer, USE_L2REG=<span class="hljs-keyword">True</span>)
model.compile(loss=<span class="hljs-string">'mean_squared_error'</span>, optimizer=curOptimizer, metrics=[<span class="hljs-string">'mse'</span>])

curModelName = <span class="hljs-string">'L2Reg'</span></pre>



<p>再次，通过所有历元将梯度控制在合理的范围内，并且模型逐渐收敛。</p>



<p id="separator-block_623b36d25bb80" class="block-separator block-separator--15">最后的话</p>



<figure class="wp-block-image size-large is-resized"><img decoding="async" loading="lazy" src="../Images/7701a538c3cf7b6bc7d245645a6c2959.png" alt="Layer gradients from the L2 regularization model generated using Neptune.ai" class="wp-image-64708" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230103154738im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Vanishing-and-exploding-gradients-9.png?resize=900%2C259&amp;ssl=1"/><figcaption><em><em>Layer gradients from the L2 regularization model generated using Neptune.ai</em> | <em><em><a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/e/GRAD1-58/charts">Source</a></em></em></em></figcaption></figure>



<p>除了我们在本文中讨论的主要技术，其他值得尝试避免/修复渐变的方法包括<a href="https://web.archive.org/web/20230103154738/https://arxiv.org/abs/1502.03167v3" target="_blank" rel="noreferrer noopener nofollow"><strong/></a>和<a href="https://web.archive.org/web/20230103154738/https://cnvrg.io/gradient-clipping/" target="_blank" rel="noreferrer noopener nofollow"> <strong>缩放输入数据</strong> </a>。这两种方法都可以使您的网络更加健壮。直觉是，在反向投影过程中，我们每一层的输入数据可能会有很大的不同(就像前一层的输出一样)。使用批量归一化允许我们固定每个图层的输入数据的均值和方差，从而防止其移动过多。有了更强大的网络，就不太可能遇到两个梯度问题。</p>







<h2 id="h-final-words">在本文中，我们讨论了与神经网络训练相关的两个主要问题-消失和爆炸梯度问题。我们解释了它们的原因和后果。我们还研究了解决这两个问题的各种方法。</h2>



<p>希望你发现这篇文章是有用的，并且学到了实用的技术来训练你自己的神经网络模型。供您参考，完整的代码可以在我的<a href="https://web.archive.org/web/20230103154738/https://github.com/YiLi225/NeptuneBlogs/blob/main/GradientsIssuesNeptune.py" target="_blank" rel="noreferrer noopener nofollow"> GitHub repo这里</a>获得，而<a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/experiments?compare=GwGgTOEIwBzkA&amp;split=bth&amp;dash=charts&amp;viewId=standard-view"> Neptune项目可以在这里</a>获得。</p>



<p>In this article, we have discussed two major issues associated with neural network training – the Vanishing and Exploding gradients problems. We explained their causes and consequences. We also walked through various approaches to address the two problems. </p>



<p>Hope you have found this article useful and learned practical techniques to use in training your own neural network models. For your reference, the full code is available in my <a href="https://web.archive.org/web/20230103154738/https://github.com/YiLi225/NeptuneBlogs/blob/main/GradientsIssuesNeptune.py" target="_blank" rel="noreferrer noopener nofollow">GitHub repo here</a> and the <a href="https://web.archive.org/web/20230103154738/https://app.neptune.ai/katyl/GradientsNew/experiments?compare=GwGgTOEIwBzkA&amp;split=bth&amp;dash=charts&amp;viewId=standard-view">Neptune project is available here</a>.</p>
        </div>
        
    </div>    
</body>
</html>