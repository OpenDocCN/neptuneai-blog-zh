<html>
<head>
<title>7 Applications of Reinforcement Learning in Finance and Trading </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>7强化学习在金融和交易中的应用</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/7-applications-of-reinforcement-learning-in-finance-and-trading#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/7-applications-of-reinforcement-learning-in-finance-and-trading#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>在本文中，我们将探索7个真实世界的交易和金融应用程序，其中强化学习用于获得性能提升。</p>



<p>好的，但是在我们进入这篇文章的本质之前，让我们定义一些我稍后会用到的概念。</p>



<p>首先，让我们快速定义强化学习:</p>



<blockquote class="wp-block-quote is-style-default"><p>一个学习过程，在这个过程中，一个主体通过试错法与其环境相互作用，以这样一种方式达到一个确定的目标:对于主体为达到其目标所做的每一个正确的步骤，主体可以最大化奖励的数量，并最小化环境给予的惩罚。</p></blockquote>



<p>酷，现在我会经常用到几个关键词:</p>



<ol><li><strong>深度强化学习(DRL): </strong>利用深度学习来逼近处于强化学习核心的价值或政策函数的算法。</li><li><strong>策略梯度强化学习技术:</strong>用于解决强化学习问题的方法。策略梯度方法的目标是直接建模和优化策略功能。</li><li><strong>深度Q学习:</strong>用神经网络逼近<strong> Q </strong>值函数。Q值函数为工作的代理创建了一个精确的矩阵，它可以“参考”这个矩阵，以使它的长期回报最大化。</li><li><strong>门控递归单元(GRU): </strong>特殊类型的递归神经网络，在门控机制的帮助下实现。</li><li><strong>门控深度Q学习策略:</strong>深度Q学习与GRU的结合。</li><li><strong>门控策略梯度策略:</strong>策略梯度技术与GRU的结合。</li><li><strong>深度递归Q网络:</strong>递归神经网络与Q学习技术的结合。</li></ol>



<p>好了，现在我们准备看看强化学习是如何在金融世界中被用来实现利润最大化的。</p>



<h2 id="h-1-trading-bots-with-reinforcement-learning">1.具有强化学习的交易机器人</h2>



<p>由强化学习驱动的机器人可以通过与交易和股票市场环境互动来学习。他们使用试错法来优化他们的学习策略，这些策略是基于股票市场上每一只股票的特征。</p>







<p>这种方法有几大优势:</p>



<ul><li>节省时间</li><li>交易机器人可以24小时交易</li><li>所有行业的交易都变得多样化</li></ul>



<p>例如，您可以使用深度Q学习项目来检查<a href="https://web.archive.org/web/20221117203659/https://github.com/pskrunner14/trading-bot" target="_blank" rel="noreferrer noopener nofollow">股票交易机器人。这里的想法是使用深度Q学习技术创建一个交易机器人，测试表明，在给定一组股票进行交易的情况下，一个经过训练的机器人能够在一段时间内进行买卖。</a></p>



<p>请注意，这个项目不是基于计算交易成本，执行交易的效率等。–所以这个项目不可能在现实世界中出类拔萃。另外，由于它的顺序方式，项目的训练是在CPU上完成的。</p>







<h2 id="h-2-chatbot-based-reinforcement-learning">2.基于聊天机器人的强化学习</h2>



<p>聊天机器人通常在<strong>序列到序列建模</strong>的帮助下进行训练，但是将强化学习添加到组合中可以为股票交易和金融带来巨大优势:</p>



<ul><li>聊天机器人可以充当经纪人，向用户运营商提供实时报价。</li><li>基于对话式UI的聊天机器人可以帮助客户解决他们的问题，而不是来自员工或后端支持团队的人。这节省了时间，并将支持人员从重复的任务中解放出来，让他们专注于更复杂的问题。</li><li>聊天机器人还可以在交易时间内给出开盘价和收盘价的建议。</li></ul>



<p><a href="https://web.archive.org/web/20221117203659/https://github.com/pochih/RL-Chatbot" target="_blank" rel="noreferrer noopener nofollow">深度强化学习聊天机器人</a>项目展示了一个基于强化学习的聊天机器人实现，使用策略梯度技术实现。</p>







<h2 id="h-3-risk-optimization-in-peer-to-peer-lending-with-reinforcement-learning">3.基于强化学习的P2P贷款风险优化</h2>



<p>P2P借贷是一种通过在线服务向个人和企业提供贷款的方式。这些在线服务为贷款人和投资者牵线搭桥。</p>



<p>在这些类型的在线市场中，强化学习派上了用场。具体来说，它可以用于:</p>



<ul><li>分析借款人的信用评分以降低风险。</li><li>预测年化回报，由于在线业务的管理费用较低，与银行提供的储蓄和投资产品相比，贷方可以期待更高的回报。</li><li>它还可以帮助估计借款人有能力偿还债务的可能性。</li></ul>



<p>使用神经网络的点对点借贷机器人顾问项目是一个用神经网络构建的在线借贷平台。它没有使用强化学习，但你可以看到这只是一种试验性的&amp;错误场景，其中RL将非常有意义。</p>







<h2 id="h-4-portfolio-management-with-deep-reinforcement-learning">4.具有深度强化学习的投资组合管理</h2>



<p>投资组合管理意味着将客户的资产投入股票，并持续管理，以帮助客户实现其财务目标。在深度策略网络强化学习的帮助下，随着时间的推移，可以优化资产的配置。</p>



<p>在这种情况下，深度强化学习的好处是:</p>



<ul><li>它提高了人类管理者的效率和成功率。</li><li>它降低了组织风险。</li><li>它增加了组织利润方面的投资回报(ROI)。</li></ul>



<p><a href="https://web.archive.org/web/20221117203659/https://github.com/selimamrouni/Deep-Portfolio-Management-Reinforcement-Learning" target="_blank" rel="noreferrer noopener nofollow">针对金融投资组合管理问题的深度强化学习框架</a>——该项目展示了投资组合管理与深度策略网络强化学习的实现。</p>







<h2 id="h-5-price-setting-strategies-with-reinforcement-learning">5.具有强化学习的定价策略</h2>



<p>复杂性和动态的股价变化是理解股价的最大挑战。为了理解这些性质，门控递归单元(GRU)网络与强化学习一起工作得很好，提供了如下优点:</p>



<ul><li>提取能够代表股票内在特征的信息性财务特征。</li><li>在交易过程中帮助决定止损和止盈。</li></ul>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/c55014faf56a85f6ca0db66c4a020112.png" alt="RL price setting" class="wp-image-29930" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203659im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/RL-price-setting.jpg?resize=512%2C264&amp;ssl=1"/><figcaption><em>Photo by Olya Kobruseva | Source: Pexels</em></figcaption></figure></div>



<p>为了支持上述说法，<a href="https://web.archive.org/web/20221117203659/https://arxiv.org/ftp/arxiv/papers/1803/1803.03916.pdf" target="_blank" rel="noreferrer noopener nofollow">时间序列的深度强化学习:玩理想化的交易游戏</a>论文显示了堆叠门控递归单元(GRU)、长短期记忆(LSTM)单元、卷积神经网络(CNN)和多层感知器(MLP)中哪一个表现最好。</p>



<p>在捕捉波浪状价格时间序列的单变量游戏中，用于模拟Q值的GRU代理表现出最佳的整体性能。</p>



<p>可以将强化学习应用于GRU的两种技术是:</p>



<ul><li>门控深度Q学习策略</li><li>门控策略梯度策略</li></ul>



<p>要更好地理解这些技术，可以看看这篇文章:<a href="https://web.archive.org/web/20221117203659/https://www.sciencedirect.com/science/article/pii/S0020025520304692?dgcid=rss_sd_all" target="_blank" rel="noreferrer noopener nofollow">深度强化学习方法的自适应股票交易策略</a>。</p>



<h2 id="h-6-recommendation-systems-with-reinforcement-learning">6.具有强化学习的推荐系统</h2>



<p>当谈到在线交易平台时，基于强化学习技术的推荐系统可以改变游戏规则。这些系统有助于在交易时向用户推荐正确的股票。</p>







<p>强化学习有助于在接受大量股票培训后选择最佳股票或共同基金，最终带来更好的投资回报。</p>



<p>这里的优势可以是:</p>



<ul><li>通过根据用户在平台上的行为提供终身选股建议来吸引现有用户。</li><li>通过推荐好股票来帮助新手交易。</li><li>从而更容易决定选择哪些股票。</li></ul>



<p>项目展示了这样一个系统的实现。</p>



<h2 id="h-7-maximizing-profit-with-minimum-capital-investments">7.以最小的资本投资实现利润最大化</h2>



<p>如果我们把以上几点结合起来，我们可以构建一个自动化系统来获得高回报，同时保持尽可能低的投资。</p>







<p>在强化学习的帮助下，可以训练一个代理人，强化学习可以从任何来源获得最小的资产，并将其分配到股票中，这可以在未来使ROI翻倍。</p>



<p>如今，RL代理已经能够学习优于人们过去应用的简单买卖策略的最优交易策略。这可以在马尔可夫决策过程(MDP)模型的帮助下，使用深度递归Q网络(DRQN)来实现。理解这个概念的一个很好的资源是<a href="https://web.archive.org/web/20221117203659/https://arxiv.org/pdf/1507.06527.pdf" target="_blank" rel="noreferrer noopener nofollow"> <strong>针对部分可观察MDP的深度递归Q学习</strong> </a> <strong>。</strong></p>



<h2 id="h-proceed-with-caution">小心行事</h2>



<p>需要补充的是，我们列出的很多项目本质上都是为了好玩而做的项目。他们是根据过去的数据训练的，没有经过适当的回溯测试。在看不见数据的情况下(例如COVID stats)，下行风险比模型预期的要大得多。</p>



<p>市场是一个复杂的系统，机器学习系统很难只根据历史数据来理解股票。基于ML的交易策略的表现可以很好，但是它也会让你耗尽你的积蓄。所以对这些项目要有所保留。</p>



<h2 id="h-conclusion">结论</h2>



<p>强化学习总是被低估。通过在本文中展示RL的金融和交易用例，我想分享关于RL如何有用的意识，为新的学习者和现有的开发者创建一条积极的道路，以更多地探索这个领域。真是个引人入胜的话题！</p>


        </div>
        
    </div>    
</body>
</html>