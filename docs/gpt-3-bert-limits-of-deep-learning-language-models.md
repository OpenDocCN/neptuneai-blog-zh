# GPT-3 或伯特能理解语言吗？深度学习语言模型的⁠—The 极限

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/gpt-3-bert-limits-of-deep-learning-language-models>

当一个话题成为《卫报》观点文章的基础时，可以有把握地认为它是主流。不寻常的是，当这个主题是一个相当小众的领域，涉及应用深度学习技术来开发自然语言模型。更不寻常的是其中一个模特(GPT-3)自己写了这篇文章！

可以理解的是，这引起了一系列末日终结者式的社交媒体热议(以及一些对《卫报》的批评，指责其误导了 GPT 3 号的能力)。

对于像 NLP 这样的领域来说，这是一个罕见和意想不到的时间，成为"*人工智能(AI)与人类*辩论的前沿和中心。这个负担通常落在机器人身上(最近的自动驾驶汽车)，因为更容易想象被人工智能驱动的机械怪物碾过或攻击。最初，可以生成文本的 NLP 模型似乎没有红眼终结者那么可怕。

然而，近年来在这一领域取得的快速进展已经产生了像 GPT-3 这样的语言模型。许多人声称，这些 LMs 理解语言是因为他们有能力撰写《卫报》评论文章、[生成](https://web.archive.org/web/20221206181234/https://analyticsindiamag.com/open-ai-gpt-3-code-generator-app-building/) React 代码，或者[执行一系列其他令人印象深刻的任务](https://web.archive.org/web/20221206181234/https://blog.exxactcorp.com/what-can-you-do-with-the-openai-gpt-3-language-model/)。

但是这些模型在简单的自然语言处理任务中表现得有多好呢？

真的有证据表明他们“明白”自己在说什么吗？

撰写《卫报》文章的 GPT-3 模型“理解”它所说的吗？

它能像人一样保护这块棋子吗？

学习新任务的能力有多强？

即使从纯粹实用的商业角度来看，理解这些模型的潜在限制也是很重要的。

*   如果它们真的像宣传的那样好，那么你的企业立即开始采用这些技术是至关重要的，因为它们将比电报、电力或铁路等技术产生更大的变革影响。
*   相反，**如果它们被过度宣传**，那么它可能会改变你在未来计划中如何看待和使用这些模型。

**为了理解 NLP，我们需要看一下这些语言模型的三个方面:**

## 

*   1 概念上的限制:阅读大量大量的文本是否有可能理解语言？如果我们试图理解人类是如何学习和使用语言的，那么机器仅从文本中学习的能力似乎会受到隐性限制。
*   技术限制:即使这些模型有可能在语言任务中发展出类似人类的技能，但目前的模型适合这项工作吗？这些模型的底层架构是否使它们无法实现其全部潜力？
*   评估限制:也许问题仅仅在于我们没有能力正确评估这些模型？目前的炒作是否与我们用来测试这些模型的 NLP 任务已经过时并且过于简单有关，因为该领域最近取得了快速的进步？

## 语言模型概念限制:我们能从文本中学到什么？

训练任何 DL 模型的大问题是数据。你通常需要很多。多少钱？越多越好，这也是最近大多数 LMs 追随的趋势。

不需要过多地研究每个模型的设计细节(我们将在下一节中讨论)，我们可以认为一般的方法是通过阅读越来越多的文本来理解语言。

**关键是文本不需要贴标签**。相反，这些模型可以阅读一本书或一篇博客文章，并试图理解单词在上下文中的含义。例如，“深度学习”一词将主要用于与“机器学习”、“神经网络”或“人工智能”相关的事物。因此，模型将开始把这些术语视为具有某种相关的上下文。随着越来越多的数据，他们将开始了解更多术语的细微差别，或者这些相关术语之间的不同用法和含义。至少理论上是这样。

作为所需数据量的例子，让我们以 [BERT](https://web.archive.org/web/20221206181234/https://arxiv.org/pdf/1810.04805.pdf) 为例。发布于 2018 年，是近年来最具影响力的模型之一，结合了 28 亿字的维基百科数据和 8 亿字的图书语料库数据，使用了 3.4 亿个参数。

[GPT-2](https://web.archive.org/web/20221206181234/https://openai.com/blog/better-language-models/) (该模型是[太危险而无法发布](https://web.archive.org/web/20221206181234/https://slate.com/technology/2019/02/openai-gpt2-text-generating-algorithm-ai-dangerous.html))在 2019 年初跟随伯特，在 800 万个网页(~40 GB 文本数据)上进行训练，包含 15 亿个参数。相比之下，OpenAIs GPT(卫报写作模型)的最新版本 GPT-3 包含高达 1750 亿个参数，并在来自各种不同文本来源的 45TB 总数据集上进行训练。

像 GPT-3 这样的模型表明，某些任务的性能随着参数的增加而提高(在这个例子中，随着任务演示或指令的增加；图中的零个、一个或几个镜头)。但这是否意味着这些模型开始“理解”语言了呢？ | *来源:*[*【GPT-3】论文*](https://web.archive.org/web/20221206181234/https://arxiv.org/pdf/2003.05002.pdf)

从高层次来看，**很容易看出这里的趋势**:创建具有更多参数的模型，让它们消费越来越多的文本数据，模型将开始在人类层面上“理解”语言。

证据表明，这种方法似乎正在发挥作用。GPT-3 似乎是最先进的模型之一，它可以很好地执行各种不同的语言任务，而不需要更多的培训。

然而，最近的一篇论文对这种方法的可行性提出了一些有趣的担忧。

### 像 GPT-3 或伯特这样的语言模型能通过章鱼测试吗？

在他们的论文“[攀登 NLU:论数据时代的意义、形式和理解](https://web.archive.org/web/20221206181234/https://openreview.net/pdf?id=GKTvAcb12b)”中，艾米丽·本德和亚历山大·柯勒考虑了像 GPT-3 或伯特这样的 LMs 是否能学会“理解”语言——不管他们能接触到多少文本或有多少参数来处理这些信息。他们提出的关键问题是形式和意义之间的**关系。**

根据他们的论文，**形式是语言的可识别的物理部分**。代表语言的标记和符号，如页面上的符号或网页上的像素和字节。

**的意思是这些元素如何与外部世界的事物联系起来**。这里需要注意的是，作者假设所讨论的模型只使用文本进行训练，而没有使用文本和图像的任何组合或其他表示外部世界的元素进行训练。从这个意义上说，就像 GPT-3 和伯特一样，这些模型试图只从形式中学习意义。

把它想象成塞尔中文教室实验(作者在论文中提到)，仅仅从形式上学习就像试图用一种你一无所知的语言交流，因为教科书和字典都是用这种奇怪的语言编写的。这类似于当前的 LMs 试图通过查看大量文本数据来做的事情。

![NLP limitations](img/02778ab96608d5be7c7d15fc580d9c1d.png)

*Could a machine learn the intent of the Napolean pose from form alone? Locked in a room, reading only text could the machine identify that the statement referred to a particular type of pose?*

你可能会问，这和章鱼有什么关系？

章鱼测试是一个有趣的思维实验，在论文中用来展示当前的 LMs 如何永远无法真正“理解”语言。你应该去看看这篇论文，它对实验有更详细的描述，这是思维实验力量的一个很好的例子，也是对充满疯狂方程式的 DL 论文的一个很好的改变。但它的要点是:

想象一下，一只章鱼 O 被放在两个人 A 和 B 之间，他们都被困在遥远的荒岛上，只能通过水下类似电报的系统进行通信。像 LMs 一样，章鱼可以监听 A 和 B 之间的对话。想象一下，它们这样做的时间足够长，可以说出 A 和 B 能够使用的几乎每个可能的单词、短语或句子。O 能以一种显示 O“理解”他们正在谈论的方式与 A 或 B 交流吗？

我们可以很容易地想象这样的场景，O 和 A 或 B 之间的琐碎对话看起来完全有效和合理。A 和 B 都不知道他们在和一只章鱼说话。GPT 3 号似乎能够做到这一点——以类似人类的方式与人交流。

然而，这只在一定程度上有效。想象一个不同的任务，其中 A 或 B 要求 O 建造一个重要的项目(如椰子弹射器)，报告它如何工作，并提供潜在的改进建议。我们在这里可以开始看到，O 没有办法“理解”如何构建它，或者所需的项目看起来像什么。形式和意义之间没有联系。

同样，随着任务性质的改变，意义和形式之间的联系变得越来越重要，这也正是 O 开始显示语言局限性的地方。

当我们想象这些场景时，不难想到像伯特或 GPT-3 这样的 LMs 很难“理解”他们在说什么，因为他们缺乏形式和意义之间的联系。他们像拼图一样把东西连在一起，识别他们在过去看到的模式。但是他们并不真正明白他们在做什么，或者为什么。

人们可能会声称这对于许多 NLP 任务并不重要，或者我们并不真正需要关心这些模型是否“理解”事物，而是它们是否能够执行类似人类的任务。也许这是一个纯粹的学术讨论，与这些模型在商业意义上是否有用无关？

即使我们假设这些模型可以仅从形态中学习足够多的知识，以接近人类的水平执行，这可能仍然不够。如果这些模型的核心架构限制了它们学习的能力，甚至是单独学习的能力，那么它们是否“理解”它们在说什么就没有意义了。这就是我们接下来要看的。

## 语言模型技术极限:LMs 是“作弊”吗？

他们没有服用提高成绩的药物，但像伯特和 GPT-3 这样的 LM 模型可能会获得“不公平”的优势。为了理解这是如何可能的，我们需要深入像 BERT 这样的模型的底层架构的细节；变压器架构。

正是这种架构声称可以帮助 LMs 从他们接受训练的大量文本数据集中学习“上下文”。但如果看起来根本不是真的在学习“语境”呢？如果 LMs 发现隐藏在文本数据中的线索怎么办？

使用这些线索，学习模型可以在特定任务上表现良好，如问答、实体识别或情感分析，但它实际上只有非常有限的语言洞察力。

当我们对底层文本数据进行微妙的更改时，问题就出现了——这些更改不会影响人类的表现，但会使像 BERT 这样的 LMs 几乎“无言以对”。如果是这种情况，那么这些模型可能甚至难以学习形式或语言语义的关键部分，而它们需要在许多重要的语言任务中表现出色。

*2012 年发表的一篇[论文中的表格，该论文创建了一个基于规则的结构，使用否定等线索来更好地执行 NLP 任务。这是一个 LMs 可以用来在 NLP 任务中“作弊”的“线索”的例子。但是，如果变形金刚使用这些简单的机制，那么它可能会对其“理解”人类语言更复杂方面的潜力提出质疑。](https://web.archive.org/web/20221206181234/https://www.aclweb.org/anthology/S12-1037.pdf)*

### 语境科学

5 月 6 日，美国总统唐纳德·特朗普[在椭圆形办公室](https://web.archive.org/web/20221206181234/https://www.whitehouse.gov/briefings-statements/remarks-president-trump-signing-proclamation-honor-national-nurses-day/)的一次活动中发表了如下声明:

“这真是我们遇到的最严重的袭击。这比珍珠港事件还糟糕。这比世贸中心还糟糕。从来没有过这样的袭击。而且应该从来没有发生过。

他在说什么？新的战争？新的恐怖袭击？也许他的下一句话有助于澄清这一点:

“它本可以在源头上被阻止。在中国本来是可以制止的。它应该在源头就被阻止，但它没有。

还不清楚？好吧，总统在前面一段的话应该提供了所需的明确性:

“…这种病毒将会消失。这是一个何时的问题。会不会小范围卷土重来？会不会以一种相当大的方式卷土重来？但我们现在知道如何更好地应对它。”

这里的关键要点是背景很重要。如果你不记得前面的段落，那么你可能不知道总统说的是冠状病毒疫情。语言是棘手的，它可能是混乱的，在任何语言环境中，我们都需要不断更新我们的上下文“缓存”，以便我们可以从我们正在处理的单词中推断出意思。

从语言和实用的角度来看，溢出的咖啡都是“脏兮兮”的！动词“spill”根据上下文可能有不同的含义，因此我们需要不断更新上下文“cache ”,以了解在每个场景中“spill”指的是什么。例子来自丹尼尔·t·威灵汉姆的书《阅读思维:理解思维如何阅读的认知方法》

2017 年发表了一篇新论文，“[注意力是你所需要的全部](https://web.archive.org/web/20221206181234/https://arxiv.org/pdf/1706.03762.pdf)”，这永远改变了 DL NLP 的格局。它仍然是你读到的任何标题的前沿，关于一个新的模型在 NLP 任务中打破性能基准。

其中一个主要原因是神经网络设计，被称为转换器，允许模型在解析文本时更容易地捕捉上下文。这在过去是一项困难的任务，因为底层架构以顺序的方式解析文本。

这意味着它必须一个单词一个单词，一个句子一个句子，所以在一个大的文本语料库上训练是非常慢的。其次，这意味着维护任何形式的长期上下文在计算上都是非常昂贵的。参考上面总统的评论，这些模型很难存储早期的上下文，即“攻击”指的是冠状病毒。

在不涉及太多细节的情况下，Transformer 架构使用键、查询和值参数，这些参数使它能够知道文本的哪一部分与这个特定的上下文最相关。想想经典的“河岸”对“钱庄”的场景。在[早期的模型](https://web.archive.org/web/20221206181234/http://jalammar.github.io/illustrated-word2vec/)中，单词的意思是静态的，它不会根据上下文而改变。所以“银行”这个词在“我把鱼竿放在河边了”和“我今天把钱存到银行了”这两个词中的意思是一样的。同样，如果没有背景，特朗普总统的评论可能会有很大不同。

![](img/1dabd383b5013ce336c268591d150ff0.png)

《变形金刚》美不胜收。如果这个图让你害怕，不要担心，现在确切地知道它是如何工作的并不重要。从这张图中可以看出，有趣的是，变压器实际上由两部分组成:编码器和解码器。不同的模型将使用该架构的不同部分。例如，伯特使用编码器部分，而 GPT 模型使用解码器部分。来源:[关注是你所需要的](https://web.archive.org/web/20221206181234/https://arxiv.org/pdf/1706.03762.pdf)

### 这都是关于注意力

Transformer 架构利用一种称为“注意”的机制来解决 NLP 中的上下文问题。注意力之前已经被其他神经网络使用过多次，但 Transformer 的独特之处在于它只使用注意力从文本中学习(因此论文标题为“注意力是你所需要的”)。

以前的模型使用注意力作为他们方法的一部分，通常是以次要的方式。 **Transformer 确实在注意力**的理念上加倍努力，将单个注意力元素打包在一起，称为“注意力头”，形成多头注意力模块。然后，它将这些多头模块堆叠在一起，形成注意力层。

简而言之，将注意力头想象为能够“聚焦”在一个单词(或单词的一部分)上，它可以告诉模型该单词对于理解当前被解析的单词有多相关或重要。

更多的注意力意味着你的模型可以回顾(或展望)一个句子或段落中的更多单词。更多层次的关注意味着你的模型可以学习更高层次的句法结构和语义。

*粗线表示被注意力集中器识别为与当前正在处理的单词的意思更相关的单词。所以“抹布”和“咖啡”比“和”或“得到”更影响“溢出”的意思。*

任何神经网络基本上都是大量的矩阵乘法，注意机制在这里也没什么不同。下表显示了一个玩具示例，展示了单词“*从关注层溢出的*”的输出。

它通过将不同的权重矩阵相乘来实现这一点，每个权重矩阵“学习”一个权重，以尝试并识别句子中的哪些单词网络应该“注意”，因为它们对特定单词的上下文很重要。

|  | Trisha | 溢出 | 她 | 咖啡 | 和 | 但 | 跳 | 向上 | 到 | 得到 | a | 破布 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |

代表单词“溢出”的最终向量由表格中所示的所有其他单词的权重组成。注意，原始结果是通过 softmax 函数得到的，所以它们加起来都是 1。因此，表示“溢出”的最终向量将由它自己的大部分含义组成，但也包含表示“Trisha”和“coffee”等向量的一些含义或权重。如果这是像“Word2Vec”这样的静态单词嵌入模式，那么“溢出”的单词将 100%来自它自己的单一含义，即它根本不使用任何上下文。

我知道，这是对 Transformer 架构的一个旋风般的概述(更详细地理解它的一个很好的资源是 Jay Alammar 关于主题的优秀的[博客文章)。但它留给我们两个关键的假设，我们可以做，这将支持 LMs 可以“理解”语言的说法:](https://web.archive.org/web/20221206181234/http://jalammar.github.io/illustrated-transformer/)

1.  像伯特和 GPT-3 **这样的模型使用 Transformer 架构的注意力机制从基于文本的数据集中学习上下文**。
2.  通过学习语境，这些模型**发展了某种程度的语言“技能”**，这使它们能够更好地完成一系列语言任务。

但是如果我们能证明这两个假设都存在疑问，那么就很难宣称这些模型能够发展出任何“理解”语言的能力。

### 伯特学——伯特学到了什么？

在他们 2019 年的论文“揭示伯特的黑暗秘密”中，作者深入研究了伯特的内部工作原理。他们的一个重要发现是 BERT 被过度参数化了。

作者通过禁用一个或多个注意力头，然后比较结果，对此进行了研究。他们的发现令人惊讶——移除注意力不仅不会影响结果，而且在某些情况下还会提高表现。

应该注意的是，并不是所有的 NLP 任务都是这样，去除一些注意力确实会对表现产生负面影响。但是这种情况发生的次数足够多，以至于作者们对 BERT 中如此多的注意力中心的相关性提出了质疑。

实验表明，你可以从 BERT 中的注意力层移除单个头部，它将在某些任务上执行相同或更好的任务。更令人惊讶的是，它们表明你可以移除整个层，即所有的注意力头，并且不会严重影响性能。

这提出了一些重要的问题。这些模型如何通过少量的注意力来学习语言的复杂性和细微差别？其他的大脑只是简单地储存他们以后使用的信息，而不是通过上下文学习规则和结构吗？

你可以说这意味着注意力是如此强大，BERT 只需要利用它的一小部分潜能就能在 NLP 任务中表现出色。

在下一节中，我们将更详细地研究这一说法，因为这也与评估数据集的结构有关。至少，**这些发现让我们质疑简单地装载越来越多的注意力头是否会导致“理解”**语言的模型。

相反，如果我们想开发真正理解语言的模型，我们可能需要修剪和重新设计这些网络。作为证据，参数巨头 GPT-3 背后的 OpenAI 团队在他们自己的论文中指出，我们可能会触及语言模型从更多参数和更多训练中学习的极限。

**" *本文描述的一般方法的一个更基本的限制——扩大任何 LM 类模型，无论是自回归模型还是双向模型——是它可能最终会遇到(或可能已经遇到)预训练目标的限制***

### 语言模型是作弊吗？

作弊呢？这与我们的主张有关，即这些模型能够通过上下文学习一些关于语言的东西，这有助于它们在 NLP 任务中表现得更好。

通过能够解析不同的句子，查看所有的单词，并以特定于上下文的方式识别出重要的单词，这些模型应该能够识别出特朗普谈论的是冠状病毒，而不是我们之前例子中的恐怖袭击。这将有助于他们在一系列 NLP 任务中表现出色，而这些任务以前超出了 NLP 模型的能力。

最近的一系列论文声称像 BERT 这样的模型并没有真正以任何有意义的方式理解语言。他们以一种创造性的方式展示了这一点，通过改变一些评估数据集，然后查看结果。首先，他们分析数据集，在这些数据集上，像 BERT 这样的模型表现得非常好，以至于它们在这项任务中超过了人类。然后他们以一种对结果解释没有影响的方式改变这些数据集。

例如，他们发现数据集中的许多短语含有否定成分，如“不”、“不会”或“不能”。使用简单的规则来“切断”这些标识符将导致高的总得分。[论文作者随后改变了这些数据集](https://web.archive.org/web/20221206181234/https://www.aclweb.org/anthology/P19-1459.pdf)，以便他们在保持数据集整体结构的同时移除这些“线索”。

对一个人来说，或者任何一个最初“恰当地”对任务进行推理的人来说，他们的分数不应该有很大的差异。这相当于说:

*“没有下雨，所以我可以去跑步了”，*

和

*“下雨了，所以我不能去跑步了”，*

也就是说，我们改变了最初的前提，但这不应该使推断正确答案的任务变得更加困难。

如果我们没有“作弊”，并且我们明白我们不能在雨中奔跑，那么我们应该在两种情况下都做出正确的推断。一个被否定的事实不应该导致我们做出错误的推断，但这正是伯特所做的。它没有表现出人类的水平，而是立即下降到比随机表现好不了多少。

现在我们知道伯特:

*   不会用所有的注意力从上下文中学习，
*   似乎不能用它所学到的来“推理”或“理解”语言
*   它似乎使用统计“线索”，像否定术语“不”和“不能”，作为一种粗糙的启发式方法来获得更好的结果。

我们能责怪模型本身吗，还是测试人员有错？

## 语言模型评估限制:像伯特或 GPT-3 这样的模型有多好？

到目前为止，我们已经考虑了一个广泛的哲学问题:当前的深度学习 NLP 模型可以仅通过文本学习理解语言吗？

即使我们假设这些模型可以仅从文本中学习高水平的语言知识，我们也要看看支撑新 Transformer 架构的内部结构——这是最新进展的关键。我们表明，这些模型是否能够扩展到能够开发类似人类的语言知识的水平，还存在一些问题。

所有这些都基于这样一个假设，即我们可以通过某种方式测试这些模型，看看它们的表现如何。我们假设有数据集和基准可以告诉我们这些模型是否真的学会了可转移的、类似人类的语言技能。

我们已经看到，像 BERT 这样的模型可以在一些测试中“作弊”，但这是一个异常值还是现代 NLP 数据集很容易被当前的 DL LMs 套件选中？如果这些模型可以使用一些简单的技巧来获得高分，那么我们将很难知道这些模型是否真的在提高他们的语言能力。

### 问正确的问题

有许多 NLP 任务可以用来评估模型。一些，如命名实体识别(NER)和词类(POS)，着眼于模型理解语言的句法和层次结构的能力。

它们代表了语言的核心部分，是高级语义发展的基础。如果我们想声称新的语言模型理解语言，那么我们想看看它们在更复杂、更高级的任务(如问答)中的表现。

在这里，模型需要理解上下文、推理和语义相似性。正如我们前面提到的，像 BERT 这样的模型已经在一些更高层次的复杂任务中表现出了类似人类的水平。

但是我们也看到了这些模型可以作弊。那么，这些模型仅仅是以比基准能跟上的更快的速度改进，还是显示出语言知识的真正迹象？

Google 发布的新数据集是一个很好的例子，说明我们需要如何开发新的基准并避免以前方法的缺陷。自然问题(NQs)数据集是一个 Q &数据集，旨在评估 LM 理解问题和解析一页文本(如维基百科页面)以找到潜在答案的能力。

关于这个数据集有趣的是作者采取的使 LMs 难以作弊的措施。这些措施表明，早期的基准和数据集可能使像 BERT 这样的模型很容易作弊。

作者采取的第一步是确保他们选择的问题是“真正的”问题。“真实”是指这些问题是人们在谷歌搜索中问的。我们对它们进行了审查，以确保它们格式良好、长度合理且连贯。

以前，像 [SQuAD](https://web.archive.org/web/20221206181234/https://rajpurkar.github.io/SQuAD-explorer/) 这样的问答数据集会要求贡献者为给定的答案创建问题。因此，给定一段文字，创造一个问题，这一段代表答案。这可能导致“启动”，即参与者首先看到答案，然后提出与答案非常相似的问题。这使得模型很容易使用“线索”找到答案。

选择问题后，NQs 参与者会得到一页文字，并被要求指出:

*   很长的回答，
*   简短的回答，
*   或者根本不可能在给定的文本中找到答案。

在某些情况下，一个答案可能既有涵盖相关问题各个方面的长答案，也有简洁地回答问题的短答案。

简短回答是包括一个或多个命名实体的简短文本。不包括答案的选项是 NQ 数据集的另一个关键区别步骤。早期的问答数据集，包括 SQuAD 的第一个版本，只包含有相应答案的问题。

一旦模型开始知道总有一个答案，那么它就可以利用这种信息找到答案，而无需真正测试其更高层次的语言技能。

### 更好的基准，更好的模型

幸运的是，NLP 社区似乎接受了我们需要像创建语言模型本身一样努力创建数据集和基准。

最近有许多论文关注于识别像 BERT 这样的模型如何利用一些经典 NLP 数据集中的弱点。

例如，在“[因错误原因而正确](https://web.archive.org/web/20221206181234/https://www.aclweb.org/anthology/P19-1334.pdf)”中，作者指出了 LMs 在没有真正理解语言基本规则的情况下，在 NLP 任务中获得高分的三种方式。他们确定了这些模型使用的三种试探法，这些试探法显示了它们缺乏理解(但由于构建较差的数据集，仍然可能导致高分):

1.  **词汇重叠**:假设“医生由演员支付报酬”与“医生向演员支付报酬”相同，
2.  **子序列**:假设“演员身边的医生跳舞了”和“演员跳舞了”一样，
3.  **成分**:假设“如果艺术家睡了，演员跑了”和“艺术家睡了”是一样的

作为这项研究的结果，我们看到了更好的基准，如[强力胶](https://web.archive.org/web/20221206181234/https://super.gluebenchmark.com/)和[极限](https://web.archive.org/web/20221206181234/https://github.com/google-research/xtreme)，在这些基准上，像伯特这样的模型很难实现类似人类的结果。这些进步与模型技术的进步一样重要，并将迫使这些模型“更加努力”以获得高分。

## 那么这些模型有多好呢？

这可能看起来很奇怪——我们研究了 LMs 的理论、技术和评估限制，现在我们将赞扬他们的成就。

问题是，通过提问，我们在推测这些模型的最终潜力。这是一个很高的标准，因为我们正在考虑这些模型是否会获得一种通用的人工智能形式，在这种形式中，它们可以在没有进一步训练的情况下学习新的任务，建立在它们当前的语言技能上，并以一种显示它们理解自己在说什么的方式与人类交流。这是令人兴奋的东西，提示终结者一样的 gif。

**需要澄清的关键问题是**虽然我们可能会质疑它们像人类一样理解语言的能力，但毫无疑问，当前的**变形金刚模型，如 BERT，已经比任何人在四五年前预测的更远更快地推进了 DL NLP 的前沿**。事实上，这些模型可以“作弊”，并且似乎只使用了他们注意力的一小部分来很好地完成 NLP 任务，这表明他们已经走了多远。

但这也增加了过度宣传这些模型的危险。也许他们永远也不会达到和你我一样的语言理解水平。也许他们不需要。

也许这些模型只需要开发更多的统计线索，它们将足以通过一系列聊天机器人和自动化 NLP 应用程序来改变商业格局，这些应用程序改变了我们搜索和使用信息的方式。他们可能不足以在短期内赢得普利策奖，但我们仍然只是触及了他们未开发潜力的皮毛。