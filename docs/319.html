<html>
<head>
<title>The Best NLP/NLU Papers from the ICLR 2020 Conference </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>ICLR 2020会议最佳NLP/NLU论文</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/iclr-2020-nlp-nlu#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/iclr-2020-nlp-nlu#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>学习代表国际会议于上周在ICLR召开，我很高兴参加了这次会议。ICLR是一个致力于<strong>研究表征学习各个方面的事件，俗称深度学习</strong>。今年的活动有点不同，因为冠状病毒疫情使它虚拟化了。然而，在线形式并没有改变活动的良好氛围。它引人入胜，互动性强，吸引了5600名与会者(是去年的两倍)。如果你对组织者如何看待这次不同寻常的在线会议安排感兴趣，你可以在<a href="https://web.archive.org/web/20220926102121/https://medium.com/@iclr_conf/gone-virtual-lessons-from-iclr2020-1743ce6164a3" target="_blank" rel="noreferrer noopener nofollow">这里</a>阅读。</p>



<p>超过1300名演讲者发表了许多有趣的论文，所以我决定创建一系列博客文章，总结他们在四个主要领域的最佳表现。你可以用最好的深度学习论文 <a href="/web/20220926102121/https://neptune.ai/blog/iclr-2020-deep-learning" target="_blank" rel="noreferrer noopener">在这里</a>赶上第一个<strong>帖子，用强化学习论文</strong> <a href="/web/20220926102121/https://neptune.ai/blog/iclr-2020-reinforcement-learning" target="_blank" rel="noreferrer noopener">在这里</a>赶上第二个<strong>帖子用生成模型论文</strong> <a href="/web/20220926102121/https://neptune.ai/blog/iclr-2020-generative-models" target="_blank" rel="noreferrer noopener">在这里</a>。</p>



<p>这是该系列的最后一篇文章，在这篇文章中，我想分享来自ICLR的10篇最佳自然语言处理/理解文章。</p>





<h2 id="Article1">最佳自然语言处理/理解论文</h2>



<h3><strong> 1。ALBERT:一个用于语言表达自我监督学习的Lite BERT】</strong></h3>



<p>一种新的预训练方法，在胶水、种族和小队基准上建立了新的最先进的结果，同时与BERT-large相比具有更少的参数。</p>



<p><em>(TL；博士，来自</em><a href="https://web.archive.org/web/20220926102121/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>)</em></p>



<p><a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=H1eA7AEtvS" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong> </a> <strong> | </strong> <a href="https://web.archive.org/web/20220926102121/https://github.com/google-research/ALBERT" target="_blank" rel="noreferrer noopener nofollow"> <strong>代码</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/be11e062e2032edaaa7f8d04d51b56fc.png" alt="" data-lazy-src="https://web.archive.org/web/20220926102121/https://lh6.googleusercontent.com/excDjmhdIS0YdP5O38HriLFDSmRlYbq-1_9dCR320xPbPnipGb49KFPZNwE6yGPwYTpySK54-P5ab-XxRI1cetgr38-6WuRGTF0x8k4pdfQU5keR7mc0Bt60gdlXT6JI8tRKhFMd?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh6.googleusercontent.com/excDjmhdIS0YdP5O38HriLFDSmRlYbq-1_9dCR320xPbPnipGb49KFPZNwE6yGPwYTpySK54-P5ab-XxRI1cetgr38-6WuRGTF0x8k4pdfQU5keR7mc0Bt60gdlXT6JI8tRKhFMd"/><noscript><img data-lazy-fallback="1" src="../Images/be11e062e2032edaaa7f8d04d51b56fc.png" alt="" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh6.googleusercontent.com/excDjmhdIS0YdP5O38HriLFDSmRlYbq-1_9dCR320xPbPnipGb49KFPZNwE6yGPwYTpySK54-P5ab-XxRI1cetgr38-6WuRGTF0x8k4pdfQU5keR7mc0Bt60gdlXT6JI8tRKhFMd"/></noscript><figcaption><em>The L2 distances and cosine similarity (in terms of degree) of the input and output embedding of each layer for BERT-large and ALBERT-large.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 2。语言表征学习的互信息最大化视角</strong></h3>



<p>单词表示是自然语言处理中的一项常见任务。在这里，作者制定了新的框架，将经典的单词嵌入技术(如Skip-gram)与基于上下文嵌入的更现代的方法(BERT，XLNet)相结合。</p>



<p><a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=Syx79eBKwr" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong> </a> <strong> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/8df71322fa410013c18f30658ba63cd8.png" alt="" data-lazy-src="https://web.archive.org/web/20220926102121/https://lh6.googleusercontent.com/7pFHoJqI-ZDdQszcVE-1Qcz9MXhzI2I4oJ_A6Q4WcYbSbSRUBJftZoOhbAvbXEZDUPMORfat8bInrv2mbSpa_8VGnGSHITc5SZB9e7F9e2qs9fFGQxMaV_NrYzwcouZ-qMcrVSFf?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh6.googleusercontent.com/7pFHoJqI-ZDdQszcVE-1Qcz9MXhzI2I4oJ_A6Q4WcYbSbSRUBJftZoOhbAvbXEZDUPMORfat8bInrv2mbSpa_8VGnGSHITc5SZB9e7F9e2qs9fFGQxMaV_NrYzwcouZ-qMcrVSFf"/><noscript><img data-lazy-fallback="1" src="../Images/8df71322fa410013c18f30658ba63cd8.png" alt="" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh6.googleusercontent.com/7pFHoJqI-ZDdQszcVE-1Qcz9MXhzI2I4oJ_A6Q4WcYbSbSRUBJftZoOhbAvbXEZDUPMORfat8bInrv2mbSpa_8VGnGSHITc5SZB9e7F9e2qs9fFGQxMaV_NrYzwcouZ-qMcrVSFf"/></noscript><figcaption><em>The left plot shows F<sub>1</sub> scores of BERT-NCE and INFOWORD as we increase the percentage of training examples on SQuAD (dev). The right plot shows F<sub>1</sub> scores of INFOWORD on SQuAD (dev) as a function of λ<sub>DIM</sub>.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 3。莫格里菲耶LSTM </strong></h3>



<p>具有最先进的语言建模结果的LSTM扩展。</p>



<p><em>(TL；博士，来自</em><a href="https://web.archive.org/web/20220926102121/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>)</em></p>



<p><a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=SJe5P6EYvS" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/e2beb53e6df825d414493613461177c6.png" alt="" data-lazy-src="https://web.archive.org/web/20220926102121/https://lh5.googleusercontent.com/ZfgJOB0iuWUK5G3mI2Hip7s6fvHxLO1JrffPeFUh6rVvHf5oFXBAmxZqAzjfi-c4BIw1idRS2rRgs9ssOPziHs9qZ2kT1kxgaR1wwdyldjYO7ihhWD6Tzc_kui_gPxUlD7sbJ4gF?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh5.googleusercontent.com/ZfgJOB0iuWUK5G3mI2Hip7s6fvHxLO1JrffPeFUh6rVvHf5oFXBAmxZqAzjfi-c4BIw1idRS2rRgs9ssOPziHs9qZ2kT1kxgaR1wwdyldjYO7ihhWD6Tzc_kui_gPxUlD7sbJ4gF"/><noscript><img data-lazy-fallback="1" src="../Images/e2beb53e6df825d414493613461177c6.png" alt="" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh5.googleusercontent.com/ZfgJOB0iuWUK5G3mI2Hip7s6fvHxLO1JrffPeFUh6rVvHf5oFXBAmxZqAzjfi-c4BIw1idRS2rRgs9ssOPziHs9qZ2kT1kxgaR1wwdyldjYO7ihhWD6Tzc_kui_gPxUlD7sbJ4gF"/></noscript><figcaption><em>Mogrifier with 5 rounds of updates. The previous state h<sup>0</sup> = h<sub>prev</sub> is transformed linearly (dashed arrows), fed through a sigmoid and gates x<sup> −1</sup> = x in an elementwise manner producing x<sup>1</sup> . Conversely, the linearly transformed x<sup>1</sup> gates h 0 and produces h<sup>2</sup> . After a number of repetitions of this mutual gating cycle, the last values of h<sup>∗</sup> and x<sup>∗</sup> sequences are fed to an LSTM cell. The prev subscript of h is omitted to reduce clutter.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 4。具有对抗网络的高保真语音合成</strong></h3>



<p>我们介绍了GAN-TTS，一个用于文本到语音转换的生成式对抗网络，其平均意见得分(MOS)为4.2。</p>



<p><em>(TL；博士，来自</em><a href="https://web.archive.org/web/20220926102121/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>)</em></p>



<p><a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=r1gfQgSFDr" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong> </a> <strong> | </strong> <a href="https://web.archive.org/web/20220926102121/https://github.com/mbinkowski/DeepSpeechDistances" target="_blank" rel="noreferrer noopener nofollow"> <strong>代码</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img src="../Images/7762870ec10e64e2e4d7ad867e1abe70.png" alt="" data-lazy-src="https://web.archive.org/web/20220926102121/https://lh4.googleusercontent.com/zEovDWJgXhAdOwjVtUvWjM20E-AXrA_dRZdz1U8GH0zSm2WDEz91Z5-Sq_nOfGhHOLF2lsSQHmmIHkQK5TTyS6nbByW-rEIvRFVI9-tgBkUukG7Q39dRiTRD7gY7ucVQ0_B2frF0?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh4.googleusercontent.com/zEovDWJgXhAdOwjVtUvWjM20E-AXrA_dRZdz1U8GH0zSm2WDEz91Z5-Sq_nOfGhHOLF2lsSQHmmIHkQK5TTyS6nbByW-rEIvRFVI9-tgBkUukG7Q39dRiTRD7gY7ucVQ0_B2frF0"/><noscript><img data-lazy-fallback="1" src="../Images/7762870ec10e64e2e4d7ad867e1abe70.png" alt="" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh4.googleusercontent.com/zEovDWJgXhAdOwjVtUvWjM20E-AXrA_dRZdz1U8GH0zSm2WDEz91Z5-Sq_nOfGhHOLF2lsSQHmmIHkQK5TTyS6nbByW-rEIvRFVI9-tgBkUukG7Q39dRiTRD7gY7ucVQ0_B2frF0"/></noscript><figcaption><em> Residual blocks used in the model. Convolutional layers have the same number of input and output channels and no dilation unless stated otherwise. h – hidden layer representation, l – linguistic features, z – noise vector, m – channel multiplier, m = 2 for downsampling blocks (i.e. if their downsample factor is greater than 1) and m = 1 otherwise, M- G’s input channels, M = 2N in blocks 3, 6, 7, and M = N otherwise; size refers to kernel size. </em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 5。重整器:高效的变压器</strong></h3>



<p>具有位置敏感散列和可逆层的高效转换器。</p>



<p><em>(TL；博士，来自</em><a href="https://web.archive.org/web/20220926102121/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>)</em></p>



<p><a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=rkgNKkHtvB" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong> </a> <strong> | </strong> <a href="https://web.archive.org/web/20220926102121/https://github.com/google/trax/tree/master/trax/models/reformer" target="_blank" rel="noreferrer noopener nofollow"> <strong>代码</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/f805259efb4d37d8b26b23d09c050b9e.png" alt="" data-lazy-src="https://web.archive.org/web/20220926102121/https://lh4.googleusercontent.com/C9UD-L5W10eXLN_l0CqtSfDBnLOdh9CT_58ILyWa1YFRNDdNp1WHdrDwp2u19kqA0-X8P5czZqZ3L2uSRhTNqEYsE35VNX75AHB2nbqvyqBB6_aTej84sKIyoIvNxgfZVa2NE5sF?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh4.googleusercontent.com/C9UD-L5W10eXLN_l0CqtSfDBnLOdh9CT_58ILyWa1YFRNDdNp1WHdrDwp2u19kqA0-X8P5czZqZ3L2uSRhTNqEYsE35VNX75AHB2nbqvyqBB6_aTej84sKIyoIvNxgfZVa2NE5sF"/><noscript><img data-lazy-fallback="1" src="../Images/f805259efb4d37d8b26b23d09c050b9e.png" alt="" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh4.googleusercontent.com/C9UD-L5W10eXLN_l0CqtSfDBnLOdh9CT_58ILyWa1YFRNDdNp1WHdrDwp2u19kqA0-X8P5czZqZ3L2uSRhTNqEYsE35VNX75AHB2nbqvyqBB6_aTej84sKIyoIvNxgfZVa2NE5sF"/></noscript><figcaption><em>An angular locality sensitive hash uses random rotations of spherically projected points to establish buckets by an argmax over signed axes projections. In this highly simplified 2D depiction, two points x and y are unlikely to share the same hash buckets (above) for the three different angular hashes unless their spherical projections are close to one another (below).</em></figcaption></figure></div>



<h3>主要作者:</h3>











<hr class="wp-block-separator"/>



<h3><strong> 6。定义:用于神经序列建模的深度分解输入令牌嵌入</strong></h3>



<p>DeFINE使用具有新的跳过连接的深度、分层、稀疏网络来高效地学习更好的单词嵌入。</p>



<p><em>(TL；博士，来自</em><a href="https://web.archive.org/web/20220926102121/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>)</em></p>



<p><a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=rJeXS04FPH" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/ca73615db19d1d3c2b7b49f42bc1acc4.png" alt="" data-lazy-src="https://web.archive.org/web/20220926102121/https://lh6.googleusercontent.com/7_QN4KJMTN6GnwL5-4MLy2wGi6UvwxZPKqimj9v10dvdI9UC-WfPK_4AviVnM10fOthmwDwpl1l3CkeoQ6IVE8C6T6weWM-bIp7Cq2nBB0iZd7DgIBts-mkE5bJkZEzAs2h1OAvT?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh6.googleusercontent.com/7_QN4KJMTN6GnwL5-4MLy2wGi6UvwxZPKqimj9v10dvdI9UC-WfPK_4AviVnM10fOthmwDwpl1l3CkeoQ6IVE8C6T6weWM-bIp7Cq2nBB0iZd7DgIBts-mkE5bJkZEzAs2h1OAvT"/><noscript><img data-lazy-fallback="1" src="../Images/ca73615db19d1d3c2b7b49f42bc1acc4.png" alt="" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh6.googleusercontent.com/7_QN4KJMTN6GnwL5-4MLy2wGi6UvwxZPKqimj9v10dvdI9UC-WfPK_4AviVnM10fOthmwDwpl1l3CkeoQ6IVE8C6T6weWM-bIp7Cq2nBB0iZd7DgIBts-mkE5bJkZEzAs2h1OAvT"/></noscript><figcaption><em>With DeFINE, Transformer-XL learns input (embedding) and output (classification) representations in low n-dimensional space rather than high m-dimensional space, thus reducing parameters significantly while having a minimal impact on the performance.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 7。深度自适应变压器</strong></h3>



<p>动态调整每个输入的计算量的序列模型。</p>



<p><em>(TL；博士，来自</em><a href="https://web.archive.org/web/20220926102121/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>)</em></p>



<p><a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=SJg7KhVKPH" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong> </a> <strong> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/a598f522f66eb065a84d1a65f63f65d7.png" alt="" data-lazy-src="https://web.archive.org/web/20220926102121/https://lh3.googleusercontent.com/fO2JD7xjUKLmTwFKyA1obny9hFhQINBcB95me_uAF1VjRtaBHrAIlxMCHQg4sN1WdahLc1Pc_IHfWSavHBquX_eitbJeyceSeaOGoNqXP_Kb7juhDucJlNNkbUzucNmLPekEi5nD?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh3.googleusercontent.com/fO2JD7xjUKLmTwFKyA1obny9hFhQINBcB95me_uAF1VjRtaBHrAIlxMCHQg4sN1WdahLc1Pc_IHfWSavHBquX_eitbJeyceSeaOGoNqXP_Kb7juhDucJlNNkbUzucNmLPekEi5nD"/><noscript><img data-lazy-fallback="1" src="../Images/a598f522f66eb065a84d1a65f63f65d7.png" alt="" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh3.googleusercontent.com/fO2JD7xjUKLmTwFKyA1obny9hFhQINBcB95me_uAF1VjRtaBHrAIlxMCHQg4sN1WdahLc1Pc_IHfWSavHBquX_eitbJeyceSeaOGoNqXP_Kb7juhDucJlNNkbUzucNmLPekEi5nD"/></noscript><figcaption><em>Training regimes for decoder networks able to emit outputs at any layer. Aligned training optimizes all output classifiers C<sub>n</sub> simultaneously assuming all previous hidden states for the current layer are available. Mixed training samples M paths of random exits at which the model is assumed to have exited; missing previous hidden states are copied from below.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 8。关于变压器的可识别性</strong></h3>



<p>我们研究了基于自我注意的BERT模型中上下文嵌入中注意分布和标记的可识别性和可解释性。</p>



<p><em>(TL；博士，来自</em><a href="https://web.archive.org/web/20220926102121/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>)</em></p>



<p><a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=BJg1f6EFDB" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/c0f1eaca33ba1de04f55144daed342ac.png" alt="" data-lazy-src="https://web.archive.org/web/20220926102121/https://lh3.googleusercontent.com/6i0mOXstSdSNJdXw3IM5t7s9MngUJMtNDIB_oTnGMHp8I8VDR-5oHEJQyoabegQubiuTTicyfXhUmIRm6BWgI0bXaNjwM6tKb935nmJ9Hn28Vy4qkD1ixWf0-k3FTguT7RR6kd71?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh3.googleusercontent.com/6i0mOXstSdSNJdXw3IM5t7s9MngUJMtNDIB_oTnGMHp8I8VDR-5oHEJQyoabegQubiuTTicyfXhUmIRm6BWgI0bXaNjwM6tKb935nmJ9Hn28Vy4qkD1ixWf0-k3FTguT7RR6kd71"/><noscript><img data-lazy-fallback="1" src="../Images/c0f1eaca33ba1de04f55144daed342ac.png" alt="" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh3.googleusercontent.com/6i0mOXstSdSNJdXw3IM5t7s9MngUJMtNDIB_oTnGMHp8I8VDR-5oHEJQyoabegQubiuTTicyfXhUmIRm6BWgI0bXaNjwM6tKb935nmJ9Hn28Vy4qkD1ixWf0-k3FTguT7RR6kd71"/></noscript><figcaption><em>(a) Each point represents the Pearson correlation coefficient of effective attention and raw attention as a function of token length. (b) Raw attention vs. (c) effective attention, where each point represents the average (effective) attention of a given head to a token type.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 9。镜像生成神经机器翻译</strong></h3>



<p>被称为神经机器翻译模型(NMT)的翻译方法依赖于作为语言对构建的大型语料库的可用性。这里，提出了一种使用生成神经机器翻译进行双向翻译的新方法。</p>



<p><a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=HkxQRTNYPH" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/40d121642d1c48cde41fb7ceb84b0d79.png" alt="" data-lazy-src="https://web.archive.org/web/20220926102121/https://lh6.googleusercontent.com/9-PgZz9TpH01jx74CIABttyIruTB_gksydsQ5n3nmCTaoxiXg7pv6-P-M7wHDlxDn4cGXWcRRjkMwFuRJEhvedxO-T99ZEfjUQThPhkjlOOk5vYiphCWd2KMc9W8MnzlzN-zO0as?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh6.googleusercontent.com/9-PgZz9TpH01jx74CIABttyIruTB_gksydsQ5n3nmCTaoxiXg7pv6-P-M7wHDlxDn4cGXWcRRjkMwFuRJEhvedxO-T99ZEfjUQThPhkjlOOk5vYiphCWd2KMc9W8MnzlzN-zO0as"/><noscript><img data-lazy-fallback="1" src="../Images/40d121642d1c48cde41fb7ceb84b0d79.png" alt="" data-original-src="https://web.archive.org/web/20220926102121im_/https://lh6.googleusercontent.com/9-PgZz9TpH01jx74CIABttyIruTB_gksydsQ5n3nmCTaoxiXg7pv6-P-M7wHDlxDn4cGXWcRRjkMwFuRJEhvedxO-T99ZEfjUQThPhkjlOOk5vYiphCWd2KMc9W8MnzlzN-zO0as"/></noscript><figcaption><em>The graphical model of MGNMT. </em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3>10。FreeLB:增强的自然语言理解对抗训练</h3>



<p>在这里，作者提出了一种新的算法，称为FreeLB，它提出了一种新的语言模型的对抗性训练方法。</p>



<p><strong> <a href="https://web.archive.org/web/20220926102121/https://openreview.net/forum?id=BygzbyHFvB" target="_blank" rel="noreferrer noopener nofollow">论文</a> | <a href="https://web.archive.org/web/20220926102121/https://github.com/zhuchen03/FreeLB" target="_blank" rel="noreferrer noopener nofollow">代码</a> </strong></p>











<hr class="wp-block-separator"/>



<h1><strong>总结</strong></h1>



<p>ICLR出版物的深度和广度相当鼓舞人心。这篇文章主要关注“自然语言处理”这个话题，这是会议期间讨论的主要领域之一。根据<a href="https://web.archive.org/web/20220926102121/https://www.analyticsvidhya.com/blog/2020/05/key-takeaways-iclr-2020/" target="_blank" rel="noreferrer noopener nofollow">本分析</a>，这些区域包括:</p>


<div class="custom-point-list">
<ol><li>深度学习(<a href="/web/20220926102121/https://neptune.ai/blog/iclr-2020-deep-learning" target="_blank" rel="noreferrer noopener">此处</a>)</li><li>强化学习(<a href="/web/20220926102121/https://neptune.ai/blog/iclr-2020-reinforcement-learning" target="_blank" rel="noreferrer noopener">此处</a>)</li><li>生成模型(<a href="/web/20220926102121/https://neptune.ai/blog/iclr-2020-generative-models" target="_blank" rel="noreferrer noopener">此处</a>)</li><li>自然语言处理/理解(包含在这篇文章中)</li></ol>
</div>


<p>为了对ICLR大学的顶级论文有一个更完整的概述，我们建立了一系列的帖子，每个帖子都专注于上面提到的一个主题。这是最后一个，所以你可能想要<strong>检查其他的</strong>以获得更完整的概述。</p>



<p>我们很乐意扩展我们的列表，所以请随意与我们分享其他有趣的NLP/NLU论文。</p>



<p>同时，祝阅读愉快！</p>




<div id="author-box-new-format-block_605afcb8e8a39" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">卡密耳鸭</h3>
    
          <p class="article__authorContent-text">人工智能研究倡导者，在MLOps领域工作。总是在寻找新的ML工具、过程自动化技巧和有趣的ML论文。偶尔会有博客作者和会议发言人。</p>
    
          
    
  </div>
</div>


<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>



<p class="has-text-color"><strong>阅读下一篇</strong></p>



<h2>自然语言处理的探索性数据分析:Python工具完全指南</h2>



<p class="has-small-font-size">11分钟阅读|作者Shahul ES |年7月14日更新</p>


<p id="block_5ffc75def9f8e" class="separator separator-10"/>



<p>探索性数据分析是任何机器学习工作流中最重要的部分之一，自然语言处理也不例外。但是<strong>你应该选择哪些工具</strong>来高效地探索和可视化文本数据呢？</p>



<p>在这篇文章中，我们将<strong>讨论和实现几乎所有的主要技术</strong>，你可以用它们来理解你的文本数据，并给你一个完成工作的Python工具的完整之旅。</p>



<h2>开始之前:数据集和依赖项</h2>



<p>在本文中，我们将使用来自Kaggle的百万新闻标题数据集。如果您想一步一步地进行分析，您可能需要安装以下库:</p>



<pre class="hljs">pip install \
   pandas matplotlib numpy \
   nltk seaborn sklearn gensim pyldavis \
   wordcloud textblob spacy textstat</pre>



<p>现在，我们可以看看数据。</p>



<pre class="hljs">news= pd.read_csv(<span class="hljs-string">'data/abcnews-date-text.csv'</span>,nrows=<span class="hljs-number">10000</span>)
news.head(<span class="hljs-number">3</span>)</pre>



<figure class="wp-block-image"><img src="../Images/56ebd3106e6c1a82edc12df9e450a7dc.png" alt="jupyter output" data-lazy-src="https://web.archive.org/web/20220926102121/https://i2.wp.com/neptune.ai/wp-content/uploads/output1.png?fit=979%2C146&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926102121im_/https://i2.wp.com/neptune.ai/wp-content/uploads/output1.png?fit=979%2C146&amp;ssl=1"/><noscript><img data-lazy-fallback="1" src="../Images/56ebd3106e6c1a82edc12df9e450a7dc.png" alt="jupyter output" data-original-src="https://web.archive.org/web/20220926102121im_/https://i2.wp.com/neptune.ai/wp-content/uploads/output1.png?fit=979%2C146&amp;ssl=1"/></noscript></figure>



<p>数据集只包含两列，发布日期和新闻标题。</p>



<p>为了简单起见，我将探索这个数据集中的前<strong> 10000行</strong>。因为标题是按<em>发布日期</em>排序的，所以实际上从2003年2月19日<em>到2003年4月7日</em>有<strong>两个月。</strong></p>



<p>好了，我想我们已经准备好开始我们的数据探索了！</p>


<a class="button continous-post blue-filled" href="/web/20220926102121/https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>