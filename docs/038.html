<html>
<head>
<title>Building MLOps Pipeline for Computer Vision: Image Classification Task [Tutorial] </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>为计算机视觉构建MLOps流水线:图像分类任务[教程]</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/mlops-pipeline-for-computer-vision-image-classification#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/mlops-pipeline-for-computer-vision-image-classification#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>Vaswani 和团队在2018年推出的<a href="https://web.archive.org/web/20221206144339/https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noreferrer noopener nofollow">变形金刚，为各种任务的深度学习模型的研发带来了重大变革。转换器利用了Bahdanau和团队从注意力机制中采用的自我注意力机制。通过这种机制，一个输入可以与其他输入进行交互，使其能够集中或关注数据的重要特征。</a></p>



<p>因此，transformers能够在各种<a href="/web/20221206144339/https://neptune.ai/category/natural-language-processing" target="_blank" rel="noreferrer noopener"> NLP </a>任务中实现最先进的结果，如机器翻译、摘要生成、文本生成等。它还在几乎所有的NLP任务中取代了RNN及其变体。事实上，随着它在NLP中的成功，变形金刚现在也被用于<a href="/web/20221206144339/https://neptune.ai/category/computer-vision" target="_blank" rel="noreferrer noopener">的计算机视觉</a>任务中。2020年，Dosovitskiy和他的团队开发了《视觉变形金刚》(ViT)，他们认为没有必要依赖CNN。基于这个前提，在本文中，我们将探索和学习ViT如何帮助完成图像分类的任务。</p>



<p>本文是一篇旨在使用<a href="https://web.archive.org/web/20221206144339/https://www.google.com/url?q=https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&amp;sa=D&amp;source=docs&amp;ust=1658825455903181&amp;usg=AOvVaw23I6LB81bRMZdj0MrMq7ID" target="_blank" rel="noreferrer noopener nofollow"> ViT </a>为计算机视觉任务<strong>构建MLOps管道的指南，它将关注与典型数据科学项目相关的以下领域:</strong></p>



<ol><li>项目的目标</li><li>硬件规格</li><li>注意力可视化</li><li>建立模型和实验跟踪</li><li>测试和推理</li><li>创建用于部署的细流应用</li><li>使用GitHub操作设置CI/CD</li><li>部署和监控</li></ol>







<p>这篇文章的代码可以在这个<a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch" target="_blank" rel="noreferrer noopener nofollow"> <strong> Github </strong> </a>链接上找到，这样你就可以跟上了。让我们开始吧。</p>



<h2 id="h-mlops-pipeline-for-image-classification-understanding-the-project">用于图像分类的MLOps管道:理解项目</h2>



<p>了解项目或客户的需求是重要的一步，因为它可以帮助我们集思广益，研究项目可能需要的各种组件，如最新的论文、存储库、相关工作、数据集，甚至基于云的部署平台。本节将重点讨论两个主题:</p>



<div id="case-study-numbered-list-block_62da4f9c88d8a" class="block-case-study-numbered-list ">

    
    <h2 id="h-"/>

    <ul class="c-list">
                    <li class="c-list__item"><span class="c-list__counter"> 1 </span>项目目的。<br/></li>
                    <li class="c-list__item"><span class="c-list__counter"> 2 </span>加速训练的硬件。</li>
            </ul>
</div>



<h3>项目目标:鸟类图像分类器</h3>



<p>该项目的目的是建立一个图像分类器来对不同种类的鸟类进行分类。由于该模型稍后将部署在云中，我们必须记住，必须对该模型进行训练，以在训练和测试数据集中获得良好的准确性分数。为了做到这一点，我们应该使用精度、召回率、混淆度、F1和AUROC分数等指标来查看模型在两个数据集上的表现。一旦模型在测试数据集上取得了好成绩，我们将创建一个web应用程序，将其部署在基于云的服务器上。</p>







<p>简而言之，这就是项目的执行方式:</p>



<div id="case-study-numbered-list-block_62da50b288d8e" class="block-case-study-numbered-list ">

    
    <h2 id="h-"/>

    <ul class="c-list">
                    <li class="c-list__item"><span class="c-list__counter"> 1 </span>用Pytorch构建深度学习模型<br/></li>
                    <li class="c-list__item"><span class="c-list__counter"> 2 </span>测试模型<br/></li>
                    <li class="c-list__item"><span class="c-list__counter"> 3 </span>创建一个简化应用<br/></li>
                    <li class="c-list__item"><span class="c-list__counter"> 4 </span>为部署创建目录及其各自的配置文件<br/></li>
                    <li class="c-list__item">最后，将其部署在谷歌云平台上</li>
            </ul>
</div>



<p>这个项目将包括您将在本文中找到的一些附加实践，例如:</p>



<ul><li>实时跟踪以监控指标，</li><li>注意力可视化，</li><li>目录结构，</li><li>所有python模块的代码格式。</li></ul>



<h3>加速训练的硬件</h3>



<p>我们将使用两套硬件进行实验:</p>



<ol><li>M1 Macbook :苹果M1处理器的效率将允许我们快速开发模型，并在更小的数据集上训练它们。一旦训练完成，我们就可以开始在本地机器上构建一个web应用程序，并在云中扩展模型之前，创建一个小型的数据接收、数据预处理、模型预测和注意力可视化管道。</li></ol>



<p><strong>注意</strong>:如果你有一台M1笔记本电脑，那么一定要在我的<a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch" target="_blank" rel="noreferrer noopener nofollow"> Github repo </a>中查看安装过程。</p>



<ol start="2"><li>Kaggle或Google Colab GPU:一旦我们的代码在我们的本地机器上正常工作，并且创建了管道，我们就可以扩展它，并在免费的Google Colab或Kaggle中对整个模型进行更长时间的训练。一旦训练完成，我们就可以将新的权重和元数据下载到我们的本地计算机，并在将web应用程序部署到云之前，测试它在看不见的数据中是否表现良好。</li></ol>



<p>现在让我们开始实现。</p>



<h2 id="h-mlops-pipeline-for-image-classification-data-preparation">用于图像分类的MLOps流水线:数据准备</h2>



<p>实施深度学习项目的第一步是规划我们将要拥有的不同python模块。尽管我们将使用Jupyter笔记本进行实验，但在开始编码之前做好一切准备总是一个好主意。规划可能包括参考代码库以及研究论文。</p>



<p>为了提高效率和便于导航，为项目创建目录结构总是一个好主意。</p>



<pre class="hljs">ViT Classification
├── notebooks
│   └── ViT.ipynb
└── source
    └──config.py
</pre>



<p>在我们的例子中，主目录称为ViT分类，它包含两个文件夹:</p>



<ol><li>笔记本:这是jupyter笔记本所有实验的地方。</li><li><strong> Source </strong>:这是所有Python模块将驻留的地方。</li></ol>



<p>随着我们的进展，我们将继续向源目录添加Python模块，我们还将创建不同的子目录来存储元数据、docker文件、README.md文件等等。</p>



<h3>建立图像分类模型</h3>



<p>如前所述，研究和规划是实现任何机器学习项目的关键。我通常首先做的是，创建一个config.py来存储与数据预处理、模型训练和推理、可视化等相关的所有参数。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/config.py" target="_blank" rel="noreferrer noopener nofollow">配置文件</a></p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Config</span>:</span>
   
   IMG_SIZE = <span class="hljs-number">32</span>
   PATCH_SIZE = <span class="hljs-number">10</span>
   CROP_SIZE = <span class="hljs-number">100</span>
   BATCH_SIZE = <span class="hljs-number">1</span>
   DATASET_SAMPLE = <span class="hljs-string">'full'</span>


   
   LR = <span class="hljs-number">0.003</span>
   OPIMIZER = <span class="hljs-string">'Adam'</span>

   
   NUM_CLASSES = <span class="hljs-number">400</span>
   IN_CHANNELS = <span class="hljs-number">3</span>
   HIDDEN_SIZE = <span class="hljs-number">768</span>
   NUM_ATTENTION_HEADS = <span class="hljs-number">12</span>
   LINEAR_DIM = <span class="hljs-number">3072</span>
   NUM_LAYERS = <span class="hljs-number">12</span>

   ATTENTION_DROPOUT_RATE = <span class="hljs-number">0.1</span>
   DROPOUT_RATE = <span class="hljs-number">0.1</span>
   STD_NORM = <span class="hljs-number">1e-6</span>
   EPS = <span class="hljs-number">1e-6</span>
   MPL_DIM = <span class="hljs-number">128</span>
   OUTPUT = <span class="hljs-string">'softmax'</span>
   LOSS_FN = <span class="hljs-string">'nll_loss'</span>

   
   DEVICE = [<span class="hljs-string">"cpu"</span>,<span class="hljs-string">"mps"</span>,<span class="hljs-string">"cuda"</span>]

   
   N_EPOCHS = <span class="hljs-number">1</span>
</pre>



<p>上面的代码块给出了参数应该是什么样子的模糊概念。随着我们取得进展，我们可以不断添加更多的参数。</p>



<p><strong>注</strong>:在设备配置部分，我已经给出了三个硬件的列表:CPU、MPS、CUDA。MPS或金属性能着色器是在M1 macbook上训练的硬件类型。</p>



<h4>资料组</h4>



<p>我们将使用的数据集是可以从Kaggle 下载的<a href="https://web.archive.org/web/20221206144339/https://www.kaggle.com/datasets/gpiosenka/100-bird-species" target="_blank" rel="noreferrer noopener nofollow">鸟类分类数据集。该数据集由400个鸟类类别组成，具有三个子集:训练、验证和测试，每个子集分别包含58388、2000和2000张图像。一旦数据下载完毕，我们就可以创建一个函数来读取和可视化图像。</a></p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img decoding="async" src="../Images/5ab2fea5b379f847775bce0092ae6ece.png" alt="sample from the datase" class="wp-image-69916" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-1.png?resize=458%2C469&amp;ssl=1"/><figcaption><em>The image above is a sample from the dataset along with the class that it belongs to | <a href="https://web.archive.org/web/20221206144339/https://www.kaggle.com/datasets/gpiosenka/100-bird-species" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<h4>准备数据</h4>



<p>我们可以继续创建一个数据加载器，将图像转换成图像张量。除此之外，我们还将执行尺寸调整、图像裁剪和规范化。一旦预处理完成，我们就可以使用DataLoader函数批量自动生成用于训练的数据。以下伪函数将让您了解我们正在努力实现的目标，您可以在代码标题中提供的链接中找到完整的代码:</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/preprocessing.py" target="_blank" rel="noreferrer noopener nofollow">预处理. py </a></p>



<pre class="hljs">

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Dataset</span><span class="hljs-params">(bs, crop_size, sample_size=<span class="hljs-string">'full'</span>)</span>:</span>
      <span class="hljs-keyword">return</span> train_data, valid_data, test_data
</pre>



<p>上面的函数有一个样本大小参数，它允许创建训练数据集的子集，以便在本地机器上进行测试。</p>



<h2 id="h-mlops-pipeline-for-image-classification-building-the-vision-transformer-using-pytorch">用于图像分类的MLOps流水线:使用Pytorch构建视觉转换器</h2>



<p>我已经按照作者在论文中对ViT的描述创建了完整的模型。这段代码的灵感来自于<a href="https://web.archive.org/web/20221206144339/https://github.com/jeonsworld/ViT-pytorch" target="_blank" rel="noreferrer noopener nofollow"> <strong> jeonsworld </strong> </a> repo，为了这个任务的目的，我增加了一些更多的细节并编辑了一些代码行。</p>



<p>我创建的模型分为9个模块，每个模块可以独立执行各种任务。为了便于理解，我们将探讨每个部分。</p>



<h3>把...嵌入</h3>



<p>变形金刚和所有的自然语言模型都有一个重要的组件叫做<strong>嵌入</strong>。它的功能通常是通过将相似的信息组合在一起来捕获语义信息。除此之外，嵌入可以跨模型学习和重用。</p>



<p>在ViT中，嵌入通过保留可以输入编码器的位置信息来达到同样的目的。同样，下面的伪代码将帮助您理解发生了什么，您也可以在代码标题中提供的链接中找到完整的代码。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/embeddings.py" target="_blank" rel="noreferrer noopener nofollow"> embedding.py </a></p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Embeddings</span><span class="hljs-params">(nn.Module)</span>:</span>


   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, img_size:int, hidden_size:int, in_channels:int)</span>:</span>


   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>


       <span class="hljs-keyword">return</span> embeddings
</pre>



<p>请注意，可以使用卷积层来创建图像的嵌入补丁。这是非常有效的，也很容易修改。</p>



<h3>编码器</h3>



<p>编码器由许多关注模块组成，关注模块本身有两个重要模块:</p>



<div id="case-study-numbered-list-block_62da5a1d88d94" class="block-case-study-numbered-list ">

    
    <h2 id="h-"/>

    <ul class="c-list">
                    <li class="c-list__item"><span class="c-list__counter"> 1 </span>自我注意机制<br/></li>
                    <li class="c-list__item"><span class="c-list__counter"> 2 </span>多层感知器(MLP)</li>
            </ul>
</div>



<h4>自我注意机制</h4>



<p>先说自我关注机制。</p>



<p>自我关注机制是整个系统的核心。它使模型能够关注数据的重要特征。它通过对不同位置的单个嵌入进行操作来计算相同序列的表示。您可以在下面找到完整代码的链接，以获得更深入的了解。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/attention.py" target="_blank" rel="noreferrer noopener nofollow"> attention.py </a></p>



<pre class="hljs">

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Attention</span><span class="hljs-params">(nn.Module)</span>:</span>
       <span class="hljs-keyword">return</span> attention_output, weights</pre>



<p>注意力块的输出将产生注意力输出以及注意力权重。后者将用于可视化使用注意机制计算的ROI。</p>



<h4>多层感知器</h4>



<p>一旦我们接收到注意力输出，我们就可以把它输入到MLP中，这将给我们一个分类的概率分布。您可以在forward函数中了解整个过程。要查看完整代码，请单击下面代码标题中提供的链接。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/linear.py" target="_blank" rel="noreferrer noopener nofollow"> linear.py </a></p>



<pre class="hljs">

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Mlp</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, hidden_size, linear_dim, dropout_rate, std_norm)</span>:</span>
       <span class="hljs-keyword">return</span> x</pre>



<p>值得注意的是，我们使用GELU作为我们的激活函数。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/509dc5a23b4b8a5b4675c8d528983787.png" alt="activation function" class="wp-image-69917" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-2.png?ssl=1"/><figcaption><em>GELU as activation function | <a href="https://web.archive.org/web/20221206144339/https://mlfromscratch.com/activation-functions-explained/#/" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<p>使用GELU的好处之一是它避免了消失梯度，这使得模型易于缩放。</p>



<h4>注意力阻断</h4>



<p>注意模块是我们组装两个模块的模块:自我注意模块和MLP模块。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/attention_block.py" target="_blank" rel="noreferrer noopener nofollow"> attention_block.py </a></p>



<pre class="hljs">

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Block</span><span class="hljs-params">(nn.Module)</span>:</span>
       <span class="hljs-keyword">return</span> x, weights
</pre>



<p>该模块还将直接从注意力机制中产生注意力权重，以及由MLP产生的分布。</p>



<p>现在让我们简单了解一下编码器。编码器本质上使我们能够创建多个注意块，给转换器更多对注意机制的控制。三个组件:编码器、变压器和ViT写在同一个模块中，即<a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/attention_block.py" target="_blank" rel="noreferrer noopener nofollow"> transformers.py </a>。</p>



<pre class="hljs">

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span><span class="hljs-params">(nn.Module)</span>:</span>
       <span class="hljs-keyword">return</span> encoded, attn_weights</pre>



<h3>变压器</h3>



<p>组装好关注模块后，我们就可以对转换器进行编码了。注意块转换器是嵌入模块和编码器模块的组合。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Transformer</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, img_size, hidden_size, in_channels, num_layers,
                num_attention_heads, linear_dim, dropout_rate, attention_dropout_rate,
                eps, std_norm)</span>:</span>
       super(Transformer, self).__init__()
       self.embeddings = Embeddings(img_size, hidden_size, in_channels)
       self.encoder = Encoder(num_layers, hidden_size, num_attention_heads,
                              linear_dim, dropout_rate, attention_dropout_rate,
                              eps, std_norm)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, input_ids)</span>:</span>
       embedding_output = self.embeddings(input_ids)
       encoded, attn_weights = self.encoder(embedding_output)
       <span class="hljs-keyword">return</span> encoded, attn_weights
</pre>



<h3>视觉变压器</h3>



<p>最后，我们可以编码我们的视觉转换器，它包括两个组件:转换器和最终的线性层。最终的线性将帮助我们找到所有类别的概率分布。它可以被描述为:</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VisionTransformer</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, img_size, num_classes, hidden_size, in_channels, num_layers,
                num_attention_heads, linear_dim, dropout_rate, attention_dropout_rate,
                eps, std_norm)</span>:</span>
       super(VisionTransformer, self).__init__()
       self.classifier = <span class="hljs-string">'token'</span>

       self.transformer=Transformer(img_size, hidden_size, in_channels,
                                    num_layers, num_attention_heads, linear_dim,
                                    dropout_rate, attention_dropout_rate, eps,
                                    std_norm)
       self.head = Linear(hidden_size, num_classes)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x, labels=None)</span>:</span>
       x, attn_weights = self.transformer(x)
       logits = self.head(x[:, <span class="hljs-number">0</span>])

       <span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
           loss_fct = CrossEntropyLoss()
           loss = loss_fct(logits.view(<span class="hljs-number">-1</span>, <span class="hljs-number">400</span>), labels.view(<span class="hljs-number">-1</span>))
           <span class="hljs-keyword">return</span> loss
       <span class="hljs-keyword">else</span>:
           <span class="hljs-keyword">return</span> logits, attn_weights</pre>



<p>请注意，网络将持续产生注意力权重，这对可视化注意力地图非常有用。</p>



<p>这是额外的小费。如果您想要查看模型的架构以及输入是如何操作的，那么使用下面的代码行。代码将为您生成一个完整的操作架构。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> torchviz <span class="hljs-keyword">import</span> make_dot
x = torch.randn(<span class="hljs-number">1</span>,config.IN_CHANNELS*config.IMG_SIZE*config.IMG_SIZE)
x = x.reshape(<span class="hljs-number">1</span>,config.IN_CHANNELS,config.IMG_SIZE,config.IMG_SIZE)
logits, attn_weights = model(x)
make_dot(logits, params=dict(list(model.named_parameters()))).render(<span class="hljs-string">"../metadata/VIT"</span>, format=<span class="hljs-string">"png"</span>)</pre>



<p>你可以在给定的<a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/metadata/VIT.png" target="_blank" rel="noreferrer noopener nofollow">链接</a>中找到图像。</p>



<p>但简而言之，这就是建筑的样子。</p>


<div class="wp-block-image">
<figure class="aligncenter size-full"><img decoding="async" src="../Images/49f15446f876f42ff14f3318d575c574.png" alt="vision transformer" class="wp-image-69918" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-3.png?ssl=1"/><figcaption><em>The architecture of vision transformer | <a href="https://web.archive.org/web/20221206144339/https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<h2 id="h-mlops-pipeline-for-image-classification-training-vision-transformer-using-pytorch">用于图像分类的MLOps流水线:使用Pytorch训练视觉转换器</h2>



<p>在培训模块中，我们将组装所有其他模块，如配置模块、预处理模块和转换器，并将包括元数据在内的参数记录到Neptune API中。记录参数最简单的方法是使用Config。__词典_ _。这会自动将类转换成字典。</p>



<p>您可以稍后创建一个函数，从字典中删除不必要的属性。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neptune_monitoring</span><span class="hljs-params">()</span>:</span>
   PARAMS = {}
   <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> Config.__dict__.items():
       <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">'__module__'</span>, <span class="hljs-string">'__dict__'</span>, <span class="hljs-string">'__weakref__'</span>, <span class="hljs-string">'__doc__'</span>]:
           PARAMS[key] = val
   <span class="hljs-keyword">return</span> PARAMS</pre>



<h3>培养</h3>



<p>训练函数非常简单明了。我在伪代码中包含了培训和评估。您可以在此找到<a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/train.py" target="_blank" rel="noreferrer noopener nofollow">完整的训练模块，或者您可以点击下面的代码标题。</a></p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/train.py" target="_blank" rel="noreferrer noopener nofollow"> train.py </a></p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_Engine</span><span class="hljs-params">(n_epochs, train_data, val_data, model, optimizer, loss_fn, device,
                monitoring=True)</span>:</span>


</pre>



<p>现在我们的训练循环已经完成，我们可以开始训练并将元数据记录到<a href="/web/20221206144339/https://neptune.ai/" target="_blank" rel="noreferrer noopener"> Neptune.ai </a>仪表板中，我们可以使用它在旅途中监控训练，保存图表和参数，并与队友共享它们。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/train.py" target="_blank" rel="noreferrer noopener nofollow"> train.py </a></p>



<pre class="hljs"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
   <span class="hljs-keyword">from</span> preprocessing <span class="hljs-keyword">import</span> Dataset
   <span class="hljs-keyword">from</span> config <span class="hljs-keyword">import</span> Config
   config = Config()
   params = neptune_monitoring(Config)

   run = neptune.init( project=<span class="hljs-string">"nielspace/ViT-bird-classification"</span>,
                       api_token=API_TOKEN)
   run[<span class="hljs-string">'parameters'</span>] = params

   model = VisionTransformer(img_size=config.IMG_SIZE,
                num_classes=config.NUM_CLASSES,
                hidden_size=config.HIDDEN_SIZE,
                in_channels=config.IN_CHANNELS,
                num_layers=config.NUM_LAYERS,
                num_attention_heads=config.NUM_ATTENTION_HEADS,
                linear_dim=config.LINEAR_DIM,
                dropout_rate=config.DROPOUT_RATE,
                attention_dropout_rate=config.ATTENTION_DROPOUT_RATE,
                eps=config.EPS,
                std_norm=config.STD_NORM)

   train_data, val_data, test_data = Dataset(config.BATCH_SIZE, config.IMG_SIZE,
                                             config.DATASET_SAMPLE)

   optimizer = optim.Adam(model.parameters(), lr=<span class="hljs-number">0.003</span>)
   train_Engine(n_epochs=config.N_EPOCHS, train_data=train_data, val_data=val_data,
               model=model,optimizer=optimizer, loss_fn=<span class="hljs-string">'nll_loss'</span>,
               device=config.DEVICE[<span class="hljs-number">1</span>], monitoring=<span class="hljs-keyword">True</span>)</pre>



<p><strong>注意</strong>:这个模型的原型是在Macbook Air M1的一个更小的数据集上完成的，这个数据集有10个类。在原型阶段，我尝试了不同的配置，并尝试了模型的架构。一旦我满意了，我就用Kaggle来训练这个模型。由于数据集有400个类，模型需要更大，并且需要更长时间的训练。</p>



<h3>实验跟踪</h3>



<p>在原型阶段，实验跟踪成为对模型进行进一步修改的一个非常方便和可靠的来源。您可以在训练期间关注模型的性能，并随后对其进行必要的调整，直到您获得一个高性能的模型。</p>



<p>Neptune API使您能够:</p>







<p>如果您想在系统中记录您的元数据，那么导入Neptune API并调用init函数。接下来，输入为项目提供的API键，就可以开始了。在这里了解更多关于如何<a href="https://web.archive.org/web/20221206144339/https://docs.neptune.ai/getting-started/installation" target="_blank" rel="noreferrer noopener">开始使用Neptune的信息。另外，</a><a href="https://web.archive.org/web/20221206144339/https://app.neptune.ai/nielspace/ViT-bird-classification/experiments?split=tbl&amp;dash=charts&amp;viewId=standard-view" target="_blank" rel="noreferrer noopener">这里是Neptune仪表板</a>，它有与这个项目相关的元数据。</p>



<pre class="hljs">run = neptune.init(project=<span class="hljs-string">"nielspace/ViT-bird-classification"</span>,
api_token=<span class="hljs-string">"API_TOKEN"</span>)</pre>



<p>一旦完成初始化，就可以开始日志记录了。例如，如果您想要:</p>



<ol><li>上传参数，使用:run['parameters'] = params。注意:确保参数是字典类的。</li><li>上传指标，使用:run['Training_loss']。log(loss.item())并运行['Training_loss']。log(loss.item())</li><li>上传模型权重，使用:run["model_checkpoints/ViT"]。上传(" model.pt ")</li><li>上传图片，使用:run["val/conf_matrix"]。上传("混淆_矩阵. png ")</li></ol>



<p>根据您优化模型的目的，有很多事情可以记录和跟踪。在我们的例子中，我们强调训练和验证的损失和准确性。</p>



<h4>记录元数据和仪表板</h4>



<p>在持续的培训过程中，您可以监控模型的性能。随着每次迭代，图形将会更新。</p>





<p>除了模型的性能，您还会发现CPU和GPU的性能。见下图。</p>





<p>您还可以找到所有的模型元数据。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221206144339/https://neptune.ai/mlops-pipeline-computer-vision-6"><img decoding="async" src="../Images/00246607383c8bceb304939411f1d713.png" alt="model metadata" class="wp-image-69921" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-6.png?ssl=1"/></a><figcaption><em>The model metadata</em></figcaption></figure></div>


<h3>使用Kaggle缩放</h3>



<p>现在，让我们缩放模型。我们将在这个项目中使用Kaggle，因为它是免费的，也因为数据集是从Kaggle下载的，所以它将很容易在平台本身上扩展和训练模型。</p>



<ol><li>我们需要做的第一件事是上传模型，将目录路径更改为Kaggle特定的路径，并启用GPU。</li></ol>



<ol start="2"><li>请注意，模型必须是复杂的，以便捕捉用于预测的相关信息。您可以通过逐渐增加隐藏层的数量并观察模型的行为来开始缩放模型。你可能不希望接触其他参数，如注意力头的数量和隐藏大小，因为它可能会抛出算术错误。</li></ol>



<ol start="3"><li>对于每一次更改，您都要让模型在所有400个类别的小数据批次中运行至少两个时期，并观察准确性是否在提高。通常，它会增加。</li></ol>



<ol start="4"><li>一旦满意，运行模型10到15个时期，对于30000个样本的子集，这将花费大约5个小时。</li></ol>



<ol start="5"><li>在训练之后，在测试数据集上检查它的性能，如果它表现良好，则下载模型权重。此时，对于400个类，模型的大小应该在650 MB左右。</li></ol>



<h3>注意力可视化</h3>



<p>如前所述，自我关注是整个视觉转换器架构的关键，有趣的是，有一种方法可以将其可视化。注意图的源代码可以在这里找到<a href="https://web.archive.org/web/20221206144339/https://github.com/jeonsworld/ViT-pytorch/blob/main/visualize_attention_map.ipynb" target="_blank" rel="noreferrer noopener nofollow">。我对它做了一点修改，并将其创建为一个单独的独立模块，可以使用转换器的输出来生成注意力地图。这里的想法是存储输入图像及其对应的注意图图像，并在README.md文件中显示。</a></p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/attention_viz.py" target="_blank" rel="noreferrer noopener nofollow"> attention_viz.py </a> ( <a href="https://web.archive.org/web/20221206144339/https://github.com/jeonsworld/ViT-pytorch/blob/main/visualize_attention_map.ipynb" target="_blank" rel="noreferrer noopener nofollow">来源</a>)</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">attention_viz</span><span class="hljs-params">(model, test_data, img_path=PATH, device=<span class="hljs-string">'mps'</span>)</span>:</span>



</pre>



<p>我们可以通过简单地调用<strong> attention_viz </strong>函数并传递相应的参数来运行这段代码。</p>



<pre class="hljs"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
   train_data, val_data, test_data = Dataset(config.BATCH_SIZE,config.IMG_SIZE, config.DATASET_SAMPLE)
   model = torch.load(<span class="hljs-string">'metadata/models/model.pth'</span>, map_location=torch.device(<span class="hljs-string">'cpu'</span>))
   attention_viz(model, test_data, PATH)</pre>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/3583c08cf327e802940ff354384a2114.png" alt="Attention Visualization" class="wp-image-69922" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-7.png?ssl=1"/><figcaption><em>The image above is an example of attention visualization. The image on the left is the original image whereas the image on the right is overlaid with the attention map. The region i.e. the face of the bird is quite bright as that area constitutes the features to which the model is paying attention </em></figcaption></figure></div>


<h3>测试和推理</h3>



<p>我们还可以在测试模块中使用<strong> attention_viz </strong>函数<strong> </strong>，我们将在测试数据上测试模型，并测量模型在各种指标上的性能，如混淆矩阵、准确度分数、f1分数、召回分数和精确度分数。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/test.py" target="_blank" rel="noreferrer noopener nofollow"> test.py </a></p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">(model, test_data)</span>:</span>
   <span class="hljs-keyword">return</span> logits_, ground, confusion_matrix

</pre>



<p>我们可以使用seaborn的热图轻松生成混淆矩阵并进行可视化，并将其保存在results文件夹中，我们还可以使用该文件夹在README.md文件中显示它。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/6f7ea638be0631523657a4103c692a2f.png" alt="confusion matrix" class="wp-image-69923" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-8.png?ssl=1"/><figcaption><em>Above is the image of a confusion matrix that is of the shape 100X100 trained for 50 epochs. As you can see the model is quite efficient to predict true positives which can be seen in the diagonals in white color. But there are few false positives across the graph which means that the model still makes wrong predictions</em></figcaption></figure></div>


<p>我们还可以生成精度和损失图，并将其存储在结果文件夹中。因此，我们可以使用Sklearn找到其他度量，但在此之前，我们必须将tensors数组转换为NumPy数组。</p>



<pre class="hljs">probs = torch.zeros(len(logits_))
y_ = torch.zeros(len(ground))
idx = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> l, o <span class="hljs-keyword">in</span> zip(logits_, ground):
   _, l = torch.max(l, dim=<span class="hljs-number">1</span>)
   probs[idx] = l
   y_[idx] = o.item()
   idx+=<span class="hljs-number">1</span>

prob = probs.to(torch.long).numpy()
y_ = y_.to(torch.long).numpy()

print(accuracy_score(y_, prob))
print(cohen_kappa_score(y_, prob))
print(classification_report(y_, prob))</pre>



<p>一旦我们对模型的性能感到满意，我们就可以通过同时创建一个Streamlit应用程序来进行推理。</p>



<h2 id="h-mlops-pipeline-for-image-classification-creating-the-app-using-streamlit">用于影像分类的MLOps管道:使用Streamlit创建应用程序</h2>



<p><a href="https://web.archive.org/web/20221206144339/https://streamlit.io/" target="_blank" rel="noreferrer noopener nofollow"> Streamlit </a>应用将是一个网络应用，我们将部署在云上。为了构建应用程序，我们必须首先pip安装streamlit，然后在新模块中导入库。</p>



<p>该模块将包含与推理模块相同的模块，我们只需要复制和粘贴评估函数，然后使用Streamlit库构建应用程序。下面是应用程序的代码。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/source/app.py" target="_blank" rel="noreferrer noopener nofollow"> app.py </a></p>



<pre class="hljs"><span class="hljs-keyword">import</span> warnings
warnings.simplefilter(action=<span class="hljs-string">'ignore'</span>, category=FutureWarning)

<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st

<span class="hljs-keyword">from</span> embeddings <span class="hljs-keyword">import</span> Embeddings
<span class="hljs-keyword">from</span> attention_block <span class="hljs-keyword">import</span> Block
<span class="hljs-keyword">from</span> linear <span class="hljs-keyword">import</span> Mlp
<span class="hljs-keyword">from</span> attention <span class="hljs-keyword">import</span> Attention
<span class="hljs-keyword">from</span> transformer <span class="hljs-keyword">import</span> VisionTransformer, Transformer, Encoder

<span class="hljs-keyword">from</span> config <span class="hljs-keyword">import</span> Config
config = Config()

st.set_option(<span class="hljs-string">'deprecation.showfileUploaderEncoding'</span>, <span class="hljs-keyword">False</span>)
st.title(<span class="hljs-string">"Bird Image Classifier"</span>)
st.write(<span class="hljs-string">""</span>)


file_up = st.file_uploader(<span class="hljs-string">"Upload an image"</span>, type = <span class="hljs-string">"jpg"</span>)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(image)</span>:</span>
   <span class="hljs-string">"""Return top 5 predictions ranked by highest probability.
   Parameters
   ----------
   :param image: uploaded image
   :type image: jpg
   :rtype: list
   :return: top 5 predictions ranked by highest probability
   """</span>
   model = torch.load(<span class="hljs-string">'model.pth'</span>)

   
   transform = transforms.Compose([
       transforms.Resize(<span class="hljs-number">128</span>),
       transforms.CenterCrop(<span class="hljs-number">128</span>),
       transforms.ToTensor(),
       transforms.Normalize(
           mean = [<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>],
           std = [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])])

   
   img = Image.open(image)
   x = transform(img)
   x = torch.unsqueeze(x, <span class="hljs-number">0</span>)
   model.eval()
   logits, attn_w = model(x)

   <span class="hljs-keyword">with</span> open(<span class="hljs-string">'../metadata/classes.txt'</span>, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
       classes = f.read().split(<span class="hljs-string">'n'</span>)

   
   prob = torch.nn.functional.softmax(logits, dim = <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] * <span class="hljs-number">100</span>
   _, indices = torch.sort(logits, descending = <span class="hljs-keyword">True</span>)
   <span class="hljs-keyword">return</span> [(classes[idx], prob[idx].item()) <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> indices[<span class="hljs-number">0</span>][:<span class="hljs-number">5</span>]]


<span class="hljs-keyword">if</span> file_up <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
   
   image = Image.open(file_up)
   st.image(image, caption = <span class="hljs-string">'Uploaded Image.'</span>, use_column_width = <span class="hljs-keyword">True</span>)
   st.write(<span class="hljs-string">""</span>)
   st.write(<span class="hljs-string">"Processing..."</span>)
   labels = predict(file_up)

   
   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> labels:
       st.write(f<span class="hljs-string">"Prediction {i[0]} score {i[1]:.2f}"</span>)</pre>



<p>但是在部署之前，我们必须在本地进行测试。为了测试应用程序，我们将运行以下命令:</p>



<pre class="hljs">streamlit run app.py</pre>



<p>一旦执行了上述命令，您将得到以下提示:</p>



<pre class="hljs">You can now view your Streamlit app <span class="hljs-keyword">in</span> your browser.

  Local URL: http://localhost:<span class="hljs-number">8501</span>
  Network URL: http://<span class="hljs-number">192.168</span><span class="hljs-number">.0</span><span class="hljs-number">.105</span>:<span class="hljs-number">8501</span></pre>



<p>复制网址粘贴到你的浏览器，app就上线了(本地)。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/0f3cbcbf2b70d7971813d84c468c0267.png" alt="Bird image classifier" class="wp-image-69924" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-9.png?ssl=1"/><figcaption><em>Copied URL</em></figcaption></figure></div>


<p>上传图片进行分类。</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/d029bcc363e5c61b62c40dd1e2c77b2f.png" alt="Uploaded image" class="wp-image-69925" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-10.png?ssl=1"/><figcaption><em>Uploaded image</em></figcaption></figure>



<p>随着ViT模型的训练和应用程序的准备，我们的目录结构应该看起来像这样:</p>



<pre class="hljs">.
├── README.md
├── metadata
│   ├── Abbott<span class="hljs-string">'s_babbler_(Malacocincla_abbotti).jpg
│   ├── classes.txt
│   ├── models
│   │   └── model.pth
│   └── results
│       ├── accuracy_loss.png
│       ├── attn.png
│       └── confusion_matrix.png
├── notebooks
│   ├── ViT.ipynb
│   └── __init__.py
└── source
    ├── __init__.py
    ├── app.py
    ├── attention.py
    ├── attention_block.py
    ├── attention_viz.py
    ├── config.py
    ├── embeddings.py
    ├── linear.py
    ├── metrics.py
    ├── preprocessing.py
    ├── test.py
    ├── train.py
    ├── transformer.py
</span></pre>



<p>现在我们开始部署应用程序。</p>



<h2 id="h-mlops-pipeline-for-image-classification-code-formatting">用于图像分类的MLOps流水线:代码格式化</h2>



<p>首先，让我们格式化我们的Python脚本。为此，我们将使用黑色。Black是一个Python脚本格式化程序。你所需要做的就是pip安装black，然后运行<strong> <em> `black ` </em> </strong>跟在python模块甚至整个目录的名字后面。对于这个项目，我运行black，然后运行包含所有python模块的源目录。</p>



<pre class="hljs">ViT-Pytorch git:(main) black source
Skipping .ipynb files <span class="hljs-keyword">as</span> Jupyter dependencies are <span class="hljs-keyword">not</span> installed.
You can fix this by running ``pip install black[jupyter]``
reformatted source/config.py
reformatted source/embeddings.py
reformatted source/attention_block.py
reformatted source/linear.py
reformatted source/app.py
reformatted source/attention_viz.py
reformatted source/attention.py
reformatted source/preprocessing.py
reformatted source/test.py
reformatted source/metrics.py
reformatted source/transformer.py
reformatted source/train.py
</pre>



<p>使用black的好处是去掉了不必要的空格，增加了双引号而不是单引号，让审查代码更快更高效。</p>



<p>下面给出了使用<strong>黑色</strong>格式化代码前后的图像。</p>


<div class="wp-block-image">
<figure class="aligncenter size-full"><a href="https://web.archive.org/web/20221206144339/https://neptune.ai/mlops-pipeline-computer-vision-11"><img decoding="async" src="../Images/d8319bc3057d2ecaf4070643386bee1d.png" alt="Examples before and after using black to format the code" class="wp-image-69926" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-11.png?ssl=1"/></a><figcaption><em>Examples before and after using black to format the code </em></figcaption></figure></div>


<p>如你所见，不必要的空格被删除了。</p>



<h2 id="h-mlops-pipeline-for-image-classification-setting-up-ci-cd">用于图像分类的MLOps管道:设置CI/CD</h2>



<p>对于我们的CI/CD流程，我们将使用<strong> Github Actions、</strong>和<strong> Google Cloud Build </strong>来集成和部署我们的Streamlit应用程序。以下步骤将帮助您创建完整的MLOps管道。</p>



<h4>创建Github存储库</h4>



<p>第一步是创建Github存储库。但在此之前，我们必须创建三个重要文件:</p>



<div id="case-study-numbered-list-block_62da67d988d97" class="block-case-study-numbered-list ">

    
    <h2 id="h-"/>

    <ul class="c-list">
                    <li class="c-list__item"><span class="c-list__counter"> 1 </span>要求. txt <br/></li>
                    <li class="c-list__item"><span class="c-list__counter"> 2 </span> makefile <br/></li>
                    <li class="c-list__item"><span class="c-list__counter"> 3 </span> main.yml</li>
            </ul>
</div>



<h4>requirements.txt</h4>



<p>requirements.txt文件必须包含模型正在使用的所有库。有两种方法可以创建requirements.txt文件。</p>



<ol><li>如果您有专门为此项目创建的专用工作环境，那么您可以运行pip freeze&gt;requirements.txt，它将为您创建一个requirements.txt文件。</li><li>如果您有一个通用的工作环境，那么您可以运行pip冻结并复制粘贴您一直在工作的库。</li></ol>



<p>该项目的requirement.txt文件如下所示:</p>



<pre class="hljs">numpy==<span class="hljs-number">1.22</span><span class="hljs-number">.3</span>
torch==<span class="hljs-number">1.12</span><span class="hljs-number">.0</span>
torchvision==<span class="hljs-number">0.12</span><span class="hljs-number">.0</span>
tqdm==<span class="hljs-number">4.64</span><span class="hljs-number">.0</span>
opencv-python==<span class="hljs-number">4.6</span><span class="hljs-number">.0</span><span class="hljs-number">.66</span>
streamlit==<span class="hljs-number">1.10</span><span class="hljs-number">.0</span>
neptune-client==<span class="hljs-number">0.16</span><span class="hljs-number">.3</span>
</pre>



<p><strong>注意:</strong>始终确保你提到的版本，以便在未来，应用程序保持稳定和最佳性能。</p>



<h4>生成文件</h4>



<p>简而言之，Makefile是一个命令提示符文件，它可以自动完成安装库和依赖项、运行Python脚本等等的整个过程。典型的Makefile如下所示:</p>



<pre class="hljs">
setup:
   python3 -m venv ~/.visiontransformer
   source ~/.visiontransformer/bin/activate
   cd .visiontransformer
install:
   pip install --upgrade pip &amp;&amp;
       pip install -r requirements.txt
run:
   python source/test.py
all: install run</pre>



<p>对于这个项目，我们的Makefile将有三个过程:</p>



<div id="case-study-numbered-list-block_62da696788d9b" class="block-case-study-numbered-list ">

    
    <h2 id="h-"/>

    <ul class="c-list">
                    <li class="c-list__item"><span class="c-list__counter"> 1 </span>设置虚拟环境并激活它。<br/></li>
                    <li class="c-list__item"><span class="c-list__counter"> 2 </span>安装所有的Python库。<br/></li>
                    <li class="c-list__item"><span class="c-list__counter"> 3 </span>运行一个测试文件。</li>
            </ul>
</div>



<p>实际上，每次我们进行新的提交时，都会执行makefile，它会自动运行test.py模块，生成最新的性能指标并更新README.md文件。</p>



<p>但是Makefile只有在我们创建一个动作触发器时才能工作。让我们来创造它。</p>



<h4>动作触发器:。github/workflow/main.yml</h4>



<p>要创建动作触发器，我们需要创建以下目录:。github/workflow，<strong> </strong>之后创建一个<strong> main.yml </strong>文件。每当repo被更新时，main.yml将创建一个动作触发器。</p>



<p>我们的目标是持续集成现有构建中的任何变更，如更新参数、模型架构，甚至UI/UX。一旦检测到更改，它将自动更新README.md文件。这个项目的main.yml被设计为在任何push或pull请求时触发工作流，但是只针对main分支。</p>



<p>在每次新提交时，该文件将激活ubuntu-latest环境，安装特定的python版本，然后执行Makefile中的特定命令。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/Makefile" target="_blank" rel="noreferrer noopener nofollow"> main.yml </a></p>



<pre class="hljs">
name: Continuous Integration <span class="hljs-keyword">with</span> Github Actions

on:
 push:
   branches: [ main ]
 pull_request:
   branches: [ main ]

jobs:
 build:
   runs-on: ubuntu-latest
   
   steps:
     - uses: actions/checkout@v2
     - name: Set up Python <span class="hljs-number">3.8</span>
       uses: actions/setup-python@v1
       <span class="hljs-keyword">with</span>:
         python-version: <span class="hljs-number">3.8</span>
     - name: Install dependencies
       run: |
         make install
         make run
</pre>



<h4>测试</h4>



<p>创建文件后，您可以将整个代码库推送到Github。上传后，您可以点击Actions选项卡，亲自查看内置进度。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/3f5ff86c1c826b723f08c2300d8f2c30.png" alt="Testing" class="wp-image-69927" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-12.png?ssl=1"/><figcaption><em>Build-in progress in the Actions tab</em></figcaption></figure></div>


<h4>部署:Google云构建</h4>



<p>测试完成后，Github README.md文件中的所有日志和结果都已更新，我们可以进入下一步，即将应用程序集成到云中。</p>



<ol><li>首先，我们将访问:<a href="https://web.archive.org/web/20221206144339/https://console.cloud.google.com/" target="_blank" rel="noreferrer noopener nofollow">https://console.cloud.google.com/</a>，然后我们将在仪表板中创建一个新项目，并将其命名为Vision Transformer Pytorch。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/b0055eab6416f32274279ff6ff55cff7.png" alt="Creating a new project" class="wp-image-69928" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-13.png?ssl=1"/><figcaption><em>Creating a new project</em></figcaption></figure></div>


<p>一旦创建了项目，您就可以导航到该项目，它看起来会像这样:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/e98838a077b6610197ed5445dd9b0d7f.png" alt="The project" class="wp-image-69929" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-14.png?ssl=1"/><figcaption><em>The project</em></figcaption></figure></div>


<p>如你所见，google cloud build在项目主页上为我们提供了各种现成的服务，如虚拟机、大查询、GKE或Kubernetes集群。但是在我们在云构建中创建任何东西之前，我们必须启用Kubernetes集群，并在项目目录中创建某个目录和它们各自的文件。</p>



<ol start="2"><li><strong> Kubernetes </strong></li></ol>



<p>让我们在创建任何文件之前设置我们的Kubernetes集群。为此，我们可以在谷歌云控制台搜索栏中搜索GKE并启用API。</p>


<div class="wp-block-image">
<figure class="aligncenter size-full"><img decoding="async" src="../Images/ef1e1b5a80ef094b7560dd3bc982919b.png" alt="Setting up Kubernetes cluster" class="wp-image-69930" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-15.png?ssl=1"/><figcaption><em>Setting up Kubernetes cluster</em></figcaption></figure></div>


<p>启用API后，我们将导航到下一页。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/d011c00846f2ffdcf41deeb7106d1fc8.png" alt="Kubernetes cluster " class="wp-image-69931" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-15b.png?ssl=1"/><figcaption><em>Kubernetes cluster</em></figcaption></figure></div>


<p>但是我们将使用内置的云shell来创建集群，而不是手动创建集群。为此，点击右上角的终端按钮，查看下图。</p>


<div class="wp-block-image">
<figure class="aligncenter size-full"><img decoding="async" src="../Images/2b0c9a2a31355488cebbb61cfe75700a.png" alt="Cloud shell" class="wp-image-69932" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-16.png?ssl=1"/><figcaption><em>Activating Cloud Shell</em></figcaption></figure></div>

<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/5ff71759198d6baeeeaee6f1b7874999.png" alt="Creating cluster by using inbuild cloud shell" class="wp-image-69933" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-17.png?ssl=1"/><figcaption><em>Creating cluster by using inbuild cloud shell</em></figcaption></figure></div>


<p>激活云shell后，我们可以键入以下命令来创建Kubernetes集群:</p>



<pre class="hljs">gcloud container clusters create project-kube --zone <span class="hljs-string">"us-west1-b"</span> --machine-type <span class="hljs-string">"n1-standard-1"</span> --num-nodes <span class="hljs-string">"1"</span></pre>



<p>这通常需要5分钟。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/af78b6a38bf783730bca11e149d5872c.png" alt="Creating Kubernetes clusters" class="wp-image-69934" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-18.png?ssl=1"/><figcaption><em>Creating Kubernetes clusters</em></figcaption></figure></div>


<p>完成后，它看起来会像这样:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/65e3baf8905966bb1f8262754030e479.png" alt="Kubernetes clustering completed" class="wp-image-69935" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-19.png?ssl=1"/><figcaption><em>Kubernetes clustering completed</em></figcaption></figure></div>


<p>现在让我们设置两个配置Kubernetes集群的文件:deployment.yml和service.yml。</p>



<p>yml文件允许我们在云中部署模型。根据要求，部署可以是淡黄色、重新创建、蓝绿色或任何其他颜色。在本例中，我们将覆盖部署。这个文件也有助于使用参数<strong>副本</strong>有效地缩放模型。下面是一个deployment.yml文件的示例。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/kubernetes/deployment.yml" target="_blank" rel="noreferrer noopener nofollow"> deployment.yml </a></p>



<pre class="hljs">

apiVersion: apps/v1
kind: Deployment
metadata:
 name: imgclass
spec:
 replicas: <span class="hljs-number">1</span>
 selector:
   matchLabels:
     app: imageclassifier
 template:
   metadata:
     labels:
       app: imageclassifier
   spec:
     containers:
     - name: cv-app
       image: gcr.io/vision-transformer-pytorch/vit:v1
       ports:
       - containerPort: <span class="hljs-number">8501</span></pre>



<p>下一个文件是service.yml文件。它本质上是将应用从容器连接到现实世界。注意到<em>容器端口</em>参数被指定为8501，我们将在我们的service.yml中为<em>目标端口</em>参数使用相同的数字。这与Streamlit用来部署应用程序的数字相同。除此之外，两个文件中的<em>应用</em>参数是相同的。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/kubernetes/service.yml" target="_blank" rel="noreferrer noopener nofollow">服务. yml </a></p>



<pre class="hljs">

apiVersion: v1
kind: Service
metadata:
 name: imageclassifier
spec:
 type: LoadBalancer
 selector:
   app: imageclassifier
 ports:
 - port: <span class="hljs-number">80</span>
   targetPort: <span class="hljs-number">8501</span></pre>



<p><strong>注意</strong>:一定要确保app的名字和版本都是小写的。</p>



<ol start="3"><li><strong> Dockerfile </strong></li></ol>



<p>现在让我们配置Dockerfile文件。该文件将创建一个Docker容器来托管我们的Streamlit应用程序。Docker是非常必要的，因为它将应用程序包装在一个易于扩展的环境中。典型的docker文件如下所示:</p>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/Dockerfile" target="_blank" rel="noreferrer noopener nofollow"> Dockerfile </a></p>



<pre class="hljs">FROM python:<span class="hljs-number">3.8</span><span class="hljs-number">.2</span>-slim-buster

RUN apt-get update

ENV APP_HOME /app
WORKDIR $APP_HOME
COPY . ./

RUN ls -la $APP_HOME/


RUN pip install -r requirements.txt


CMD [ <span class="hljs-string">"streamlit"</span>, <span class="hljs-string">"run"</span>,<span class="hljs-string">"app.py"</span> ]
</pre>



<p>Dockerfile包含一系列命令，这些命令:</p>



<ul><li>安装Python版本。</li><li>将本地代码复制到容器映像。</li><li>安装所有库。</li><li>执行Streamlit应用程序。</li></ul>



<p>请注意，我们使用的是Python 3.8，因为一些依赖项使用的是最新的Python版本。</p>



<ol start="4"><li><strong> cloudbuild.yaml </strong></li></ol>



<p>在Google Cloudbuild中，cloudbuild.yml文件将所有工件缝合在一起，创建了一个无缝管道。它有三个主要步骤:</p>



<ul><li>使用当前目录中的Docker文件构建Docker容器。</li><li>将容器推送到google容器注册表。</li><li>在Kubernetes引擎中部署容器。</li></ul>



<p><a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch/blob/main/cloudbuild.yaml" target="_blank" rel="noreferrer noopener nofollow">云构建. yml </a></p>



<pre class="hljs">steps:
- name: <span class="hljs-string">'gcr.io/cloud-builders/docker'</span>
 args: [<span class="hljs-string">'build'</span>, <span class="hljs-string">'-t'</span>, <span class="hljs-string">'gcr.io/vision-transformer-pytorch/vit:v1'</span>, <span class="hljs-string">'.'</span>]
 timeout: <span class="hljs-number">180</span>s
- name: <span class="hljs-string">'gcr.io/cloud-builders/docker'</span>
 args: [<span class="hljs-string">'push'</span>, <span class="hljs-string">'gcr.io/vision-transformer-pytorch/vit:v1'</span>]
- name: <span class="hljs-string">"gcr.io/cloud-builders/gke-deploy"</span>
 args:
 - run
 - --filename=kubernetes/ 
 - --location=us-west1-b
 - --cluster=project-kube</pre>



<p><strong>注意</strong>:请交叉检查deployment.yml和cloudbuild.yml文件中的容器名等参数。此外，还要用clouldbuild.yml文件中的集群名称交叉检查您之前创建的集群名称。最后，确保<em>文件名</em>参数与deployment.yml和service.yml所在的Kubernetes目录相同。</p>



<p>创建文件后，整个项目的文件结构应该如下所示:</p>



<pre class="hljs">.
├── Dockerfile
├── .github/workflow/main.yml
├── Makefile
├── README.md
├── cloudbuild.yaml
├── kubernetes
│   ├── deployment.yml
│   └── service.yml
├── metadata
│   ├── Abbott<span class="hljs-string">'s_babbler_(Malacocincla_abbotti).jpg
│   ├── classes.txt
│   ├── models
│   │   └── model.pth
│   └── results
│       ├── accuracy_loss.png
│       ├── attn.png
│       └── confusion_matrix.png
├── notebooks
│   ├── ViT.ipynb
│   └── __init__.py
├── requirements.txt
└── source
    ├── __init__.py
    ├── app.py
    ├── attention.py
    ├── attention_block.py
    ├── attention_viz.py
    ├── config.py
    ├── embeddings.py
    ├── linear.py
    ├── metrics.py
    ├── preprocessing.py
    ├── test.py
    ├── train.py
    ├── transformer.py
    └── vit-pytorch.ipynb
</span></pre>



<ol start="5"><li><strong>克隆和测试</strong></li></ol>



<p>现在，让我们在google cloud build项目中克隆GitHub repo，将其放入cd中，并运行cloudbuild.yml文件。使用以下命令:</p>






<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/f70ca83dce3d23d0d9f4a96713273bf6.png" alt="clone the GitHub repo" class="wp-image-69936" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-20.png?ssl=1"/><figcaption><em>Cloning the GitHub repo</em></figcaption></figure></div>


<ul><li><em> gcloud builds提交–配置cloudbuild.yaml </em></li></ul>



<p>部署过程将如下所示:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/518ca7f89f8e3c056a18c8c1bb810cc3.png" alt="The deployment process" class="wp-image-69937" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-21.png?ssl=1"/><figcaption><em>The deployment process </em></figcaption></figure></div>


<ol start="6"><li>部署大约需要10分钟，这取决于各种因素。如果一切都正确执行，您将会看到这些步骤用绿色标记进行了颜色编码。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/c01b59aca3c6719a2e5d58735569bd82.png" alt="Succcessful deployment " class="wp-image-69938" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-22.png?ssl=1"/><figcaption><em>Succcessful deployment  </em></figcaption></figure></div>


<ol start="7"><li>部署成功后，您可以在Kubernetes引擎的Services &amp; Ingress选项卡中找到应用程序的端点。单击端点，它会将您导航到Streamlit应用程序。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-full"><img decoding="async" src="../Images/7687b5a280f8743ffeb3b3fcc2aeae8b.png" alt="The endpoints " class="wp-image-69939" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-23.png?ssl=1"/><figcaption><em>The endpoints </em></figcaption></figure></div>

<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/90e804e1867db1da3ad0d860e2f19ac5.png" alt="The Streamlit app" class="wp-image-69940" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-24.png?ssl=1"/><figcaption><em>The Streamlit app</em></figcaption></figure></div>


<p><strong>附加提示:</strong></p>



<ol><li>确保在所有*中使用小写的应用程序名称和项目id。yml配置文件。</li><li>交叉检查所有*的论点。yml配置文件。</li><li>由于您是在虚拟环境中拷贝您的存储库，请交叉检查所有目录和文件路径。</li><li>如果在云构建过程中出现错误，请寻找一个命令来帮助您解决在错误语句中发现的错误。请看下图，以便更好地理解；我突出显示了在重新运行云构建命令之前需要执行的命令。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/21eaf2006b1f8071eca1b60d37d5c19a.png" alt="an error in the cloud build process" class="wp-image-69941" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-25.png?ssl=1"/><figcaption><em>An error in the cloud build process</em></figcaption></figure></div>


<h4>云构建集成</h4>



<p>现在，我们将把Google cloud构建集成到Github repo中。这将创建一个触发操作，每当在回购中进行更改时，该操作将更新构建。</p>



<ol><li>在市场中搜索谷歌云构建</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/5daf38df5b1b1634248257c37083f8c9.png" alt="Searching for Google Cloud Build" class="wp-image-69942" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-26.png?ssl=1"/><figcaption><em>Searching for Google Cloud Build</em></figcaption></figure></div>


<ol start="2"><li>选择要连接的回购。在这种情况下，它将是ViT-Pytorch并保存它。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/023029f23b3887b38d4b9e5eb36ccb6f.png" alt="Selecting the repo" class="wp-image-69943" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-27.png?ssl=1"/><figcaption><em>Selecting the repo</em></figcaption></figure></div>


<ol start="3"><li>在Google Cloud Build中，我们将转到Cloud Build页面，并单击Triggers选项卡来创建触发器。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/73b0c529536c0852c3df02ae3427429b.png" alt="creating triggers" class="wp-image-69944" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-28.png?ssl=1"/><figcaption><em>Creating triggers</em></figcaption></figure></div>


<ol start="4"><li>单击“创建触发器”后，我们将被导航到下面的页面。这里我们将提到触发器名称，选择将触发cloudbuild.yml文件的事件，并选择项目存储库。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/199279208cb2fb3d11862b7794b08098.png" alt="Trigger settings " class="wp-image-69945" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-29.png?ssl=1"/><figcaption><em>Trigger settings</em></figcaption></figure></div>


<ol start="5"><li>遵循认证过程。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/0d34c94440b64c1f4c74dbe63153aa3e.png" alt="authentication process" class="wp-image-69946" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-30.png?ssl=1"/><figcaption><em>Authentication process</em></figcaption></figure></div>


<ol start="6"><li>连接存储库。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/5976dbe423bebcb83b5ab923ed0cf693.png" alt="Connecting the repository" class="wp-image-69947" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-31.png?ssl=1"/><figcaption><em>Connecting the repository</em></figcaption></figure></div>


<ol start="7"><li>最后，创建触发器。</li></ol>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/e1f5a1c7cb155076a631342914618e84.png" alt="creating the trigger" class="wp-image-69948" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-32.png?ssl=1"/><figcaption><em>Creating the trigger</em></figcaption></figure></div>


<p>既然已经创建了触发器，那么您在Github repo中所做的所有更改都将被自动检测到，并且部署也将被更新。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/0b650976d86b2bd7e981ad75b13456fd.png" alt="Created trigger" class="wp-image-69949" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206144339im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlops-pipeline-computer-vision-33.png?ssl=1"/><figcaption><em>Created trigger </em></figcaption></figure></div>


<h4>监控模型衰减</h4>



<p>随着时间的推移，模型会衰退，这将影响预测能力。我们需要定期监控性能。一种方法是偶尔在新数据集上测试模型，并在我之前提到的指标上进行评估，如F1分数、准确度分数、精确度分数等。</p>



<p>监控模型性能的另一个有趣的方法是使用AUROC指标，它测量模型的区分性能。因为此项目是多分类项目，所以您可以将其转换为二元分类项目，然后检查模型的性能。如果模型的性能已经衰退，那么必须用新的样本和更大的样本再次训练模型。如果真的需要，那么也要修改架构。</p>



<p><a href="https://web.archive.org/web/20221206144339/https://gist.github.com/khizirsiddiqui/559a91dab223944fb83f8480715d2582" target="_blank" rel="noreferrer noopener nofollow">这里的</a>是代码的链接，它将允许您测量AUROC分数。</p>



<h2 id="h-conclusion">结论</h2>



<p>在本文中，我们学习了使用Pytorch和Streamlit通过Vision Transformer构建一个图像分类器应用程序。我们还看到了如何使用Github操作和技术(如Kubernetes、Dockerfile和Makefile)在Google云平台上部署应用程序。</p>



<p>本项目的重要收获:</p>



<ol><li>更大的数据需要更大的模型，这本质上需要更多时代的训练。</li><li>在创建原型实验时，减少类的数量，并测试准确性是否随着每个历元而增加。在Kaggle或Colab等云服务上使用GPU之前，尝试不同的配置，直到您确信模型的性能正在提高。</li><li>使用各种性能指标，如混淆指标、精确度、召回率、混淆指标、f1和AUROC。</li><li>一旦部署了模型，就可以偶尔而不是频繁地对模型进行监控。</li><li>为了进行监控，使用像AUROC分数这样的性能指标是很好的，因为它会自动创建阈值并绘制模型的真阳性率和假阳性率。有了AUROC评分，就可以很容易地比较模型以前和当前的性能。</li><li>只有当模型发生显著漂移时，才应该重新训练模型。由于像这样的模型需要大量的计算资源，频繁的重新训练可能是昂贵的。</li></ol>



<p>我希望你发现这篇文章信息丰富，实用。你可以在这个<a href="https://web.archive.org/web/20221206144339/https://github.com/Nielspace/ViT-Pytorch" target="_blank" rel="noreferrer noopener nofollow"> Github repo </a>中找到完整的代码。也可以随意与他人分享。</p>



<h3>参考</h3>



<ol><li><a href="https://web.archive.org/web/20221206144339/https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noreferrer noopener nofollow">一幅图像值16×16个字:大规模图像识别的变形金刚</a></li><li><a href="https://web.archive.org/web/20221206144339/https://arxiv.org/abs/2111.05464" target="_blank" rel="noreferrer noopener nofollow">变形金刚比CNN更健壮吗？</a></li><li><a href="https://web.archive.org/web/20221206144339/https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html" target="_blank" rel="noreferrer noopener nofollow">https://www . kdnugges . com/2022/01/machine-learning-models-die-silence . html</a></li><li><a href="https://web.archive.org/web/20221206144339/https://github.com/jeonsworld/ViT-pytorch" target="_blank" rel="noreferrer noopener nofollow">https://github.com/jeonsworld/ViT-pytorch</a></li><li><a href="https://web.archive.org/web/20221206144339/https://gist.github.com/khizirsiddiqui/559a91dab223944fb83f8480715d2582" target="_blank" rel="noreferrer noopener nofollow">https://gist . github . com/khizirsiddiqui/559 a 91 dab 223944 FB 83 f 8480715d 2582</a></li><li><a href="https://web.archive.org/web/20221206144339/https://github.com/srivatsan88/ContinousModelDeploy" target="_blank" rel="noreferrer noopener nofollow">https://github.com/srivatsan88/ContinousModelDeploy</a></li><li><a href="https://web.archive.org/web/20221206144339/https://neptune.ai/blog/mlops-pipeline-for-nlp-machine-translation" target="_blank" rel="noreferrer noopener nofollow">为NLP构建MLOps管道:机器翻译任务</a></li></ol>
        </div>
        
    </div>    
</body>
</html>