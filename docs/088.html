<html>
<head>
<title>Continuous Control With Deep Reinforcement Learning </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>具有深度强化学习的连续控制</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/continuous-control-with-deep-reinforcement-learning#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/continuous-control-with-deep-reinforcement-learning#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>这一次我想探索如何深入利用<a href="/web/20221206055216/https://neptune.ai/blog/reinforcement-learning-basics-markov-chain-tree-search" target="_blank" rel="noreferrer noopener">强化学习</a>，例如让一个人形模型行走。这种任务是连续控制任务。这种任务的解决方案不同于你可能知道并使用深度Q网络(DQN)玩Atari游戏(如Pong)的方案。</p>



<p>我将讨论连续控制环境的特征。然后，我将向您介绍演员-评论家架构，并展示最先进的演员-评论家方法，软演员-评论家(SAC)的例子。最后，我们将深入研究代码。我将简要解释它是如何在令人惊叹的SpinningUp框架中实现的。我们走吧！</p>



<h2 id="h-what-is-continuous-control">什么是持续控制？</h2>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" src="../Images/5383c7e38b55cde7dd5d5b4f545a0f2e.png" alt="Continuous Control with Deep Reinforcement Learning" class="wp-image-53267" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206055216im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Continuous-Control-with-Deep-Reinforcement-Learning_4.gif?ssl=1"/><figcaption><em>Continuous Control with Deep Reinforcement Learning | Source: <a href="https://web.archive.org/web/20221206055216/https://openai.com/blog/roboschool/" target="_blank" rel="noreferrer noopener nofollow">Roboschool</a></em></figcaption></figure></div>



<p>遇见人形。它是一个三维的两足机器人环境。它的观测值是描述机器人运动学属性的376维向量。它的动作是17维向量，指定施加在机器人关节上的扭矩。目标是尽可能快地向前跑…并且不要摔倒。</p>



<p>动作是连续取值的向量。这与您可能已经从Atari环境中知道的一组固定的可能操作非常不同。它需要一个策略，不返回所有可能操作的分数或质量，而只返回一个要执行的操作。不同的策略输出需要不同的培训策略，我们将在下一节探讨这一点。</p>



<h2 id="h-what-is-mujoco">什么是MuJoCo？</h2>



<p>MuJoCo 是一个快速精确的物理模拟引擎，旨在研究和开发机器人、生物力学、图形和动画。OpenAI Gym及其包含的人形环境利用它来模拟环境动态。我在这里写了关于安装和使用它的整个帖子<a href="/web/20221206055216/https://neptune.ai/blog/installing-mujoco-to-work-with-openai-gym-environments" target="_blank" rel="noreferrer noopener">。对于这个帖子，我们不需要这个。</a></p>







<h2 id="h-off-policy-actor-critic-methods">不符合政策的行动者-批评家方法</h2>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/89ff9b700e117ad569a86ca13cc49e9b.png" alt="Reinforcement Learning" class="wp-image-53266" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206055216im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Continuous-Control-with-Deep-Reinforcement-Learning_5.png?resize=768%2C296&amp;ssl=1"/><figcaption><em>Reinforcement Learning | Source: Sutton &amp; Barto, Reinforcement Learning: An Introduction, 2nd edition</em></figcaption></figure></div>



<p>让我们回顾一下:强化学习(RL)是学习做什么——如何将情况映射到行动——以最大化某种累积奖励的概念。RL由一个为了学习而在环境中活动的代理组成。环境对每个代理的动作提供响应，并反馈给代理。奖励被用作强化信号，而状态被用来制约代理人的决策。</p>



<p>真正的目标是找到一个最优的政策。策略告诉代理在它发现自己处于任何状态时应该如何表现。这是代理对环境目标的映射。</p>







<p>如上图所示，演员-评论家架构将代理分为两部分，演员<strong>和评论家<strong>分别为</strong>和</strong>。</p>



<ul><li><strong>参与者</strong>代表策略——它学习从状态到动作的映射。</li><li><strong>评论家</strong>代表Q函数——它学习评估每个动作在每个可能的状态下有多好。你可以看到演员利用评论家的评价来改进政策。</li></ul>



<p>为什么要使用这样的构造？如果你已经知道Q-Learning ( <a href="https://web.archive.org/web/20221206055216/https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0" target="_blank" rel="noreferrer noopener nofollow">这里</a>你可以了解它)，你知道训练Q-函数对于解决RL任务是有用的。Q函数会告诉你每个动作在任何状态下有多好。然后，您可以简单地选择最佳行动。当你有一套固定的行动时，很容易做到，你只需简单地评估每一个行动，然后选择最好的！</p>



<p>然而，在一个动作是连续的情况下该怎么办呢？你不能评估每一个值，你可以评估一些值并选择最好的，但它会产生自身的问题，例如分辨率——评估多少个值以及评估哪些值？演员就是这些问题的答案。它近似离散情况下的argmax算子。它只是被训练来预测最好的行动，如果我们能和批评家一起评估每一个可能的行动。下面我们描述软演员评论家(SAC)的例子。</p>



<h3>伪代码中的SAC</h3>







<p>SAC的批评者是经过训练的非政策，这意味着它可以重用由旧的、训练较少的政策收集的数据。第11-13行中的政策外批评家训练利用了与DQN非常相似的技术，例如，它使用目标Q网络来稳定训练。不符合策略的方法比符合策略的方法(如<a href="https://web.archive.org/web/20221206055216/https://spinningup.openai.com/en/latest/algorithms/ppo.html" target="_blank" rel="noreferrer noopener nofollow"> PPO </a>)样本效率更高，因为我们可以构建经验重放缓冲区，其中每个收集的数据样本都可以多次重复用于训练——与符合策略的训练相反，符合策略的训练仅在一次更新后就丢弃数据！</p>



<p>您可以在第1行看到critics和replay buffer被初始化，在第2行看到target critics。我们用两个批评家来对抗论文中描述的高估误差:“双Q-learning”和“Actor-Critic中的寻址函数近似误差”，你可以在这里了解更多。然后，在第4-8行中收集数据并提供给重放缓冲区。第14行更新策略，第15行更新目标网络。</p>



<p>您可能会注意到评论家和演员的更新都包括一些额外的日志术语。这是最大熵正则化，防止代理人过多地利用其可能不完美的知识，并奖励有前途的行动的探索。如果你想详细了解它，我推荐你阅读<a href="https://web.archive.org/web/20221206055216/https://spinningup.openai.com/en/latest/algorithms/sac.html" target="_blank" rel="noreferrer noopener nofollow">这个</a>资源。</p>



<h2 id="h-soft-actor-critic-in-code">代码中的软演员评论家</h2>



<p>我们将在深度RL-TF2实施框架中与<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2" target="_blank" rel="noreferrer noopener nofollow">一起工作。repo自述文件中有安装说明。注意，你暂时不用安装MuJoCo。我们将在OpenAI Gym suite的Pendulum-v0环境中运行一个示例软演员-评论家代理。让我们投入进去吧！</a></p>



<h3>钟摆v0环境</h3>







<p>摆-v0是连续控制环境，其中:</p>



<p id="separator-block_61ae42570368b" class="block-separator block-separator--15"> </p>



<div id="medium-table-block_61ae425d0368c" class="block-medium-table c-table__outer-wrapper ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">行动</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">一维中只有一个关节的扭矩</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>三维向量，其中前两维代表摆的位置——它们是摆角的cos和sin第三维是摆角速度</p> </div></td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"><p/></div></td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>



<p id="separator-block_61680220f582a" class="block-separator block-separator--15"> </p>



<p>你可以把它想象成类似人形的更复杂的机器人的简化模型。人形机器人由许多相似的二维或三维关节组成。</p>



<h2 id="h-training-the-sac-agent">培训SAC代理</h2>



<p>在回购中，SAC代理代码是<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/tree/main/spinup_bis/algos/tf2/sac" target="_blank" rel="noreferrer noopener nofollow">这里是</a>。core.py文件包括演员-评论家模型的工厂方法和其他实用程序。sac.py包括重放缓冲区定义和上述训练算法的实现。我建议您浏览一下，并尝试将上面的伪代码行映射到该文件中的实际实现。然后，检查我的列表:</p>



<ul><li>伪代码的第1-2行的初始化在sac.py的第<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L159" target="_blank" rel="noreferrer noopener nofollow"> 159行</a>–<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L179" target="_blank" rel="noreferrer noopener nofollow">179行</a>中实现，</li><li>伪代码的第3行的主循环在sac.py中的第<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L264" target="_blank" rel="noreferrer noopener nofollow"> 264 </a>行实现，</li><li>伪代码第4-8行的数据收集在sac.py中的第<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L270" target="_blank" rel="noreferrer noopener nofollow">270</a>–<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L295" target="_blank" rel="noreferrer noopener nofollow">295</a>行实现。</li><li>伪代码第9-11行的更新处理在sac.py的第<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L298" target="_blank" rel="noreferrer noopener nofollow"> 298行</a>–<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L300" target="_blank" rel="noreferrer noopener nofollow">300行</a>中实现，</li><li>伪代码第12-15行的参数更新在sac.py的第<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L301" target="_blank" rel="noreferrer noopener nofollow"> 301 </a>行调用，在第<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L192" target="_blank" rel="noreferrer noopener nofollow">192</a>–<a href="https://web.archive.org/web/20221206055216/https://github.com/awarelab/spinningup_tf2/blob/f6fd8f679d76d43d44a2388549eb2c123cf0e743/spinup_bis/algos/tf2/sac/sac.py#L240" target="_blank" rel="noreferrer noopener">2</a>T6】4T8】0行实现。</li><li>sac.py中的其余代码主要是日志处理和一些样板代码。</li></ul>



<p>Pendulum-v0环境中的示例训练是在repo根中的run_example.py中实现的。简单地像这样运行它:python run_example.py。在训练之后——或者在200，00 0个环境步骤之后——训练将自动完成并在。/out/checkpoint目录。</p>



<p>下面是培训开始和结束时的日志示例。请注意AverageTestEpReturn是如何变小的——从一个巨大的负数变到接近零的值，这是最大回报。回报是负的，因为代理人因为钟摆不在目标位置而受到惩罚:垂直，零角速度和零力矩。</p>



<p>在我配有英特尔i5处理器的MacBook上，培训耗时482秒(约8分钟)。</p>



<h3>训练前</h3>



<pre class="hljs">---------------------------------------
|      AverageEpRet |       <span class="hljs-number">-1.48e+03</span> |
|          StdEpRet |             <span class="hljs-number">334</span> |
|          MaxEpRet |            <span class="hljs-number">-973</span> |
|          MinEpRet |       <span class="hljs-number">-1.89e+03</span> |
|  AverageTestEpRet |        <span class="hljs-number">-1.8e+03</span> |
|      StdTestEpRet |             <span class="hljs-number">175</span> |
|      MaxTestEpRet |       <span class="hljs-number">-1.48e+03</span> |
|      MinTestEpRet |       <span class="hljs-number">-1.94e+03</span> |
|             EpLen |             <span class="hljs-number">200</span> |
|         TestEpLen |             <span class="hljs-number">200</span> |
| TotalEnvInteracts |           <span class="hljs-number">2e+03</span> |
|     AverageQ1Vals |       <span class="hljs-number">-4.46e+03</span> |
|         StdQ1Vals |         <span class="hljs-number">7.1e+04</span> |
|         MaxQ1Vals |           <span class="hljs-number">0.744</span> |
|         MinQ1Vals |           <span class="hljs-number">-63.3</span> |
|     AverageQ2Vals |       <span class="hljs-number">-4.46e+03</span> |
|         StdQ2Vals |        <span class="hljs-number">7.11e+04</span> |
|         MaxQ2Vals |            <span class="hljs-number">0.74</span> |
|         MinQ2Vals |           <span class="hljs-number">-63.5</span> |
|      AverageLogPi |           <span class="hljs-number">-35.2</span> |
|          StdLogPi |             <span class="hljs-number">562</span> |
|          MaxLogPi |            <span class="hljs-number">3.03</span> |
|          MinLogPi |           <span class="hljs-number">-8.33</span> |
|            LossPi |            <span class="hljs-number">17.4</span> |
|            LossQ1 |            <span class="hljs-number">2.71</span> |
|            LossQ2 |            <span class="hljs-number">2.13</span> |
|    StepsPerSecond |        <span class="hljs-number">4.98e+03</span> |
|              Time |             <span class="hljs-number">3.8</span> |
---------------------------------------
</pre>



<h3>训练后</h3>



<pre class="hljs">---------------------------------------
|      AverageEpRet |            <span class="hljs-number">-176</span> |
|          StdEpRet |            <span class="hljs-number">73.8</span> |
|          MaxEpRet |           <span class="hljs-number">-9.95</span> |
|          MinEpRet |            <span class="hljs-number">-250</span> |
|  AverageTestEpRet |            <span class="hljs-number">-203</span> |
|      StdTestEpRet |            <span class="hljs-number">55.3</span> |
|      MaxTestEpRet |            <span class="hljs-number">-129</span> |
|      MinTestEpRet |            <span class="hljs-number">-260</span> |
|             EpLen |             <span class="hljs-number">200</span> |
|         TestEpLen |             <span class="hljs-number">200</span> |
| TotalEnvInteracts |           <span class="hljs-number">2e+05</span> |
|     AverageQ1Vals |       <span class="hljs-number">-1.56e+04</span> |
|         StdQ1Vals |        <span class="hljs-number">2.48e+05</span> |
|         MaxQ1Vals |           <span class="hljs-number">-41.8</span> |
|         MinQ1Vals |            <span class="hljs-number">-367</span> |
|     AverageQ2Vals |       <span class="hljs-number">-1.56e+04</span> |
|         StdQ2Vals |        <span class="hljs-number">2.48e+05</span> |
|         MaxQ2Vals |           <span class="hljs-number">-42.9</span> |
|         MinQ2Vals |            <span class="hljs-number">-380</span> |
|      AverageLogPi |             <span class="hljs-number">475</span> |
|          StdLogPi |        <span class="hljs-number">7.57e+03</span> |
|          MaxLogPi |            <span class="hljs-number">7.26</span> |
|          MinLogPi |           <span class="hljs-number">-10.6</span> |
|            LossPi |            <span class="hljs-number">61.6</span> |
|            LossQ1 |            <span class="hljs-number">2.01</span> |
|            LossQ2 |            <span class="hljs-number">1.27</span> |
|    StepsPerSecond |        <span class="hljs-number">2.11e+03</span> |
|              Time |             <span class="hljs-number">482</span> |
---------------------------------------
</pre>



<h2 id="h-visualizing-the-trained-policy">可视化经过培训的策略</h2>



<p>现在，保存了训练好的模型，我们可以运行它，看看效果如何！运行此脚本:</p>



<pre class="hljs">python run_policy.py --model_path ./out/checkpoint --env_name Pendulum-v0</pre>



<p>在回购根中。你会看到你的经纪人接连播放10集！是不是很酷？你的代理人受过训练能完美地垂直调整钟摆吗？我的不是。你可以试着玩玩run_example.py文件中的超参数(代理的函数参数)，让代理找到更好的策略。小提示:我发现早点结束训练可能会有帮助。所有超参数都在sac.py文件中的SAC的docstring中定义。</p>



<p>你可能会奇怪，为什么每一集都不一样？这是因为每次环境重置和新一集开始时，初始条件(钟摆起始角度和速度)都是随机的。</p>



<h2 id="h-conclusions">结论</h2>



<p>你的下一步是在一些更复杂的环境中训练SAC，如人形或MuJoCo套件中的任何其他环境。<a href="/web/20221206055216/https://neptune.ai/blog/installing-mujoco-to-work-with-openai-gym-environments" target="_blank" rel="noreferrer noopener"> <em>安装MuJoCo与OpenAI健身房环境一起工作</em> </a>是我写的关于如何安装MuJoCo并访问这些复杂环境的指南。它还描述了要跟踪的有用诊断。你可以在强化学习框架中的<a href="/web/20221206055216/https://neptune.ai/blog/logging-in-reinforcement-learning-frameworks" target="_blank" rel="noreferrer noopener"> <em>日志中了解更多关于记录这些诊断的信息——你需要知道的</em> </a>。也有其他实现算法的框架可以解决连续控制任务。在这篇文章中阅读它们:<a href="/web/20221206055216/https://neptune.ai/blog/best-benchmarks-for-reinforcement-learning" target="_blank" rel="noreferrer noopener"> <em>强化学习的最佳基准:终极清单</em> </a>。感谢您的参与，下次再见！</p>
        </div>
        
    </div>    
</body>
</html>