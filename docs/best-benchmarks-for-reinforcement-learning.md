# 强化学习的最佳基准:终极清单

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/best-benchmarks-for-reinforcement-learning>

在这篇文章中，我将与你分享我的支持训练强化学习(RL)代理的环境库。RL 研究的基础，甚至玩 RL 或者学习 RL 的基础，都是环境。它是你运行你的算法来评估它有多好的地方。我们将探索 23 种不同的基准，因此我保证您会发现一些有趣的东西！

但首先，我们将做一个简短的介绍，如果你刚刚开始学习 RL，你应该寻找什么。不管你目前的知识水平如何，我建议你浏览一下整个列表。我希望它能激励你继续做好工作，并激励你以不同于标准基准的方式开始自己的项目！

## 经验法则

如果你对专门研究**离散动作空间** (PPO、DQN、彩虹……)的算法感兴趣，例如，动作输入可以是雅达利 2600 游戏控制器上的按钮，那么你应该看看[开放 AI 健身房](https://web.archive.org/web/20221206030210/https://gym.openai.com/)中的雅达利环境。这些包括 Pong，Breakout，Space Invaders，Seaquest 等等。

另一方面，如果你对专门研究**连续动作空间** (DDPG、TD3、SAC、…)的算法更感兴趣，比如说，动作输入是学习走路的人形机器人关节上的扭矩，那么你应该看看 [OpenAI Gym](https://web.archive.org/web/20221206030210/https://gym.openai.com/) 和 [DeepMind Control Suite](https://web.archive.org/web/20221206030210/https://github.com/deepmind/dm_control) 中的 MuJoCo 环境。PyBullet Gymperium 是一种免费的替代品。更艰苦的环境包括机器人在[开放人工智能健身房](https://web.archive.org/web/20221206030210/https://gym.openai.com/)。

如果你还不知道你感兴趣的是什么，那么我建议你在 OpenAI 健身房里玩玩[经典控制](https://web.archive.org/web/20221206030210/https://gym.openai.com/envs/#classic_control)环境，并阅读《T2》深度学习。

介绍够了，来看看基准吧！

## 基准

这一部分的第一部分只是一个列表，按字母顺序列出了所有 23 个基准。接下来，我添加了每个基准测试创建者的一些描述，向您展示它的用途。

### RL 基准列表

## A

体验假说是这样一种观点，即“智能出现在主体与环境的相互作用中，并且是感觉运动活动的结果”。 *Habitat* 是一个用于研究嵌入式人工智能的模拟平台。
想象一下，走到一个家用机器人面前，问“嘿，机器人，你能去看看我的笔记本电脑在不在我的桌子上吗？如果有的话，拿给我”。或者问一个以自我为中心的人工智能助理(坐在你的智能眼镜上):“嘿——我最后一次看到我的钥匙是在哪里？”。AI Habitat 能够在将学到的技能转移到现实之前，在高度逼真的&高效 3D 模拟器中训练这种具体化的 AI 智能体(虚拟机器人和以自我为中心的助手)。

如果你学习具有物理或虚拟化身的智能系统，这将是最适合你的。

## B

bsuite 是一组精心设计的实验，调查强化学习(RL)代理的核心能力，有两个主要目标。

*   收集清晰的、信息丰富的、可扩展的问题，这些问题抓住了设计有效的、通用的学习算法中的关键问题。
*   通过代理在这些共享基准上的表现来研究他们的行为。

这个库自动评估和分析这些基准上的任何代理。它有助于对 RL 中的核心问题进行可重复和可访问的研究，并最终设计出优秀的学习算法。

## D

*dm_control* 软件包是一个 **Python 库和任务套件**的集合，用于关节式身体模拟中的强化学习代理。MuJoCo 包装器提供了方便的函数和数据结构绑定，可以创建自己的任务。

此外，控制套件是一组具有标准化结构的固定任务，旨在作为性能基准。它包括像 HalfCheetah，Humanoid，Hopper，Walker，Graber 等经典任务(见图)。运动框架提供了运动任务(如足球)的高级抽象和示例。还包括一组可配置的操作任务，包括机械臂和拼接砖。

该软件包的介绍性教程可通过[协作笔记本](https://web.archive.org/web/20221206030210/https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb)获得。

*DeepMind Lab* 是通过 ioquake3 和其他开源软件，基于 Quake III Arena 的 3D 学习环境。DeepMind 实验室为学习代理提供了一套具有挑战性的 3D 导航和解谜任务。它的主要目的是作为人工智能研究的试验台，代理人必须根据视觉观察采取行动。

DeepMind 记忆任务套件是一组 13 个不同的机器学习任务，需要记忆来解决。构造它们是为了让我们评估特定于内存的维持集的泛化性能。

*Psychlab* 是 DeepMind 实验室第一人称 3D 游戏世界内部的模拟心理学实验室。Psychlab 实现了经典的实验室心理学实验，因此它们可以与人类和人工代理一起工作。Psychlab 有一个简单灵活的 API，使用户能够轻松地创建自己的任务。例如，Psychlab 包括几个经典的实验范例，包括视觉搜索、变化检测、随机点运动辨别和多目标跟踪。

## G

Google Research Football 是一个新颖的 RL 环境，代理人的目标是掌握世界上最受欢迎的运动——足球！模仿流行的足球视频游戏，足球环境提供了一个高效的基于物理的 3D 足球模拟，其中代理控制他们队中的一个或所有足球运动员，学习如何在他们之间传球，并设法克服对手的防守以得分。足球环境提供了一组要求严格的研究问题，称为足球基准，以及足球学院，一组越来越难的 RL 场景。
它非常适合多智能体和多任务研究。它还允许您创建自己的学院场景，以及使用模拟器，基于包括的例子全新的任务。

## M

元强化学习算法可以使机器人通过利用先前的经验来学习如何学习，从而更快地获得新技能。 *Meta-World* 是一个开源的模拟基准，用于元强化学习和多任务学习，由 50 个不同的机器人操纵任务组成。作者旨在提供足够广泛的任务分布，以评估 meta-RL 算法对新行为的推广能力。

MineRL 是一个始于卡耐基梅隆大学的研究项目，旨在开发《我的世界》人工智能的各个方面。简而言之，MineRL 由两个主要部分组成:

*   [MineRL-v0 数据集](https://web.archive.org/web/20221206030210/https://minerl.io/dataset/)–最大的模仿学习数据集之一，拥有超过 6000 万帧记录的人类球员数据。该数据集包括一组环境，这些环境突出了现代强化学习中的许多最困难的问题:稀疏奖励和分层政策。
*   用于在《我的世界》进行人工智能研究的丰富的 python3 包。这包括两个主要的子模块:*MineRL . env*——《我的世界》的一组不断增长的 OpenAI 健身房环境和*MineRL . data*——用于试验 MineRL-v0 数据集的主要 python 模块。

![RL benchmarks - Multiagent](img/d3c7c743fca8b64d8b6e799641942589.png)

多代理自动课程中[紧急工具使用的环境生成代码。这是一篇有趣的论文，我强烈推荐你阅读。作者观察到代理人在玩简单的捉迷藏游戏时逐渐发现更复杂的工具使用。通过在模拟的捉迷藏环境中的训练，代理人建立了一系列六种不同的策略和反策略。这种简单环境中自我监督的涌现复杂性进一步表明，多主体协同适应可能有一天会产生极其复杂和智能的行为。](https://web.archive.org/web/20221206030210/https://arxiv.org/abs/1909.07528)

它使用“[world gen:Randomized MuJoCo environments](https://web.archive.org/web/20221206030210/https://github.com/openai/mujoco-worldgen)”允许用户生成复杂的、高度随机化的环境。如果你想创造自己的环境，你也应该尝试一下！

## O

Gym 除了是最广为人知的基准之外，还是一个开发和比较强化学习算法的神奇工具包。它支持教导代理从行走模拟人形机器人(需要 MuJoCo，免费选择见 [PyBullet Gymperium](https://web.archive.org/web/20221206030210/https://github.com/benelot/pybullet-gym) )到玩像 Pong 或 Pinball 这样的 Atari 游戏。我个人在研究中用得最多。它非常容易使用，而且是当今的标准配置。你应该好好了解一下。

*健身房复古*可以认为是 OpenAI 健身房的延伸。它可以让你将经典的视频游戏转化为开放式人工智能健身房环境，用于强化学习，并集成了约 1000 个游戏。它使用各种支持 Libretro API 的模拟器，使得添加新的模拟器相当容易。

OpenSpiel 是一个环境和算法的集合，用于游戏中一般强化学习和搜索/规划的研究。OpenSpiel 支持 n 人(单个和多个代理)零和、合作和一般和、一次性和连续、严格轮流和同时移动、完美和不完美信息游戏，以及传统的多代理环境，如(部分和完全可观察的)网格世界和社会困境。OpenSpiel 还包括分析学习动力和其他常见评估指标的工具。游戏被表示为程序性的扩展形式的游戏，带有一些自然的扩展。为了提高效率，核心的 API 和游戏都是用 C++实现的，为了便于使用，它们都暴露在 Python 中。

## P

*Procgen Benchmark* 由 16 个独特的环境组成，旨在测量强化学习中的样本效率和泛化能力。这个基准是评估泛化的理想选择，因为在每个环境中都可以生成不同的训练集和测试集。该基准也非常适合于评估样本效率，因为所有环境都给 RL 代理带来了各种各样令人信服的挑战。环境的内在多样性要求代理学习健壮的策略；过度适应状态空间中的狭窄区域是不够的。换句话说，当代理人面对不断变化的水平时，概括的能力成为成功的一个不可或缺的组成部分。

PyBullet Gymperium 是 OpenAI Gym MuJoCo 环境等的开源实现。这些都是具有挑战性的连续控制环境，比如训练人形机器人行走。它的酷之处在于，它不需要用户安装 MuJoCo，这是一个需要付费许可证才能运行 30 天以上的商业物理引擎。

## 稀有

[现实世界强化学习的挑战](https://web.archive.org/web/20221206030210/https://arxiv.org/abs/1904.12901)论文确定并描述了一组九个挑战，这些挑战目前阻碍了强化学习(RL)代理在现实世界应用和产品中的应用。它还描述了一个评估框架和一组环境，可以评估 RL 算法对现实系统的潜在适用性。从那时起，[对现实世界强化学习的挑战进行了实证研究](https://web.archive.org/web/20221206030210/https://arxiv.org/pdf/2003.11881.pdf)论文，实现了九个描述的挑战中的八个，并分析了它们对各种最先进的 RL 算法的影响。
这是用于执行该分析的代码库，也是用于围绕这些挑战进行可重复实验的通用平台。它被称为 *realworldrl-suite* (真实世界强化学习(RWRL)套件)。

*RLCard* 是一个卡牌游戏中强化学习(RL)的工具包。它通过易于使用的界面支持多卡环境。游戏包括 21 点，UNO，限制德州扑克，等等！它还允许您创建自己的环境。RLCard 的目标是在强化学习和不完美信息博弈之间架起一座桥梁。

*RL 不插电*是一套离线强化学习的基准测试。RL Unplugged 旨在促进易用性，它为数据集提供了统一的 API，一旦建立了通用管道，从业者就可以轻松地处理套件中的所有数据。它包括最常见基准的数据集:雅达利、DeepMind 移动、DeepMind 控制套件、Realworld RL、DeepMind 实验室和 bsuite。

## S

Screeps 是一款大型多人在线即时战略游戏(phwee，很多)。每个玩家都可以在一个所有玩家共享的世界中创建自己的殖民地。这样的殖民地可以开采资源，建造单位，征服领土。随着你征服更多的领域，你在游戏世界中的影响力也在增长，你扩大自己影响的能力也在增长。然而，这需要你付出很多努力，因为多个玩家可能会瞄准同一个领域。最重要的是，你建立了一个能做所有这些的人工智能！

Screeps 是为有编程技能的人开发的。与其他 RTS 游戏不同，你在 Screeps 中的单位可以在没有你参与的情况下对事件做出反应——前提是你已经对它们进行了适当的编程。

毒蛇。AI 是一个简单而强大的新颖框架，帮助开发者创建游戏代理。用熟悉的 Python 代码，将你拥有的任何视频游戏变成一个成熟的沙盒环境进行实验。比如 GTA 里的这个[自动驾驶智能体。该框架首先为机器学习& AI 研究提供了一个有价值的工具。作为一个业余爱好者来说，使用它也是非常有趣的(而且很容易上瘾)！](https://web.archive.org/web/20221206030210/https://www.youtube.com/watch?v=rvnHikUJ9T0)

*PySC2* 为 RL 代理提供了与星际争霸 2 交互的接口，获取观察和发送动作。它将暴雪娱乐的星际争霸 2 机器学习 API 暴露为 Python RL 环境。这是 DeepMind 和暴雪的合作，将星际争霸 2 开发成一个丰富的 RL 研究环境。 *PySC2* 有很多预配置的小游戏地图，用于对 RL 代理进行基准测试。

## T

这是一个开源项目，使游戏和模拟成为训练智能代理的环境。Unity 提供了最先进算法的实现(基于 PyTorch ),使游戏开发者和爱好者能够轻松训练 2D、3D 和 VR/AR 游戏的智能代理。然而，研究人员可以使用提供的简单易用的 Python API，通过强化学习、模仿学习、神经进化或任何其他方法来训练代理！例如，参见[马拉松环境](https://web.archive.org/web/20221206030210/https://github.com/Unity-Technologies/marathon-envs)。

## W

这是官方的 Python 实现 [WordCraft:一个基准测试常识代理](https://web.archive.org/web/20221206030210/https://larel-ws.github.io/assets/pdfs/wordcraft_an_environment_for_benchmarking_commonsense_agents.pdf)的环境。快速解决各种现实任务的能力需要对世界有常识性的理解。为了更好地利用常识对代理进行研究，你应该试试 WordCraft，一个基于[小炼金术 2](https://web.archive.org/web/20221206030210/https://littlealchemy2.com/) 的 RL 环境。小炼金术 2 是一个有趣和令人上瘾的游戏，它允许玩家组合元素来创造更多的元素。这种轻量级环境运行速度很快，并且建立在受现实世界语义启发的实体和关系之上。

## 结论

我们的 RL 基准测试列表到此结束。我真的不能告诉你应该选哪一个。对于一些人来说，在“经验法则”一节中描述的更经典的基准，如 OpenAI Gym 或 DM Control Suite，将是最合适的。对于其他人来说，这是不够的，他们可能会想投入一些不那么累的东西，如 Unity ML-agents 或 Screeps。

就我个人而言，有一次我和 GRF 一起工作，看到我的经纪人如何学习踢足球和进球很有趣。目前，我正在进行一些更基础的研究，并使用公认的 OpenAI Gym MuJoCo 环境测试我的代理，这在其他方面很有趣，比如看到我的方法真的有效。

无论你的选择是什么，我希望这个列表能帮助你让你的 RL 研究更令人兴奋！