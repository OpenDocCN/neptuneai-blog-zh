# 你需要知道的最好的 8 个机器学习模型部署工具

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/best-8-machine-learning-model-deployment-tools>

机器学习在科技界并不新鲜。它为许多行业带来了革命性的变化，能够实现渠道自动化，并增加业务工作流程的灵活性。

我们如何在生产环境中创建和部署训练有素的模型 API 由机器学习生命周期的许多方面控制。 [MLOps](/web/20220926090659/https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective) 的概念对于处理复杂的 ML 部署环境非常有益。

实施可靠的 MLOps 可以为投资机器学习的公司带来巨大的好处。理解使用和执行什么是这个难题的一个重要部分。学习和适应简化整体工作流程的新工具是另外一回事。

本文列出了用于模型部署的最佳 MLOps 工具。帮助您扩展和管理机器学习生命周期的所有要素，包括服务、监控和管理 API 端点。

如果您想将训练好的模型部署为端点，您可以使用 TensorFlow 服务来实现。

它允许您创建一个 REST API 端点来服务于训练好的模型。TensorFlow Serving 是一个健壮的高性能系统，用于服务机器学习模型。

您可以轻松部署最先进的机器学习算法，同时维护与其各自端点相同的服务器架构。它足够强大，可以为不同类型的模型和数据以及 TensorFlow 模型提供服务。

它是由谷歌创建的，许多顶级公司都使用它。将模型作为集中的模型库是一个很好的方法。服务架构对于大量用户同时访问模型是足够有效的。

如果由于大量请求而出现任何阻塞，可以使用负载平衡器轻松维护。总体而言，该系统具有较高的可扩展性和可维护性。

**TensorFlow 发球优势:**

*   一旦部署模型准备就绪，这个工具就可以轻松地提供服务。
*   它可以向同一个模型发起批量请求，因此可以有效地利用硬件。
*   它还提供了模型版本管理。
*   该工具易于使用，并负责建模和服务管理。

**TensorFlow 发球缺点:**

*   加载新型号或更新旧型号时，无法确保零停机时间。
*   仅适用于张量流模型。

如果您正在寻找一个开源工具来组织您的整个 ML 生命周期，这可能是您的平台。

MLflow 提供管理 ML 流程和部署的解决方案。它可以进行实验、复制、部署，或者作为一个中央模型注册中心。

该平台可以被个人开发者和团队用于 ML 部署。它可以整合到任何编程生态系统中。该库旨在满足各种技术需求，并可用于不同的机器学习库。

组织整个 ML 生命周期围绕着四个主要功能:跟踪、项目、模型和模型注册。

它有助于简化自动化 ML 模型跟踪的过程。但是一个缺点是它不能自动处理模型定义。这意味着向模型定义添加额外的工作需要手动完成。

**MLflow 优点:**

*   模型跟踪机制很容易建立。
*   它为服务提供了非常直观的 API。
*   日志记录实用且简单，因此很容易进行实验。
*   代码优先的方法。

**MLflow 缺点:**

*   向模型添加额外的工作并不是自动的。
*   对于将模型部署到不同的平台来说，这并不容易，也不理想。

Kubeflow 的主要目标是维护机器学习系统。这是为 Kubernetes 设计的强大套件。

主要操作包括打包，组织 docker 容器，帮助维护整个机器学习系统。

它简化了机器学习工作流的开发和部署，从而使模型具有可追溯性。它提供了一套强大的 ML 工具和架构框架来有效地执行各种 ML 任务。

多功能 UI 仪表板使管理和跟踪实验、任务和部署运行变得容易。笔记本功能使我们能够使用指定的平台开发套件与 ML 系统进行交互。

组件和管道是模块化的，可以重复使用以提供快速解决方案。这个平台是 Google 通过 Kubernetes 为 TensorFlow 任务服务而启动的。后来，它扩展到执行整个 ML 管道的多云、多架构框架。

**Kubeflow 优点:**

*   一致的基础架构，提供监控、运行状况检查、每次复制，以及对新功能的扩展。
*   简化新团队成员的入职流程。
*   标准化流程有助于建立安全性并更好地控制基础架构。

Kubeflow cons :

*   难以手动设置和配置。
*   高可用性不是自动的，需要手动配置。
*   这个工具的学习曲线很陡。

Cortex 是一个开源的多框架工具，非常灵活，可以用作模型服务工具，也可以用于模型监控等目的。

凭借其处理不同机器学习工作流的能力，它允许您完全控制模型管理操作。它还可以作为使用 SageMaker 工具提供模型的替代方案，以及基于 AWS 服务(如 Elastic Kubernetes Service (EKS)、Lambda 或 Fargate)的模型部署平台。

Cortex 扩展到 Docker、Kubernetes、TensorFlow Serving 和 TorchServe 等开源项目。它可以与 cohesion 中的任何 ML 库或工具一起工作。它提供端点的可扩展性来管理负载。

它允许您在单个 API 端点中部署多个模型。它还可以作为一种解决方案，在不停止服务器的情况下更新已经存在的生产端点。它覆盖了模型监控工具的足迹，监督端点的性能以及预测数据。

**皮质优点**:

*   当网络流量波动时，允许 API 安全的自动扩展功能。
*   支持多个平台，如 Keras、TensorFlow、Scikit-learn、PyTorch 等..
*   更新型号时不会停机。

**皮层 cons** :

*   设置过程可能有些令人望而生畏。

Seldon.io 提供了 Seldon core，这是一个开源的框架。该框架简化并加速了 ML 模型和实验部署。

它处理和服务在任何其他开源 ML 框架中构建的模型。ML 模型部署在 Kubernetes。当它与 Kubernetes 一起伸缩时，它使我们能够使用最先进的 Kubernetes 特性，比如定制资源定义来处理模型图。

Seldon 还提供了将您的项目与持续集成和部署(CI/CD)工具相连接的能力，以扩展和更新模型部署。

它有一个警报系统，当监控生产中的模型出现问题时，它会通知您。您可以定义模型来解释某些预测。该工具在云中可用，在内部也可用。

**谢顿优点**:

*   定制离线模型。
*   向外部客户公开 API 的实时预测。
*   简化部署过程。

**谢顿缺点:**

*   设置可能有点复杂。
*   对新来的人来说，学习可能很难。

BentoML 简化了构建机器学习服务的过程。它为部署和维护生产级 API 提供了一个基于 Python 的标准架构。这种架构允许用户使用任何 ML 框架轻松打包训练好的模型，用于在线和离线模型服务。

BentoML 的高性能模型服务器支持自适应微批处理，以及独立于业务逻辑扩展模型推理工人的能力。UI 仪表板提供了一个集中的系统来组织模型和监控部署过程。

其模块化设计使配置可在现有 GitOps 工作流中重复使用，自动 docker 映像生成使生产部署成为一个简单的版本化流程。

多用途框架解决了 ML 模型的服务、组织和部署。主要重点是连接数据科学和 DevOps 部门，以实现更高效的工作环境，并产生高性能的可扩展 API 端点。

欢迎使用:

*   易于大规模部署预测服务的实用格式
*   以单一统一的格式支持高性能模型服务和部署
*   支持部署到多个平台，而不仅仅是 Kubernetes

膨润土辅料:

*   不注重实验管理。
*   不支持开箱即用的水平扩展。

AWS Sagemaker 是亚马逊提供的强大服务。它让 ML 开发者能够快速构建、训练和部署机器学习模型。

它通过删除一些复杂的步骤来简化整个机器学习过程，从而提供高度可扩展的 ML 模型。

机器学习开发生命周期是一个复杂的迭代过程。它迫使你整合复杂的工具和工作流程。这项任务可能要求很高，令人恼火，而且可能会消耗你大量的时间。更不用说配置时出错的麻烦了。

Sagemaker 使这一过程变得更加容易，在一个集中的工具集中提供了用于机器学习的所有组件。没有必要配置每一个，因为它已经安装并准备使用。

这以最小的努力和成本加速了模型的生产和部署。该工具可用于使用任何 ML 框架创建的端点。它还提供预测跟踪和捕获，以及日程监控。

**AWS Sagemaker 优点**:

*   设置过程很简单，可以用 Jupyter 笔记本运行。因此，脚本的管理和部署得到了简化。
*   基于您使用的功能，成本是模块化的。
*   模型训练在多台服务器上完成。

AWS Sagemaker 缺点:

*   初级开发人员的陡峭学习曲线。
*   严格的工作流程很难定制。
*   仅适用于 AWS ecostystem

Torchserve 是一个 Pytorch 模型服务框架。它简化了训练有素的 PyTorch 模型的大规模部署。它消除了为模型部署编写定制代码的需要。

Torchserve 由 AWS 设计，是 PyTorch 项目的一部分。对于那些使用 PyTorch 环境构建模型的人来说，这使得设置变得很容易。

它支持低延迟的轻量级服务。部署的模型具有高性能和广泛的可伸缩性。

Torchserve 为一些 ML 任务提供了内置库，比如对象检测或文本分类。它可以节省你花在编写代码上的时间。它提供了强大的特性，比如多模型服务、用于 A/B 测试的模型版本控制、用于监控的指标以及用于应用程序集成的 RESTful 端点。

火炬服务的优点:

*   扩展部署的模型得到了简化。
*   服务端点是轻量级的，具有高性能规模。

火炬服务的缺点:

*   因为这个工具是实验性的，所以经常会发生变化和更新。
*   仅适用于 PyTorch 型号

## 结论

创建和部署高性能和可扩展的机器学习模型是具有挑战性的任务。

幸运的是，本文中列出的部署工具和框架可以帮助您创建健壮的 ML 模型，并快速、轻松地部署它们。

处理和组织全面的机器学习生命周期绝非易事。这些工具将帮助你节省时间和精力。

祝你好运！

### 资源:

[ML 生产基础设施工具 Aparna Dhinakaran](https://web.archive.org/web/20220926090659/https://towardsdatascience.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving-fcfc75c4a362)

### 克里斯萨纳瓦特·卡韦桑穆昂

InstaMobile 的开发者关系
只是一个典型的内向开发者，美国——没有上瘾，对人工智能和区块链充满热情。

* * *

**阅读下一篇**

## 你需要知道的最好的机器学习模型管理工具

9 分钟阅读|作者弗拉基米尔·利亚申科| 2021 年 7 月 14 日更新

开发您的模型是 ML 项目工作的重要部分。这通常是一个艰难的挑战。

每个数据科学家都必须面对它，还有困难，比如失去实验的线索。这些困难很可能既烦人又不明显，会让你时不时感到困惑。

这就是为什么简化管理 ML 模型的过程是有好处的，幸运的是有几个工具可以做到这一点。这些工具有助于:

*   实验跟踪
*   模型版本控制
*   测量推理时间
*   团队协作
*   资源监控

因此，寻找和使用适合您的项目的工具是常识和良好的实践。

在本文中，我们将探索模型管理工具的**前景。我将尝试向您展示各种工具，并强调它们的优点。**

我们将涵盖:

*   选择**模型管理工具**的标准
*   **模型管理工具** : **Neptune、亚马逊 SageMaker、Azure 机器学习、Domino 数据科学平台、Google Cloud AI 平台、Metaflow、MLflow**

[Continue reading ->](/web/20220926090659/https://neptune.ai/blog/best-machine-learning-model-management-tools)

* * *