<html>
<head>
<title>Dimensionality Reduction for Machine Learning </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>机器学习的降维</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/dimensionality-reduction#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/dimensionality-reduction#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>数据构成了任何机器学习算法的基础，没有它，数据科学就不可能发生。有时，它可能包含大量的特性，有些甚至不是必需的。这种冗余信息使得建模变得复杂。此外，由于高维度，通过可视化来解释和理解数据变得困难。这就是降维发挥作用的地方。</p>



<p>在本文中，您将了解到:</p>



<ol><li>什么是降维？</li><li>什么是维度的诅咒？</li><li>用于降维的工具和库</li><li>用于降维的算法</li><li>应用程序</li><li>优点和缺点</li></ol>



<h2 id="h-what-is-dimensionality-reduction">什么是降维？</h2>



<p>降维是减少数据集中要素数量的任务。在像<a href="/web/20221219034305/https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why" target="_blank" rel="noreferrer noopener">回归</a>或<a href="/web/20221219034305/https://neptune.ai/blog/image-classification-tips-and-tricks-from-13-kaggle-competitions" target="_blank" rel="noreferrer noopener">分类</a>这样的机器学习任务中，经常有太多的变量需要处理。这些变量也被称为<strong>特性</strong>。特征数量越多，建模就越困难，这就是所谓的<strong>维度诅咒</strong>。这将在下一节详细讨论。</p>



<p>此外，这些特征中的一些可能非常多余，会向数据集添加噪声，并且在训练数据中包含它们是没有意义的。这就是需要减少特征空间的地方。</p>



<p>降维的过程实质上是将数据从高维特征空间转换到低维特征空间。同时，在转换过程中不丢失数据中存在的有意义的属性也很重要。</p>



<p>降维通常用于数据可视化以理解和解释数据，以及用于机器学习或深度学习技术以简化手头的任务。</p>



<h3>维度的诅咒</h3>



<p>众所周知，ML/DL算法需要大量的数据来学习不变性、模式和表示。如果该数据包含大量特征，这可能会导致维数灾难。维数灾难首先由<a href="https://web.archive.org/web/20221219034305/https://zbmath.org/0103.12901" target="_blank" rel="noreferrer noopener nofollow"> Bellman </a>提出，描述了为了以一定的精度估计任意函数，估计所需的特征或维数呈指数增长。对于产生更多<strong>稀疏性</strong>的大数据来说尤其如此。</p>



<p>数据的稀疏性通常指的是具有零值的特征；这并不意味着缺少值。如果数据具有大量稀疏特征，则空间和计算复杂性会增加。Oliver<a href="https://web.archive.org/web/20221219034305/https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1421" target="_blank" rel="noreferrer noopener nofollow">Kuss【2002】</a>表明在稀疏数据上训练的模型在测试数据集中表现不佳。换句话说，训练期间的模型学习噪声，并且它们不能很好地概括。因此他们过度适应。</p>



<p>当数据稀疏时，训练数据集中的观察值或样本很难聚类，因为高维数据会导致数据集中的每个观察值看起来彼此等距。如果数据是有意义的和非冗余的，那么将存在相似数据点聚集在一起的区域，此外，它们必须具有统计显著性。</p>



<p>高维数据带来的问题有:</p>



<ol><li>冒着过度适应机器学习模型的风险。</li><li>相似特征聚类困难。</li><li>增加了空间和计算时间的复杂性。</li></ol>



<p>另一方面，非稀疏数据或密集数据是具有非零特征的数据。除了包含非零特征之外，它们还包含有意义且非冗余的信息。</p>



<p>为了解决维数灾难，使用了降维等方法。降维技术对于将稀疏特征转换为密集特征非常有用。此外，降维还用于清洗数据和特征提取。</p>







<p>最流行的降维库是<strong> scikit-learn </strong> (sklearn)。该库由三个主要的降维算法模块组成:</p>



<ol><li>分解算法<ul><li>主成分分析</li><li>核主成分分析</li><li>非负矩阵分解</li><li>奇异值分解</li></ul></li><li>流形学习算法<ul><li>t分布随机邻居嵌入</li><li>光谱嵌入</li><li>局部线性嵌入</li></ul></li><li>判别分析<ul><li>线性判别分析</li></ul></li></ol>



<p>当谈到深度学习时，可以构建像自动编码器这样的算法来降低维度，并学习特征和表示。Pytorch、Pytorch Lightning、Keras和TensorFlow等框架用于创建自动编码器。</p>







<h2 id="h-algorithms-for-dimensionality-reduction">降维算法</h2>



<p>先说第一类算法。</p>



<h3>分解算法</h3>



<p>scikit-learn中的分解算法涉及维数约减算法。我们可以使用以下命令调用各种技术:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA, KernelPCA, NMF</pre>



<h4>主成分分析</h4>



<p>主成分分析(PCA)是一种降维方法，通过保留在高维输入空间中测量的<strong>方差</strong>来寻找低维空间。这是一种无监督的降维方法。</p>



<p>PCA变换是线性变换。它包括寻找主成分的过程，即把特征矩阵分解成特征向量。这意味着当数据集的分布是非线性时，PCA将是无效的。</p>



<p>让我们用python代码来理解PCA。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pca</span><span class="hljs-params">(X=np.array<span class="hljs-params">([])</span>, no_dims=<span class="hljs-number">50</span>)</span>:</span>

    print(<span class="hljs-string">"Preprocessing the data using PCA..."</span>)
    (n, d) = X.shape
    Mean = np.tile(np.mean(X, <span class="hljs-number">0</span>), (n, <span class="hljs-number">1</span>))
    X = X - Mean
    (l, M) = np.linalg.eig(np.dot(X.T, X))
    Y = np.dot(X, M[:, <span class="hljs-number">0</span>:no_dims])
    <span class="hljs-keyword">return</span> Y</pre>



<p>PCA的实现非常简单。我们可以将整个过程定义为四个步骤:</p>



<ol><li><strong>标准化</strong>:通过取原始数据集与整个数据集的平均值之间的差值，将数据转换到一个通用的标度。这将使分布0居中。</li><li><strong>求协方差</strong>:协方差会帮助我们理解均值和原始数据之间的关系。</li><li><strong>确定主成分</strong>:主成分可以通过计算特征向量和特征值来确定。<strong>特征向量</strong>是一组特殊的向量，帮助我们理解数据的结构和属性，这些数据将成为主要成分。另一方面，<strong>特征值</strong>帮助我们确定主成分。最高的特征值及其对应的特征向量构成了最重要的主分量。</li><li><strong>最终输出</strong>:是标准化矩阵和特征向量的点积。请注意，列或特征的数量将会改变。</li></ol>



<p>减少数据变量的数量不仅降低了复杂性，还降低了机器学习模型的准确性。然而，由于特征数量较少，因此易于探索、可视化和分析，这也使得机器学习算法的计算成本较低。简而言之，主成分分析的思想是减少数据集的变量数量，同时尽可能多地保留信息。</p>



<p>我们也来看看sklearn为PCA提供的模块和函数。</p>



<p>我们可以从加载最多的数据集开始:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_digits
digits = load_digits()
digits.data.shape</pre>



<p>(1797, 64)</p>



<p>数据由8×8像素图像组成，这意味着它们是64维的。为了了解这些点之间的关系，我们可以使用PCA将它们投影到更低的维度，如2-D:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA

pca = PCA(<span class="hljs-number">2</span>)  
projected = pca.fit_transform(digits.data)
print(digits.data.shape)
print(projected.shape)</pre>



<p>(1797, 64)</p>



<p>(1797, 2)</p>



<p>现在，让我们画出前两个主要成分。</p>



<pre class="hljs">plt.scatter(projected[:, <span class="hljs-number">0</span>], projected[:, <span class="hljs-number">1</span>],
            c=digits.target, edgecolor=<span class="hljs-string">'none'</span>, alpha=<span class="hljs-number">0.5</span>,
            cmap=plt.cm.get_cmap(<span class="hljs-string">'spectral'</span>, <span class="hljs-number">10</span>))
plt.xlabel(<span class="hljs-string">'component 1'</span>)
plt.ylabel(<span class="hljs-string">'component 2'</span>)
plt.colorbar();</pre>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" src="../Images/a88ccf09dc4c306696fb5636b373601c.png" alt="Principal Component Analysis (PCA)" class="wp-image-54935" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_5.png?resize=523%2C406&amp;ssl=1"/><figcaption><em>Dimensionality reduction technique: PCA | Source: Author</em></figcaption></figure></div>



<p>我们可以看到，在大多数情况下，PCA最优地找到了可以非常有效地聚类相似分布的主成分。</p>



<h4>内核PCA (KPCA)</h4>



<p>我们之前描述的PCA变换是线性变换，对于非线性分布无效。要处理非线性分布，基本思想是使用核技巧。</p>



<p>内核技巧只是一种将非线性数据投影到更高维度空间并分离不同数据分布的方法。一旦分布被分离，我们可以使用主成分分析来线性分离它们。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/1f6ddbde18f5fac09b7adcf51a7a9e50.png" alt="Kernel PCA" class="wp-image-54929" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_2.png?ssl=1"/><figcaption><em>Dimensionality reduction technique: KPCA | Source: Author</em></figcaption></figure></div>



<p>核PCA使用核函数ϕ来计算非线性映射的数据的点积。换句话说，函数ϕ通过创建原始特征的非线性组合，将原始d维特征映射到更大的k维特征空间。</p>



<p>假设数据集x包含两个特征x1和x2:</p>







<p id="separator-block_617a5ffbf9f91" class="block-separator block-separator--5"> </p>



<p>应用内核技巧后，我们得到:</p>







<p id="separator-block_617a6001f9f92" class="block-separator block-separator--5"> </p>



<p>为了更直观地理解内核PCA，让我们定义一个不能线性分离的特征空间。</p>



<pre class="hljs">​​<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_circles
<span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> KernelPCA
np.random.seed(<span class="hljs-number">0</span>)
X, y = make_circles(n_samples=<span class="hljs-number">400</span>, factor=<span class="hljs-number">.3</span>, noise=<span class="hljs-number">.05</span>)</pre>



<p>现在，让我们绘制并查看我们的数据集。</p>



<pre class="hljs">plt.figure(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">10</span>))
plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, aspect=<span class="hljs-string">'equal'</span>)
plt.title(<span class="hljs-string">"Original space"</span>)
reds = y == <span class="hljs-number">0</span>
blues = y == <span class="hljs-number">1</span>

plt.scatter(X[reds, <span class="hljs-number">0</span>], X[reds, <span class="hljs-number">1</span>], c=<span class="hljs-string">"red"</span>,
           s=<span class="hljs-number">20</span>, edgecolor=<span class="hljs-string">'k'</span>)
plt.scatter(X[blues, <span class="hljs-number">0</span>], X[blues, <span class="hljs-number">1</span>], c=<span class="hljs-string">"blue"</span>,
           s=<span class="hljs-number">20</span>, edgecolor=<span class="hljs-string">'k'</span>)
plt.xlabel(<span class="hljs-string">"$x_1$"</span>)
plt.ylabel(<span class="hljs-string">"$x_2$"</span>)</pre>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/f2431ee7e8c38ee95dae6aae00b7173e.png" alt="Kernel PCA" class="wp-image-54928" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_7.png?resize=456%2C460&amp;ssl=1"/><figcaption><em>Dimensionality reduction technique: KPCA | Source: Author</em></figcaption></figure></div>



<p>正如您在该数据集中看到的，这两个类不能线性分离。现在，让我们定义内核PCA，看看它是如何分离这个特征空间的。</p>



<pre class="hljs">kpca = KernelPCA(kernel=<span class="hljs-string">"rbf"</span>, fit_inverse_transform=<span class="hljs-keyword">True</span>, gamma=<span class="hljs-number">10</span>, )
X_kpca = kpca.fit_transform(X)
plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, aspect=<span class="hljs-string">'equal'</span>)
plt.scatter(X_kpca[reds, <span class="hljs-number">0</span>], X_kpca[reds, <span class="hljs-number">1</span>], c=<span class="hljs-string">"red"</span>,
           s=<span class="hljs-number">20</span>, edgecolor=<span class="hljs-string">'k'</span>)
plt.scatter(X_kpca[blues, <span class="hljs-number">0</span>], X_kpca[blues, <span class="hljs-number">1</span>], c=<span class="hljs-string">"blue"</span>,
           s=<span class="hljs-number">20</span>, edgecolor=<span class="hljs-string">'k'</span>)
plt.title(<span class="hljs-string">"Projection by KPCA"</span>)
plt.xlabel(<span class="hljs-string">r"1st principal component in space induced by $phi$"</span>)
plt.ylabel(<span class="hljs-string">"2nd component"</span>)</pre>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/223ae8a4b1791e12609786fdfc57ba32.png" alt="Kernel PCA" class="wp-image-54934" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_11.png?resize=584%2C432&amp;ssl=1"/><figcaption><em>Dimensionality reduction technique: KPCA | Source: Author</em></figcaption></figure></div>



<p>应用KPCA后，它能够线性分离数据集中的两个类。</p>



<h4>奇异值分解</h4>



<p>奇异值分解或SVD是实矩阵或复矩阵的因式分解方法。当处理稀疏数据集时，它是有效的；有许多零条目的数据集。这种类型的数据集通常出现在推荐系统、评级和评论数据集中，等等。</p>



<p>SVD的思想是将形状nXp的每个矩阵分解成A = USV <sup> T </sup>，其中U是正交矩阵，S是对角矩阵，<sup>T3】V<sup>T</sup>也是正交矩阵。</sup></p>







<p>SVD的优点是正交矩阵捕获了原始矩阵A的结构，这意味着当乘以其他数时，它们的属性不会改变。这可以帮助我们近似a。</p>



<p>现在让我们用代码来理解SVD。为了更好地理解该算法，我们将使用scikit-learn提供的人脸数据集。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_lfw_people
lfw_people = fetch_lfw_people(min_faces_per_person=<span class="hljs-number">70</span>, resize=<span class="hljs-number">0.4</span>)</pre>



<p>绘制图像以了解我们正在处理的内容。</p>



<pre class="hljs">X = lfw_people.images.reshape(img_count, img_width * img_height)
X0_img = X[<span class="hljs-number">0</span>].reshape(img_height, img_width)

plt.imshow(X0_img, cmap=plt.cm.gray)</pre>







<p>创建一个函数，以便于图像的可视化。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw_img</span><span class="hljs-params">(img_vector, h=img_height, w=img_width)</span>:</span>
   plt.imshow( img_vector.reshape((h,w)), cmap=plt.cm.gray)
   plt.xticks(())
   plt.yticks(())
draw_img(X[<span class="hljs-number">49</span>])</pre>







<p>在应用SVD之前，最好将数据标准化。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler

scaler = StandardScaler(with_std=<span class="hljs-keyword">False</span>)
Xstd = scaler.fit_transform(X)</pre>



<p>标准化后，这就是图像的外观。</p>







<p>值得注意的是，我们总是可以通过执行逆变换来恢复原始图像。</p>



<pre class="hljs">Xorig = scaler.inverse_transform(Xstd)
draw_img(Xorig[<span class="hljs-number">49</span>])</pre>







<p>现在，我们可以应用NumPy中的SVD函数，并将矩阵分解为三个矩阵。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> numpy.linalg <span class="hljs-keyword">import</span> svd

U, S, VT = svd(Xstd)</pre>



<p>为了检查这个函数是否有效，我们总是可以执行三个矩阵的矩阵乘法。</p>



<pre class="hljs">US = U*S
Xhat = US @ VT[<span class="hljs-number">0</span>:<span class="hljs-number">1288</span>,:]


Xhat_orig = scaler.inverse_transform(Xhat)
draw_img(Xhat_orig[<span class="hljs-number">49</span>])</pre>







<p>现在，让我们进行降维。为此，我们只需减少正交矩阵的特征数量。</p>



<pre class="hljs">Xhat_500 = US[:, <span class="hljs-number">0</span>:<span class="hljs-number">500</span>] @ VT[<span class="hljs-number">0</span>:<span class="hljs-number">500</span>, :]

Xhat_500_orig = scaler.inverse_transform(Xhat_500)

draw_img(Xhat_500_orig[<span class="hljs-number">49</span>])</pre>







<p>我们可以进一步减少更多的功能，看看结果。</p>



<pre class="hljs">Xhat_100 = US[:, <span class="hljs-number">0</span>:<span class="hljs-number">100</span>] @ VT[<span class="hljs-number">0</span>:<span class="hljs-number">100</span>, :]

Xhat_100_orig = scaler.inverse_transform(Xhat_100)

draw_img(Xhat_100_orig[<span class="hljs-number">49</span>])</pre>







<p>现在，让我们创建一个函数，允许我们减少图像的尺寸。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dim_reduce</span><span class="hljs-params">(US_, VT_, dim=<span class="hljs-number">100</span>)</span>:</span>

   Xhat_ = US_[:, <span class="hljs-number">0</span>:dim] @ VT_[<span class="hljs-number">0</span>:dim, :]

   <span class="hljs-keyword">return</span> scaler.inverse_transform(Xhat_)</pre>



<p>用不同数量的特征绘制图像。</p>



<pre class="hljs">dim_vec = [<span class="hljs-number">50</span>, <span class="hljs-number">100</span>, <span class="hljs-number">200</span>, <span class="hljs-number">400</span>, <span class="hljs-number">800</span>]

plt.figure(figsize=(<span class="hljs-number">1.8</span> * len(dim_vec), <span class="hljs-number">2.4</span>))

<span class="hljs-keyword">for</span> i, d <span class="hljs-keyword">in</span> enumerate(dim_vec):
   plt.subplot(<span class="hljs-number">1</span>, len(dim_vec), i + <span class="hljs-number">1</span>)
   draw_img(dim_reduce(US, VT, d)[<span class="hljs-number">49</span>])</pre>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/d6890cb5280847894d7187692621d866.png" alt="Singular Value Decomposition" class="wp-image-54932" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_9.png?ssl=1"/><figcaption><em>Dimensionality reduction technique: SVD | Source: Author</em></figcaption></figure></div>



<p>如你所见，第一幅图像包含最少数量的特征，但它仍然可以构建图像的抽象版本，随着我们增加特征，我们最终获得原始图像。这证明了SVD可以保留数据的基本结构。</p>



<h4>非负矩阵分解(NMF)</h4>



<p>NMF是一种无监督的机器学习算法。当一个维数为mXn的非负输入矩阵X被提供给算法时，它被分解成两个非负矩阵W和H的乘积。W的维数为mXp，而H的维数为pXn。</p>







<p><strong>其中Y = W.H </strong></p>



<p>从上面的等式可以看出，要分解矩阵，我们需要最小化距离。最广泛使用的距离函数是平方Frobenius范数；这是欧几里德范数在矩阵上的扩展。</p>



<p>同样值得注意的是，这个问题通常是不可解的，这就是为什么它是近似的。事实证明，NMF有利于数据集的基于部件的表示，即NMF提供了一种<strong>高效的</strong>、<strong>分布式表示、</strong>，并且可以帮助发现数据中感兴趣的结构。</p>



<p>让我们用代码来理解NMF。我们将使用在SVD中使用的相同数据。</p>



<p>首先，我们将使模型符合数据。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMF
model = NMF(n_components=<span class="hljs-number">200</span>, init=<span class="hljs-string">'nndsvd'</span>, random_state=<span class="hljs-number">0</span>)
W = model.fit_transform(X)
V = model.components_</pre>



<p>NMF需要一点时间来分解数据。一旦数据被分解，我们就可以可视化分解的组件。</p>



<pre class="hljs">num_faces = <span class="hljs-number">20</span>
plt.figure(figsize=(<span class="hljs-number">1.8</span> * <span class="hljs-number">5</span>, <span class="hljs-number">2.4</span> * <span class="hljs-number">4</span>))

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, num_faces):
   plt.subplot(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, i + <span class="hljs-number">1</span>)
   draw_img(V[i])</pre>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/f2b91986a9a5eca8f519a8e4477b2836.png" alt="Non-negative Matrix Factorization" class="wp-image-54925" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_10.png?resize=676%2C693&amp;ssl=1"/><figcaption><em>Dimensionality reduction technique: NMF | Source: Author</em></figcaption></figure></div>



<p>从上图我们可以看出，NMF捕捉数据底层结构的效率非常高。同样值得一提的是，NMF只获取了线性属性。</p>



<p><strong>NMF的优势</strong>:</p>



<ol><li>数据压缩和可视化</li><li>对噪声的鲁棒性</li><li>更容易理解</li></ol>







<h3>流形学习</h3>



<p>到目前为止，我们已经看到了只涉及线性变换的方法。但是，当我们有一个非线性数据集时，我们该怎么办呢？</p>



<p>流形学习是一种无监督学习，旨在对非线性数据集进行降维。同样，scikit-learn提供了一个由各种非线性降维技术组成的模块。我们可以通过这个命令调用这些类或技术:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNE, LocallyLinearEmbedding, SpectralEmbedding</pre>



<h4>t分布随机邻居嵌入(t-SNE)</h4>



<p>t-分布式随机邻居嵌入或t-SNE是一种非常适合数据可视化的降维技术。与简单地最大化方差的PCA不同，t-SNE最小化两个分布之间的差异。本质上，它在低维空间中重建了高维空间的分布，而不是最大化方差，甚至没有使用核技巧。</p>



<p>我们可以通过三个简单的步骤对SNE霸王龙有一个高层次的了解:</p>



<ol><li>它首先为高维样本创建一个概率分布。</li><li>然后，它为低维嵌入中的点定义了类似的分布。</li><li>最后，它试图最小化两个分布之间的KL-散度。</li></ol>



<p>现在我们用代码来理解一下。对于SNE霸王龙，我们将再次使用MNIST数据集。首先，我们导入TSNE，然后导入数据。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNE
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_digits

digits = load_digits()
print(digits.data.shape)





<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>):
   plt.figure(figsize=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))
   plt.imshow(digits.images[i])
   plt.show()</pre>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/70be75d61df30ba735443957a65240ef.png" alt="t-Distributed Stochastic Neighbor Embedding" class="wp-image-54936" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_19.png?resize=287%2C560&amp;ssl=1"/><figcaption><em>Dimensionality reduction technique: t-SNE | Source: Author</em></figcaption></figure></div>



<p>然后我们将使用np.vstack按顺序存储这些数字。</p>



<pre class="hljs">X = np.vstack([digits.data[digits.target==i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>)])
Y = np.hstack([digits.target[digits.target==i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>)])</pre>



<p>我们将对数据集应用t-SNE。</p>



<pre class="hljs">digits_final = TSNE(perplexity=<span class="hljs-number">30</span>).fit_transform(X)</pre>



<p>我们现在将创建一个函数来可视化数据。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot</span><span class="hljs-params">(x, colors)</span>:</span>
    palette = np.array(sb.color_palette(<span class="hljs-string">"hls"</span>, <span class="hljs-number">10</span>))  

   
   f = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))
   ax = plt.subplot(aspect=<span class="hljs-string">'equal'</span>)
   sc = ax.scatter(x[:,<span class="hljs-number">0</span>], x[:,<span class="hljs-number">1</span>], lw=<span class="hljs-number">0</span>, s=<span class="hljs-number">40</span>,c=palette[colors.astype(np.int)])
   
   txts = []
   <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):
       
       xtext, ytext = np.median(x[colors == i, :], axis=<span class="hljs-number">0</span>)
       txt = ax.text(xtext, ytext, str(i), fontsize=<span class="hljs-number">24</span>)
       txt.set_path_effects([pe.Stroke(linewidth=<span class="hljs-number">5</span>, foreground=<span class="hljs-string">"w"</span>),
                             pe.Normal()])
       txts.append(txt)
   <span class="hljs-keyword">return</span> f, ax, txts</pre>



<p>现在，我们对转换后的数据集执行数据可视化。</p>



<pre class="hljs">plot(digits_final,Y)</pre>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/a7297afe5a7593de21afc3d145f3b835.png" alt="t-Distributed Stochastic Neighbor Embedding" class="wp-image-54926" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_21.png?resize=524%2C504&amp;ssl=1"/><figcaption><em>Dimensionality reduction technique: t-SNE | Source: Author</em></figcaption></figure></div>



<p>可以看出，SNE霸王龙对数据进行了完美的聚类。与PCA相比，t-SNE在非线性数据上表现良好。t-SNE的缺点是，当数据很大时，它会消耗大量的时间。所以最好先进行主成分分析，然后再进行t-SNE。</p>



<h4>局部线性嵌入(LLE)</h4>



<p>局部线性嵌入或LLE是一种非线性和无监督的降维机器学习方法。LLE利用数据的局部结构或拓扑，并将其保存在低维特征空间中。</p>



<p>LLE优化速度更快，但在噪音数据上失败。</p>



<p>让我们将整个过程分成三个简单的步骤:</p>



<ol><li>找到数据点的最近邻。</li><li>通过将每个数据点近似为其k个最近邻点的加权线性组合并最小化它们与其线性表示之间的平方距离来构建权重矩阵。</li><li>通过使用基于<strong>特征向量的优化</strong>技术将权重映射到低维空间。</li></ol>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/395f6ddd9b242eb0c5e6ad4dc56d0163.png" alt="Locally Linear Embedding" class="wp-image-54927" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_24.png?resize=649%2C639&amp;ssl=1"/><figcaption><em>Dimensionality reduction technique: LLE | Source: S. T. Roweis and L. K. Saul, Nonlinear dimensionality reduction by locally linear embedding</em></figcaption></figure></div>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/ed04bb41bd926f3933e2fb442d03815c.png" alt="Locally Linear Embedding" class="wp-image-54933" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221219034305im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Dimensionality-Reduction-for-Machine-Learning_14.png?resize=518%2C389&amp;ssl=1"/><figcaption><em>Dimensionality reduction technique: LLE | Source: <a href="https://web.archive.org/web/20221219034305/https://scikit-learn.org/stable/modules/manifold.html#manifold" target="_blank" rel="noreferrer noopener nofollow">Scikit Learn</a></em></figcaption></figure></div>



<h4>光谱嵌入</h4>



<p>谱嵌入是另一种非线性降维技术，也是一种无监督的机器学习算法。谱嵌入旨在基于低维表示找到不同类别的聚类。</p>



<p>我们可以再次将整个过程分成三个简单的步骤:</p>



<ol><li><strong>预处理</strong>:构建数据或图形的拉普拉斯矩阵表示。</li><li><strong>分解</strong>:计算构造好的矩阵的特征值和特征向量，然后将每个点映射到一个更低维的表示上。谱嵌入利用了第二小特征值及其对应的特征向量。</li><li><strong>聚类</strong>:根据表示法，将点分配给两个或多个聚类。聚类通常使用k-means聚类来完成。</li></ol>



<p><strong>应用</strong>:光谱嵌入在图像<strong>分割中得到应用</strong>。</p>



<h3>判别分析</h3>



<p>判别分析是scikit-learn提供的另一个模块。可以使用以下命令调用它:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.discriminant_analysis <span class="hljs-keyword">import</span> LinearDiscriminantAnalysis</pre>



<h4>线性判别分析(LDA)</h4>



<p>LDA是一种算法，用于查找数据集中要素的线性组合。像PCA一样，LDA也是一种基于线性变换的技术。但与PCA不同，它是一种监督学习算法。</p>



<p>LDA计算方向，即可以创建决策边界并最大化多个类之间的分离的线性判别式。对于多类分类任务也非常有效。</p>



<p>为了对LDA有一个更直观的理解，考虑绘制两个类的关系，如下图所示。</p>







<p>解决这个问题的一种方法是将所有数据点投影到x轴上。</p>







<p>这种方法会导致信息丢失，而且是多余的。</p>







<p>更好的方法是计算数据中所有点之间的距离，并拟合一条穿过这些点的新直线。这条新线现在可以用来投影所有的点。</p>







<p>这条新线最小化了方差，并通过最大化两个类之间的距离来有效地对它们进行分类。</p>







<p>LDA也可以用于多元数据。它使数据推断变得非常简单。可以使用以下5个步骤计算LDA:</p>



<ol><li>计算数据集中不同类别的d维均值向量。</li><li>计算散布矩阵(类间和类内散布矩阵)。散布矩阵用于估计协方差矩阵。当协方差矩阵难以计算或者两个随机变量的联合可变性难以计算时，就要这样做。</li><li>计算散布矩阵的特征向量(e1，e2，e3…ed)和相应的特征值(λ1，λ2，…，λd)。</li><li>将特征向量按特征值递减排序，选择k个特征值最大的特征向量，形成d×k维矩阵W(其中每列代表一个特征向量)。</li><li>使用这个d×k特征向量矩阵将样本变换到新的子空间上。这可以通过矩阵乘法来概括:Y=X×W(其中X是表示n个样本的n×d维矩阵，Y是新子空间中的变换后的n×k维样本)。</li></ol>



<p>要了解LDA，你可以看看这篇文章。</p>



<h2 id="h-applications-of-dimentionality-reduction">降维的应用</h2>



<p>降维在许多实际应用中都有应用，其中包括:</p>



<ul><li>客户关系管理</li><li>文本分类</li><li>图像检索</li><li>入侵检测</li><li>医学图像分割</li></ul>



<h2 id="h-advantages-and-disadvantages-of-dimentionality-reduction">降维的利与弊</h2>



<p><strong>降维的优势</strong>:</p>



<ul><li>它通过减少特征来帮助数据压缩。</li><li>它减少了存储。</li><li>它使机器学习算法在计算上高效。</li><li>它还有助于移除多余的特征和噪声。</li><li>它解决了维度的诅咒</li></ul>



<p><strong>降维的缺点</strong>:</p>



<ul><li>这可能会导致一些数据丢失。</li><li>准确性大打折扣。</li></ul>



<h2 id="h-final-thoughts">最后的想法</h2>



<p>在本文中，我们学习了降维以及维数灾难。我们通过数学细节和代码讨论了降维中使用的不同算法。</p>



<p>值得一提的是，这些算法应该根据手头的任务来使用。例如，如果你的数据是线性的，那么使用分解方法，否则使用流形学习技术。</p>



<p>首先将数据可视化，然后决定使用哪种方法，这被认为是一种良好的做法。此外，不要把自己局限在一种方法上，而是探索不同的方法，看看哪一种是最合适的。</p>



<p>我希望你从这篇文章中学到了一些东西。快乐学习。</p>



<h3>参考</h3>



<ol><li><a href="https://web.archive.org/web/20221219034305/https://www.geeksforgeeks.org/dimensionality-reduction/" target="_blank" rel="noreferrer noopener nofollow">降维介绍–GeeksforGeeks</a></li><li><a href="https://web.archive.org/web/20221219034305/https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/" target="_blank" rel="noreferrer noopener nofollow">机器学习降维介绍</a></li><li><a href="https://web.archive.org/web/20221219034305/https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202" target="_blank" rel="noreferrer noopener nofollow">主成分分析:回顾与最新进展</a></li><li><a href="https://web.archive.org/web/20221219034305/https://towardsdatascience.com/linear-discriminant-analysis-in-python-76b8b17817c2" target="_blank" rel="noreferrer noopener nofollow">Python中的线性判别分析|作者Cory Maklin </a></li><li><a href="https://web.archive.org/web/20221219034305/https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html" target="_blank" rel="noreferrer noopener nofollow">在机器学习模型中使用稀疏特征</a></li><li><a href="https://web.archive.org/web/20221219034305/https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html" target="_blank" rel="noreferrer noopener nofollow">必知:什么是维度的诅咒？</a></li><li><a href="https://web.archive.org/web/20221219034305/https://analyticsindiamag.com/curse-of-dimensionality-and-what-beginners-should-do-to-overcome-it/" target="_blank" rel="noreferrer noopener nofollow">维数灾难和初学者应该如何克服它</a></li><li><a href="https://web.archive.org/web/20221219034305/https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/" target="_blank" rel="noreferrer noopener nofollow">Python的6种降维算法</a></li><li>硬化API参考</li><li><a href="https://web.archive.org/web/20221219034305/https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank" rel="noreferrer noopener nofollow"> t分布随机邻居嵌入</a></li><li><a href="https://web.archive.org/web/20221219034305/https://www.sciencedirect.com/science/article/pii/B9780124095458000029" target="_blank" rel="noreferrer noopener nofollow">特征选择和提取</a></li></ol>
        </div>
        
    </div>    
</body>
</html>