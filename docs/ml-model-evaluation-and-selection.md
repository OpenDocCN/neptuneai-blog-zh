# 机器学习中模型评估和选择的最终指南

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/ml-model-evaluation-and-selection>

为了正确评估你的机器学习模型并选择最佳模型，你需要为你的问题选择一个**好的验证策略**和**可靠的评估指标**。

一个好的**验证(评估)策略**基本上就是你如何分割你的数据来估计未来的测试性能。它可以像训练测试分割一样简单，也可以是复杂的分层 k 倍策略。

一旦您知道您可以估计未来的模型性能，您需要**选择一个适合您的问题的指标**。如果你理解了分类和回归度量，那么大多数其他复杂的度量(例如，在对象检测中)就相对容易掌握。

当你搞定那两个人，你就很棒。

在这篇文章中，我将谈论:

*   选择一个好的评估方法(重采样、交叉验证等)
*   流行的(不太为人所知的)分类和回归指标
*   以及机器学习中的偏差/方差权衡。

所以让我们开始吧。

可以使用 [neptune.ai.](/web/20230313155430/https://neptune.ai/) 基于度量、参数、学习曲线、预测图像、数据集版本等来比较实验和模型。

它使得模型评估和选择方式更加容易。

[![Experiment and model comparison in neptune.ai](img/4f594650d179676745457c35643a221b.png)](https://web.archive.org/web/20230313155430/https://app.neptune.ai/showcase/example-project-tensorflow-keras/experiments?split=tbl&dash=charts&viewId=979b96ce-302a-4a92-b973-a01c05e3ef4a&spl=4a21e7e9-abab-4f9f-8723-41f5ae4f71e8&product_tour_id=314370)

[Play with this project live](https://web.archive.org/web/20230313155430/https://app.neptune.ai/showcase/example-project-tensorflow-keras/experiments?split=tbl&dash=charts&viewId=979b96ce-302a-4a92-b973-a01c05e3ef4a&spl=4a21e7e9-abab-4f9f-8723-41f5ae4f71e8&product_tour_id=314370) 

更多信息:

–[查看实验对比文档](https://web.archive.org/web/20230313155430/https://docs.neptune.ai/app/comparison/)
–[了解我们的实验跟踪产品](https://web.archive.org/web/20230313155430/https://neptune.ai/product/experiment-tracking)
–[查看关于对比实验和模型的视频项目和其他资源](https://web.archive.org/web/20230313155430/https://neptune.ai/resources/?s=compare)

为了确保我们在同一页上，让我们把定义放在一边。

## 什么是模型评估？

**模型评估**是在选定的评估设置上评估模型性能的过程。这是通过计算量化性能指标(如 F1 分数或 RMSE)或由主题专家定性评估结果来完成的。您选择的机器学习**评估指标应该反映您希望使用机器学习解决方案优化的业务指标**。

## 什么是选型？

**模型选择**是为给定任务选择最佳 ml 模型的过程。这是通过在根据设计的评估方案计算的所选评估指标上比较各种候选模型来完成的。选择**正确的评估模式**，无论是简单的训练测试分割还是复杂的交叉验证策略，都是构建任何机器学习解决方案的**至关重要的第一步****。**

 **## 如何评价机器学习模型，选出最好的？

我们将深入探讨这个问题，但让我一步一步地告诉你:

**第一步:选择合适的验证策略**。不能强调这一点，没有一个可靠的方法来验证您的模型性能，再多的超参数调整和最先进的模型也不会帮助您。

第二步:选择正确的评估指标。找出你的模型背后的商业案例，并尝试使用与此相关的机器学习指标。通常情况下，没有一个度量标准适合这个问题。

因此，计算多个指标，并在此基础上做出决策。有时，您需要将经典的 ML 指标与主题专家评估结合起来。这没关系。

第三步:记录你的实验结果。无论你使用电子表格还是专用的[实验跟踪器](https://web.archive.org/web/20230313155430/https://neptune.ai/product/experiment-tracking)，确保记录所有重要的指标、学习曲线、数据集版本和配置。你以后会感谢自己的。

第四步:对比实验，选出一个赢家。不管您选择什么样的指标和验证策略，最终，您都希望找到最好的模型。但是没有一个模型是真正最好的，但是有些已经足够好了。

因此，确保了解什么对您的问题足够好，一旦您达到了这一点，就转移到项目的其他部分，如模型部署或管道编排。

## 机器学习中的模型选择(选择模型验证策略)

### 重采样方法

顾名思义，重采样方法是重新排列数据样本的简单技术，用于检查模型在未经训练的数据样本上是否表现良好。换句话说，重采样有助于我们理解**模型是否能很好地概括**。

#### 随机分裂

随机拆分用于将一定百分比的数据随机抽样到训练集、测试集中，最好是验证集中。这种方法的优点是原始群体很有可能在所有三个集合中得到很好的代表。用更正式的术语来说，随机分裂将防止数据的有偏抽样。

在模型选择中注意验证集的使用是非常重要的。验证集是第二个测试集，有人可能会问，为什么有两个测试集？

在特征选择和模型调整过程中，测试集用于模型评估。这意味着选择模型参数和特征集，使得它们在测试集上给出最佳结果。因此，具有完全看不见的数据点(在调整和特征选择模块中没有使用)的验证集被用于最终评估。

#### 基于时间的分割

有**种类型的数据不可能随机分割**。例如，如果我们必须为天气预报训练一个模型，我们不能将数据随机分为训练集和测试集。这将打乱季节模式！这种数据通常被称为时间序列。

在这种情况下，使用时间分割。训练集可以包含最近三年和今年 10 个月的数据。最后两个月可以留给测试或验证集。

还有一个 ***窗口*** **集合**的**概念——其中模型被训练到一个特定的日期，并在未来的日期被反复测试，使得训练窗口保持增加一天的偏移(因此，测试集合也减少一天)。这种方法的优点是，当测试集非常小(比如 3 到 7 天)时，它可以稳定模型并防止过度拟合。**

但是，时序数据的缺点是事件或数据点不是 ***相互独立的*** 。一个事件可能会影响随后的每个数据输入。

例如，执政党的更迭可能会极大地改变未来几年的人口统计数据。或者臭名昭著的冠状病毒疫情将对未来几年的经济数据产生巨大影响。

在这种情况下，没有机器学习模型可以从过去的数据中学习，因为事件前后的数据点有很大的差异。

#### k 倍交叉验证

交叉验证技术的工作原理是**随机改组数据集，然后将其分成 k 组。此后，在对每个组进行迭代时，该组需要被认为是一个测试集，而所有其他组一起加入训练集。在测试组上测试该模型，并且对 k 个组继续该过程。**

因此，在该过程结束时，在 k 个不同的测试组上有 k 个不同的结果。然后，通过选择具有最高分数的模型，可以容易地选择最佳模型。

#### 分层 K 折叠

分层 k-fold 的过程类似于 k-fold 交叉验证的过程，只有一个单点差异–**与 K-Fold 交叉验证不同，分层 K-Fold 会考虑目标变量的值。**

例如，如果目标变量是具有两个类别的分类变量，那么分层 k-fold 确保当与训练集比较时，每个测试 fold 获得两个类别的相等比率。

这使得模型评估更准确，模型训练更少偏差。

#### 引导程序

Bootstrap 是获得稳定模型的最有效的方法之一。它接近于随机分裂技术，因为它遵循随机采样的概念。

第一步是选择样本大小(通常等于原始数据集的大小)。此后，必须从原始数据集中随机选择一个样本数据点，并将其添加到引导样本中。添加后，需要将样品放回原始样品中。这个过程需要重复 N 次，其中 N 是样本大小。

因此，它是一种重采样技术，通过用替换的 从原始数据集 ***中采样数据点来创建引导样本。这意味着引导示例可以包含同一数据点的多个实例。***

该模型在引导样本上进行训练，然后在所有那些没有进入引导样本的数据点上进行评估。这些样品被称为*样品。*

 *比较和评估交叉验证的结果可能会变得棘手。

医疗保健初创公司 Theta Tech AI 为此使用了海王星实验跟踪器。

> “通过验证集进行分组对我们来说非常重要，许多其他人也将受益于验证分组功能。”–罗伯特·托特博士 西塔科技人工智能创始人

[![Visualizing cross validatiokn results in neptune.ai](img/fe8e146f3cf4873d8466c8d15e6e9725.png)](https://web.archive.org/web/20230313155430/https://app.neptune.ai/showcase/example-project-tensorflow-keras/experiments?split=tbl&dash=charts&viewId=979b96ce-302a-4a92-b973-a01c05e3ef4a&spl=4a21e7e9-abab-4f9f-8723-41f5ae4f71e8&product_tour_id=314370)

[Play with this project live](https://web.archive.org/web/20230313155430/https://app.neptune.ai/o/common/org/showroom/e/SHOW-3700/dashboard/fold-0-96502660-c0e9-4409-acaa-a4eac08207e0) 

更多信息:

概率测度

概率度量**不仅考虑模型性能，还考虑模型复杂性**。模型复杂性是对模型捕捉数据中的差异的能力的度量。

### 例如，像线性回归算法这样的高偏差模型不太复杂，而另一方面，神经网络的复杂性非常高。

这里需要注意的另一个要点是，在概率度量中考虑的**模型性能**是仅根据训练集计算的**。通常不需要保持测试组。**

然而，一个相当大的缺点在于，概率测量没有考虑模型的不确定性，并且有可能选择较简单的模型而不是复杂的模型。

赤池信息标准(AIC)

众所周知，每个模型都不完全准确。总是存在一些信息损失，这可以使用 KL 信息度量来测量。Kulback-Liebler 或 KL 散度是两个变量的概率分布差异的度量。

#### 统计学家 Hirotugu Akaike 考虑了 KL 信息和最大似然(在最大似然中，在给定参数和指定概率分布的情况下，希望最大化观察数据点 X 的条件概率)之间的关系，并发展了信息标准(或 IC)的概念。因此，赤池的 IC 或 AIC 是信息损失的度量。这就是如何捕获两个不同模型之间的差异，并建议选择信息损失最少的模型。

K =独立变量或预测值的数量

L =模型的最大似然

*   N =训练集中数据点的数量(在小数据集的情况下尤其有用)
*   AIC 的局限性在于它不太擅长概化模型，因为它倾向于选择丢失较少训练信息的复杂模型。
*   贝叶斯信息准则(BIC)

BIC 源自贝叶斯概率概念，适用于在最大似然估计下训练的模型。

#### K =独立变量的数量

L =最大似然

*   N =训练集中样本/数据点的数量
*   BIC 因模型的复杂性而对其不利，当数据集的大小不是很小时，最好使用它(否则它倾向于停留在非常简单的模型上)。
*   最小描述长度(MDL)

MDL 来源于信息论，信息论处理熵之类的量，这些量测量用概率分布或随机变量表示事件所需的平均位数。

#### MDL 或最小描述长度是表示模型所需的最小位数。

d =模型

D =模型做出的预测

*   L(h) =表示模型所需的位数
*   L(D | h) =表示模型预测所需的位数
*   结构风险最小化(SRM)
*   机器学习模型面临着从一组有限的数据中定义一个通用理论的不可避免的问题。这导致过度拟合的情况，其中模型偏向于作为其主要学习源的训练数据。SRM 试图平衡模型的复杂性和数据拟合的成功性。

#### 如何评估 ML 模型(选择性能指标)

可以使用多种指标来评估模型。然而，评估指标的正确选择是至关重要的，并且通常取决于正在解决的问题。对各种度量标准的清楚理解可以帮助评估者找到问题陈述和度量标准的适当匹配。

## 分类指标

对于每个分类模型预测，可以构建一个称为 ***混淆矩阵*** 的矩阵，其展示了正确和错误分类的测试用例的数量。

### 大概是这样的(考虑到 1-正和 0-负是目标类):

TN:正确分类的阴性病例数

TP:正确分类的阳性病例数

*   FN:被错误归类为阴性的阳性病例数
*   FP:正确归类为阳性的阴性病例数
*   准确(性)
*   准确性是最简单的度量，可以定义为正确分类的测试用例的数量除以测试用例的总数。

#### 它可以应用于大多数一般问题，但在涉及不平衡数据集时不是很有用。

例如，如果我们在银行数据中检测欺诈，欺诈与非欺诈案例的比例可能是 1:99。在这种情况下，如果使用准确性，通过预测所有测试案例为非欺诈，该模型将证明是 99%准确的。99%准确的模型将完全无用。

如果一个模型训练很差，以至于它预测所有 1000 个数据点为非欺诈，那么它将错过 10 个数据点。如果测量准确性，它将显示该模型正确预测了 990 个数据点，因此，它的准确性为(990/1000)*100 = 99%！

这就是为什么准确性是模型健康的虚假指标。

因此，对于这种情况，需要一个指标来关注模型完全遗漏的十个欺诈数据点。

精确

精度是用于识别分类正确性的度量。

#### 直观上，这个等式是正确的肯定分类与预测的肯定分类总数的比率。分数越大，精度越高，这意味着模型正确分类正类的能力越强。

在预测性维护的问题中(必须提前预测机器何时需要维修)，精确度发挥了作用。维护成本通常很高，因此，不正确的预测会给公司带来损失。在这种情况下，模型正确分类阳性类别和降低假阳性数量的能力至关重要！

回忆

回忆告诉我们在阳性病例总数中正确识别的阳性病例数。

#### 回到欺诈问题，召回值在欺诈案例中非常有用，因为高召回值将表明在欺诈总数中识别出大量欺诈案例。

F1 分数

F1 分数是召回率和准确率的调和平均值，因此平衡了两者的优势。

#### 这在召回率和精确度都很重要的情况下很有用——比如识别可能需要修理的飞机部件。在这种情况下，需要精确以节省公司成本(因为飞机零件极其昂贵),需要召回以确保机器稳定且不会对人类生命构成威胁。

AUC-ROC

ROC 曲线是**真阳性率**(回忆)对**假阳性率** (TN / (TN+FP))的曲线图。AUC-ROC 代表受试者操作特征下的面积，面积越大，模型性能越好。

#### 如果曲线在 50%对角线附近，则表明模型随机预测了输出变量。

原木损失

**对数损失是一个非常有效的分类指标**，相当于-1*对数(似然函数)，其中似然函数表明模型认为观察到的结果集的可能性有多大。

#### 由于似然函数提供了非常小的值，因此解释它们的更好方法是将这些值转换为对数，并添加负值以颠倒度量的顺序，这样较低的损失分数表明模型更好。

增益和提升图表

增益和提升图是评估模型性能的工具，就像混淆矩阵一样，但有细微而显著的区别。混淆矩阵决定了模型在整个群体或整个测试集上的性能，而增益和提升图评估模型在部分群体上的性能。因此，我们对每%的人口(x 轴)都有一个分数(y 轴)。

#### 提升图衡量模型与随机预测相比带来的改进。这种改进被称为“提升”。

k 线图

K-S 图或 Kolmogorov-Smirnov 图确定了两种分布之间的分离程度——正类分布和负类分布。差异越大，模型就越能更好地区分正面和负面案例。

#### 回归度量

回归模型提供了一个连续的输出变量，不同于具有离散输出变量的分类模型。因此，用于评估回归模型的度量被相应地设计。

### 均方差或 MSE

MSE 是一个简单的指标，它计算实际值和预测值之间的差异(误差)，将其平方，然后提供所有误差的平均值。

#### MSE 对异常值非常敏感，即使在拟合良好的模型预测中存在一些异常值，MSE 也会显示出非常高的误差值。

均方根误差或 RMSE

RMSE 是 MSE 的根源，并且是有益的，因为它有助于降低更接近实际值的误差范围，使其更易于解释。

#### 记录比每次模型训练运行所需更多的指标通常是一个好主意。

但是你必须把它们形象化并进行比较。这就是用于[实验跟踪](https://web.archive.org/web/20230313155430/https://neptune.ai/product/experiment-tracking)的工具的用处。

您可以使用 [neptune.ai.](/web/20230313155430/https://neptune.ai/) 在一个地方可视化、组织和比较您的所有模型指标。

更多信息:

–[看 2 分钟产品走一遍](https://web.archive.org/web/20230313155430/https://neptune.ai/resources/walk-through)
–[看实验对比文档](https://web.archive.org/web/20230313155430/https://docs.neptune.ai/app/comparison/)
–[了解我们的实验跟踪产品](https://web.archive.org/web/20230313155430/https://neptune.ai/product/experiment-tracking)
–[查看关于对比实验和模型的视频项目和其他资源](https://web.archive.org/web/20230313155430/https://neptune.ai/resources/?s=compare)

[![Experiment and model comparison in neptune.ai](img/4f594650d179676745457c35643a221b.png)](https://web.archive.org/web/20230313155430/https://app.neptune.ai/showcase/example-project-tensorflow-keras/experiments?split=tbl&dash=charts&viewId=979b96ce-302a-4a92-b973-a01c05e3ef4a&spl=4a21e7e9-abab-4f9f-8723-41f5ae4f71e8&product_tour_id=314370)

[Play with this project live](https://web.archive.org/web/20230313155430/https://app.neptune.ai/showcase/example-project-tensorflow-keras/experiments?split=tbl&dash=charts&viewId=979b96ce-302a-4a92-b973-a01c05e3ef4a&spl=4a21e7e9-abab-4f9f-8723-41f5ae4f71e8&product_tour_id=314370) 

平均绝对误差

MAE 是绝对误差值(实际值–预测值)的平均值。

#### 如果想在一定程度上忽略异常值，可以选择 MAE，因为它通过去除平方项显著降低了异常值的损失。

均方根对数误差

在 RMSLE 中，遵循与 RMSE 方程相同的方程，除了增加了对数函数以及实际值和预测值。

#### x 是实际值，y 是预测值。这有助于通过对数函数淡化较高的错误率来缩小离群值的影响。此外，RMSLE 通过使用日志帮助捕获**相对** **误差**(通过比较所有误差值)。

r 平方

R-Square 有助于确定目标变量的方差比例，该比例可在独立变量或预测值的帮助下获得。

然而，R-square 有一个巨大的问题。比方说，一个新的不相关特征被添加到一个分配了权重 w 的模型中。如果模型发现新的预测值和目标变量之间绝对没有相关性，则 w 为 0。然而，由于随机性，几乎总是存在小的相关性，这增加了小的正权重(w > 0 ),并且由于过拟合，实现了新的最小损失。

这就是为什么 R 平方随着任何新特性的增加而增加。因此，它的**无法在添加新功能时降低价值**限制了它识别模型是否在功能较少的情况下做得更好的能力。

调整后的 R 平方

调整的 R-Square 解决了 R-Square 的问题，因为它无法通过添加功能来降低值。随着更多功能的加入，分数会降低。

#### 这里的分母是魔术元素，它随着特性数量的增加而增加。因此，需要显著增加 R ² 来增加整体值。

聚类度量

聚类算法预测数据点组，因此基于距离的度量是最有效的。

### 邓恩指数

邓恩指数侧重于识别具有低方差(在集群的所有成员中)并且紧凑的集群。不同聚类的平均值也需要相距很远。

#### δ(Xi，Yj)是簇间距离，即 Xi 和 Xj 之间的距离

∈( Xk)是聚类 Xk 的聚类间距离，即聚类 Xk 内的距离

*   然而，邓恩指数的缺点是随着更高的聚类数和更多的维度，计算成本增加。
*   轮廓系数

轮廓系数在-1 到+1 的范围内追踪一个聚类中的每个点与其他聚类中的每个点的接近程度。：

#### 较高的轮廓值(接近+1)表示来自两个不同聚类的采样点距离较远。

0 表示这些点接近决策边界

*   并且接近-1 的值表明这些点被错误地分配给该聚类。
*   肘法
*   肘形法用于确定数据集中的聚类数，方法是将 x 轴上的聚类数与 y 轴上的方差百分比相对照。x 轴上曲线突然弯曲的点(肘部)被认为是建议最佳聚类数的点。

#### ml 模型选择中的权衡

偏差与方差

## 在高层次上，机器学习是统计和计算的结合。机器学习的关键围绕着算法或模型的概念，事实上，它们是类固醇的统计估计。

### 然而，根据数据分布的不同，任何给定的模型都有一些限制。它们中没有一个是完全准确的，因为它们只是 ***(即使使用类固醇)*** 。这些限制俗称 ***偏差*** 和 ***方差*** 。

具有高偏差的**模型会由于不太注意训练点而过于简化(例如:在线性回归中，不管数据分布如何，模型将总是假设线性关系)。**

当一个模型被假设严格控制时，就会出现偏差——就像线性回归模型假设输出变量和独立变量之间的关系是一条直线。当实际值与独立变量非线性相关时，这导致 ***欠拟合*** 。

具有高方差的**模型将通过不对其之前未见过的测试点进行概括来将其自身限制于训练数据(例如:[随机森林](https://web.archive.org/web/20230313155430/https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why)，其中 max_depth = None)。**

当一个模型过于关注训练集并且非常接近地学习变化时，方差很高，从而影响了泛化能力。这就导致 ***过拟合*** 。

当限制很微妙时，问题就出现了，比如当我们必须在随机森林算法和梯度推进算法之间进行选择，或者在同一决策树算法的两个变体之间进行选择。两者都趋向于具有高方差和低偏差。

最佳模型是具有最低偏差和方差的模型，因为这两个属性是间接成比例的，所以实现这一点的唯一方法是在两者之间进行权衡。因此，模型选择应该使偏差和方差交叉，如下图所示。

这可以通过迭代地[调整使用中的模型](https://web.archive.org/web/20230313155430/https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide)的 ***超参数*** 来实现(超参数是馈入模型函数的输入参数)。在每次迭代之后，必须使用合适的度量标准进行模型评估。

学习曲线

![](img/8286ad41f11a9059b7ddca7024e80fb8.png)

*Soure: Analytics Vidhya*

跟踪模型训练或构建进度的最佳方式是使用学习曲线。这些曲线有助于识别一组超参数组合中的最佳点，并在模型选择和模型评估过程中提供大量帮助。

### 通常，学习曲线是一种在 y 轴上跟踪模型性能的学习或改进，在 x 轴上跟踪时间或经验的方法。

两个最受欢迎的学习曲线是:

**培训学习曲线**–它有效地绘制了培训过程中的评估指标分数，从而有助于跟踪模型在培训过程中的学习或进度。

**验证学习曲线**–在该曲线中，评估指标分数相对于验证集的时间绘制。

*   有时可能会发生这样的情况，训练曲线显示出改进，但是验证曲线显示出性能下降。
*   这表明模型过度拟合，需要恢复到之前的迭代。换句话说，验证学习曲线确定了模型的泛化能力。

因此，在训练学习曲线和验证学习曲线之间有一个折衷，并且模型选择技术必须依赖于两条曲线相交并且处于最低点的点。

好吧，但是你实际上是怎么做的呢？

下一步是什么

评估 ML 模型并选择性能最佳的模型是制作前的主要工作之一。

## 希望通过这篇文章，您已经学会了如何正确地建立模型验证策略，以及如何为您的问题选择度量标准。

你已经准备好进行一系列的实验，看看什么有效。

随之而来的另一个问题是跟踪实验参数、使用的数据集、配置和结果。

弄清楚如何可视化和比较所有这些模型和结果。

为此，您可能想看看:

其他资源

Kaggle 竞赛的交叉验证和评估策略:

## 评估指标和可视化:

Cross-validation and evaluation strategies from Kaggle competitions:

实验跟踪视频和真实案例研究:

Evaluation metrics and visualization:

Experiment tracking videos and real-world case studies:***