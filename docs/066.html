<html>
<head>
<title>Self-Supervised Learning and Its Applications </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>自我监督学习及其应用</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/self-supervised-learning#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/self-supervised-learning#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>在过去的十年里，人工智能的研究和发展突飞猛进，特别是在2012年ImageNet比赛结果公布之后。焦点主要集中在<a href="https://web.archive.org/web/20220926104145/https://www.upgrad.com/blog/types-of-supervised-learning/" target="_blank" rel="noreferrer noopener nofollow">监督学习方法</a>上，这些方法需要大量的标记数据来为特定用例训练系统。</p>



<p>在本文中，我们将探索<strong>自我监督学习(SSL)</strong>——机器学习社区中的一个热门研究话题。</p>



<h2>什么是自我监督学习(SSL)算法？</h2>



<p><strong>自我监督学习(SSL) </strong>是一种不断发展的机器学习技术，旨在解决标签数据过度依赖带来的挑战。多年来，使用机器学习方法构建智能系统在很大程度上依赖于高质量的标记数据。因此，高质量注释数据的成本是整个训练过程中的一个主要瓶颈。</p>



<p>人工智能研究人员的首要任务之一是开发具有非结构化数据的自我学习机制，这些机制可以以低成本规模化通用人工智能系统的研发。实际上，要收集和标注各种各样的数据是不可能的。</p>



<p>为了解决这个问题，研究人员正在研究能够捕捉数据中细微差别的自我监督学习(SSL)技术。</p>



<p>在我们进入自我监督学习之前，让我们先了解一些在构建智能系统中使用的流行学习方法的背景。</p>



<h3>1.监督学习</h3>



<p>一种流行的学习技术，用于根据特定任务的标记数据训练神经网络。你可以把监督学习想象成一个教室，一个老师用许多例子教学生。例如用于对象分类。</p>







<h3>2.无监督学习</h3>



<p>无监督学习是一种深度学习技术，用于发现数据中的隐含模式，而无需对标记数据进行显式训练。与监督学习不同，它不需要注释和反馈回路来进行训练。例如用于聚类。</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-full is-resized"><img data-attachment-id="62561" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_18" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_18.png?fit=701%2C299&amp;ssl=1" data-orig-size="701,299" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_18" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_18.png?fit=300%2C128&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_18.png?fit=701%2C299&amp;ssl=1" src="../Images/e87779e01b41b42120cd9929ea50337e.png" alt="Unsupervised Learning" class="wp-image-62561 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_18.png?resize=767%2C327&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_18.png?resize=767%2C327&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62561" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_18" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_18.png?fit=701%2C299&amp;ssl=1" data-orig-size="701,299" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_18" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_18.png?fit=300%2C128&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_18.png?fit=701%2C299&amp;ssl=1" src="../Images/e87779e01b41b42120cd9929ea50337e.png" alt="Unsupervised Learning" class="wp-image-62561" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_18.png?resize=767%2C327&amp;ssl=1"/></noscript><figcaption><em>Unsupervised learning | <a href="https://web.archive.org/web/20220926104145/https://data-flair.training/blogs/types-of-machine-learning-algorithms/">Source</a></em></figcaption></figure></div>



<h3>3.半监督学习</h3>



<p>半监督学习是一种机器学习方法，其中我们有输入数据，输入数据的一部分被标记为输出。它是监督学习和非监督学习的混合。</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-full is-resized"><img data-attachment-id="62577" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_2" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_2.png?fit=700%2C448&amp;ssl=1" data-orig-size="700,448" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_2" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_2.png?fit=300%2C192&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_2.png?fit=700%2C448&amp;ssl=1" src="../Images/0f039216184f2b33ec9cc9b728f872ab.png" alt="Semi-supervised learning" class="wp-image-62577 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_2.png?resize=793%2C506&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_2.png?resize=793%2C506&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62577" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_2" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_2.png?fit=700%2C448&amp;ssl=1" data-orig-size="700,448" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_2" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_2.png?fit=300%2C192&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_2.png?fit=700%2C448&amp;ssl=1" src="../Images/0f039216184f2b33ec9cc9b728f872ab.png" alt="Semi-supervised learning" class="wp-image-62577" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_2.png?resize=793%2C506&amp;ssl=1"/></noscript><figcaption><em>Semi-supervised learning | <a href="https://web.archive.org/web/20220926104145/https://medium.com/enjoy-algorithm/supervised-unsupervised-and-semi-supervised-learning-64ee79b17d10" target="_blank" rel="noreferrer noopener nofollow"> Source</a></em></figcaption></figure></div>



<p>在我们只有少量标记数据点来训练模型的情况下，半监督学习可能是有用的。训练过程可以使用一小块已标记的数据，并对数据集的其余部分进行伪标记。</p>



<p>例如，一个学生被老师教了几个问题，他必须自己想出其余问题的答案。</p>



<h3>4.强化学习</h3>



<p>强化学习<strong> <em> </em> </strong>是一种利用奖励反馈策略训练AI智能体在特定情境下学习环境行为的方法。</p>



<p>举个例子:把它想象成一个在游戏中努力赢得舞台的孩子。</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-full"><img data-attachment-id="62572" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_7" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_7.png?fit=700%2C420&amp;ssl=1" data-orig-size="700,420" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_7" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_7.png?fit=300%2C180&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_7.png?fit=700%2C420&amp;ssl=1" src="../Images/e993bc162eef02eb310c08bac0c9f9e1.png" alt="Reinforcement learning process" class="wp-image-62572 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_7.png?resize=700%2C420&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_7.png?resize=700%2C420&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62572" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_7" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_7.png?fit=700%2C420&amp;ssl=1" data-orig-size="700,420" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_7" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_7.png?fit=300%2C180&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_7.png?fit=700%2C420&amp;ssl=1" src="../Images/e993bc162eef02eb310c08bac0c9f9e1.png" alt="Reinforcement learning process" class="wp-image-62572" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_7.png?resize=700%2C420&amp;ssl=1"/></noscript><figcaption><em>Reinforcement learning process | <a href="https://web.archive.org/web/20220926104145/https://blog.clairvoyantsoft.com/reinforcement-learning-the-third-paradigm-of-machine-learning-1d7f61af9be4" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<h2>什么是自我监督学习？</h2>



<p>自我监督学习是一个机器学习过程，其中模型训练自己从输入的另一部分学习输入的一部分。它也被称为预测学习或借口学习。</p>



<p>在这个过程中，通过自动生成标签将无监督问题转化为有监督问题。为了利用大量的未标记数据，设置正确的学习目标以从数据本身获得监督是至关重要的。</p>



<p>自我监督学习方法的过程是从输入的任何未隐藏部分中识别输入的任何隐藏部分。</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img data-attachment-id="62570" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_9" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_9.png?fit=1628%2C792&amp;ssl=1" data-orig-size="1628,792" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_9" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_9.png?fit=300%2C146&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_9.png?fit=1024%2C498&amp;ssl=1" src="../Images/e411fe804130568e1d13d3aa85a0dca8.png" alt="Self-supervised learning" class="wp-image-62570 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_9.png?resize=768%2C373&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_9.png?resize=768%2C373&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62570" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_9" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_9.png?fit=1628%2C792&amp;ssl=1" data-orig-size="1628,792" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_9" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_9.png?fit=300%2C146&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_9.png?fit=1024%2C498&amp;ssl=1" src="../Images/e411fe804130568e1d13d3aa85a0dca8.png" alt="Self-supervised learning" class="wp-image-62570" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_9.png?resize=768%2C373&amp;ssl=1"/></noscript><figcaption><em>Self-supervised learning | <a href="https://web.archive.org/web/20220926104145/https://www.youtube.com/watch?v=7I0Qt7GALVk" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p>例如，在自然语言处理中，如果我们有几个单词，使用自我监督学习我们可以完成句子的其余部分。同样，在视频中，我们可以根据可用的视频数据预测过去或未来的帧。自我监督学习使用数据的结构来利用大型数据集的各种监督信号——所有这些都不依赖于标签。</p>



<h2>自我监督学习和无监督学习有什么区别？</h2>



<p>许多人混淆了这两个术语，并互换使用。然而，这两种学习技巧有不同的目标。</p>



<p>自监督学习和无监督学习方法可以被认为是互补的学习技术，因为两者都不需要标记数据集。无监督学习可以被认为是自监督学习的超集，因为它没有任何反馈循环。相反，自我监督学习有许多监督信号，在训练过程中起反馈作用。</p>



<p>一种更简单的说法是,“无监督”学习技术非常关注模型而不是数据，而“自我监督学习”技术的工作方式正好相反。然而，无监督学习方法擅长聚类和降维，而自监督学习是用于回归和分类任务的借口方法。</p>



<h2>为什么我们需要自我监督学习？</h2>



<p>自我监督学习的出现是因为在其他学习过程中持续存在以下问题:</p>


<div class="custom-point-list">
<ul><li><strong>高成本</strong>:大部分学习方法都需要标注数据。就时间和金钱而言，高质量标记数据的成本非常高。</li><li><strong>漫长的生命周期:</strong>数据准备生命周期是开发ML模型的漫长过程。它需要根据培训框架进行清理、过滤、注释、审查和重组。</li><li><strong> Generic AI: </strong>自我监督学习框架离将人类认知嵌入机器又近了一步。</li></ul>
</div>


<p>现在让我们来谈谈自我监督学习在不同领域的效用。</p>



<h2>自监督学习在计算机视觉中的应用</h2>



<p>多年来，计算机视觉中学习方法的焦点一直是朝着完善模型架构和假设我们拥有高质量数据的方向发展。然而，在现实中，如果没有高成本的时间和精力，很难获得高质量的图像数据，从而导致次优的训练模型。</p>



<p>最近，研究重点的很大一部分已经放在开发跨不同应用的计算机视觉中的自监督方法上。用未标记的数据训练模型的能力加快了整体训练过程，并使模型能够在不引入标记偏差的情况下学习潜在的语义特征。</p>



<p>为了训练自监督模型，主要有两个阶段:</p>





<p>我们用于预训练的任务被称为借口任务。借口任务(也称为监督任务)的目的是指导模型学习数据的中间表示。它有助于理解潜在的结构意义，这对实际的下游任务是有益的。</p>



<p>生成模型可以被认为是自我监督的模型，但是具有不同的目标。例如，在GANs中，它们用于为鉴别器生成逼真的图像，而自我监督训练的目的是识别可用于各种任务的良好特征，而不仅仅是欺骗鉴别器。</p>









<p>下游任务是托词模型到具体任务的<strong> <em>知识转移过程</em> </strong>。向下游任务提供更少量的标记数据。</p>



<p>视觉领域中的下游任务也称为目标任务，可以是对象识别、对象分类、对象再识别等。在托词模型上做了微调。</p>







<p>研究人员已经为使用SSL方法训练不同的基于图像的任务提出了许多想法。</p>



<h3>补丁定位</h3>



<p><strong>目的:</strong>托辞任务的目的是使用自我监督学习来识别图像中不同斑块之间的关系。</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-full is-resized"><img data-attachment-id="62558" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_21" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_21.png?fit=620%2C575&amp;ssl=1" data-orig-size="620,575" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_21" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_21.png?fit=300%2C278&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_21.png?fit=620%2C575&amp;ssl=1" src="../Images/93d8759929ccc0bf06fe62e148f95a63.png" alt="Patch localization in image" class="wp-image-62558 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_21.png?resize=561%2C520&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_21.png?resize=561%2C520&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62558" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_21" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_21.png?fit=620%2C575&amp;ssl=1" data-orig-size="620,575" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_21" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_21.png?fit=300%2C278&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_21.png?fit=620%2C575&amp;ssl=1" src="../Images/93d8759929ccc0bf06fe62e148f95a63.png" alt="Patch localization in image" class="wp-image-62558" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_21.png?resize=561%2C520&amp;ssl=1"/></noscript><figcaption><em>Patch localization in image | <a href="https://web.archive.org/web/20220926104145/https://arxiv.org/pdf/1505.05192.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p><strong>训练算法</strong> <a href="https://web.archive.org/web/20220926104145/https://arxiv.org/pdf/1505.05192.pdf" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong></a><strong/></p>


<div class="custom-point-list">
<ol><li>从图像中随机抽取一块样本。</li><li>最近邻:假设第一个面片位于3×3网格的中间，第二个面片从其8个相邻位置采样。</li><li>引入增强功能，如补片之间的间隙、<a href="https://web.archive.org/web/20220926104145/https://en.wikipedia.org/wiki/Chromatic_aberration" target="_blank" rel="noreferrer noopener nofollow">色差</a>、补片的下采样和上采样，以处理像素化和色彩抖动。这有助于模型不过度拟合某些低电平信号。</li><li>该任务的目的是识别8个相邻位置中的哪一个是第二块。该任务被设计成一个超过8类的分类问题。</li></ol>
</div>


<p>在完成托词任务时，重要的是要确保它不是在学习与全局模式下的高级潜在特征相比微不足道的模式。例如，像面片之间的边界纹理这样的低级线索可以被认为是微不足道的特征。然而，对于某些图像，存在一个微不足道的解决方案。这是由于相机镜头效应造成的，称为色差，色差是由于不同波长的光的焦点不同而产生的。</p>



<p>卷积神经网络能够通过检测品红色(蓝色+红色)和绿色之间的差异来学习补丁的相对位置。最近邻实验证明，很少有小块从绝对相同的位置提取区域，因为小块显示相似的像差。</p>



<h3>上下文感知像素预测</h3>



<p><strong>目的:</strong>使用编码器-解码器基于图像的整体上下文来预测图像中未知块的像素值。</p>



<p><strong>训练算法</strong> <a href="https://web.archive.org/web/20220926104145/https://arxiv.org/pdf/1604.07379.pdf" target="_blank" rel="noreferrer noopener nofollow"> <strong>论文</strong></a><strong/></p>


<div class="custom-point-list">
<ol><li>使用普通的编码器-解码器架构来训练托词任务。</li><li>编码器(<a href="https://web.archive.org/web/20220926104145/https://arxiv.org/abs/1604.07379" target="_blank" rel="noreferrer noopener nofollow"> Pathak等人，2016 </a>)使用具有涂黑区域的输入图像产生图像的潜在特征表示。</li><li>解码器使用来自编码器的潜在特征表示，并使用重建损失(MSE)来估计丢失的图像区域。</li><li>编码器和解码器之间的通道式全连接层允许解码器中的每个单元对整个图像内容进行推理。</li></ol>
</div>


<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img data-attachment-id="62559" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_20" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_20.png?fit=1380%2C682&amp;ssl=1" data-orig-size="1380,682" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_20" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_20.png?fit=300%2C148&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_20.png?fit=1024%2C506&amp;ssl=1" src="../Images/ab9de4ab458a858471ead4ba90c5a47c.png" alt="Context encoder architecture" class="wp-image-62559 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_20.png?resize=801%2C396&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_20.png?resize=801%2C396&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62559" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_20" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_20.png?fit=1380%2C682&amp;ssl=1" data-orig-size="1380,682" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_20" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_20.png?fit=300%2C148&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_20.png?fit=1024%2C506&amp;ssl=1" src="../Images/ab9de4ab458a858471ead4ba90c5a47c.png" alt="Context encoder architecture" class="wp-image-62559" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_20.png?resize=801%2C396&amp;ssl=1"/></noscript><figcaption><em>Context encoder architecture | <a href="https://web.archive.org/web/20220926104145/https://arxiv.org/pdf/1604.07379.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p><strong>损失函数</strong></p>



<p>训练中使用的损失函数是<strong> <em>重建</em> </strong>损失和<strong> <em>对抗</em> </strong>损失。</p>



<p><strong>重建损失</strong></p>


<div class="custom-point-list">
<ul><li>重建(L2)损失负责捕获相对于完整图像背景的显著特征。</li><li>重建损失定义为输入图像的<em>归一化屏蔽距离x </em>。<ul><li><strong> M </strong>:对应于被去除图像区域的二进制掩码，对于输入像素值为0，不考虑像素时为1。</li><li><strong> F: </strong>产生编码器输出的函数</li></ul></li></ul>
</div>






<p><strong>敌对损失</strong></p>


<div class="custom-point-list">
<ul><li>对手损失被建模以使预测看起来真实，并学习它被训练的输入数据的潜在空间。</li><li>因为鉴别器D能够利用修补区域和原始上下文中的永久不连续性，所以只有生成器G针对输入掩码进行调节。</li></ul>
</div>






<p><strong>关节损失</strong></p>


<div class="custom-point-list">
<ul><li>联合损失是通过结合重建和对抗损失发展起来的</li><li>然而，在实验中，作者意识到，只有在不利损失的情况下，修复效果最好。</li></ul>
</div>






<p>语义修复是使用SSL方法通过辅助监督和学习强特征表示来实现的。早在2016年，这篇论文就是使用SSL方法训练竞争形象模型的早期先驱之一。</p>






<h2>自监督学习在自然语言处理中的应用</h2>



<p>在SSL成为主流计算机视觉研究的一部分之前，SSL已经在自然语言处理(NLP)领域取得了巨大的进步。从文档处理应用程序、文本建议、句子完成等等，语言模型几乎无处不在。</p>



<p>然而，自从2013年发表了革新NLP领域的<strong> <em> Word2Vec </em> </strong>论文以来，这些模型的学习能力已经发生了演变。单词嵌入方法的想法很简单:我们不需要一个模型来预测下一个单词，我们可以让它根据之前的上下文来预测下一个单词。</p>



<p>由于这些进步，我们能够通过单词嵌入的分布来获得有意义的表示，这可以用于许多场景，如句子完成、单词预测等。如今，NLP中最流行的SSL方法之一是<strong> <em> BERT。</em>T3】</strong></p>



<p>在过去的十年中，NLP领域的研究和开发不断涌现。让我们在下面提取一些重要的。</p>



<h3>下一句预测</h3>



<p>在下一句预测(NSP)中，我们从一个文档中选取两个同时出现的句子，并从相同或不同的文档中随机选取一个句子，比如句子A、句子B和句子c。然后我们询问模型句子A相对于句子B的相对位置？–并且模型输出IsNextSentence或IsNotNextSentence。我们对所有组合都这样做。</p>



<p><strong>考虑以下场景:</strong></p>


<div class="custom-point-list">
<ol><li>放学后，迈克回家了。</li><li>将近50年后，载人登月任务终于开始了。</li><li>一回到家，迈克看着网飞放松。</li></ol>
</div>


<p>如果我们让一个人重新排列任何两个符合我们逻辑理解的句子，他们很可能会选择<strong> <em>句子1 </em> </strong>，然后是<strong> <em>句子3 </em> </strong>。</p>



<p>这个模型的主要目的是基于长期的上下文依赖来预测句子。</p>



<p><a href="https://web.archive.org/web/20220926104145/https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noreferrer noopener nofollow"> <strong> <em>来自变形金刚的双向编码器表示(BERT) </em> </strong> </a>谷歌人工智能团队的研究人员发表的一篇论文已经成为自然语言推理(MNLI)、问答(SQuAD)等几项NLP任务的黄金标准。</p>



<p>对于这样的下游任务，BERT提供了一种很好的方法来捕捉句子之间的关系，这是通过其他语言建模技术不可能实现的。下面是下一句预测的工作原理。</p>






<div class="custom-point-list">
<ol><li>为了让BERT处理各种下游任务，输入表示能够明确地表示一对在单个序列中打包在一起的句子。“序列”是指BERT的输入令牌序列。</li></ol>
</div>

<div class="custom-point-list">
<ol start="2"><li>每个序列的第一个标记总是一个特殊的分类标记([CLS])。对应于该令牌的最终隐藏状态被用作分类任务的聚集序列表示。</li></ol>
</div>

<div class="custom-point-list">
<ol start="3"><li>我们用两种方法区分句子。首先，我们用一个特殊的标记([SEP])将它们分开。第二，我们向每个标记添加一个学习嵌入，指示它是属于句子A还是句子b。</li></ol>
</div>

<div class="custom-point-list">
<ol start="4"><li>我们将输入嵌入表示为E，将特殊[CLS]标记的最终隐藏向量表示为C，将第I个<sup>输入标记的最终隐藏向量表示为T<sub>I .</sub>该向量C用于下一句预测(NSP)</sup></li></ol>
</div>


<p><strong>这个任务可以从下面的例子来理解:</strong></p>







<p>如果你想利用BERT模型来完成这个任务，你可以参考拥抱脸文档。</p>



<h3>自回归语言建模</h3>



<p>虽然像BERT这样的自动编码模型利用自监督学习来完成像句子分类(next或not)这样的任务，但自监督方法的另一个应用是文本生成领域。</p>



<p>像GPT(预训练生成转换器)这样的自回归模型是在经典的语言建模任务中预训练的——在阅读了所有前面的单词后预测下一个单词。这种模型对应于变压器的解码器部分，并且在整个句子的顶部使用了一个遮罩，以便注意力只能够看到文本中之前的内容，而不是之后的内容。</p>



<p>让我们通过查看GPT的训练框架来更深入地了解这些模型是如何工作的。</p>



<p>培训程序包括两个阶段:</p>


<div class="custom-point-list">
<ol><li><strong>无监督预训练</strong></li></ol>
</div>


<p>第一阶段是在大型文本语料库上学习高容量语言模型。</p>



<p>给定一个无监督的记号集U = {u <sub> 1 </sub>，。。。，u <sub> n </sub> }，我们使用标准语言建模目标来最大化以下可能性:</p>







<p>其中k是上下文窗口的大小，并且条件概率P使用具有参数θ的神经网络来建模。使用随机梯度下降来训练这些参数。</p>



<p>这里训练的模型是语言模型的多层transformer解码器，它是transformer的变体。该模型在输入上下文标记上应用多头自关注操作，随后是逐位置前馈层，以产生目标标记上的输出分布:</p>







<p>其中U =(U<sub>—k</sub>，。。。，u<sub>—1</sub>是记号的上下文向量，n是层数，W <sub> e </sub>是记号嵌入矩阵，W <sub> p </sub>是位置嵌入矩阵。这种受约束的自我关注(每个标记都可以关注其左侧的上下文)将自我监督的方法带入了画面。</p>








<p>在这一步中，我们假设一个带标签的数据集C，其中每个实例由一系列输入标记x <sup> 1 </sup>组成。。。输入通过我们预先训练的模型，以获得最终变压器块的激活h<sup>m</sup>T6】l，然后将其馈送到添加的线性输出层，该输出层具有参数W <sub> y </sub>以预测y:</p>







<p>这为我们提供了以下最大化目标:</p>







<p>将语言建模作为微调的辅助目标有助于学习——提高监督模型的泛化能力，加速收敛。具体来说，我们优化以下目标(权重为λ):</p>







<p>总的来说，在微调过程中，我们需要的唯一额外参数是W <sub> y </sub>，以及分隔符标记的嵌入。</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img data-attachment-id="62562" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_17" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_17.png?fit=1086%2C438&amp;ssl=1" data-orig-size="1086,438" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_17" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_17.png?fit=300%2C121&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_17.png?fit=1024%2C413&amp;ssl=1" src="../Images/e53ab999edc5f44b4c380b4cd7e2e755.png" alt="(left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks" class="wp-image-62562 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_17.png?resize=854%2C344&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_17.png?resize=854%2C344&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62562" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_17" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_17.png?fit=1086%2C438&amp;ssl=1" data-orig-size="1086,438" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_17" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_17.png?fit=300%2C121&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_17.png?fit=1024%2C413&amp;ssl=1" src="../Images/e53ab999edc5f44b4c380b4cd7e2e755.png" alt="(left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks" class="wp-image-62562" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_17.png?resize=854%2C344&amp;ssl=1"/></noscript><figcaption><em>(left) Transformer architecture and training objectives used in this work <br/>(right) Input transformations for fine-tuning on different tasks | <a href="https://web.archive.org/web/20220926104145/https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p>在上图中，左边是Transformer架构和培训目标，右边是针对不同任务进行微调的输入转换。我们将所有结构化输入转换为令牌序列，由我们预先训练的模型进行处理，然后是线性+softmax层。对于不同的任务，需要不同的处理，就像对于文本蕴涵，我们连接前提(p)，包含文本和假设(h)，包含文本，标记序列，中间有分隔符标记($)。</p>



<p>对最初的GPT模型进行了多次改进，要了解如何将它用于您自己的用例，您可以参考此<a href="https://web.archive.org/web/20220926104145/https://huggingface.co/gpt2" target="_blank" rel="noreferrer noopener nofollow">页</a>。</p>






<h2>自我监督学习应用:工业案例研究</h2>



<p>到目前为止，我们已经讨论了如何使用自我监督的方法来训练流行的模型，以及如何自己训练或使用可用库中的模型。</p>



<p><strong>现在，让我们来看看业界是如何利用这项技术来解决关键问题的。</strong></p>



<h3>1.脸书的仇恨言论检测</h3>



<blockquote class="wp-block-quote"><p>“我们认为，自我监督学习(SSL)是在人工智能系统中建立背景知识和近似常识形式的最有前途的方法之一。”</p><cite><a href="https://web.archive.org/web/20220926104145/https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/" target="_blank" rel="noreferrer noopener nofollow">AI Scientists</a>, Facebook</cite></blockquote>



<p>脸书不仅通过基础、开放的科学研究在许多领域推进自我监督学习技术<a href="https://web.archive.org/web/20220926104145/https://ai.facebook.com/blog/advances-in-content-understanding-self-supervision-to-protect-people/" target="_blank" rel="noreferrer noopener nofollow"/>,而且他们还将这一前沿工作应用于生产，以快速提高其产品中内容理解系统的准确性，从而确保人们在其平台上的安全。</p>



<p>一个这样的例子是XLM，脸书人工智能的跨多种语言训练语言系统的方法，不依赖手动标记的数据集来提高仇恨言论检测。</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img data-attachment-id="62571" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_8" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_8.png?fit=1920%2C653&amp;ssl=1" data-orig-size="1920,653" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_8" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_8.png?fit=300%2C102&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_8.png?fit=1024%2C348&amp;ssl=1" src="../Images/988f75cf36496100ed3acde1bf531d67.png" alt="Hate-speech detection at Facebook" class="wp-image-62571 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_8.png?resize=823%2C279&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_8.png?resize=823%2C279&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62571" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_8" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_8.png?fit=1920%2C653&amp;ssl=1" data-orig-size="1920,653" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_8" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_8.png?fit=300%2C102&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_8.png?fit=1024%2C348&amp;ssl=1" src="../Images/988f75cf36496100ed3acde1bf531d67.png" alt="Hate-speech detection at Facebook" class="wp-image-62571" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_8.png?resize=823%2C279&amp;ssl=1"/></noscript><figcaption><em>Hate-speech detection at Facebook | <a href="https://web.archive.org/web/20220926104145/https://ai.facebook.com/blog/how-ai-is-getting-better-at-detecting-hate-speech/" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p>这种自我监督学习的应用使得他们的模型更加健壮，他们的平台更加安全。让我们简单谈谈XLM是什么，以及它是如何做出如此改变的。</p>



<h4>XLM</h4>



<h5><strong> </strong>型号</h5>



<p>它是一个基于Transformers的架构，使用三个语言建模目标之一进行预训练:</p>


<div class="custom-point-list">
<ol><li><strong>随意语言建模(CLM): </strong>根据句子中前面的单词，对一个单词的概率进行建模，即P(w <sub> t </sub> |w <sub> 1 </sub>，。。。，w<sub>t1</sub>，θ)。</li></ol>
</div>

<div class="custom-point-list">
<ol start="2"><li>掩蔽顾岚<strong>年龄建模(MLM):</strong>BERT的掩蔽语言建模目标，即使用[MASK]关键字掩蔽随机选择的标记，并尝试预测它们。</li></ol>
</div>

<div class="custom-point-list">
<ol start="3"><li><strong>翻译语言建模(TLM):</strong>MLM的新增和扩展，它不考虑单语文本流，而是连接平行句子，如下图所示。源句子和目标句子中的单词都被屏蔽。为了预测英语句子中隐藏的单词，模型可以关注周围的英语单词或法语翻译，鼓励模型对齐英语和法语表示。如果英语上下文不足以推断被屏蔽的英语单词，该模型还可以利用法语上下文。</li></ol>
</div>


<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img data-attachment-id="62569" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_10" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_10.png?fit=1600%2C870&amp;ssl=1" data-orig-size="1600,870" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_10" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_10.png?fit=300%2C163&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_10.png?fit=1024%2C557&amp;ssl=1" src="../Images/ed1a7cc0945c6dacca0ff07a8e97713e.png" alt="Cross-lingual language model pretraining" class="wp-image-62569 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_10.png?resize=755%2C410&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_10.png?resize=755%2C410&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62569" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_10" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_10.png?fit=1600%2C870&amp;ssl=1" data-orig-size="1600,870" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_10" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_10.png?fit=300%2C163&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_10.png?fit=1024%2C557&amp;ssl=1" src="../Images/ed1a7cc0945c6dacca0ff07a8e97713e.png" alt="Cross-lingual language model pretraining" class="wp-image-62569" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_10.png?resize=755%2C410&amp;ssl=1"/></noscript><figcaption><em>Cross-lingual language model pertaining | <a href="https://web.archive.org/web/20220926104145/https://arxiv.org/pdf/1901.07291.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p>因此，XLM是一个跨语言的语言模型，其预训练可以在CLM、MLM或MLM与TLM结合使用的情况下进行。现在，让我们来看看XLM带来的好处。</p>



<p><strong>性能分析</strong></p>


<div class="custom-point-list">
<ol><li><strong>跨语言分类</strong></li></ol>
</div>


<p>XLM为零射击跨语言分类提供了更好的句子编码器初始化，并能够通过MLM方法在相同的句子编码器上获得71.5%的准确度，从而实现最先进的(SOTA)性能。结合MLM和TLM将性能进一步提高到75.1%。</p>


<div class="custom-point-list">
<ol start="2"><li><strong>机器翻译系统</strong></li></ol>
</div>


<p>类似于第一点，它提供了监督和非监督神经机器翻译系统的更好的初始化。具有MLM目标的预训练显示了在无监督系统的情况下的显著改进，而相同的目标导致了在有监督系统中的SOTA性能，BLEU得分为38.5。</p>


<div class="custom-point-list">
<ol start="3"><li><strong>低资源语言的语言模型</strong></li></ol>
</div>


<p>对于资源较少的语言来说，利用类似但资源较多的语言中的数据通常是有益的，尤其是当它们共享很大一部分词汇表时。发现XLM通过利用来自印地语(一种相对流行的具有大量资源的语言)的信息来改进尼泊尔语语言模型(一种低资源语言)，因为它们共享相同的Devnagari文字。</p>


<div class="custom-point-list">
<ol start="4"><li><strong>无监督的跨语言单词嵌入</strong></li></ol>
</div>


<p>XLM在跨语言单词嵌入方面优于先前的工作，在源单词和它们的翻译之间达到了0.69的SOTA水平皮尔逊相关分数。</p>



<p>有了这样的进步，XLM确实在自然语言处理方面有所作为。</p>



<h3>2.谷歌的医学影像分析模型</h3>



<p>在医学领域，训练深度学习模型一直是一项困难的任务，因为标记的数据有限，而且标注这些数据既耗时又昂贵。为了解决这个问题，谷歌的研究团队引入了一种新的多实例对比学习(MICLe)方法，该方法使用每个患者病例的潜在病理的多个图像，为自我监督学习构建更多信息的阳性对。</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img data-attachment-id="62566" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_13" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_13.png?fit=1092%2C1328&amp;ssl=1" data-orig-size="1092,1328" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_13" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_13.png?fit=247%2C300&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_13.png?fit=842%2C1024&amp;ssl=1" src="../Images/d1e9c54d3819d36ef128fdaf7d8b766d.png" alt="Google’s medical imaging analysis model" class="wp-image-62566 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_13.png?resize=568%2C690&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_13.png?resize=568%2C690&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62566" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_13" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_13.png?fit=1092%2C1328&amp;ssl=1" data-orig-size="1092,1328" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_13" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_13.png?fit=247%2C300&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_13.png?fit=842%2C1024&amp;ssl=1" src="../Images/d1e9c54d3819d36ef128fdaf7d8b766d.png" alt="Google’s medical imaging analysis model" class="wp-image-62566" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_13.png?resize=568%2C690&amp;ssl=1"/></noscript><figcaption><em>Google’s medical imaging analysis model | <a href="https://web.archive.org/web/20220926104145/https://arxiv.org/pdf/2101.05224.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p>关于图示的方法，需要记住几件事:</p>


<div class="custom-point-list">
<ul><li>第一步是使用SimCLR进行的，这是Google为图像的自我监督表示学习设计的另一个框架。我们将很快讨论它。</li><li>与步骤(1)不同，步骤(2)和(3)是特定于任务和数据集的。</li></ul>
</div>


<p>所以还是一步一步来。</p>



<h4>步骤1:sim clr框架</h4>



<p>它代表了视觉表征对比学习的一个简单框架，极大地推动了自监督和半监督学习的发展，并在有限的类别标记数据下实现了图像分类的新纪录。</p>


<div class="custom-point-list">
<ul><li>SimCLR首先在未标记的数据集上学习图像的通用表示，然后可以用少量的标记图像进行微调，以实现给定分类任务的良好性能(就像医学成像任务一样)。</li></ul>
</div>

<div class="custom-point-list">
<ul><li>遵循一种称为对比学习的方法<em>，通过同时最大化同一图像的不同变换视图之间的一致性和最小化不同图像的变换视图之间的一致性来学习通用表示。</em>使用这种对比目标来更新神经网络的参数使得对应视图的表示彼此“吸引”,而不对应视图的表示彼此“排斥”。</li></ul>
</div>

<div class="custom-point-list">
<ul><li>首先，SimCLR从原始数据集中随机抽取示例，使用简单扩充的组合将每个示例转换两次，创建两组对应的视图。</li></ul>
</div>

<div class="custom-point-list">
<ul><li>然后，它使用基于ResNet架构的CNN来计算图像表示。</li></ul>
</div>

<div class="custom-point-list">
<ul><li>最后，SimCLR使用全连接网络(即，MLP)计算图像表示的非线性投影，这放大了不变特征并最大化了网络识别同一图像的不同变换的能力。</li></ul>
</div>






<p>经训练的模型不仅在识别同一图像的不同变换方面做得很好，而且还学习相似概念的表示(例如，椅子对狗)，这些概念稍后可以通过微调与标签相关联。</p>



<h4>米克尔</h4>



<p>在用SimCLR对未标记的自然图像完成初始预训练之后，训练该模型以捕捉医学图像数据集的特殊特征。这也可以用SimCLR来完成，但是这种方法只能通过增强来构建阳性对，而不能轻易地利用患者的元数据来构建阳性对。因此这里使用MICLe。</p>


<div class="custom-point-list">
<ul><li>给定给定患者病例的多个图像，MICLe通过从来自同一患者病例的两个不同图像中绘制两个裁剪来构建用于自我监督对比学习的正对。这种图像可以从不同的视角拍摄，并显示具有相同潜在病理的不同身体部位。</li></ul>
</div>

<div class="custom-point-list">
<ul><li>这为自监督学习算法提供了一个很好的机会，以直接方式学习对视点、成像条件和其他混淆因素的变化具有鲁棒性的表示。</li></ul>
</div>






<h4>第三步:微调</h4>


<div class="custom-point-list">
<ul><li>该模型在微调期间被端到端地训练，使用预训练网络的权重作为下游监督任务数据集的初始化。</li><li>对于微调期间的数据增强，在两个任务(皮肤病学和胸部x光)中对图像进行了随机颜色增强、带大小调整的裁剪、模糊、旋转和翻转。</li><li>对于预训练策略和下游微调任务的每个组合，执行广泛的超参数搜索。</li></ul>
</div>


<h4>技术性能分析</h4>


<div class="custom-point-list">
<ol><li>自监督学习利用未标记的特定领域医学图像，并且显著优于监督ImageNet预训练。</li></ol>
</div>


<div class="wp-block-image is-style-default"><figure class="aligncenter size-full is-resized"><img data-attachment-id="62557" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_22" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_22.png?fit=640%2C323&amp;ssl=1" data-orig-size="640,323" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_22" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_22.png?fit=300%2C151&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_22.png?fit=640%2C323&amp;ssl=1" src="../Images/3ab8efd93960c760224a510c99e60f8d.png" alt="Comparison of supervised and self-supervised pre-training, followed by supervised fine-tuning using two architectures on dermatology and chest X-ray classification." class="wp-image-62557 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_22.png?resize=633%2C319&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_22.png?resize=633%2C319&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62557" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_22" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_22.png?fit=640%2C323&amp;ssl=1" data-orig-size="640,323" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_22" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_22.png?fit=300%2C151&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_22.png?fit=640%2C323&amp;ssl=1" src="../Images/3ab8efd93960c760224a510c99e60f8d.png" alt="Comparison of supervised and self-supervised pre-training, followed by supervised fine-tuning using two architectures on dermatology and chest X-ray classification." class="wp-image-62557" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_22.png?resize=633%2C319&amp;ssl=1"/></noscript><figcaption><em>Comparison of supervised and self-supervised pre-training, followed by supervised fine-tuning using two architectures on dermatology and chest X-ray classification | <a href="https://web.archive.org/web/20220926104145/https://ai.googleblog.com/2021/10/self-supervised-learning-advances.html" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<div class="custom-point-list">
<ol start="2"><li>自我监督预训练模型可以更好地概括分布变化，其中最小预训练导致最大增益。这是一个有价值的发现，因为分布转移下的泛化对临床应用至关重要。</li></ol>
</div>


<div class="wp-block-image is-style-default"><figure class="aligncenter size-full"><img data-attachment-id="62578" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_1" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_1.png?fit=658%2C422&amp;ssl=1" data-orig-size="658,422" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_1" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_1.png?fit=300%2C192&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_1.png?fit=658%2C422&amp;ssl=1" src="../Images/accd55fa769812bd3229f8805eabf4d6.png" alt="Evaluation of models on distribution-shifted datasets" class="wp-image-62578 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_1.png?resize=658%2C422&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_1.png?resize=658%2C422&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62578" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_1" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_1.png?fit=658%2C422&amp;ssl=1" data-orig-size="658,422" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_1" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_1.png?fit=300%2C192&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_1.png?fit=658%2C422&amp;ssl=1" src="../Images/accd55fa769812bd3229f8805eabf4d6.png" alt="Evaluation of models on distribution-shifted datasets" class="wp-image-62578" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_1.png?resize=658%2C422&amp;ssl=1"/></noscript><figcaption><em>Evaluation of models on distribution-shifted datasets | <a href="https://web.archive.org/web/20220926104145/https://arxiv.org/pdf/2101.05224.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<div class="custom-point-list">
<ol start="3"><li>使用自监督模型的预训练可以补偿医学图像分类的低标签效率，并且在采样的标签部分中，自监督模型始终优于监督基线。事实上，MICLe仅使用ResNet-50 (4x)的20%的训练数据和ResNet152 (2x)的30%的训练数据就能够匹配基线。</li></ol>
</div>


<div class="wp-block-image is-style-default"><figure class="aligncenter size-full"><img data-attachment-id="62560" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_19" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_19.png?fit=652%2C736&amp;ssl=1" data-orig-size="652,736" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_19" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_19.png?fit=266%2C300&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_19.png?fit=652%2C736&amp;ssl=1" src="../Images/445682417f7fa47d8114d892bba43519.png" alt="Top-1 accuracy for dermatology condition classification for MICLe, SimCLR, and supervised models under different unlabeled pretraining dataset and varied sizes of label fractions" class="wp-image-62560 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_19.png?resize=652%2C736&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_19.png?resize=652%2C736&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="62560" data-permalink="https://web.archive.org/web/20220926104145/https://neptune.ai/self-supervised-learning-and-its-applications_19" data-orig-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_19.png?fit=652%2C736&amp;ssl=1" data-orig-size="652,736" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Self-Supervised Learning and Its Applications_19" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_19.png?fit=266%2C300&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926104145/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_19.png?fit=652%2C736&amp;ssl=1" src="../Images/445682417f7fa47d8114d892bba43519.png" alt="Top-1 accuracy for dermatology condition classification for MICLe, SimCLR, and supervised models under different unlabeled pretraining dataset and varied sizes of label fractions" class="wp-image-62560" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926104145im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Self-Supervised-Learning-and-Its-Applications_19.png?resize=652%2C736&amp;ssl=1"/></noscript><figcaption><em>Top-1 accuracy for dermatology condition classification for MICLe, SimCLR, and supervised models under different unlabeled pretraining dataset and varied sizes of label fractions | <a href="https://web.archive.org/web/20220926104145/https://arxiv.org/pdf/2101.05224.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<h2>自我监督学习的挑战</h2>



<p>到目前为止，我们已经讨论了自我监督学习如何在机器学习社区的几乎每个领域取得进展，但它也有一些缺点。自我监督学习试图实现“一种方法解决所有问题”的方法，但它远没有实现。SSL领域的一些关键挑战是:</p>


<div class="custom-point-list">
<ul><li><strong>准确性:</strong>虽然SSL技术的前提是不使用带标签的数据，但这种方法的缺点是您要么需要大量数据来生成准确的伪标签，要么会牺牲准确性。需要注意的是，在初始步骤中进行训练时，生成的不准确标签会产生反作用。</li></ul>
</div>

<div class="custom-point-list">
<ul><li><strong>计算效率:</strong>由于多阶段训练(1。生成伪标签2。在伪标签上训练)与监督学习相比，训练模型所花费的时间较高。此外，当前的SSL方法需要大量的数据来实现接近监督学习的准确性。</li></ul>
</div>

<div class="custom-point-list">
<ul><li><strong>托辞任务:</strong>为你的用例选择合适的托辞任务非常重要。例如，如果您选择autoencoder作为您的托词任务，其中图像被压缩，然后重新生成，它也会试图模仿原始图像的噪声，如果您的任务是生成高质量的图像，这种托词任务将弊大于利。</li></ul>
</div>


<h2>关键要点</h2>



<p>在本文中，我们了解了什么是自我监督学习，为什么它越来越受欢迎，以及与它相关的风险和挑战是什么。我们还讨论了使用这种方法训练的流行模型，并深入探讨了大型科技公司如何利用自我监督学习来解决一些真正紧迫的问题。</p>



<p>总结一下我们目前所学的知识:</p>


<div class="custom-point-list">
<ul><li>在我们处理与数据相关的挑战的用例中，自我监督学习是一种福气。从用于数据集准备的资源不足到耗时的标注问题，不一而足。</li><li>另一个好处是下游任务，即迁移学习。模型可以在未标记的数据集上以自我监督的方式进行预训练，然后可以针对特定的用例进行进一步的微调。</li><li>作为前两点的结果，如果你想建立一个可扩展的ML模型，自我监督学习显然是一种可行的方法。</li><li>然而与此同时，人们必须意识到使用这种方法所附带的条件。</li></ul>
</div>


<p>虽然我们试图在本文中涵盖很多内容，但显然我们讨论的内容并不详尽。关于自我监督学习还有很多东西要学。如果您想了解更多有关其当前和潜在使用案例的信息，您可以参考以下资料:</p>





<p>快乐学习！</p>



<h3>参考</h3>






<div id="author-box-new-format-block_622f274a1284f" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">德瓦尔·沙阿</h3>
    
          <p class="article__authorContent-text">对智能软件系统很好奇。作为我日常生活的一部分，我编写程序，写关于人工智能的新研究趋势，阅读关于技术、文化、政治和体育的书籍。</p>
    
          
    
  </div>
</div>



<div id="author-box-new-format-block_6228b534b8857" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">阿布舍克·贾</h3>
    
          <p class="article__authorContent-text">一个好奇的家伙，目前正在建造模型，希望有一天能建造天网。跟随这个空间，学习未被理清的数据科学概念，并站在未来的正确一边！</p>
    
          
    
  </div>
</div>


<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>



<p class="has-text-color"><strong>阅读下一篇</strong></p>



<h2>如何构建和管理自然语言处理(NLP)项目</h2>



<p class="has-small-font-size">Dhruvil Karani |发布于2020年10月12日</p>


<p id="block_5ffc75def9f8e" class="separator separator-10"/>



<p>如果说我在ML行业工作中学到了什么的话，那就是:<strong>机器学习项目很乱。</strong></p>



<p>这并不是说人们不想把事情组织起来，只是在项目过程中有很多事情很难组织和管理。</p>



<p>你可以从头开始，但有些事情会阻碍你。</p>



<p>一些典型的原因是:</p>


<div class="custom-point-list">
<ul><li>笔记本中的快速数据探索，</li><li>取自github上的研究报告的模型代码，</li><li>当一切都已设置好时，添加新的数据集，</li><li>发现了数据质量问题并且需要重新标记数据，</li><li>团队中的某个人“只是快速地尝试了一些东西”,并且在没有告诉任何人的情况下改变了训练参数(通过argparse传递),</li><li>从高层推动将原型转化为产品“仅此一次”。</li></ul>
</div>


<p>多年来，作为一名机器学习工程师，我学到了一堆<strong>东西，它们可以帮助你保持在事物的顶端，并检查你的NLP项目</strong>(就像你真的可以检查ML项目一样:)。</p>



<p>在这篇文章中，我将分享我在从事各种数据科学项目时学到的关键指针、指南、技巧和诀窍。许多东西在任何ML项目中都是有价值的，但有些是NLP特有的。</p>


<a class="button continous-post blue-filled" href="/web/20220926104145/https://neptune.ai/blog/how-to-structure-and-manage-nlp-projects-templates" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>