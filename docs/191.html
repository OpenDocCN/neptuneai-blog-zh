<html>
<head>
<title>Cross-Entropy Loss and Its Applications in Deep Learning </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>交叉熵损失及其在深度学习中的应用</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>在21世纪，大多数企业都在使用机器学习和深度学习来自动化他们的流程、决策、提高疾病检测的效率等。公司如何优化这些模型？他们如何确定模型的效率？评估模型效率的一种方法是<strong>准确性。</strong>精度越高，模型效率越高。因此，有必要通过优化模型来提高精度；通过应用<a href="https://web.archive.org/web/20221206001915/https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23" target="_blank" rel="noreferrer noopener nofollow"> <strong>损失函数</strong> </a> <strong>。</strong>在这篇文章中，我们学习下面的内容，更侧重于<a href="https://web.archive.org/web/20221206001915/https://machinelearningmastery.com/cross-entropy-for-machine-learning/" target="_blank" rel="noreferrer noopener nofollow"> <strong>交叉熵函数</strong> </a>。</p>



<ul><li>什么是损失函数？</li><li>离散损失函数和连续损失函数的区别。</li><li>交叉熵损失函数。(在二分类和多类分类中，理解交叉熵公式)</li><li>在深度学习框架中应用交叉熵；PyTorch和TensorFlow。</li></ul>



<h2 id="h-loss-function">损失函数</h2>



<p>在大多数情况下，误差函数和损失函数的意思是相同的，但有微小的区别。</p>



<blockquote class="wp-block-quote"><p>一个<strong>误差函数</strong>测量/计算我们的模型偏离正确预测的程度。<br/>一个<strong>损失函数</strong>对误差进行运算，以量化特定大小/方向的误差有多糟糕，该误差受到导致不正确预测的负面后果的影响。</p></blockquote>



<p>损失函数可以是离散的，也可以是连续的。</p>







<h3>连续和离散误差/损失函数</h3>



<p>我们将用两个例子来理解连续和离散损失函数。</p>



<p><em> <strong>插图1 </strong> </em></p>



<p>想象一下，你想在一个阴天从一座大山的山顶上下来。你是如何选择正确的方向一直走到底部的？</p>



<p>你将不得不考虑所有可能的方向，并选择一个使你下降最多的方向<em> </em>。你朝着选定的方向前进，从而降低高度，重复同样的过程，总是降低<em>高度</em>，直到你到达你的目标=山脚。<br/></p>







<div class="is-layout-flex wp-container-6 wp-block-columns">




<div class="is-layout-flow wp-block-column">
<p>注意，我们用高度来衡量我们离底部有多远。降低<em>高度</em>意味着我们离目标更近了。我们可以将高度称为误差函数(测量/计算我们离底部有多远)。</p>
</div>
</div>



<p><em> <strong>插图2 </strong> </em></p>



<p>让我们看另一个例子。哪个误差函数适合解决下面的问题？蓝点代表通过考试的学生，而红点代表失败的学生。</p>



<div class="is-layout-flex wp-container-9 wp-block-columns">




<div class="is-layout-flow wp-block-column">
<p>我们开发了一个模型来预测学生是不及格还是及格。下图中的线条代表模型预测。</p>
</div>
</div>



<div class="is-layout-flex wp-container-12 wp-block-columns">




<div class="is-layout-flow wp-block-column">
<p>一个红点在蓝色区域，一个蓝点在红色区域，这意味着预测线导致2个错误。</p>



<p>你如何解决这个错误？</p>
</div>
</div>



<div class="is-layout-flex wp-container-15 wp-block-columns">




<div class="is-layout-flow wp-block-column">
<p>为了解决这个错误，我们移动这条线以确保所有的正面和负面预测都在正确的区域。</p>
</div>
</div>



<p>在大多数现实生活中的机器学习应用程序中，我们很少像上面那样对预测线进行如此剧烈的移动。我们应用小步骤来最小化误差。如果我们在上面的例子中移动小步长，我们可能会以相同的误差结束，这就是<strong>离散误差函数</strong>的情况。</p>



<p>然而，在<strong>图1 </strong>中，由于山的坡度不同，我们可以检测到我们高度的微小变化(误差)并采取必要的措施，这是<strong>连续误差函数</strong>的情况。</p>



<h4>Sigmoid函数</h4>



<p>为了将误差函数从离散型转换为连续型误差函数，我们需要对每个学生的线性得分值应用一个激活函数，这将在后面讨论。</p>



<p>比如图2  中的<strong> <em>，模型预测输出决定一个学生会及格还是不及格；这个模型回答了这个问题，学生A会通过SAT考试吗？</em></strong></p>



<p>一个连续的问题是，学生A通过SAT考试的可能性有多大？答案是30%或70%等等。，有可能。</p>



<p>我们如何确保我们的模型预测输出在(0，1)范围内或连续？我们对每个学生的线性分数应用一个激活函数。我们的例子是我们所说的二进制分类，其中有两类，要么通过，要么失败。在这种情况下，应用的激活函数被称为<strong>s形激活函数。</strong></p>



<p>通过以上做法，错误不再是两个学生没有通过SAT考试，而是学生每个错误的总和。</p>



<p>在图2中使用概率可以更容易地对每个学生的误差(他们离及格有多远)求和，从而更容易地逐步移动预测线，直到我们获得最小的求和误差。</p>







<p>下面的公式表示sigmoid函数(<strong> x </strong>是每个点的值):</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/bd56292312fccc59320dfdd9c8d588d1.png" alt="" class="wp-image-42895" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-1.png?ssl=1"/></figure>



<h4>Softmax函数</h4>



<p>在最后几段中，我们发现sigmoid激活对二进制分类问题有效。有两个以上的类要分类的情况会怎样？如下图所示:</p>







<p>我们试图回答的问题是:颜色是蓝色、绿色还是红色？</p>



<p>在这种情况下，回答不是是/否，而是两者之一(绿色、蓝色或红色)</p>



<p>如何将响应从(蓝色、绿色和红色)转换成可能的颜色(绿色/红色/蓝色)？</p>



<p>在深度学习中，该模型将线性回归应用于每个输入，即输入特征的线性组合，并且表示为:</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/64f1651dc3327ce0385654773087646e.png" alt="" class="wp-image-42898" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-2.png?ssl=1"/></figure>



<p>您可以查看线性回归的基础知识以获得更多理解。</p>



<p>假设线性回归函数根据类别/输入参数/特征给出以下分数:</p>



<p>蓝色= 2，绿色= 1，红色= -1</p>



<p>获得概率的最简单方法是:</p>







<p>上述转换将适用于正分数。如果有负分怎么办，记住，概率一定在0-1之间？比如红色班，分数为负；我们如何将分数转化为正值？</p>



<p>我们在所有分数上使用<strong>指数</strong>:</p>



<div class="is-layout-flex wp-container-23 wp-block-columns">
<div class="is-layout-flow wp-block-column">
<p>蓝色=</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/5d60943c594fa4f41f7ae09e7025a376.png" alt="" class="wp-image-42915" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-6.png?ssl=1"/></figure>



<p>=0.705</p>
</div>



<div class="is-layout-flow wp-block-column">
<p>绿色=</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/4454e581b1299bae216d1ce104eff3bb.png" alt="" class="wp-image-42917" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-7.png?ssl=1"/></figure>



<p>=0.259</p>
</div>



<div class="is-layout-flow wp-block-column">
<p>红色=</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/8e16a4ee4785ee21c1c5ebe68fe410a7.png" alt="" class="wp-image-42918" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-8.png?ssl=1"/></figure>



<p>=0.035</p>
</div>
</div>



<p>指数将概率转换为0-1的范围</p>



<p>我们有<strong> n个类，</strong>我们要找出<strong> <em> </em> </strong> <strong>个类的概率x </strong>会，用线性得分<strong> A1，A2… An，</strong>来计算每个类的概率。</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/83e749162c44bfafe44066e6563e53c5.png" alt="" class="wp-image-42921" title="f of i equals the fraction with numerator e raised to the A i power and denominator the sum from n equals i to c of e raised to the A n power" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-9.png?ssl=1"/></figure>



<p>上面的函数是<strong> softmax激活函数，</strong>其中<strong> i </strong>是类名。</p>



<p>为了理解交叉熵，有必要讨论损失函数和激活函数，即将离散预测转化为连续预测。我们现在将深入交叉熵函数。</p>



<h2 id="h-cross-entropy">交叉熵</h2>



<p>克劳德·香农在他1948年的论文《交流的数学理论》中引入了信息熵的概念。根据香农的说法，随机变量的<strong>熵</strong>是变量可能结果中固有的“信息”、“惊喜”或“不确定性”的平均水平。<br/> <br/>听起来熟悉吗？</p>



<p>我们可以看到，随机变量的熵与我们引入的概念的误差函数有关。不确定度的平均水平指的是误差。</p>



<p><strong> <em>交叉熵</em> </strong>建立在信息论熵的思想上，测量给定随机变量/事件集的两个概率分布之间的差异。</p>



<p>交叉熵可以应用于二分类和多分类问题。我们将讨论在每种情况下使用交叉熵的区别。</p>



<h3>二元交叉熵</h3>



<p>让我们考虑一下前面的例子，我们回答一个学生是否会通过SAT考试。在这种情况下，我们和四个学生一起工作。我们有两个模型，A和B，预测这四个学生通过考试的可能性，如下图所示。</p>



<p><strong>注。</strong> <em>前面我们讨论过</em> <em>“在深度学习中，模型对每个输入应用线性回归，即输入特征的线性组合。</em></p>



<p>每个模型将<em>线性回归函数(f(x) = wx + b) </em>应用于每个学生，以生成线性分数。然后使用<em> sigmoid函数</em>将线性得分转换为概率。让我们假设这两个模型给出了图表的概率，其中蓝色区域表示通过，而红色区域表示失败。</p>



<div class="is-layout-flex wp-container-26 wp-block-columns">
<div class="is-layout-flow wp-block-column">
<p>模型A</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/2c8ed0fe7562092b550b7daf6d47233f.png" alt="Binary cross-entropy model " class="wp-image-42876" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Binary-cross-entropy-model-1.png?ssl=1"/></figure>
</div>



<div class="is-layout-flow wp-block-column">
<p>模型B</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/95c840bbefd384ce02facf3797d8bfce.png" alt="Binary cross-entropy model " class="wp-image-42877" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Binary-cross-entropy-model-2.png?ssl=1"/></figure>
</div>
</div>



<p>上图显示，模型B比模型A执行得更好，因为它正确地对各自区域的所有学生进行了分类。所有概率的乘积决定了模型的最大似然。</p>



<p><strong>乘积概率</strong> : <em>同时发生的两个(或多个)独立事件的概率是通过乘以事件各自的概率来计算的。</em></p>



<p>我们想通过乘以每个独立学生的概率来计算模型的总概率。</p>



<div class="is-layout-flex wp-container-29 wp-block-columns">
<div class="is-layout-flow wp-block-column">
<p>产品概率模型A:</p>



<p>0.1 * 0.7 * 0.6 * 0.2   = 0.0084</p>
</div>



<div class="is-layout-flow wp-block-column">
<p>产品概率模型B:</p>



<p>0.8 * 0.6 * 0.7 * 0.9 =  0.3024</p>
</div>
</div>



<p>模型B的产品概率优于模型a。</p>



<p><em>产品概率</em>在我们有几个项目要预测时效果更好，但现实生活中的模型预测却不是这样。</p>



<p>例如，如果我们有一个满是1000名学生的班级，不管你的模型有多好，乘积概率总是接近于0。如果我们也改变一个概率，产品会发生剧烈变化，给人一种模型表现良好的错误印象。因此，我们需要使用<strong>对数函数将乘积转换为总和。</strong></p>



<p>测井模型A:</p>



<p>对数(0.1) +对数(0.7) +对数(0.6) +对数(0.2)</p>



<p>-1  +  -0.154    +   -0.221     + -0.698   = -2.073</p>



<p>测井模型B:</p>



<p>对数(0.8) +对数(0.6) +对数(0.7) +对数(0.9)</p>



<p> -0.09 + -0.22 + -0.15  + -0.045 = -0.505</p>



<p>介于0和1之间的数的对数总是负数。以上是评价我们模型性能的更好方法吗？不完全是。相反，我们将采用预测概率的负对数。</p>



<p>负对数模型A:</p>



<p>-对数(0.1)+-对数(0.7)+-对数(0.6)+-对数(0.2)</p>



<p>1  +  0.154    +   0.221     + 0.698 = 2.073</p>



<p>负对数模型B:</p>



<p>-对数(0.8) +-对数(0.6)+-对数(0.7)+-对数(0.9)</p>



<p> 0.09 + 0.22 + 0.15  + 0.045 =  0.505</p>



<p><strong>交叉熵损失</strong>是<strong> </strong> <em>每个学生预测概率的负对数之和。</em> <strong> <em> </em> </strong> <em>模型A的交叉熵损失为2.073；B型的是0.505。交叉熵很好地衡量了每个模型的有效性。</em></p>



<h4>二元交叉熵公式</h4>



<p>在我们的四个学生预测中——模型B:</p>




<table class="tg">
<thead>
  <tr>
    <th class="tg-j0s4">A</th>
    <th class="tg-j0s4">B</th>
    <th class="tg-j0s4">C</th>
    <th class="tg-j0s4">D</th>
    <th class="tg-j0s4">可能性</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-d5c0">P1=0.8(失败)</td>
    <td class="tg-d5c0">P2=0.6(失败)</td>
    <td class="tg-d5c0">P3=0.7(通过)</td>
    <td class="tg-d5c0">P4=0.9(通过)</td>
    <td class="tg-d5c0">通过概率</td>
  </tr>
  <tr>
    <td class="tg-d5c0">1–P1</td>
    <td class="tg-d5c0">1–P2</td>
    <td class="tg-d5c0">P3</td>
    <td class="tg-d5c0">P4</td>
    <td class="tg-d5c0">如果学生通过else 0，则yi = 1，因此:</td>
  </tr>
  <tr>
    <td class="tg-d5c0" colspan="5">y1= 0</td>
  </tr>
  <tr>
    <td class="tg-d5c0">y2 = 0</td>
    <td class="tg-d5c0">y3 = 1</td>
    <td class="tg-d5c0">y4 = 1</td>
    <td class="tg-d5c0">学生C的交叉熵:</td>
    <td class="tg-d5c0">蓝色代表学生证。红色代表学生失败。</td>
  </tr>
</tbody>
</table>



<p id="separator-block_60632b935146c" class="block-separator block-separator--10"><strong>学生A的交叉熵</strong></p>



<p>请注意，我们使用每个学生的预测概率来计算交叉熵。我们将纳入公式，包括概率是如何产生的。之前，我们讨论了二分类中使用的sigmoid激活函数，用于将线性函数得分转换为概率。这是使用激活的交叉熵函数:</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/fd175fc5ab82045c9895a3fefebfff7d.png" alt="" class="wp-image-42922" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-10.png?ssl=1"/></figure>



<p>其中:</p>



<p><strong>S</strong><strong><sub>I</sub></strong>–输入/权重</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/afd82ce366fae0a39f3646d8cc566332.png" alt="" class="wp-image-42923" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-11.png?ssl=1"/></figure>



<p><strong>f</strong>–这种情况下的激活功能，</p>







<p><strong>t</strong>–目标预测</p>



<ul><li><strong>I—</strong>要预测的类。</li><li><strong> BCE </strong> =</li><li>多类交叉熵/分类交叉熵</li><li>对于多类分类问题，我们使用多类交叉熵。假设我们需要创建一个模型来预测水果的类型/种类。我们有三种不同容器的水果(橘子、苹果、柠檬)。</li></ul>



<p><span>水果</span></p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/866338ba391578fcdbcacc52e17a59d8.png" alt="" class="wp-image-42925" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-12.png?ssl=1"/></figure>



<h3><span>容器一</span> <br/> <span>概率</span></h3>



<p><span>容器B </span> <br/> <span>概率</span></p>




<table class="tg">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
  <tr>
    <th class="tg-q6j4"><span>容器C </span> <br/> <span>概率</span></th>
    <th class="tg-q6j4"><span>橙色</span></th>
    <th class="tg-q6j4"><span> 0.7 </span></th>
    <th class="tg-q6j4"><span> 0.3 </span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-gf8u"><span> 0.1 </span></td>
    <td class="tg-gf8u"><span>苹果</span></td>
    <td class="tg-gf8u"><span> 0.2 </span></td>
    <td class="tg-gf8u"><span> 0.4 </span></td>
  </tr>
  <tr>
    <td class="tg-gf8u"><span> 0.5 </span></td>
    <td class="tg-gf8u"><span>柠檬</span></td>
    <td class="tg-gf8u"><span> 0.1 </span></td>
    <td class="tg-gf8u"><span> 0.3 </span></td>
  </tr>
  <tr>
    <td class="tg-gf8u"><span> 0.4 </span></td>
    <td class="tg-gf8u">每个容器的概率总和需要为1。</td>
    <td class="tg-gf8u"><span>容器A </span></td>
    <td class="tg-gf8u"><span>容器B </span></td>
  </tr>
</tbody>
</table>



<p id="separator-block_6063233a51469" class="block-separator block-separator--10"><span>集装箱C </span></p>



<p><span>正确的水果放在</span> <br/> <span>各自的容器里</span></p>




<table class="tg">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
  <tr>
    <th class="tg-q6j4"><span>橙子</span></th>
    <th class="tg-q6j4"><span>柠檬</span></th>
    <th class="tg-q6j4"><span>柠檬</span></th>
    <th class="tg-q6j4"><span>预测的概率</span> <br/> <span>证明水果是正确的</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-gf8u"><span> 0.7 </span></td>
    <td class="tg-gf8u"><span> 0.3 </span></td>
    <td class="tg-gf8u"><span> 0.4 </span></td>
    <td class="tg-gf8u">产品概率= 0.7 * 0.3 * 0.4 = 0.084</td>
  </tr>
  <tr>
    <td class="tg-gf8u">交叉熵=-log(0.7)+–log(0.3)+-log(0.4)= 1.073</td>
    <td class="tg-gf8u">多类交叉熵公式</td>
    <td class="tg-gf8u">让我们将概率值指定为变量:</td>
    <td class="tg-gf8u"><span>水果</span></td>
  </tr>
</tbody>
</table>



<p id="separator-block_606323b45146a" class="block-separator block-separator--10"><span>容器一</span> <br/> <span>概率</span></p>



<p><span>容器B </span> <br/> <span>概率</span></p>



<p><span>容器C </span> <br/> <span>概率</span></p>



<h4><span>橙色</span></h4>



<p><span>P1</span>T2一</p>




<table class="tg">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
  <tr>
    <th class="tg-q6j4"><span>P1</span>T2 B</th>
    <th class="tg-q6j4"><span>P1</span>T2【C3】</th>
    <th class="tg-q6j4"><span>苹果</span></th>
    <th class="tg-q6j4"><span>p2</span>T2</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-gf8u"><span>p2</span>T2 B</td>
    <td class="tg-gf8u"><span>P2</span>T2【C3】</td>
    <td class="tg-gf8u"><span>柠檬</span></td>
    <td class="tg-gf8u"><span>P3</span>T2</td>
  </tr>
  <tr>
    <td class="tg-gf8u"><span>P3</span>T2 B</td>
    <td class="tg-gf8u"><span>P3</span>T2【C3】</td>
    <td class="tg-gf8u">容器A中的橙子、苹果或柠檬的概率是多少？我们分别有<em> 0.7、0.2和0.1 </em>。</td>
    <td class="tg-gf8u">如果容器A包含特定的水果，则其y1值等于1；否则为0。</td>
  </tr>
  <tr>
    <td class="tg-gf8u">y1 <sub> A，–</sub>如果是橙子</td>
    <td class="tg-gf8u">y2<sub>A–</sub>如果是苹果</td>
    <td class="tg-gf8u">y3<sub>A—</sub>如果是柠檬。</td>
    <td class="tg-gf8u">容器A的交叉熵:</td>
  </tr>
</tbody>
</table>



<p id="separator-block_606324e55146b" class="block-separator block-separator--10">容器B的交叉熵:</p>



<p>容器C的交叉熵:</p>



<p>设我们的类(1，2，3)等于I，容器(A，B，C)等于j。</p>



<ul><li>交叉熵容器A:</li><li>交叉熵容器B:</li><li>交叉熵容器C:</li></ul>



<p>在总交叉熵损失中，我们的类由I定义；因此，我们可以将(y1，y2，y3)等同于I:</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/7bcd01a3ec93913342a67738faea489c.png" alt="" class="wp-image-42926" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-13.png?ssl=1"/></figure>



<p><strong> <em>总交叉熵</em> </strong>:</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/28fbd564dd5bdc815fe748490232a666.png" alt="" class="wp-image-42928" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-14.png?ssl=1"/></figure>



<p>我们使用<em>总交叉熵</em>公式计算多类分类中的交叉熵。</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/be5af5d0a5bf77160b95bd7ef121e6e1.png" alt="" class="wp-image-42929" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-15.png?ssl=1"/></figure>



<p>整合激活功能:</p>



<p><strong> <em>多类交叉熵</em> </strong></p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/8bd9f97ec7bd24e2413832500a811950.png" alt="" class="wp-image-42930" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-16.png?ssl=1"/></figure>



<p><strong> CE= </strong></p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/b3629fee912cb92383dd571d344eb9a8.png" alt="" class="wp-image-42932" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-17.png?ssl=1"/></figure>



<p>如何应用交叉熵？</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/844c02c85c0c180857ae0c8bdca06d7c.png" alt="" class="wp-image-42933" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-18.png?ssl=1"/></figure>



<p>我们已经讨论了交叉熵损失用于二分类和多分类。让我们看看如何应用交叉熵的例子:</p>



<p>PyTorch</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/e034a38756fc1d9a2a91427df02f2305.png" alt="" class="wp-image-42934" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-19.png?ssl=1"/></figure>



<p><strong> 1。使用Pytorch </strong>简单说明二元交叉熵</p>



<p>请确保您安装了PyTorch遵循此处的指南<a href="https://web.archive.org/web/20221206001915/https://pytorch.org/get-started/locally/" target="_blank" rel="noreferrer noopener nofollow">。</a></p>







<p>使用PyTorch <a href="https://web.archive.org/web/20221206001915/https://pytorch.org/docs/stable/generated/torch.randn.html" target="_blank" rel="noreferrer noopener nofollow"> random </a>生成输入特征(X)和标签(y)值。</p>



<p>让我们来看看X的值:</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/782930fac62a9a928fb2ed157c965bc3.png" alt="" class="wp-image-42936" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206001915im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Cross-entropy-equation-20.png?ssl=1"/></figure>



<h2 id="h-how-to-apply-cross-entropy">Y的值:</h2>



<p>在我们的讨论中，我们使用<strong> sigmoid函数</strong>作为输入的激活函数。我们将把PyTorch <a href="https://web.archive.org/web/20221206001915/https://pytorch.org/docs/stable/generated/torch.sigmoid.html" target="_blank" rel="noreferrer noopener nofollow"> sigmoid模块</a>传递给我们的输入(X)特性。</p>



<h3><a href="https://web.archive.org/web/20221206001915/https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html" target="_blank" rel="noreferrer noopener nofollow"> Pytorch二元交叉熵损失</a>:</h3>



<p><strong> 2。使用Pytorch的分类交叉熵</strong></p>



<p>PyTorch分类<a href="https://web.archive.org/web/20221206001915/https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" target="_blank" rel="noreferrer noopener nofollow">交叉熵</a>模块，softmax激活函数已经应用于公式。因此，我们不会像在前面的例子中那样使用激活函数。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn</pre>



<p>我们仍然使用PyTorch随机数来生成输入要素(X)和标注(y)值。</p>



<pre class="hljs">X = torch.randn(<span class="hljs-number">10</span>)
y = torch.randint(<span class="hljs-number">2</span>, (<span class="hljs-number">10</span>,), dtype=torch.float)</pre>



<p>由于这是一个多类问题，输入要素有五个类(class_0，class_1，class_2，class_3，class_4)</p>



<pre class="hljs">print(X)</pre>



<pre class="hljs">tensor([ <span class="hljs-number">0.0421</span>, <span class="hljs-number">-0.6606</span>,  <span class="hljs-number">0.6276</span>,  <span class="hljs-number">1.2491</span>, <span class="hljs-number">-1.1535</span>, <span class="hljs-number">-1.4137</span>,  <span class="hljs-number">0.8967</span>, <span class="hljs-number">-1.1786</span>,
        <span class="hljs-number">-1.3214</span>,  <span class="hljs-number">0.2828</span>])</pre>



<p>多类交叉熵计算如下:</p>



<pre class="hljs">print(y)</pre>



<pre class="hljs">tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>])</pre>



<p>跨不同深度学习框架计算交叉熵是一样的；让我们看看如何在TensorFlow中实现同样的功能。</p>



<pre class="hljs">X_continous_values = torch.sigmoid(X)
print(X_continous_values)
</pre>



<pre class="hljs">tensor([<span class="hljs-number">0.5105</span>, <span class="hljs-number">0.3406</span>, <span class="hljs-number">0.6519</span>, <span class="hljs-number">0.7772</span>, <span class="hljs-number">0.2398</span>, <span class="hljs-number">0.1957</span>, <span class="hljs-number">0.7103</span>, <span class="hljs-number">0.2353</span>, <span class="hljs-number">0.2106</span>,
        <span class="hljs-number">0.5702</span>])</pre>



<p>海王星与<a href="https://web.archive.org/web/20221206001915/https://docs.neptune.ai/essentials/integrations/deep-learning-frameworks/pytorch" target="_blank" rel="noreferrer noopener"> PyTorch </a>的融合</p>



<pre class="hljs">loss = nn.BCELoss()(X_continous_values, y)
print(loss)
</pre>



<pre class="hljs">tensor(<span class="hljs-number">1.0966</span>)</pre>



<p>TensorFlow</p>



<p><strong> 1。二元交叉熵:</strong></p>



<p>假设我们的实际值和预测值如下:</p>



<p>使用tensor flow<a href="https://web.archive.org/web/20221206001915/https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy" target="_blank" rel="noreferrer noopener nofollow">BinaryCrossentropy()</a>模块:</p>



<pre class="hljs">X = torch.randn(<span class="hljs-number">10</span>, <span class="hljs-number">5</span>)
print(X)
</pre>



<pre class="hljs">tensor([[<span class="hljs-number">-0.5698</span>, <span class="hljs-number">-0.0558</span>, <span class="hljs-number">-0.2550</span>,  <span class="hljs-number">1.6812</span>,  <span class="hljs-number">0.0238</span>],
        [<span class="hljs-number">-2.1786</span>,  <span class="hljs-number">1.3923</span>, <span class="hljs-number">-0.2363</span>, <span class="hljs-number">-0.4601</span>, <span class="hljs-number">-1.4949</span>],
        [ <span class="hljs-number">1.3679</span>,  <span class="hljs-number">1.2853</span>,  <span class="hljs-number">0.4087</span>, <span class="hljs-number">-0.5743</span>, <span class="hljs-number">-0.2752</span>],
        [ <span class="hljs-number">2.1995</span>,  <span class="hljs-number">0.1469</span>, <span class="hljs-number">-0.1661</span>,  <span class="hljs-number">0.4617</span>, <span class="hljs-number">-0.4395</span>],
        [<span class="hljs-number">-0.5686</span>, <span class="hljs-number">-0.7453</span>, <span class="hljs-number">-0.1455</span>, <span class="hljs-number">-0.5304</span>,  <span class="hljs-number">0.3020</span>],
        [<span class="hljs-number">-0.1489</span>, <span class="hljs-number">-0.9143</span>, <span class="hljs-number">-1.5282</span>, <span class="hljs-number">-0.5023</span>,  <span class="hljs-number">1.2751</span>],
        [<span class="hljs-number">-1.3830</span>, <span class="hljs-number">-0.6535</span>,  <span class="hljs-number">0.5392</span>, <span class="hljs-number">-2.2050</span>, <span class="hljs-number">-1.4138</span>],
        [<span class="hljs-number">-0.5592</span>,  <span class="hljs-number">1.5028</span>,  <span class="hljs-number">0.0442</span>, <span class="hljs-number">-1.5487</span>, <span class="hljs-number">-0.1522</span>],
        [ <span class="hljs-number">0.7436</span>, <span class="hljs-number">-1.8956</span>,  <span class="hljs-number">1.0145</span>, <span class="hljs-number">-0.2974</span>, <span class="hljs-number">-2.0576</span>],
        [ <span class="hljs-number">0.1003</span>,  <span class="hljs-number">0.6604</span>, <span class="hljs-number">-1.3535</span>, <span class="hljs-number">-0.3053</span>, <span class="hljs-number">-0.4034</span>]])</pre>



<pre class="hljs">y = torch.randint(<span class="hljs-number">5</span>, (<span class="hljs-number">10</span>,))
print(y)</pre>



<pre class="hljs">tensor([<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])</pre>



<p><strong> 2。分类交叉熵</strong></p>



<pre class="hljs">loss = nn.CrossEntropyLoss()(X, y)
print(loss)
</pre>



<pre class="hljs">tensor(<span class="hljs-number">1.9732</span>)</pre>



<p>假设我们有三个类(猫、狗、熊)要预测。我们实际的形象/阶级是一只狗；因此，我们理论上有(0，1，0)。其中1表示实际图像，0表示图像不是狗。我们的价值观将是:</p>



<section id="blog-intext-cta-block_61b35c73e2992" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>假设模型预测图像有5%可能是猫，85%是狗，10%是熊。那么我们的预测值将是:</p>
    
    </section>



<h3>使用TensorFlow <a href="https://web.archive.org/web/20221206001915/https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy" target="_blank" rel="noreferrer noopener nofollow">分类交叉熵()</a>模块，我们计算损失如下:</h3>



<p>海王星与<a href="https://web.archive.org/web/20221206001915/https://docs.neptune.ai/essentials/integrations/deep-learning-frameworks/tensorflow-keras" target="_blank" rel="noreferrer noopener">张量流</a>的融合</p>



<pre class="hljs"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf</pre>



<p>结论</p>



<pre class="hljs">actual_values = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
predicted_values = [<span class="hljs-number">.5</span>, <span class="hljs-number">.7</span>, <span class="hljs-number">.2</span>, <span class="hljs-number">.3</span>, <span class="hljs-number">.5</span>, <span class="hljs-number">.6</span>]</pre>



<p>本文涵盖了损失函数的核心概念，主要是交叉熵。我希望它能让你更好地理解交叉熵，以及它是如何用于二元和多类分类问题的，并且你能够在你的案例场景中应用它。</p>



<pre class="hljs">binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()
loss = binary_cross_entropy(actual_values, predicted_values)
print(loss.numpy)</pre>



<pre class="hljs"><span class="hljs-number">0.53984624</span></pre>



<p>资源:</p>



<p>Let’s say we have three classes(cat, dog, bear) to predict. Our actual image/class is a dog; therefore, we have theoretically (0, 1, 0). Where 1 represents the actual image and 0, where the image is not a dog. Our values will be:</p>



<pre class="hljs">actual_values = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]</pre>



<p>Hypothetically the model predicts that the image is 5% likely to be a cat, 85% a dog, and 10% a bear. Then our predicted values will be:</p>



<pre class="hljs">predicted_values = [<span class="hljs-number">0.05</span>, <span class="hljs-number">0.85</span>, <span class="hljs-number">0.10</span>]</pre>



<p>Using the TensorFlow <a href="https://web.archive.org/web/20221206001915/https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy" target="_blank" rel="noreferrer noopener nofollow">Categorical Cross Entropy() </a>module, we calculate loss as follows:</p>



<pre class="hljs">loss = tf.keras.losses.CategoricalCrossentropy()
loss = loss(actual_values, predicted_values)
print(loss.numpy)
</pre>



<pre class="hljs"><span class="hljs-number">0.1625189</span></pre>



<section id="blog-intext-cta-block_61b35c91e2993" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>Neptune’s integration with <a href="https://web.archive.org/web/20221206001915/https://docs.neptune.ai/essentials/integrations/deep-learning-frameworks/tensorflow-keras" target="_blank" rel="noreferrer noopener">TensorFlow</a></p>
    
    </section>



<h2 id="h-conclusion">Conclusion</h2>



<p>This article covers the core concepts of Loss functions, mainly the Cross-Entropy. I hope it gives you a better understanding of cross-entropy and how it’s used for both binary and multi-class classification problems and that you are in a position to apply it in your case scenario.</p>



<h3>Resources:</h3>







<p/>
        </div>
        
    </div>    
</body>
</html>