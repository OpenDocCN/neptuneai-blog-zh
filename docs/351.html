<html>
<head>
<title>10 Real-Life Applications of Reinforcement Learning </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>强化学习的10个现实应用</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/reinforcement-learning-applications#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/reinforcement-learning-applications#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>在<strong>强化学习(RL) </strong>中，智能体按照<strong>奖励</strong>和<strong>惩罚</strong>机制接受训练。代理人因正确的移动而得到奖励，因错误的移动而受到惩罚。在这样做的时候，代理人试图最小化错误的移动，最大化正确的移动。</p>





<p>在本文中，我们将看看强化学习在现实世界中的一些应用。</p>







<h2 id="h-applications-in-self-driving-cars">自动驾驶汽车中的应用</h2>



<p>各种论文提出了<a href="https://web.archive.org/web/20230228161547/https://arxiv.org/pdf/2002.00444.pdf" target="_blank" rel="noreferrer noopener nofollow">深度强化学习</a>用于<strong>自动驾驶</strong>。在自动驾驶汽车中，有各种各样的方面需要考虑，例如各个地方的速度限制，可驾驶区，避免碰撞——仅举几例。</p>



<p>一些可以应用强化学习的自动驾驶任务包括轨迹优化、运动规划、动态路径、控制器优化和基于场景的高速公路学习策略。</p>



<p>比如，可以通过学习自动泊车政策来实现泊车。使用<a href="https://web.archive.org/web/20230228161547/https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56" target="_blank" rel="noreferrer noopener nofollow"> Q-Learning </a>可以实现变道，而超车可以通过学习超车策略来实现，同时避免碰撞并在其后保持稳定的速度。</p>



<p>AWS DeepRacer 是一款自主赛车，旨在在物理赛道上测试RL。它使用摄像头来可视化跑道，并使用强化学习模型来控制油门和方向。</p>





<p>Wayve.ai已经成功地将强化学习应用于训练汽车如何在一天内<strong>驾驶。</strong>他们使用深度强化学习算法来处理车道跟随任务。他们的网络架构是一个具有4个卷积层和3个全连接层的深度网络。以下示例显示了车道跟踪任务。中间的图像代表驾驶员的视角。</p>



<figure class="wp-block-video aligncenter"><video autoplay="" loop="" muted="" src="https://web.archive.org/web/20230228161547im_/https://neptune.ai/wp-content/uploads/2022/11/10-Real-Life-Applications-of-Reinforcement-Learning.mp4"/><figcaption class="wp-element-caption"><a href="https://web.archive.org/web/20230228161547/https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning/" target="_blank" rel="noreferrer noopener nofollow"><strong>Source</strong></a></figcaption></figure>







<p id="separator-block_54e14ae7492eec50bbf3d8fd1970aff2" class="block-separator block-separator--15"> </p>



<h2 id="h-industry-automation-with-reinforcement-learning">具有强化学习的工业自动化</h2>



<p>在工业强化中，基于学习的<strong>机器人</strong>被用来执行各种任务。除了这些机器人比人更有效率这一事实之外，它们还能完成对人来说很危险的任务。</p>



<p>一个很好的例子是<a href="https://web.archive.org/web/20230228161547/https://deepmind.com/blog/article/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control" target="_blank" rel="noreferrer noopener nofollow"> Deepmind使用人工智能代理来冷却谷歌数据中心</a>。这导致<strong>能源支出</strong>减少了40%。这些中心现在完全由人工智能系统控制，无需人工干预。显然仍然有来自数据中心专家的监督。该系统的工作方式如下:</p>



<ul>
<li>每五分钟从数据中心获取数据快照，并将其输入深度神经网络</li>



<li>然后，它预测不同的组合将如何影响未来的能源消耗</li>



<li>确定在维持安全标准的同时将导致最小功耗的行动</li>



<li>在数据中心发送和实施这些操作</li>
</ul>



<p>这些动作由本地控制系统验证。</p>



<h2 id="h-reinforcement-learning-applications-in-trading-and-finance">强化学习在交易和金融中的应用</h2>



<p>有监督的<strong> </strong> <a href="https://web.archive.org/web/20230228161547/https://arxiv.org/ftp/arxiv/papers/1803/1803.03916.pdf" target="_blank" rel="noreferrer noopener nofollow"> <strong>时间序列</strong> </a>模型可以用于预测未来的销售以及预测<strong>股票价格</strong>。然而，这些模型并不能决定在特定的股票价格下应该采取的行动。进入强化学习(RL)。RL代理可以决定这样的任务；是持有、买入还是卖出。RL模型使用市场基准标准进行评估，以确保其性能最佳。</p>



<p>这种自动化为流程带来了一致性，不像以前的方法，分析师必须做出每一个决策。例如，IBM 有一个复杂的基于强化学习的平台，能够进行<strong>金融交易</strong>。它根据每笔金融交易的盈亏来计算回报函数。</p>



<h2 id="h-reinforcement-learning-in-nlp-natural-language-processing">自然语言处理中的强化学习</h2>



<p>在NLP中，RL可以用于<strong>文本摘要</strong>、<strong>问答、</strong>和<strong>机器翻译</strong>等等。</p>



<p>这篇<a href="https://web.archive.org/web/20230228161547/https://homes.cs.washington.edu/~eunsol/papers/acl17eunsol.pdf" target="_blank" rel="noreferrer noopener nofollow">论文的作者</a> Eunsol Choi、Daniel Hewlett和Jakob Uszkoreit提出了一种基于强化学习的方法来回答给定的长文本问题。他们的方法是首先从文档中选择一些与回答问题相关的句子。然后用慢速RNN来回答所选的句子。</p>





<p>在本文中，监督学习和强化学习的组合被用于<a href="https://web.archive.org/web/20230228161547/https://arxiv.org/pdf/1705.04304.pdf" target="_blank" rel="noreferrer noopener nofollow">抽象文本摘要。该报由罗曼·保卢斯、蔡明·熊&amp;理查德·索歇署名。他们的目标是在较长的文档中使用注意力的、基于RNN的编码器-解码器模型时，解决在<strong>摘要</strong>中面临的问题。本文的作者提出了一种具有新型内部注意力的神经网络，该网络关注输入并分别连续产生输出。他们的训练方法是标准监督单词预测和强化学习的结合。</a></p>





<p>在机器翻译方面，来自科罗拉多大学和马里兰大学的作者提出了一种基于强化学习的同步T2机器翻译方法。这项工作有趣的地方在于，它有能力学习何时信任预测的单词，并使用RL来确定何时等待更多输入。</p>





<p>来自斯坦福大学、俄亥俄州立大学和微软研究院的研究人员已经将Deep RL用于对话生成。深度RL可以用于在<strong>聊天机器人对话中模拟未来的奖励。</strong>使用两个虚拟代理模拟对话。策略梯度方法用于奖励包含重要对话属性的序列，如连贯性、信息性和易于回答。</p>





<p>更多NLP应用可以在<a href="https://web.archive.org/web/20230228161547/https://github.com/adityathakker/awesome-rl-nlp" target="_blank" rel="noreferrer noopener nofollow">这里</a>或者<a href="https://web.archive.org/web/20230228161547/https://www.future-processing.com/blog/the-future-of-natural-language-processing/" target="_blank" rel="noreferrer noopener">这里</a>找到。</p>



<h2 id="h-reinforcement-learning-applications-in-healthcare">强化学习在医疗保健中的应用</h2>



<p>在医疗保健领域，患者可以<strong>根据从RL系统学习到的政策</strong>接受治疗。RL能够使用先前的经验找到最优策略，而不需要关于生物系统的数学模型的先前信息。这使得这种方法比医疗保健中其他基于控制的系统更适用。</p>



<p>医疗保健中的RL被归类为慢性病或重症护理、自动医疗诊断和其他一般领域中的<a href="https://web.archive.org/web/20230228161547/https://arxiv.org/pdf/1908.08796.pdf" target="_blank" rel="noreferrer noopener nofollow">动态治疗方案(DTRs) </a>。</p>





<p>在DTRs中，输入是一组对患者的临床观察和评估。输出是每个阶段的治疗方案。这些类似于RL中的状态。在DTRs中应用RL是有利的，因为它能够确定依赖于时间的决定，以便在特定时间对患者进行最佳治疗。</p>



<p><a href="https://web.archive.org/web/20230228161547/https://arxiv.org/pdf/1908.08796.pdf" target="_blank" rel="noreferrer noopener nofollow">RL在医疗保健中的使用</a>也通过考虑治疗的延迟效应而改善了长期结果。</p>



<p>RL还被用于发现和产生用于慢性疾病的最佳DTR。</p>



<p>您可以通过浏览这篇<a href="https://web.archive.org/web/20230228161547/https://arxiv.org/pdf/1908.08796.pdf" target="_blank" rel="noreferrer noopener nofollow">文章</a>来深入了解医疗保健领域的RL应用。</p>



<h2 id="h-reinforcement-learning-applications-in-engineering">强化学习在工程中的应用</h2>



<p>在工程前沿，脸书开发了<strong>开源强化学习平台</strong>——<a href="https://web.archive.org/web/20230228161547/https://engineering.fb.com/ml-applications/horizon/" target="_blank" rel="noreferrer noopener nofollow">地平线</a>。该平台使用强化学习来优化大规模生产系统。脸书在内部使用了Horizon:</p>



<ul>
<li>个性化建议</li>



<li>向用户发送更有意义的通知</li>



<li>优化视频流质量。</li>
</ul>



<p>Horizon还包含以下工作流:</p>



<ul>
<li>模拟环境</li>



<li>分布式数据预处理平台</li>



<li>在生产中培训和导出模型。</li>
</ul>



<p>视频显示中强化学习的一个经典示例是根据视频缓冲区的状态和来自其他机器学习系统的估计，为用户提供低比特率或高比特率的视频。</p>



<p>Horizon能够处理类似生产的问题，例如:</p>



<ul>
<li>大规模部署</li>



<li>特征标准化</li>



<li>分布学习</li>



<li>提供和处理包含高维数据和数千种要素类型的数据集。</li>
</ul>



<h2 id="h-reinforcement-learning-in-news-recommendation">新闻推荐中的强化学习</h2>



<p>用户偏好可能经常改变，因此基于评论和喜欢向用户推荐新闻的<a href="https://web.archive.org/web/20230228161547/http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf" target="_blank" rel="noreferrer noopener nofollow"><strong/></a>可能会很快过时。通过强化学习，RL系统可以跟踪读者的返回行为。</p>



<p>构建这样的系统将涉及获取新闻特征、读者特征、上下文特征和读者新闻特征。新闻特写包括但不限于内容、标题和出版商。阅读器功能指的是阅读器如何与内容互动，例如点击和分享。上下文特征包括新闻方面，例如新闻的时间和新鲜度。然后根据这些用户行为定义奖励。</p>



<h2 id="h-reinforcement-learning-in-gaming">游戏中的强化学习</h2>



<p>让我们来看看<strong>游戏</strong>前沿的一个应用，具体来说就是<strong> AlphaGo Zero </strong>。使用强化学习，AlphaGo Zero能够从零开始学习围棋。它通过与自己对抗来学习。经过40天的自我训练，Alpha Go Zero能够超越被称为<em>大师</em>的Alpha Go版本，该版本击败了<a href="https://web.archive.org/web/20230228161547/https://deepmind.com/alphago-china" target="_blank" rel="noreferrer noopener nofollow">世界排名第一的柯洁</a>。它只使用棋盘上的黑白棋子作为输入特征和一个单一的神经网络。依赖于单个神经网络的简单树搜索用于评估位置移动和样本移动，而不使用任何<a href="https://web.archive.org/web/20230228161547/https://en.wikipedia.org/wiki/Monte_Carlo_method" target="_blank" rel="noreferrer noopener nofollow">蒙特卡罗</a>展开。</p>



<h2 id="h-real-time-bidding-reinforcement-learning-applications-in-marketing-and-advertising">实时竞价——强化学习在营销和广告中的应用</h2>



<p>在这篇<a href="https://web.archive.org/web/20230228161547/https://arxiv.org/pdf/1802.09756.pdf" target="_blank" rel="noreferrer noopener nofollow">论文</a>中，作者提出了具有多智能体强化学习的<strong>实时竞价</strong>。使用聚类方法和给每个聚类分配一个策略投标代理来处理大量广告客户的处理。为了平衡广告商之间的竞争与合作，提出了一种分布式协调多代理竞价(DCMAB)。</p>



<p>在市场营销中，准确定位个人的能力至关重要。这是因为正确的目标显然会带来高投资回报。本文的研究基于中国最大的电子商务平台淘宝。所提出的方法优于最先进的单代理强化学习方法。</p>



<h2 id="h-reinforcement-learning-in-robotics-manipulation">机器人操作中的强化学习</h2>



<p>深度学习和强化学习<a href="https://web.archive.org/web/20230228161547/https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html" target="_blank" rel="noreferrer noopener nofollow">的使用可以训练机器人</a>，这些机器人有能力抓住各种物体——甚至是那些在训练中看不见的物体。例如，这可以用于在装配线上制造产品。</p>



<p>这是通过结合大规模分布式优化和一种叫做<a href="https://web.archive.org/web/20230228161547/https://arxiv.org/abs/1806.10293" rel="nofollow"> QT-Opt </a>的<a href="https://web.archive.org/web/20230228161547/https://en.wikipedia.org/wiki/Q-learning" target="_blank" rel="noreferrer noopener nofollow">深度Q学习</a>来实现的。QT-Opt对连续动作空间的支持使其适用于机器人问题。首先离线训练一个模型，然后在真实的机器人上进行部署和微调。</p>



<p>谷歌人工智能将这种方法应用于<strong>机器人抓取</strong>，其中7个真实世界的机器人在4个月的时间内运行了800个机器人小时。</p>





<p>在<a href="https://web.archive.org/web/20230228161547/https://www.youtube.com/watch?v=W4joe3zzglU" target="_blank" rel="noreferrer noopener nofollow">这个实验</a>中，QT-Opt方法在700次抓取之前看不到的物体的尝试中，有96%的抓取成功。Google AI之前的方法有78%的成功率。</p>



<h2 id="h-final-thoughts">最后的想法</h2>



<p>虽然强化学习仍然是一个非常活跃的研究领域，但在推进该领域并将其应用于现实生活方面已经取得了重大进展。</p>



<p>在本文中，就强化学习的应用领域而言，我们仅仅触及了皮毛。希望这能激发你的好奇心，让你更深入地探索这个领域。如果你想了解更多，看看这个<a href="https://web.archive.org/web/20230228161547/https://github.com/aikorea/awesome-rl" target="_blank" rel="noreferrer noopener nofollow">棒极了的回购</a>——没有双关语，还有<a href="https://web.archive.org/web/20230228161547/https://github.com/dennybritz/reinforcement-learning" target="_blank" rel="noreferrer noopener nofollow">这个</a>。</p>
        </div>
        
    </div>    
</body>
</html>