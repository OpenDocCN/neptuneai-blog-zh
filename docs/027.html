<html>
<head>
<title>Feature Selection Methods and How to Choose Them </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>特征选择方法以及如何选择它们</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/feature-selection-methods#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/feature-selection-methods#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>你有没有发现自己坐在屏幕前想知道什么样的功能将帮助你的机器学习模型最好地学习它的任务？我打赌你有。数据准备往往会消耗大量数据科学家和机器学习工程师的时间和精力，准备好数据以供学习算法使用是一项不小的壮举。</p>



<p>数据准备流程中的关键步骤之一是<strong>特征选择</strong>。你可能知道一句流行的谚语:垃圾进，垃圾出。你用什么来喂养你的模型至少和模型本身一样重要，如果不是更重要的话。</p>



<p>在本文中，我们将:</p>


<div class="custom-point-list">
<ul><li>在数据准备流程中，查看功能选择在其他功能相关任务中的位置</li><li>并讨论它对任何机器学习项目的成功如此重要的多种原因。</li><li>接下来，我们将回顾不同的特性选择方法，并讨论一些技巧和提示来改善它们的结果。</li><li>然后，我们将一瞥Boruta(最先进的特征选择算法)的幕后，看看一种结合不同特征选择方法的巧妙方法</li><li>我们还将了解特性选择在行业中是如何被利用的。</li></ul>
</div>


<p>让我们开始吧！</p>



<h2>什么是特征选择，什么不是？</h2>



<p>让我们从定义感兴趣的对象开始。</p>



<p>什么是特征选择？简而言之，它是选择用于训练机器学习模型的特征子集的过程。</p>



<p>这就是特征选择，但同样重要的是理解特征选择不是什么——它既不是特征提取/特征工程，也不是降维。</p>



<p>特征提取和特征工程是描述基于领域知识从现有特征创建新特征的相同过程的两个术语。这会产生比原来更多的特性，应该在特性选择之前执行。首先，我们可以进行特征提取，以得出许多潜在有用的特征，然后我们可以执行特征选择，以挑选出确实会提高模型性能的最佳子集。</p>



<p><a href="/web/20220926085303/https://neptune.ai/blog/dimensionality-reduction" target="_blank" rel="noreferrer noopener">降维</a>又是一个概念。它有点类似于特征选择，因为两者都旨在减少特征的数量。然而，它们在实现这一目标的方式上有很大的不同。虽然特征选择选择原始特征的子集来保留并丢弃其他特征，但是维度减少技术将原始特征投影到更少维度的空间上，从而创建全新的特征集。如果需要的话，降维应该在特征选择之后运行，但是在实践中，它不是这个就是那个。</p>



<p>现在我们知道了什么是特征选择，以及它如何对应于其他与特征相关的数据准备任务。但是我们为什么需要它呢？</p>



<h2>我们需要特性选择的7个原因</h2>



<p>一个流行的说法是，现代机器学习技术在没有特征选择的情况下做得很好。毕竟，一个模型应该能够知道特定的特性是无用的，并且应该关注其他的特性，对吗？</p>



<p>嗯，这个推理在某种程度上是有道理的。理论上，线性模型可以给无用的特征分配零权重，而基于树的模型应该很快学会不要对它们进行分割。然而，在实践中，当输入不相关或多余时，许多事情可能会在培训中出错——这两个术语将在后面详述。除此之外，还有许多其他原因可以解释为什么简单地将所有可用的特性都转储到模型中可能不是一个好主意。我们来看看最突出的七个。</p>



<p><strong> 1。无关和多余的特征</strong></p>



<p>有些功能可能与手头的问题无关。这意味着它们与目标变量没有关系，与模型设计要解决的任务完全无关。丢弃不相关的特征将防止模型拾取它可能携带的虚假相关性，从而避免过度拟合。</p>



<p>不过，冗余功能是另一种动物。冗余意味着两个或更多特征共享相同的信息，并且除了一个之外，所有特征都可以被安全地丢弃而不会丢失信息。请注意，在存在另一个相关特征的情况下，一个重要特征也可能是多余的。应丢弃冗余要素，因为它们可能会在训练过程中造成许多问题，例如线性模型中的多重共线性。</p>



<p><strong> 2。维度的诅咒</strong></p>



<p>特征选择技术在特征很多但训练样本很少的场景中尤其不可或缺。这种情况存在所谓的维数灾难:在一个非常高维的空间中，每个训练样本与所有其他样本相距如此之远，以至于模型无法学习任何有用的模式。解决方案是降低特征空间的维数，例如，通过特征选择。</p>



<p><strong> 3。训练时间</strong></p>



<p>功能越多，训练时间越多。这种权衡的细节取决于所使用的特定学习算法，但是在需要实时进行再训练的情况下，人们可能需要将自己限制在几个最佳特性上。</p>



<p><strong> 4。部署工作</strong></p>



<p>功能越多，机器学习系统在生产中就变得越复杂。这带来了多种风险，包括但不限于高维护成本、<a href="https://web.archive.org/web/20220926085303/https://towardsdatascience.com/8-hazards-menacing-machine-learning-systems-in-production-5c470baa0163" target="_blank" rel="noreferrer noopener nofollow">纠缠、未申报的消费者或修正级联</a>。</p>



<p><strong> 5。可解释性</strong></p>



<p>有了太多的特性，我们就失去了模型的<a href="/web/20220926085303/https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools" target="_blank" rel="noreferrer noopener">可解释性。虽然并不总是主要的建模目标，但是解释和说明模型的结果通常是重要的，并且在一些受管制的领域中，甚至可能构成法律要求。</a></p>



<p><strong> 6。奥卡姆剃刀</strong></p>



<p>根据这个所谓的节俭定律，只要性能相同，简单的模型应该比复杂的模型更受青睐。这也和机器学习工程师的克星，过度拟合有关。不太复杂的模型不太可能过度拟合数据。</p>



<p><strong> 7。数据模型兼容性</strong></p>



<p>最后，还有数据模型兼容性的问题。虽然原则上，这种方法应该是数据优先，这意味着收集和准备高质量的数据，然后选择一个适合这些数据的模型，但现实生活可能正好相反。</p>



<p>你可能试图复制一篇特定的研究论文，或者你的老板可能建议使用特定的模型。在这种模型优先的方法中，您可能被迫选择与您开始训练的模型兼容的特性。例如，许多模型无法处理数据中的缺失值。除非您<a href="https://web.archive.org/web/20220926085303/https://towardsdatascience.com/handling-missing-data-5be11eddbdd" target="_blank" rel="noreferrer noopener nofollow">非常了解您的插补方法</a>，否则您可能需要删除不完整的特征。</p>



<h2>不同的特征选择方法</h2>



<p>所有不同的特征选择方法可以分为四类，每一类都有其优点和缺点。有无监督和有监督的方法。后者可以进一步分为包装器、过滤器和嵌入式方法。让我们逐一讨论。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img data-attachment-id="71268" data-permalink="https://web.archive.org/web/20220926085303/https://neptune.ai/feature-selection-methods-1" data-orig-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-1.png?fit=1999%2C1176&amp;ssl=1" data-orig-size="1999,1176" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="feature-selection-methods-1" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-1.png?fit=300%2C176&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-1.png?fit=1024%2C602&amp;ssl=1" src="../Images/00f312942d6057f15339f251a09388e3.png" alt="Different approaches to feature selection" class="wp-image-71268 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-1.png?resize=767%2C452&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926085303im_/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-1.png?resize=767%2C452&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="71268" data-permalink="https://web.archive.org/web/20220926085303/https://neptune.ai/feature-selection-methods-1" data-orig-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-1.png?fit=1999%2C1176&amp;ssl=1" data-orig-size="1999,1176" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="feature-selection-methods-1" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-1.png?fit=300%2C176&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-1.png?fit=1024%2C602&amp;ssl=1" src="../Images/00f312942d6057f15339f251a09388e3.png" alt="Different approaches to feature selection" class="wp-image-71268" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926085303im_/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-1.png?resize=767%2C452&amp;ssl=1"/></noscript><figcaption><em>Feature selection methods | Source: author </em></figcaption></figure></div>


<h3>无监督特征选择方法</h3>



<p>就像无监督学习是在无标签数据中寻找模式的学习类型一样，无监督特征选择方法也是不利用任何标签的方法。换句话说，他们不需要访问机器学习模型的目标变量。</p>



<p>你可能会问，如果不分析一个特性与模型目标的关系，我们怎么能说它对模型不重要呢？嗯，在某些情况下，这是可能的。我们可能希望放弃以下功能:</p>


<div class="custom-point-list">
<ul><li>零或接近零的方差。(几乎)不变的特征提供的学习信息很少，因此是不相关的。</li><li>许多缺失值。虽然删除不完整的特征<a href="https://web.archive.org/web/20220926085303/https://towardsdatascience.com/handling-missing-data-5be11eddbdd" target="_blank" rel="noreferrer noopener nofollow">不是处理缺失数据的首选</a> red方式，但这通常是一个好的开始，如果缺失太多条目，这可能是唯一明智的做法，因为这些特征可能无关紧要。</li><li>高度多重共线性；多重共线性意味着不同要素之间的相关性很强，这可能表示存在冗余问题。</li></ul>
</div>


<h4>实践中的无监督方法</h4>



<p>现在让我们讨论无监督特征选择方法的实际实现。就像大多数其他机器学习任务一样，scikit-learn包，特别是其“sklearn.feature_selection”模块，可以很好地服务于特征选择。然而，在某些情况下，一个人需要接触到其他地方。在这里，以及在本文的其余部分，让我们用“x”表示一个数组或数据帧，所有潜在的特征为列，观察值为行，目标向量为“y”。</p>


<div class="custom-point-list">
<ul><li>th<em>e</em>` sk learn . feature _ selection。VarianceThreshold` transformer将默认删除所有零方差特征。我们还可以传递一个阈值作为参数，让它移除方差低于阈值的特征。</li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> VarianceThreshold


sel = VarianceThreshold(threshold=<span class="hljs-number">0.05</span>)
X_selection = sel.fit_transform(X)
</pre>


<div class="custom-point-list">
<ul><li>为了删除缺少值的列，可以在数据框上使用pandas `. dropna(axis = 1)`方法。</li></ul>
</div>


<pre class="hljs">X_selection = X.dropna(axis=<span class="hljs-number">1</span>)</pre>


<div class="custom-point-list">
<ul><li>要移除具有高度多重共线性的要素，我们首先需要测量它。一种流行的多重共线性测量方法是方差膨胀因子或VIF。它是在statsmodels包中实现的。</li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">from</span> statsmodels.stats.outliers_influence <span class="hljs-keyword">import</span> variance_inflation_factor


vif_scores = [variance_inflation_factor(X.values, feature)<span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> range(len(X.columns))]
</pre>



<p>按照惯例，VIF大于10的列被视为存在多重共线性，但如果另一个阈值看起来更合理，则可以选择该阈值。</p>



<h3>包装特征选择方法</h3>



<p>包装器方法是指一系列受监督的特征选择方法，这些方法使用模型来对不同的特征子集进行评分，以最终选择最佳的一个。每个新的子集用于训练一个模型，然后在保留集上评估该模型的性能。选择产生最佳模型性能的特征子集。包装方法的一个主要优点是，它们倾向于为特定的模型类型提供性能最佳的特性集。</p>



<p>然而，与此同时，它也有局限性。包装器方法很可能会过度适应模型类型，如果想在不同的模型中尝试它们，它们产生的特性子集可能不会通用化。</p>



<p>包装器方法的另一个显著缺点是它们需要大量的计算。它们需要训练大量的模型，这可能需要一些时间和计算能力。</p>



<p>流行的包装方法包括:</p>


<div class="custom-point-list">
<ul><li><strong>反向选择</strong>，我们从包含所有可用功能的完整模型开始。在随后的迭代中，我们一次删除一个特征，总是在模型性能度量中产生最大增益的那个，直到我们达到期望的特征数量。</li><li><strong>正向选择</strong>，反向工作:我们从一个零特征的空模型开始，一次贪婪地添加一个特征，以最大化模型的性能。</li><li><strong>递归特征消除</strong>，或RFE，精神上类似于逆向选择。它也从一个完整的模型开始，一个接一个地迭代消除特征。区别在于选择要丢弃的特征的方式。RFE并不依赖于拒不接受的模型性能指标，而是基于从模型中提取的特征重要性来做出决策。这可以是线性模型中的特征权重、基于树的模型中的杂质减少或排列重要性(适用于任何模型类型)。</li></ul>
</div>


<h4>实践中的包装方法</h4>



<p>谈到包装器方法，scikit-learn为我们提供了:</p>


<div class="custom-point-list">
<ul><li>向后和向前特征选择可以通过SequentialFeatureSelector转换器实现。例如，为了使用k-最近邻分类器作为正向选择中的评分模型，我们可以使用以下代码片段:</li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SequentialFeatureSelector

knn = KNeighborsClassifier(n_neighbors=<span class="hljs-number">3</span>)
sfs = SequentialFeatureSelector(knn, n_features_to_select=<span class="hljs-number">3</span>, direction=”forward”)
sfs.fit(X, y)
X_selection = sfs.transform(X)
</pre>


<div class="custom-point-list">
<ul><li>递归特征消除以非常相似的方式实现。下面是一个基于支持向量分类器的特征重要性实现RFE的片段。</li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE

svc = SVC(kernel=<span class="hljs-string">"linear"</span>)
rfe = RFE(svc, n_features_to_select=<span class="hljs-number">3</span>)
rfe.fit(X, y)
X_selection = rfe.transform(X)
</pre>



<h3>过滤特征选择方法</h3>



<p>受监督家族的另一个成员是过滤方法。它们可以被认为是包装器的更简单、更快速的替代品。为了评估每个特征的有用性，他们简单地分析其与模型目标的统计关系，使用诸如相关性或互信息之类的度量作为模型性能度量的代理。</p>



<p>不仅过滤方法比包装器快，而且它们更通用，因为它们是模型不可知的；他们不会过度适应任何特定的算法。它们也很容易解释:如果一个特征与目标没有统计关系，它就会被丢弃。</p>



<p>然而，另一方面，过滤方法有一个主要缺点。他们孤立地看待每个特征，评估它与目标的关系。这使得他们倾向于丢弃有用的特征，这些特征本身是目标的弱预测器，但是当与其他特征结合时，为模型增加了很多价值。</p>



<h4>实践中的过滤方法</h4>



<p>现在让我们来看看如何实现各种过滤方法。这些将需要更多的粘合代码来实现。首先，我们需要计算每个特征和目标之间的期望相关性度量。然后，我们将根据结果对所有特征进行排序，并保留所需数量(前K个或前30%)的相关性最强的特征。幸运的是，scikit-learn提供了一些实用程序来帮助这一努力。</p>


<div class="custom-point-list">
<ul><li>为了保留与目标具有最强Pearson相关性的前2个特征，我们可以运行:</li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> r_regression, SelectKBest

X_selection = SelectKBest(r_regression, k=<span class="hljs-number">2</span>).fit_transform(X, y)</pre>


<div class="custom-point-list">
<ul><li>类似地，为了保留前30%的特性，我们将运行:</li></ul>
</div>


<pre class="hljs">	<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> r_regression, SelectPercentile

	X_selection = SelectPercentile(r_regression, percentile=<span class="hljs-number">30</span>).fit_transform(X, y)</pre>



<p>“SelectKBest”和“SelectPercentile”方法也适用于自定义或非scikit-learn相关性测量，只要它们返回长度等于特征数量的向量，每个特征的数量表示其与目标的关联强度。现在让我们看看如何计算所有不同的相关性度量(我们稍后将讨论它们的含义以及何时选择它们)。</p>


<div class="custom-point-list">
<ul><li>Spearman的Rho、Kendall Tau和点-双列相关都可以在scipy包中获得。这就是如何获得x中每个特征的值。</li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats

rho_corr = [stats.spearmanr(X[:, f], y).correlation <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> range(X.shape[<span class="hljs-number">1</span>])]

tau_corr = [stats.kendalltau(X[:, f], y).correlation <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> range(X.shape[<span class="hljs-number">1</span>])]

pbs_corr = [stats.pointbiserialr(X[:, f], y).correlation <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> range(X.shape[<span class="hljs-number">1</span>])]
</pre>


<div class="custom-point-list">
<ul><li>卡方、互信息和ANOVA F-score都在scikit-learn中。请注意，互信息有一个单独的实现，这取决于目标是否是名义上的。</li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> chi2
<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> mutual_info_regression
<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> mutual_info_classif
<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> f_classif

chi2_corr = chi2(X, y)[<span class="hljs-number">0</span>]
f_corr = f_classif(X, y)[<span class="hljs-number">0</span>]
mi_reg_corr = mutual_info_regression(X, y)
mi_class_corr = mutual_info_classif(X, y)
</pre>


<div class="custom-point-list">
<ul><li>Cramer的V可以从最近的scipy版本(1.7.0或更高版本)获得。</li></ul>
</div>


<pre class="hljs"><span class="hljs-keyword">from</span> scipy.stats.contingency <span class="hljs-keyword">import</span> association

v_corr = [association(np.hstack([X[:, f].reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>), y.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)]), method=<span class="hljs-string">"cramer"</span>) <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> range(X.shape[<span class="hljs-number">1</span>])]
</pre>



<h3>嵌入式特征选择方法</h3>



<p>我们将讨论的最后一种特征选择方法是将其嵌入学习算法本身。这个想法是结合两个世界的优点:过滤器的速度，同时获得特定模型的最佳子集，就像从包装器中一样。</p>



<h4>实践中的嵌入式方法</h4>



<p>最典型的例子是套索回归。它基本上只是正则化的线性回归，其中特征权重在损失函数中向零收缩。因此，许多要素的权重最终为零，这意味着它们将从模型中被丢弃，而权重非零的其余要素将被包括在内。</p>



<p>嵌入式方法的问题在于，没有多少算法内置了特征选择。LASSO旁边的另一个例子来自计算机视觉:<a href="https://web.archive.org/web/20220926085303/https://towardsdatascience.com/autoencoders-from-vanilla-to-variational-6f5bb5537e4a" target="_blank" rel="noreferrer noopener nofollow">带有瓶颈层的自动编码器</a>迫使网络忽略图像中一些最无用的特征，而专注于最重要的特征。除此之外，没有多少有用的例子。</p>



<h2>过滤器特征选择方法:有用的技巧和提示</h2>



<p>正如我们所看到的，包装方法速度慢，计算量大，并且是特定于模型的，并且没有多少嵌入式方法。因此，过滤器通常是特征选择方法的首选。</p>



<p>同时，他们需要最专业的知识和对细节的关注。虽然嵌入式方法开箱即用，包装器实现起来也相当简单(尤其是当人们只调用scikit-learn函数时)，但过滤器需要一点统计复杂性。现在让我们把注意力转向滤波方法，并更详细地讨论它们。</p>



<p>过滤方法需要评估每个特征和目标之间的统计关系。虽然听起来很简单，但事情远比看起来简单。有许多统计方法来衡量两个变量之间的关系。为了知道在特定情况下选择哪一个，我们需要回想一下我们的第一个STATS101类，并复习一下<a href="https://web.archive.org/web/20220926085303/https://towardsdatascience.com/data-measurement-levels-dfa9a4564176" target="_blank" rel="noreferrer noopener nofollow">数据测量级别</a>。</p>



<h3>数据测量级别</h3>



<p>简而言之，变量的测量水平描述了数据的真实含义以及对这些数据有意义的数学运算的类型。有四种测量级别:标称、顺序、间隔和比率。</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img data-attachment-id="71269" data-permalink="https://web.archive.org/web/20220926085303/https://neptune.ai/feature-selection-methods-2" data-orig-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-2.png?fit=610%2C198&amp;ssl=1" data-orig-size="610,198" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="feature-selection-methods-2" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-2.png?fit=300%2C97&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-2.png?fit=610%2C198&amp;ssl=1" src="../Images/7d462ef3b5813eb586b71af6884a8d7a.png" alt="Tabel with data measurement levels" class="wp-image-71269 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-2.png?resize=693%2C225&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926085303im_/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-2.png?resize=693%2C225&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="71269" data-permalink="https://web.archive.org/web/20220926085303/https://neptune.ai/feature-selection-methods-2" data-orig-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-2.png?fit=610%2C198&amp;ssl=1" data-orig-size="610,198" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="feature-selection-methods-2" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-2.png?fit=300%2C97&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-2.png?fit=610%2C198&amp;ssl=1" src="../Images/7d462ef3b5813eb586b71af6884a8d7a.png" alt="Tabel with data measurement levels" class="wp-image-71269" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926085303im_/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-2.png?resize=693%2C225&amp;ssl=1"/></noscript><figcaption><em>Data measurement levels | <a href="https://web.archive.org/web/20220926085303/https://towardsdatascience.com/data-measurement-levels-dfa9a4564176" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>

<div class="custom-point-list">
<ul><li>标称特征，例如颜色(“红”、“绿”或“蓝”)在值之间没有排序；他们只是根据这些观察结果对<em> </em>进行分组。</li></ul>
</div>

<div class="custom-point-list">
<ul><li>序数特征，如教育水平(“初级”、“中级”、“高级”)表示顺序，但不是特定水平之间的差异(我们不能说“初级”和“中级”之间的差异与“中级”和“高级”之间的差异相同)。</li></ul>
</div>

<div class="custom-point-list">
<ul><li>间隔要素(如以摄氏度为单位的温度)保持间隔相等(25度和20度之间的差异与30度和25度之间的差异相同)。</li></ul>
</div>

<div class="custom-point-list">
<ul><li>最后，比率特征，如以美元表示的价格，以有意义的零为特征，这允许我们计算两个数据点之间的比率:我们可以说4美元是2美元的两倍。</li></ul>
</div>


<p>为了选择正确的统计工具来度量两个变量之间的关系，我们需要考虑它们的度量水平。</p>



<h3>测量各种数据类型的相关性</h3>



<p>当我们比较的两个变量，即特征和目标，都是区间或比率时，我们可以使用最流行的相关性度量:皮尔逊相关性，也称为<strong>皮尔逊相关性</strong>。</p>



<p>这很好，但是皮尔逊相关性有两个缺点:它假设两个变量都是正态分布的，并且它只测量它们之间的线性相关性。当相关性是非线性时，皮尔逊的r不会检测到它，即使它真的很强。</p>



<p>你可能听说过由Alberto Cairo编译的Datasaurus数据集。它由13对变量组成，每个变量都具有相同的非常弱的皮尔逊相关性-0.06。一旦我们把它们标绘出来，就很快变得显而易见，这些对实际上有很强的相关性，尽管是以非线性的方式。</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img data-attachment-id="71270" data-permalink="https://web.archive.org/web/20220926085303/https://neptune.ai/feature-selection-methods-3" data-orig-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-3.png?fit=597%2C426&amp;ssl=1" data-orig-size="597,426" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="feature-selection-methods-3" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-3.png?fit=300%2C214&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-3.png?fit=597%2C426&amp;ssl=1" src="../Images/4d0f5f906b8eec14eebabfdd7de023e7.png" alt="The Datasaurus dataset" class="wp-image-71270 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-3.png?resize=597%2C426&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926085303im_/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-3.png?resize=597%2C426&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="71270" data-permalink="https://web.archive.org/web/20220926085303/https://neptune.ai/feature-selection-methods-3" data-orig-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-3.png?fit=597%2C426&amp;ssl=1" data-orig-size="597,426" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="feature-selection-methods-3" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-3.png?fit=300%2C214&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-3.png?fit=597%2C426&amp;ssl=1" src="../Images/4d0f5f906b8eec14eebabfdd7de023e7.png" alt="The Datasaurus dataset" class="wp-image-71270" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926085303im_/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-3.png?resize=597%2C426&amp;ssl=1"/></noscript><figcaption><em>The Datasaurus dataset by Alberto Cairo | <a href="https://web.archive.org/web/20220926085303/https://www.autodesk.com/research/publications/same-stats-different-graphs" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<p>当预期非线性关系时，应考虑皮尔逊相关性的替代方法之一。最受欢迎的两个是:</p>


<div class="custom-point-list">
<ol><li><strong>斯皮尔曼秩相关(斯皮尔曼ρ)，</strong></li></ol>
</div>


<p>对于比率/区间变量，Spearman秩相关是Pearson相关的替代方法。顾名思义，它只查看等级值，即它根据变量中特定数据点的相对位置来比较两个变量。它能够捕捉非线性关系，但没有免费的午餐:由于只考虑排名而不是确切的数据点，我们丢失了一些信息。</p>


<div class="custom-point-list">
<ol start="2"><li><strong>肯德尔秩相关(Kendall Tau)。</strong></li></ol>
</div>


<p>另一种基于等级的相关性度量是肯德尔等级相关性。它在精神上类似于Spearman的相关性，但表述方式略有不同(Kendall的计算基于一致和不一致的值对，与Spearman基于偏差的计算相反)。肯德尔通常被认为对数据中的异常值更稳健。</p>



<p>如果至少有一个被比较的变量是有序类型，Spearman或Kendall等级相关是可行的。由于序数数据只包含关于等级的信息，所以它们都是完美的匹配，而皮尔逊的线性相关性用处不大。</p>



<p>另一种情况是两个变量都是名义变量。在这种情况下，我们可以从几个不同的相关性度量中进行选择:</p>


<div class="custom-point-list">
<ul><li><strong> Cramer的V </strong>，它将两个变量之间的关联捕捉到一个从零(无关联)到一(一个变量完全由另一个变量决定)的数字中。</li><li><strong>卡方统计</strong>通常用于检验两个变量之间的相关性。缺乏依赖性意味着特定的特性没有用。</li><li><strong>互信息</strong>衡量两个变量之间相互依赖程度的指标，旨在量化从一个变量中提取的关于另一个变量的信息量。</li></ul>
</div>


<p>选哪个？没有放之四海而皆准的答案。通常，每种方法都有一些优点和缺点。众所周知，克拉默的V字高估了该协会的实力。互信息作为一种非参数方法，需要更大的数据样本来产生可靠的结果。最后，卡方检验不能提供关系强度的信息，而只能提供关系是否存在的信息。</p>



<p>我们已经讨论了这样的场景，其中我们比较的两个变量都是区间或比率，当它们中至少有一个是序数时，以及当我们比较两个名义变量时。最后可能遇到的是比较一个名义变量和一个非名义变量。</p>



<p>在这种情况下，两种最广泛使用的相关性度量是:</p>


<div class="custom-point-list">
<ul><li><strong> ANOVA F-score </strong>，当一个变量是连续的而另一个是名义变量时的卡方当量，</li><li><strong>点-双列相关</strong>一种专门用于评估二进制变量和连续变量之间关系的相关性度量。</li></ul>
</div>


<p>再说一次，没有灵丹妙药。F分数只捕捉线性关系，而点-双列相关作出一些强正态性假设，在实践中可能不成立，破坏了它的结果。</p>



<p>说了这么多，在特定情况下应该选择哪种方法呢？下表有望在这个问题上提供一些指导。</p>


<p id="block_631894b437194" class="separator separator-20"/>


<div class="medium-table">
        <div class="mt-row heading">
            <p class="mt-col">变量1</p>
            <p class="mt-col">变量2</p>
            <p class="mt-col">方法</p>
            <p class="mt-col">评论</p>
        </div>
    
            <div class="mt-row">
                            
                            
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <table>
<tbody>
<tr>
<td><span>仅捕捉线性关系，假设正态性</span></td>
</tr>
</tbody>
</table>
                                    </div>
                    </div>
            <div class="mt-row">
                            <p class="mt-col">                                                                                                     </p>
                            <p class="mt-col"><span class="column-name">变量2: </span></p>
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <table>
<tbody>
<tr>
<td><span>当预期非线性关系时</span></td>
</tr>
</tbody>
</table>
                                    </div>
                    </div>
            <div class="mt-row">
                            <p class="mt-col">                                                                                                     </p>
                            <p class="mt-col"><span class="column-name">变量2: </span></p>
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <p><span>当预期非线性关系时</span></p>
                                    </div>
                    </div>
            <div class="mt-row">
                            
                            
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <table>
<tbody>
<tr>
<td><span>仅基于等级，捕捉非线性</span></td>
</tr>
</tbody>
</table>
                                    </div>
                    </div>
            <div class="mt-row">
                            <p class="mt-col">                                                                                                     </p>
                            <p class="mt-col"><span class="column-name">变量2: </span></p>
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <p><span>类似于Rho，但对异常值更稳健</span></p>
                                    </div>
                    </div>
            <div class="mt-row">
                            
                            
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <table>
<tbody>
<tr>
<td><span>可能高估相关强度</span></td>
</tr>
</tbody>
</table>
                                    </div>
                    </div>
            <div class="mt-row">
                            <p class="mt-col">                                                                                                     </p>
                            <p class="mt-col"><span class="column-name">变量2: </span></p>
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <table>
<tbody>
<tr>
<td><span>没有相关强度的信息</span></td>
</tr>
</tbody>
</table>
                                    </div>
                    </div>
            <div class="mt-row">
                            <p class="mt-col">                                                                                                     </p>
                            <p class="mt-col"><span class="column-name">变量2: </span></p>
                            <div class="mt-col">
                                        <span class="column-name">
                        Method:
                    </span>
                                                                <p><span>相互信息</span></p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <table>
<tbody>
<tr>
<td><span>需要许多数据样本。</span></td>
</tr>
</tbody>
</table>
                                    </div>
                    </div>
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Variable 2:
                    </span>
                                                                <p><span>区间/比率</span> <span> /序数</span></p>
                                    </div>
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <table>
<tbody>
<tr>
<td><span>仅捕捉线性关系</span></td>
</tr>
</tbody>
</table>
                                    </div>
                    </div>
            <div class="mt-row">
                            <p class="mt-col">                                                                                                     </p>
                            <p class="mt-col"><span class="column-name">变量2: </span></p>
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Comments:
                    </span>
                                                                <table>
<tbody>
<tr>
<td><span>做出强正态假设</span></td>
</tr>
</tbody>
</table>
                                    </div>
                    </div>
    </div>


<p id="block_6318949637193" class="separator separator-15"/>



<p class="has-text-align-center"><em>不同方法的比较</em></p>



<h2>不要俘虏:博鲁塔不需要人类的参与</h2>



<p>说到特性选择，不能不提到Boruta。回到2010年，当<a href="https://web.archive.org/web/20220926085303/https://www.jstatsoft.org/article/view/v036i11" target="_blank" rel="noreferrer noopener nofollow">第一次以R包的形式发布</a>时，它作为一种革命性的特征选择算法迅速成名。</p>



<h3>为什么博鲁塔是游戏规则的改变者？</h3>



<p>到目前为止，我们讨论的所有其他方法都需要人类做出任意的决定。无监督方法需要我们为特征移除设置方差或VIF阈值。包装器要求我们决定想要保留的特性的数量。过滤器需要我们选择相关度和要保留的特征数量。嵌入式方法让我们选择正则化强度。博鲁塔不需要这些。</p>



<p>Boruta是一个简单但统计优雅的算法。它使用来自随机森林模型的特征重要性度量来选择特征的最佳子集，并且它通过引入两个聪明的想法来做到这一点。</p>


<div class="custom-point-list">
<ol><li>首先，特性的重要性分数不能相互比较。相反，每个特征的重要性与其随机化版本的重要性相竞争。为了实现这一点，Boruta随机排列每个特征来构建它的“影子”版本。</li></ol>
</div>


<p class="has-text-align-left">然后，在整个特征集上训练随机森林，包括新的阴影特征。阴影特征中的最大特征重要性用作阈值。在最初的特征中，只有那些重要性高于这个阈值的特征得分。换句话说，只有比随机向量更重要的特征才会被加分。</p>



<p>这个过程反复重复多次。由于每次随机排列不同，阈值也不同，因此不同的特征可能得分。经过多次迭代后，每个原始特征都有一些指向其名称的点。</p>


<div class="custom-point-list">
<ol start="2"><li>最后一步是根据每个特性的得分来决定是保留还是丢弃它。博鲁塔的两个聪明想法中的另一个出现了:我们可以使用<a href="https://web.archive.org/web/20220926085303/https://towardsdatascience.com/6-useful-probability-distributions-with-applications-to-data-science-problems-2c0bee7cef28" target="_blank" rel="noreferrer noopener nofollow">二项式分布</a>来模拟分数。</li></ol>
</div>


<p>每一次迭代都被认为是一次独立的试验。如果该特性在给定的迭代中得了分，它就是一个保留它的投票；如果没有，那就投票决定放弃它。先验地，我们不知道某个特征是否重要，所以该特征得分的预期试验百分比是50%。因此，我们可以用p=0.5的二项式分布来模拟分数。如果我们的特征得分明显高于这个次数，它就被认为是重要的并被保留。如果得分次数少得多，它就被认为不重要并被丢弃。如果它在大约50%的试验中得分，它的状态是未解决的，但是为了保守起见，我们可以保留它。</p>



<p>例如，如果我们让Boruta运行100次试验，那么每个特性的预期得分将是50。如果它接近零，我们丢弃它，如果它接近100，我们保留它。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><a href="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?ssl=1" data-lbwps-width="1356" data-lbwps-height="1110" data-lbwps-srcsmall="https://neptune.ai/wp-content/uploads/feature-selection-methods-4.png"><img data-attachment-id="71271" data-permalink="https://web.archive.org/web/20220926085303/https://neptune.ai/feature-selection-methods-4" data-orig-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?fit=1356%2C1110&amp;ssl=1" data-orig-size="1356,1110" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="feature-selection-methods-4" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?fit=300%2C246&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?fit=1024%2C838&amp;ssl=1" src="../Images/34fdefbb1e87d557628561b9c6aa7c28.png" alt="Graph with example of Boruta" class="wp-image-71271 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?resize=512%2C419&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20220926085303im_/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?resize=512%2C419&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="71271" data-permalink="https://web.archive.org/web/20220926085303/https://neptune.ai/feature-selection-methods-4" data-orig-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?fit=1356%2C1110&amp;ssl=1" data-orig-size="1356,1110" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="feature-selection-methods-4" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?fit=300%2C246&amp;ssl=1" data-large-file="https://web.archive.org/web/20220926085303/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?fit=1024%2C838&amp;ssl=1" src="../Images/34fdefbb1e87d557628561b9c6aa7c28.png" alt="Graph with example of Boruta" class="wp-image-71271" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20220926085303im_/https://i0.wp.com/neptune.ai/wp-content/uploads/feature-selection-methods-4.png?resize=512%2C419&amp;ssl=1"/></noscript></a><figcaption><em>Boruta example | Source: author </em></figcaption></figure></div>


<p>Boruta在许多Kaggle比赛中已经证明非常成功，总是值得尝试。它还被成功地用于<a href="https://web.archive.org/web/20220926085303/https://www.mdpi.com/1996-1073/14/10/2779" target="_blank" rel="noreferrer noopener nofollow">预测建筑供暖的能耗</a>或<a href="https://web.archive.org/web/20220926085303/https://www.researchgate.net/publication/353955153_An_application_of_Machine_learning_with_Boruta_Feature_selection_to_Improve_NO2_pollution_prediction" target="_blank" rel="noreferrer noopener nofollow">预测空气污染</a>。</p>



<p>有一个非常直观的Python包来实现Boruta，名为<a href="https://web.archive.org/web/20220926085303/https://github.com/scikit-learn-contrib/boruta_py" target="_blank" rel="noreferrer noopener nofollow"> BorutaPy </a>(现在是scikit-learn-contrib的一部分)。这个包的GitHub readme演示了用Boruta运行特性选择是多么容易。</p>



<h2>选择哪种特征选择方法？为自己构建一个投票选择器</h2>



<p>我们已经讨论了许多不同的特征选择方法。他们每个人都有自己的优点和缺点，做出自己的假设，并以不同的方式得出结论。选哪个？还是我们必须选择？在许多情况下，将所有这些不同的方法组合在一起会使最终的特性选择器比它的每个子部分都更强大。</p>



<h3>灵感</h3>



<p>一种方法是受集合决策树的启发。在这类模型中，包括随机森林和许多流行的梯度提升算法，人们训练多个不同的模型，并让它们对最终预测进行投票。本着类似的精神，我们可以为自己构建一个投票选择器。</p>



<p>想法很简单:实现我们已经讨论过的几个特性选择方法。您的选择可能会受到您的时间、计算资源和数据测量水平的影响。只要你能负担得起，就尽可能多的运行不同的方法。然后，对于每个特征，写下建议将该特征保留在数据集中的选择方法的百分比。如果超过50%的方法投票支持保留该特性，那么就保留它——否则，就放弃它。</p>



<p>这种方法背后的思想是，虽然一些方法可能由于其固有的偏见而对一些特征做出错误的判断，但是方法的集合应该正确地得到有用的特征集。让我们看看在实践中如何实现！</p>



<h3>实施</h3>



<p>让我们构建一个简单的投票选择器，它集成了三种不同的特性选择方法:</p>


<div class="case-study-numbered-list">
    <h2/>
    <ul>
                    <li><span> 1 </span>一种基于皮尔逊相关的滤波方法。<br/></li>
                    <li><span> 2 </span>一种基于多重共线性的无监督方法。<br/></li>
                    <li><span> 3 </span>一个包装器，递归特征消除。</li>
            </ul>
</div>



<p>让我们看看这样一个投票选择器可能是什么样子。</p>



<p>进行进口。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> compress

<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE, r_regression, SelectKBest
<span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVR
<span class="hljs-keyword">from</span> statsmodels.stats.outliers_influence <span class="hljs-keyword">import</span> variance_inflation_factor</pre>



<p>接下来，我们的VotingSelector类在init构造函数之上包含四个方法。其中三个实现了我们想要集成的三种特征选择技术:</p>


<div class="case-study-numbered-list">
    <h2/>
    <ul>
                    <li><span> 1 </span> _select_pearson()进行皮尔逊相关滤波<br/></li>
                    <li><span> 2 </span> _select_vif()用于基于方差膨胀因子的无监督方法<br/></li>
                    <li>用于rbf包装器的<span> 3 </span> _select_rbf()</li>
            </ul>
</div>



<p>这些方法中的每一种都将特征矩阵X和目标y作为输入。基于VIF的方法不会使用目标，但是我们仍然使用这个参数来保持所有方法的接口一致，这样我们可以方便地在以后的循环中调用它们。除此之外，每个方法都接受一个关键字参数字典，我们将使用它来传递依赖于方法的参数。解析完输入后，每个方法调用我们之前讨论过的适当的sklearn或statsmodels函数，返回要保留的特性名称列表。</p>



<p>投票魔术发生在select()方法中。在这里，我们简单地迭代三个选择方法，对于每个特征，我们记录它是否应该根据这个方法被保留(1)或丢弃(0)。最后，我们对这些投票取平均值。对于每个特性，如果这个平均值大于投票阈值0.5(这意味着三个方法中至少有两个投票保留了一个特性)，我们就保留它。</p>



<p>这是整个类的代码。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VotingSelector</span><span class="hljs-params">()</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
       self.selectors = {
           <span class="hljs-string">"pearson"</span>: self._select_pearson,
           <span class="hljs-string">"vif"</span>: self._select_vif,
           <span class="hljs-string">"rfe"</span>: self._select_rfe,
       }
       self.votes = <span class="hljs-keyword">None</span>

<span class="hljs-meta">   @staticmethod</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_select_pearson</span><span class="hljs-params">(X, y, **kwargs)</span>:</span>
       selector = SelectKBest(r_regression, k=kwargs.get(<span class="hljs-string">"n_features_to_select"</span>, <span class="hljs-number">5</span>)).fit(X, y)
       <span class="hljs-keyword">return</span> selector.get_feature_names_out()

<span class="hljs-meta">   @staticmethod</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_select_vif</span><span class="hljs-params">(X, y, **kwargs)</span>:</span>
       <span class="hljs-keyword">return</span> [
           X.columns[feature_index]
           <span class="hljs-keyword">for</span> feature_index <span class="hljs-keyword">in</span> range(len(X.columns))
           <span class="hljs-keyword">if</span> variance_inflation_factor(X.values, feature_index) &lt;= kwargs.get(<span class="hljs-string">"vif_threshold"</span>, <span class="hljs-number">10</span>)
       ]

<span class="hljs-meta">   @staticmethod</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_select_rfe</span><span class="hljs-params">(X, y, **kwargs)</span>:</span>
       svr = SVR(kernel=<span class="hljs-string">"linear"</span>)
       rfe = RFE(svr, n_features_to_select=kwargs.get(<span class="hljs-string">"n_features_to_select"</span>, <span class="hljs-number">5</span>))
       rfe.fit(X, y)
       <span class="hljs-keyword">return</span> rfe.get_feature_names_out()

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">select</span><span class="hljs-params">(self, X, y, voting_threshold=<span class="hljs-number">0.5</span>, **kwargs)</span>:</span>
       votes = []
       <span class="hljs-keyword">for</span> selector_name, selector_method <span class="hljs-keyword">in</span> self.selectors.items():
           features_to_keep = selector_method(X, y, **kwargs)
           votes.append(
               pd.DataFrame([int(feature <span class="hljs-keyword">in</span> features_to_keep) <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> X.columns]).T
           )
       self.votes = pd.concat(votes)
       self.votes.columns = X.columns
       self.votes.index = self.selectors.keys()
       features_to_keep = list(compress(X.columns, self.votes.mean(axis=<span class="hljs-number">0</span>) &gt; voting_threshold))
       <span class="hljs-keyword">return</span> X[features_to_keep]

</pre>



<p>让我们看看它在实践中的效果。我们将加载scikit-learn内置的臭名昭著的波士顿住房数据。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_boston
boston = load_boston()
X = pd.DataFrame(boston[<span class="hljs-string">"data"</span>], columns=boston[<span class="hljs-string">"feature_names"</span>])
y = boston[<span class="hljs-string">"target"</span>]

</pre>



<p>现在，运行特征选择就像这样简单:</p>



<pre class="hljs">vs = VotingSelector()
X_selection = vs.select(X, y)</pre>



<p>结果，我们得到的特征矩阵只剩下三个特征。</p>



<pre class="hljs">      ZN  CHAS     RM
<span class="hljs-number">0</span>    <span class="hljs-number">18.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">6.575</span>
<span class="hljs-number">1</span>     <span class="hljs-number">0.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">6.421</span>
<span class="hljs-number">2</span>     <span class="hljs-number">0.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">7.185</span>
<span class="hljs-number">3</span>     <span class="hljs-number">0.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">6.998</span>
<span class="hljs-number">4</span>     <span class="hljs-number">0.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">7.147</span>
..    ...   ...    ...
<span class="hljs-number">501</span>   <span class="hljs-number">0.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">6.593</span>
<span class="hljs-number">502</span>   <span class="hljs-number">0.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">6.120</span>
<span class="hljs-number">503</span>   <span class="hljs-number">0.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">6.976</span>
<span class="hljs-number">504</span>   <span class="hljs-number">0.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">6.794</span>
<span class="hljs-number">505</span>   <span class="hljs-number">0.0</span>   <span class="hljs-number">0.0</span>  <span class="hljs-number">6.030</span>
[<span class="hljs-number">506</span> rows x <span class="hljs-number">3</span> columns]
</pre>



<p>我们还可以通过打印<em>对</em>的投票来一瞥我们的每个方法是如何投票的</p>



<pre class="hljs">        CRIM  ZN  INDUS  CHAS  NOX  RM  AGE  DIS  RAD  TAX  PTRATIO  B  LSTAT
pearson     <span class="hljs-number">0</span>   <span class="hljs-number">1</span>      <span class="hljs-number">0</span>     <span class="hljs-number">1</span>    <span class="hljs-number">0</span>   <span class="hljs-number">1</span>    <span class="hljs-number">0</span>    <span class="hljs-number">1</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>        <span class="hljs-number">0</span>  <span class="hljs-number">1</span>      <span class="hljs-number">0</span>
vif         <span class="hljs-number">1</span>   <span class="hljs-number">1</span>      <span class="hljs-number">0</span>     <span class="hljs-number">1</span>    <span class="hljs-number">0</span>   <span class="hljs-number">0</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>        <span class="hljs-number">0</span>  <span class="hljs-number">0</span>      <span class="hljs-number">0</span>
rfe         <span class="hljs-number">0</span>   <span class="hljs-number">0</span>      <span class="hljs-number">0</span>     <span class="hljs-number">1</span>    <span class="hljs-number">1</span>   <span class="hljs-number">1</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>        <span class="hljs-number">1</span>  <span class="hljs-number">0</span>      <span class="hljs-number">1</span></pre>



<p>最初的13个专栏只剩下3个，我们可能会不高兴。幸运的是，我们可以很容易地通过修改特定方法的参数来减少选择的限制。这可以通过简单地向select调用添加适当的参数来实现，这要感谢我们如何传递kwargs。</p>



<p>皮尔逊和RFE方法需要保留预定数量的特征。默认值为5，但我们可能希望将其增加到8。我们还可以修改VIF阈值，该阈值是方差膨胀因子的值，高于该值时，由于多重共线性，我们会丢弃某个要素。按照惯例，这个阈值被设置为10，但是增加到15会导致更多的特性被保留。</p>



<pre class="hljs">vs = VotingSelector()
X_selection = vs.select(X, y, n_features_to_select=<span class="hljs-number">8</span>, vif_threshold=<span class="hljs-number">15</span>)</pre>



<p>这样，我们还剩下七个特征。</p>



<pre class="hljs">        CRIM  ZN  INDUS  CHAS  NOX  RM  AGE  DIS  RAD  TAX  PTRATIO  B  LSTAT
pearson     <span class="hljs-number">1</span>   <span class="hljs-number">1</span>      <span class="hljs-number">0</span>     <span class="hljs-number">1</span>    <span class="hljs-number">0</span>   <span class="hljs-number">1</span>    <span class="hljs-number">1</span>    <span class="hljs-number">1</span>    <span class="hljs-number">1</span>    <span class="hljs-number">0</span>        <span class="hljs-number">0</span>  <span class="hljs-number">1</span>      <span class="hljs-number">0</span>
vif         <span class="hljs-number">1</span>   <span class="hljs-number">1</span>      <span class="hljs-number">1</span>     <span class="hljs-number">1</span>    <span class="hljs-number">0</span>   <span class="hljs-number">0</span>    <span class="hljs-number">0</span>    <span class="hljs-number">1</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>        <span class="hljs-number">0</span>  <span class="hljs-number">0</span>      <span class="hljs-number">1</span>
rfe         <span class="hljs-number">1</span>   <span class="hljs-number">0</span>      <span class="hljs-number">1</span>     <span class="hljs-number">1</span>    <span class="hljs-number">1</span>   <span class="hljs-number">1</span>    <span class="hljs-number">0</span>    <span class="hljs-number">1</span>    <span class="hljs-number">0</span>    <span class="hljs-number">0</span>        <span class="hljs-number">1</span>  <span class="hljs-number">0</span>      <span class="hljs-number">1</span></pre>



<p>我们的VotingSelector类是一个简单但通用的模板，您可以将其扩展到任意数量的特征选择方法。作为一种可能的扩展，您还可以将传递给select()的所有参数视为建模管道的超参数，并对它们进行优化，以便最大化下游模型的性能。</p>



<h2>Big Tech的功能选择</h2>



<p>GAFAM等大型科技公司拥有数以千计的生产中的机器学习模型，是如何在野外操作特征选择的主要例子。让我们看看这些科技巨头对此有什么看法！</p>



<h3>谷歌</h3>



<p>ML的规则是谷歌机器学习最佳实践的便利汇编。在其中，谷歌的工程师指出，该模型可以学习的参数数量大致为</p>



<p>与它可以访问的数据量成比例。因此，我们拥有的数据越少，需要丢弃的特征就越多。他们的粗略指导方针(来源于基于文本的模型)是用1000个训练样本使用十几个特征，或者用1000万个训练样本使用100，000个特征。</p>



<p>文档中的另一个关键点涉及模型部署问题，这也会影响特性选择。</p>


<div class="custom-point-list">
<ul><li>首先，可供选择的特性集可能会受到推理时产品中可用特性的限制。如果模型上线时没有一个很好的特性，你可能会被迫放弃它。</li></ul>
</div>

<div class="custom-point-list">
<ul><li>第二，一些特性可能容易出现<a href="https://web.archive.org/web/20220926085303/https://towardsdatascience.com/dont-let-your-model-s-quality-drift-away-53d2f7899c09" target="_blank" rel="noreferrer noopener nofollow">数据漂移</a>。虽然解决漂移是一个复杂的话题，但有时最好的解决方案可能是从模型中完全删除有问题的特性。</li></ul>
</div>


<h3>脸谱网</h3>



<p>几年前，在2019年，脸书提出了自己的神经网络合适的特征选择算法，以便在训练大规模模型的同时节省计算资源。他们在自己的脸书新闻数据集上进一步测试了这种算法，以便在使用更少维度的输入的同时尽可能高效地对相关项目进行排序。你可以在这里阅读全部内容<a href="https://web.archive.org/web/20220926085303/https://research.facebook.com/publications/feature-selection-for-facebook-feed-ranking-system-via-a-group-sparsity-regularized-training-algorithm/" target="_blank" rel="noreferrer noopener nofollow">。</a></p>



<h2>离别赠言</h2>



<p>感谢阅读到最后！我希望这篇文章能让您相信特性选择是数据准备流程中的一个关键步骤，并为您提供一些如何实现它的指导。</p>



<p>不要犹豫，在社交媒体上联系我，讨论这里涵盖的主题或任何其他机器学习主题。快乐精选！</p>



<h3>参考资料</h3>






<div id="author-box-new-format-block_625956d8f9d5e" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">迈克尔·奥莱塞克</h3>
    
          <p class="article__authorContent-text">有统计学背景的机器学习工程师。我身兼数职，曾在一家咨询公司、一家人工智能初创公司和一家软件公司工作过。旅行者、通晓多种语言者、数据科学博客作者和讲师，以及终身学习者。查看他的网站，了解更多信息。</p>
    
          
    
  </div>
</div>


<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator has-css-opacity"/>



<p class="has-text-color"><strong>阅读下一篇</strong></p>



<h2>真实世界的MLOps示例:超因子中的模型开发</h2>



<p class="has-small-font-size">6分钟阅读|作者斯蒂芬·奥拉德勒| 2022年6月28日更新</p>


<p id="block_5ffc75def9f8e" class="separator separator-10"/>



<p>在“真实世界的MLOps示例”系列的第一部分中，<a href="https://web.archive.org/web/20220926085303/https://www.linkedin.com/in/jules-belveze" target="_blank" rel="noreferrer noopener">MLOps工程师Jules Belveze </a>将带您了解<a href="https://web.archive.org/web/20220926085303/https://hypefactors.com/" target="_blank" rel="noreferrer noopener"> Hypefactors </a>的模型开发流程，包括他们构建的模型类型、他们如何设计培训渠道，以及您可能会发现的其他有价值的细节。享受聊天！</p>



<h3 id="company-profile">公司简介</h3>



<p><a href="https://web.archive.org/web/20220926085303/https://hypefactors.com/" target="_blank" rel="noreferrer noopener"> Hypefactors </a>提供一体化媒体智能解决方案，用于管理公关和沟通、跟踪信任度、产品发布以及市场和金融情报。他们运营着大型数据管道，实时传输世界各地的媒体数据。人工智能用于许多以前手动执行的自动化操作。</p>



<h3 id="guest-introduction">嘉宾介绍</h3>



<h4>你能向我们的读者介绍一下你自己吗？</h4>



<p>嘿，斯蒂芬，谢谢你邀请我！我叫朱尔斯。我26岁。我在巴黎出生和长大，目前住在哥本哈根。</p>



<h4>嘿茱尔斯！谢谢你的介绍。告诉我你的背景以及你是如何成为催眠师的。</h4>



<p>我拥有法国大学的统计学和概率学士学位以及普通工程学硕士学位。除此之外，我还毕业于丹麦的丹麦技术大学，主修深度学习的数据科学。我对多语言自然语言处理非常着迷(并因此专攻它)。在微软的研究生学习期间，我还研究了高维时间序列的异常检测。</p>



<p>今天，我在一家名为Hypefactors的媒体智能技术公司工作，在那里我开发NLP模型，帮助我们的用户从媒体领域获得洞察力。对我来说，目前的工作是有机会从原型一直到产品进行建模。我想你可以叫我书呆子，至少我的朋友是这么形容我的，因为我大部分空闲时间不是编码就是听迪斯科黑胶。</p>



<h3 id="model-development-at-hypefactors">超因子模型开发</h3>



<h4>你能详细说明你在Hypefactors建立的模型类型吗？</h4>



<p>尽管我们也有计算机视觉模型在生产中运行，但我们主要为各种用例构建<a href="https://web.archive.org/web/20220926085303/https://neptune.ai/blog/category/natural-language-processing" target="_blank" rel="noreferrer noopener"> NLP(自然语言处理)</a>模型。我们需要覆盖多个国家和处理多种语言。多语言方面使得用“经典机器学习”方法开发变得困难。我们在<a href="https://web.archive.org/web/20220926085303/https://github.com/huggingface/transformers" target="_blank" rel="noreferrer noopener">变形金刚库</a>的基础上打造深度学习模型。</p>



<p>我们在生产中运行各种模型，从跨度提取或序列分类到文本生成。这些模型旨在服务于不同的用例，如主题分类、情感分析或总结。</p>


<a class="button continous-post blue-filled" href="/web/20220926085303/https://neptune.ai/blog/mlops-examples-model-development-in-hypefactors" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator has-css-opacity"/>
</div></div>
</div>
      </div>    
</body>
</html>