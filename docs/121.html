<html>
<head>
<title>Natural Language Processing with Hugging Face and Transformers </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>拥抱脸和变形金刚的自然语言处理</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/natural-language-processing-with-hugging-face-and-transformers#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/natural-language-processing-with-hugging-face-and-transformers#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>NLP是机器学习的一个分支，旨在帮助计算机和智能系统像人类一样理解文本和口语。</p>



<p>NLP驱动计算机程序执行各种各样非常有用的任务，如文本翻译、响应口头命令或在眨眼之间总结大量文本。您很有可能以下列形式与NLP技术进行过交互:</p>



<ul>
<li>声控全球定位系统</li>



<li>智能机器人和数字助理</li>



<li>客服聊天机器人</li>



<li>基本上，任何涉及使用STT和TTS技术的数字服务</li>
</ul>



<p>我们将讨论NLP的两个关键技术，但首先是NLP的基础——抽象层。</p>



<h2 id="h-layers-of-abstraction-in-nlp">NLP中的抽象层</h2>



<p>从书面文本中提取意义涉及几个抽象层，这些抽象层通常与不同的研究领域相关，但相互之间有很好的协同作用。</p>



<p>这些研究领域包括:</p>



<ul>
<li><strong> <em>形态学</em> </strong>层次，研究词的结构和构词。</li>



<li><strong><em/></strong>词汇分析正是着眼于词汇和标记的构成，以及它们各自的词性。</li>



<li><strong> <em>句法</em> </strong>分析负责使用词法分析阶段输出的词性标注将单词分组为连贯的短语。</li>



<li><strong><em/></strong>语义处理然后通过将句法特征与给定的上下文相关联以及消除具有多个定义的单词的歧义来评估所形成的句子的意义和含义。</li>



<li>最后是<strong> <em>语篇</em> </strong>层面，这里的处理是关于对文本结构和意义的分析，而不仅仅是单个句子，在单词和句子之间建立联系。</li>
</ul>



<p>当前最先进的NLP技术将所有这些层结合起来，产生非常类似于人类语音的出色结果。最重要的是，NLP将人类语言的多种基于规则的建模与统计和深度学习模型相结合。</p>



<p>最近需求量很大的深度学习方法需要大量带注释的数据来学习和识别相关的相关性。像BERT，GPT2，GPT3或RoBERTA这样的著名模型消耗了大量的训练数据，它们只能由负担得起成本的大规模公司进行训练。例如，训练<a href="https://web.archive.org/web/20221206005212/https://towardsdatascience.com/the-future-of-ai-is-decentralized-848d4931a29a#:~:text=Training%20GPT%2D3%20reportedly%20cost,a%20single%20training%20run%C2%B9." target="_blank" rel="noreferrer noopener nofollow"> GPT-3据报道花费了12，000，000美元…一次训练运行</a>。</p>







<h2 id="h-the-transformers-library">变形金刚图书馆</h2>



<h3>注意力机制</h3>



<p>2018年出现的一个趋势是基于注意力的算法，这是一个由谷歌R&amp;D部门研究和开发的概念，并于2017年在著名的“<a href="https://web.archive.org/web/20221206005212/https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noreferrer noopener nofollow">注意力是你所需要的全部</a>”论文中首次发布。</p>



<p>注意力是一种模仿我们大脑内部认知结构的技术。它增强并本能地关注数据的特定部分，而淡化其余部分。因此，当面对复杂和大量的数据时，这种机制节省了时间和能量处理。</p>



<p>变形金刚网络大量利用注意力机制来实现高端表现力。因此，变压器在许多领域的NLP深度学习模型的架构中被广泛采用。</p>





<h3>单词嵌入</h3>



<p>在每个NLP模型的中心，都有一个预处理阶段，将有意义的单词和句子转换成实数向量。换句话说，嵌入是一种允许具有相似意思的单词具有相似表示的单词表示类型。</p>



<p>它们是文本的分布式表示，也许是深度学习方法在挑战自然语言处理问题上令人印象深刻的表现的关键突破之一。</p>







<p>单词嵌入的根源<a href="https://web.archive.org/web/20221206005212/https://en.wikipedia.org/wiki/Distributional_semantics" target="_blank" rel="noreferrer noopener nofollow"> <strong> <em>分布语义学</em> </strong> </a>理论试图根据单词周围的上下文来表征单词。例如，单词“演员”在句子“演员崩溃”中的意思与在句子“我的演员朋友给我发了这个链接”中的意思不同。共享相似上下文的单词也共享相似的意思。</p>



<p>单词嵌入有几种方法。它们可以追溯到人工智能的早期，并且是基于降维的方法。这些方法(称为“n-gram”或“k-means”)获取文本语料库，并通过在单词共现矩阵中找到聚类来将其缩减到固定的维数。目前最受欢迎的方法是基于Word2Vec，最初是由谷歌研究人员托马斯·米科洛夫、程凯、格雷格·科拉多和Quoc Le在2013年推出的。</p>





<h2 id="h-sentiment-analysis-leveraging-bert">利用BERT进行情感分析</h2>



<p><strong> BERT </strong>代表<strong> B </strong>方向<strong>E</strong>n编码器<strong> R </strong>代表来自<strong> T </strong>变压器。这是谷歌人工智能在2018年底开发的一种架构，提供以下功能:</p>



<ul>
<li>设计成深度双向的。从令牌的左右上下文中有效地捕获信息。</li>



<li>与前辈相比，在学习速度方面效率极高。</li>



<li>它结合了掩码语言模型(MLM)和下一句预测(NSP)。</li>



<li>这是一个通用的深度学习模型，可用于分类、问答、翻译、摘要等。</li>
</ul>







<p>最初，用未标记的数据对BERT进行预训练。之后，模型输出输入的特定表示。</p>



<p>再训练过程可以以各种方式完成，或者通过从零开始构建判别或生成模型，或者对公共数据库中的现有模型进行微调。通过利用预先训练的模型，有可能将学习从一个领域转移到另一个领域，而无需花费从头开始学习所需的时间和精力。</p>



<p><strong> <em>注</em> </strong> <em>:我已经简要描述了实际的机制是如何工作的，更多细节我推荐'</em> <a href="https://web.archive.org/web/20221206005212/https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" target="_blank" rel="noreferrer noopener nofollow"> <em> BERT解释:NLP语言模型的现状</em> </a> <em> ' </em></p>



<h3>用BERT迁移学习</h3>



<p>微调BERT以执行特定任务的方法相对简单。虽然BERT可用于各种NLP应用，但微调过程需要在核心模型中添加一个小层。例如:</p>



<p><strong>分类任务—</strong>在变压器模块上使用分类层(情感分析)。</p>



<p><strong> Q &amp; A </strong>相关任务——模型收到一个关于文本序列的问题，并被要求在命题中标出正确答案。伯特被训练学习两个向量，它们标记了答案的开始和结束。例如:班，1.1版</p>



<p><strong>命名实体识别NER: </strong>模型被输入一个文本序列，并被要求识别特定的实体(国家、组织、人、动物等)。)，并给它们贴上标签。这同样适用于这里，预先训练以识别实体的分类层与变换器块“组装”在一起以适合整个架构。</p>





<h3>为什么抱脸？</h3>



<p><a href="https://web.archive.org/web/20221206005212/https://huggingface.co/" target="_blank" rel="noreferrer noopener nofollow">拥抱脸</a>是一个大型开源社区，它迅速成为预先训练的深度学习模型的诱人中心，主要针对NLP。他们自然语言处理的核心操作模式围绕着转换器的使用。</p>





<p>用Python编写的Transformers库公开了一个配置良好的API，以利用过多的深度学习架构来完成像前面讨论的那些最先进的NLP任务。</p>



<p>正如您可能已经猜到的，一个核心的启动价值是可重用性——所有可用的模型都带有一组预先训练好的权重，您可以针对您的特定用途进行微调。</p>



<h3>从拥抱脸开始</h3>



<p>在官方文档中，您可以找到所有具有相关库结构的组件。</p>



<h4>拥抱脸轮毂Repos</h4>



<p>它们有基于git的存储库，可以作为存储，并且可以包含项目的所有文件，提供类似github的特性，例如:</p>



<ul>
<li>版本控制，</li>



<li>提交历史记录和分支差异。</li>
</ul>



<p>它们还提供了优于常规Github回购的重要优势:</p>



<ul>
<li>关于已启动任务、模型训练、指标等的有用元数据，</li>



<li>测试推理的浏览器预览，</li>



<li>用于生产就绪环境的API，</li>



<li>HUB中有10多个框架:Transformers、Asteroid、ESPnet等等。</li>
</ul>



<p><strong> <em>查看这里:</em> </strong> <a href="https://web.archive.org/web/20221206005212/https://huggingface.co/docs/libraries" target="_blank" rel="noreferrer noopener nofollow"> <strong> <em>抱抱脸库</em> </strong> </a></p>



<h4>拥抱面部小工具</h4>



<p>一组现成的预训练模型，用于在web预览中测试推理。</p>



<p>一些例子:</p>







<p><strong> <em>查看这里:</em> </strong> <a href="https://web.archive.org/web/20221206005212/https://huggingface-widgets.netlify.app/" target="_blank" rel="noreferrer noopener nofollow"> <strong> <em>抱脸小工具</em> </strong> </a></p>



<h3>拥抱脸的伯特模型</h3>



<p>现在，让我们试着做我们一直在谈论的事情。我们想要微调BERT来分析亚马逊上购买商品的一些商业评论，并确定评论的<em>正面</em>、<em>负面、</em>和<em>中立</em>。</p>



<h4>亚马逊产品评论数据集</h4>



<p>该数据集包括超过1480万条来自购买、评级、文本、有用性投票等的产品评论。这些数据将证明对我们的任务非常有用，因为评论是由人们真实地做出的，以反映他们对给定产品的意见。文本倾向于主观性，因此本质上可以分为积极的，消极的和中性的类别。</p>



<p><strong> <em>下载数据集的链接</em> </strong> <em> : </em> <a href="https://web.archive.org/web/20221206005212/http://jmcauley.ucsd.edu/data/amazon/" target="_blank" rel="noreferrer noopener nofollow"> <em>亚马逊商品评论</em> </a></p>



<p>数据集的示例如下所示:</p>



<pre class="hljs">{
  <span class="hljs-string">"reviewerID"</span>: <span class="hljs-string">"A2SUAM1J3GNN3B"</span>,
  <span class="hljs-string">"asin"</span>: <span class="hljs-string">"0000013714"</span>,
  <span class="hljs-string">"reviewerName"</span>: <span class="hljs-string">"J. McDonald"</span>,
  <span class="hljs-string">"helpful"</span>: [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
  <span class="hljs-string">"reviewText"</span>: <span class="hljs-string">"I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!"</span>,
  <span class="hljs-string">"overall"</span>: <span class="hljs-number">5.0</span>,
  <span class="hljs-string">"summary"</span>: <span class="hljs-string">"Heavenly Highway Hymns"</span>,
  <span class="hljs-string">"unixReviewTime"</span>: <span class="hljs-number">1252800000</span>,
  <span class="hljs-string">"reviewTime"</span>: <span class="hljs-string">"09 13, 2009"</span>
}</pre>



<p>为了确定每个类别，我们将依靠亚马逊评级系统。“总体”键值表示该产品的总体评级。我们可以为每个类别建立一个可测量的范围:</p>



<ul>
<li>正面反馈:4-5颗星</li>



<li>负面反馈:从0-2星</li>



<li>中性:3颗星</li>
</ul>



<p>由于数据涵盖了大量不同的主题和类别，建议缩小范围，只选择其中的一小部分。我选择了这些:</p>



<ul>
<li>汽车的</li>



<li>服装、鞋子和珠宝</li>



<li>电子学</li>



<li>手机和配件</li>
</ul>



<p>最后，我们将使用小版本的数据集，以避免处理能力过载。</p>



<h4>变形金刚图书馆</h4>



<p><strong>安装抱脸变形金刚库</strong></p>



<ol>
<li>使用conda创建您的虚拟环境:</li>
</ol>



<pre class="hljs">conda create --name bert_env python=<span class="hljs-number">3.6</span></pre>



<ol start="2">
<li>安装支持cuda的Pytorch(如果您有专用的GPU，或者没有专用的CPU版本):</li>
</ol>



<pre class="hljs">conda install pytorch torchvision torchaudio cudatoolkit=<span class="hljs-number">10.2</span> -c pytorch</pre>



<ol start="3">
<li>从conda通道安装变压器版本4.0.0:</li>
</ol>



<pre class="hljs">conda install -c huggingface transformers</pre>



<ol start="4">
<li>安装火炬视觉:</li>
</ol>



<pre class="hljs">pip install torchvision</pre>



<ol start="5">
<li>安装pytorch-nlp包中的Bert预训练版本:</li>
</ol>



<pre class="hljs">pip install pytorch-pretrained-bert pytorch-nlp</pre>



<p>我们将使用Pytorch版本的BERT uncased，由Hugging Face提出。</p>





<h4>训练模型</h4>



<p><strong>从预处理数据开始</strong></p>



<p>对于训练数据，我们只需要“总体”和“回顾”属性。我们将使用包含<strong> 1 </strong>(正值)、<strong> 2 </strong>(负值)和<strong> 0 </strong>(中性)的“情绪数据”创建一个新列。根据总体得分，每一行都将标有这些数字。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
review_data = pd.read_json(<span class="hljs-string">'/content/Dataset_final.json'</span>, lines=<span class="hljs-keyword">True</span>)
sentiment_data = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> review_data[[<span class="hljs-string">'overall'</span>]].values:
    <span class="hljs-keyword">if</span> i &gt;= <span class="hljs-number">4</span>:
      sentiment_data.append(<span class="hljs-number">1</span>) 
    <span class="hljs-keyword">elif</span> i &lt; <span class="hljs-number">3</span>:
      sentiment_data.append(<span class="hljs-number">2</span>) 
    <span class="hljs-keyword">else</span>:
      sentiment_data.append(<span class="hljs-number">0</span>) 

sentiment = pd.DataFrame(sentiment_data)
review_data[<span class="hljs-string">'sentiment'</span>] = sentiment</pre>



<p><strong>启动NSP进程</strong></p>



<p>将[CLS]和[SEP]标签放在每个复习句子的前面。</p>



<pre class="hljs">sentences = [<span class="hljs-string">"[CLS] "</span> + query + <span class="hljs-string">" [SEP]"</span> <span class="hljs-keyword">for</span> query <span class="hljs-keyword">in</span> review_data[<span class="hljs-string">'reviewText'</span>]]</pre>



<h4>BERT令牌嵌入</h4>



<p>在深入这部分代码之前，我们需要解释一下令牌嵌入及其工作原理。嵌入令牌提供了关于文本内容的信息。首先要做的是将我们的文本转换成一个向量。</p>



<p>BERT使用内部算法将输入的单词分解成记号。BERT实施的流程包括三个阶段:</p>



<ul>
<li>令牌嵌入</li>



<li>嵌入位置</li>



<li>嵌入段</li>
</ul>



<p>PyTorch的BertTokenizer模块将负责内部的所有逻辑。将每个输入句子分成合适的记号，然后将它们编码成数字向量。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> pytorch_pretrained_bert <span class="hljs-keyword">import</span> BertTokenizer


tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">'bert-base-uncased'</span>, do_lower_case=<span class="hljs-keyword">True</span>)
tokenized_texts = [tokenizer.tokenize(sent) <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]</pre>



<p>填充输入标记，并使用BERT标记器将标记转换为它们在BERT词汇表中的索引号:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences

MAX_LEN = <span class="hljs-number">512</span>

input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) <span class="hljs-keyword">for</span> txt <span class="hljs-keyword">in</span> tokenized_texts],
                          maxlen=MAX_LEN, dtype=<span class="hljs-string">"long"</span>, truncating=<span class="hljs-string">"post"</span>, padding=<span class="hljs-string">"post"</span>)

input_ids = [tokenizer.convert_tokens_to_ids(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tokenized_texts]
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=<span class="hljs-string">"long"</span>, truncating=<span class="hljs-string">"post"</span>, padding=<span class="hljs-string">"post"</span>)</pre>



<p>创建注意掩码，每个标记的掩码为1，后跟填充的0:</p>



<pre class="hljs">attention_masks = []
<span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> input_ids:
  seq_mask = [float(i&gt;<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> seq]
  attention_masks.append(seq_mask)</pre>



<p><strong>分离数据并为训练做准备</strong></p>



<p>拆分列车并测试拆分:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels.values,  random_state=<span class="hljs-number">2018</span>, test_size=<span class="hljs-number">0.2</span>)
train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=<span class="hljs-number">2018</span>, test_size=<span class="hljs-number">0.2</span>)</pre>



<p>将数据转换为torch张量，并使用特定的batch_size创建Dataloader迭代器:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset, DataLoader, RandomSampler,
SequentialSampler

train_inputs = torch.tensor(train_inputs)
validation_inputs = torch.tensor(validation_inputs)
train_labels = torch.tensor(train_labels)
validation_labels = torch.tensor(validation_labels)
train_masks = torch.tensor(train_masks)
validation_masks = torch.tensor(validation_masks)

batch_size = <span class="hljs-number">8</span>
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)</pre>



<p>实例化模型:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> pytorch_pretrained_bert <span class="hljs-keyword">import</span> BertAdam, BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>, num_labels=<span class="hljs-number">3</span>)
model.cuda()</pre>



<p>定义优化的超参数:</p>



<pre class="hljs">param_optimizer = list(model.named_parameters())
no_decay = [<span class="hljs-string">'bias'</span>, <span class="hljs-string">'gamma'</span>, <span class="hljs-string">'beta'</span>]
optimizer_grouped_parameters = [
    {<span class="hljs-string">'params'</span>: [p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> param_optimizer <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> any(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay)],
     <span class="hljs-string">'weight_decay_rate'</span>: <span class="hljs-number">0.01</span>},
    {<span class="hljs-string">'params'</span>: [p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> param_optimizer <span class="hljs-keyword">if</span> any(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay)]  <span class="hljs-string">'weight_decay_rate'</span>: <span class="hljs-number">0.0</span>}
]

optimizer = BertAdam(optimizer_grouped_parameters, lr=<span class="hljs-number">2e-5</span>, warmup=<span class="hljs-number">.1</span>)</pre>



<p>定义训练循环:</p>



<pre class="hljs">
train_loss_set = []

epochs = <span class="hljs-number">2</span>

<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(epochs, desc=<span class="hljs-string">"Epoch"</span>):

  
  model.train()
  
  tr_loss = <span class="hljs-number">0</span>
  nb_tr_examples, nb_tr_steps = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>
  
  <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> enumerate(train_dataloader):
    
    
    batch = tuple(t.to(device) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> batch)
    
    b_input_ids, b_input_mask, b_labels = batch
    
    optimizer.zero_grad()
    
    loss = model(b_input_ids, token_type_ids=<span class="hljs-keyword">None</span>, attention_mask=b_input_mask, labels=b_labels)
    train_loss_set.append(loss.item())
    
    loss.backward()
    
    optimizer.step()
    
    tr_loss += loss.item()
    nb_tr_examples += b_input_ids.size(<span class="hljs-number">0</span>)
    nb_tr_steps += <span class="hljs-number">1</span></pre>



<p>开始跟踪培训损失，看看模型实际上如何改进。</p>



<p>将模型置于评估模式，评估一个批次的预测:</p>



<pre class="hljs">
  model.eval()
  
  eval_loss, eval_accuracy = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>
  nb_eval_steps, nb_eval_examples = <span class="hljs-number">0</span>, <span class="hljs-number">0</span></pre>



<pre class="hljs"><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> validation_dataloader:
    
    batch = tuple(t.to(device) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> batch)
    
    b_input_ids, b_input_mask, b_labels = batch
    
    <span class="hljs-keyword">with</span> torch.no_grad():
      
      logits = model(b_input_ids, token_type_ids=<span class="hljs-keyword">None</span>, attention_mask=b_input_mask)
    
    logits = logits.detach().cpu().numpy()
    label_ids = b_labels.to(<span class="hljs-string">'cpu'</span>).numpy()
    tmp_eval_accuracy = flat_accuracy(logits, label_ids)
    eval_accuracy += tmp_eval_accuracy
    nb_eval_steps += <span class="hljs-number">1</span></pre>



<p>可以通过绘制train_loss_set列表来看一下训练损耗。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
plt.figure(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">8</span>))
plt.title(<span class="hljs-string">"Training loss"</span>)
plt.xlabel(<span class="hljs-string">"Batch"</span>)
plt.ylabel(<span class="hljs-string">"Loss"</span>)
plt.plot(train_loss_set)
plt.show()</pre>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/6103159c0069dbfef0e1a79a3cd6eb66.png" alt="NLP BERT training loss" class="wp-image-49460" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206005212im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/NLP-BERT-training-loss.png?ssl=1"/><figcaption class="wp-element-caption"><em>Training loss plot</em></figcaption></figure></div>


<p>训练完成后，您可以使用torch.save()将其保存为检查点。</p>



<pre class="hljs">torch.save(model, <span class="hljs-string">'/bert_final_version.pth'</span>)</pre>



<p>本节的目标是向您展示一个简单的演示，说明如何使用Hugging Face提供的预训练版本的BERT，并使用特定的数据集对其进行微调，以执行所需的任务。</p>



<p>一旦保存了您的检查点，您就可以(例如)在API中使用它作为服务来将tweets或其他类似的文本内容分为正面、负面或中性类别。</p>



<p>要了解更多，请查看我以前关于对话人工智能的文章，在那里我使用Django API作为后端服务来服务模型推理→<a href="/web/20221206005212/https://neptune.ai/blog/conversational-ai-nvidia-tools-guide" target="_blank" rel="noreferrer noopener">Nvidia支持的对话人工智能架构:工具指南</a></p>



<p>我在下面给你留了google Colab笔记本的链接，在那里你会找到运行这个实验的所有代码:<a href="https://web.archive.org/web/20221206005212/https://colab.research.google.com/drive/15Kp-wVEY5oDKDH_NlfOYSSEQ-OSa0RK8?usp=sharing" target="_blank" rel="noreferrer noopener nofollow">情感分析BERT </a></p>



<h2 id="h-conclusion">结论</h2>



<p>我真诚地推荐你检查拥抱脸的工作，他们有优秀的教程和文章来快速自信地让你开始NLP和深度学习。</p>



<p>还有，用Fastai和PyTorch看《程序员深度学习》这本书。他们有关于深度学习NLP方法的精彩章节，完全用Pytorch和Fast AI编码。这很容易，你将很快开始编写你的模型。</p>



<p>一如既往，如有任何问题，请随时通过我的电子邮件联系我:hachcham.ayman@gmail.com</p>



<h2 id="h-references">参考</h2>




        </div>
        
    </div>    
</body>
</html>