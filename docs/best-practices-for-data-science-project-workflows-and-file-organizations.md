# 数据科学项目工作流和文件组织的最佳实践

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/best-practices-for-data-science-project-workflows-and-file-organizations>

自从 2012 年《哈佛商业评论》发表文章称*数据科学是 21 世纪最性感的工作*以来，这个领域已经席卷了所有行业。数据科学工作从来不会错过排名第一的最佳工作列表——一个工作列表每天可以收到 200-300 名申请人也就不足为奇了。福布斯的一篇文章称，“从 2010 年到 2020 年，世界上创建、捕获、复制和消费的数据量从 1.2 万亿千兆字节增加到 59 万亿千兆字节，几乎增长了 5000%”。按照目前的速度，我们完全淹没在数据中，各行各业的企业都在寻找利用数据的方法。

对一些人来说，这可能似曾相识，但对经历过软件工程繁荣时期的其他人来说，他们可能会看到一个熟悉的模式正在形成。实际上，每个从事软件工程的人都想保持或发展他们在市场中的竞争优势，因此他们决心向市场交付高质量的产品，这反过来导致了方法和所用工具的革命——一些例子包括敏捷、DevOps、CI/CD 等等。

然而，软件工程和数据科学的不同之处在于，软件工程师本质上是为了构建而学习，而对于数据科学家来说，学习通常发生在构建之后。尽管如此，这并不是说数据科学没有任何东西可以从它的表亲软件工程中学到。事实上，现实恰恰相反。

## 什么是数据科学工作流？

通常，工作流描述了人们执行任务以完成工作的方式。为了说明一个典型的工作流程是什么样子的，我们将使用图表或清单列出一系列应该按顺序完成的步骤。

> 工作流由编排好的可重复的业务活动模式组成，通过将资源系统地组织成转换材料、提供服务或处理信息的流程来实现。它可以描述为一系列操作，声明为一个人或一组人的工作，一个员工组织，或者一个或多个简单或复杂的机制【**来源** : [维基百科](https://web.archive.org/web/20221201180359/https://en.wikipedia.org/wiki/Workflow)

这是数据科学工作流的本质；数据科学工作流阐述了数据科学项目中采取的不同步骤。由于工作流的目的是说明如何完成工作，因此使用定义良好的数据科学工作流非常有用，因为它可以提醒所有团队成员**哪些工作已经完成，哪些工作尚未完成。**

## 工作流程的开发

“数据科学工作流来自哪里？”的简单答案是软件工程，但是和大多数事情一样，它真的没有那么简单。首先，软件工程师是工程师，因此无论何时他们学习新东西，目标都是构建。另一方面，数据科学家在成为工程师之前是科学家(如果他们有任何工程能力的话)，因此，只有在他们学会之后才能进行构建。

因此，数据科学家与代码的关系不同于工程师。数据科学家很少会在做实验时考虑编码最佳实践，相反，他们希望从实验的见解中学到一些有价值的东西。因此，**数据科学家利用代码从数据**中获得洞察力，并得出项目开始时制定的有趣问题的答案。

尽管存在差异，数据科学团队使用的最佳实践实际上是从软件开发最佳实践中借鉴来的。尽管存在大量的开发工作流，但是它们之间的一个共同点是，它们通常都包括定义规范、编写代码、代码审查、代码测试、代码集成以及将系统部署到生产环境中的步骤，以便它能够为业务服务。

同样，数据科学工作流也有其自身的共性。

## 数据科学工作流的一般方面

由于数据科学问题的本质——我们从一开始就不知道结果——很难定义一个在处理数据科学问题时应该普遍使用的具体模板。根据问题和数据的不同，您希望如何完成任务的路线图可能会有所不同，因此团队需要定义一个合适的结构。

尽管如此，当我们处理许多不同的问题时，不管数据集如何，我们都会看到非常常见的步骤。让我们来看看这些步骤。

***注*** *:下面定义的过程绝不是线性的。数据科学项目是相当迭代的，许多阶段是重复和/或重新访问的。*

### 定义问题

定义一个问题并不像看起来那么简单，因为有许多因素需要考虑，以确保解决正确的问题。定义问题时要考虑的问题如下:

*   我们试图解决什么问题？
*   我们的客户在使用我们的产品/服务时面临哪些挑战？
*   我们想更多地了解哪些见解？
*   我们目前面临哪些问题？

清楚地陈述问题的能力更像是一门艺术，但这是进行任何数据科学项目之前必不可少的第一步。如果没有一个团队所有成员都遵循的指南针，很容易花费大量时间做大量事情，而在增加业务价值方面没有太大进展。

### 数据收集

数据是工业数据科学项目中出现瓶颈最多的地方；很少有我们需要的所有数据都是现成的，因此，我们知道一些获取数据的技术是很重要的。这种现象被称为*数据采集*。

根据维基百科的描述，数据采集被描述为对测量现实世界物理条件的信号进行采样，并将结果样本转换为计算机可以操纵的数字数值的过程[ **来源** : [维基百科](https://web.archive.org/web/20221201180359/https://en.wikipedia.org/wiki/Data_acquisition)。

获取数据的方法有很多种，下面是一些想法:

*   公开日期
*   产品干预
*   数据扩充
*   从本质上讲，数据可以来自各种来源——这里有一些问题，但是详细描述它们超出了本文的范围。

数据探索

### 一旦收集的数据可供数据科学家访问，花时间熟悉这些数据是很重要的。

在这一阶段，在寻找模式和异常的同时，对数据进行假设是很重要的。你还想确定正在解决的问题的类型，例如，这是监督学习任务还是非监督学习任务？是分类任务还是回归任务？我们是在试图预测什么还是在推断什么？

**监督学习**包括建立一个模型，该模型学习基于输入-输出对的例子将输入映射到输出的函数。

*   **无监督学习**涉及建立一个从无标签数据中学习模式的模型。
*   **分类**是监督学习的一种形式，指的是模型输出为离散标签的建模问题。
*   **回归**是监督学习的一种形式，指的是模型输出连续的建模问题。
*   主要的要点是，我们希望足够好地理解我们的数据，以开发假设，当我们进入工作流的下一个阶段——数据建模时，我们可能会测试这些假设。

建模

### 一旦我们对数据进行了全面的研究，我们就会对我们所面临的问题类型有一个更好的了解，并且有希望在前一阶段产生一些我们可以尝试的假设。

由于数据科学是一门科学，我们可能需要测试各种不同的解决方案，然后才能得出我们希望如何推进项目的结论。每个实验或迭代将涉及 3 个阶段；

**构建**涉及使用训练数据学习和推广机器学习算法。

*   **拟合**涉及测量机器学习模型归纳为从未见过的与训练数据相似的例子的能力。
*   **验证**涉及使用来自训练数据的不同部分的测试数据来评估训练的模型。
*   阅读如何在专业视图中使用深入分析来[比较 Neptune](https://web.archive.org/web/20221201180359/https://docs.neptune.ai/you-should-know/comparing-runs) 中的多次运行。

传达结果

### 许多数据科学家在职业生涯的早期阶段沉迷于机器学习算法和艺术状态，然而，随着时间的推移，许多人开始意识到他们的注意力应该转移到所谓的*软技能上。*

清楚地传达您的结果是作为一名数据科学家最重要的技能之一，因为您将会做大量的工作。在这一阶段，数据科学家需要将发现、结果和/或故事反馈给各种利益相关者。在大多数情况下，这些利益相关者并不总是完全扎根于数据科学的人，因此能够改变您的信息以使您的受众能够理解是数据科学家工作流程中非常重要的一部分。

现有工作流程

## 数据科学工作流在该领域并不是一个新的专长，事实上，有许多框架可供团队选择。

Blitzstein & Pfister 工作流程

### 也被称为*数据科学流程*，Blitzstein 和 Pfister 为[哈佛 CS 109](https://web.archive.org/web/20221201180359/http://cs109.github.io/2015/) 课程精心设计了一个数据科学工作流框架。Blitzstein 表示，该课程的目标是为决定参加该课程的人介绍数据科学调查的整体流程。

Blitzstein 和 Pfister 工作流由数据科学项目的 5 个关键阶段组成；

第一阶段:问一个有趣的问题

*   **第二阶段:**获取数据
*   **第三阶段:**探索数据
*   **阶段 4:** 对数据进行建模
*   **第 5 阶段:**沟通并可视化结果
*   如前所述，数据科学工作流不是一个线性过程，相反，它是一个非线性的、迭代性极强的过程。数据科学过程强调首先建立一个问题的重要性，该问题将指导您的工作流，以及迭代的重要性，这可以通过将每个阶段与下一个阶段和之前的上一个步骤链接起来的线条来看出。

CRISP-DM

### CRISP-DM 是数据挖掘跨行业标准流程的缩写，是最受认可的数据科学工作流框架之一；CRISP-DM 的想法是创建一个可以跨行业使用的标准化流程。CRISP-DM 由 6 个迭代阶段组成，每个阶段都有应该完成的指定任务，以及一组可交付成果(即文档)。类似于 Blitzstein 和 Pfister 的工作流程，项目可以根据需要返回到之前的阶段。

如上述流程图所示，CRISP-DM 工作流程包括以下 6 个关键阶段:

**第一阶段**:业务理解

*   **第二阶段**:数据理解
*   **第三阶段**:数据准备
*   **第四阶段**:建模
*   **第五阶段**:评估
*   第六阶段:部署
*   OSEMN

### OSENM 最初是在希拉里·梅森和克里斯·维金斯 2010 年发表的一篇名为[*的博客文章中描述的，这是一种数据科学的分类法*](https://web.archive.org/web/20221201180359/http://www.dataists.com/2010/09/a-taxonomy-of-data-science/#:~:text=All%20of%20these%20are%20partially,%2C%20which%20rhymes%20with%20possum).) ，人们经常通过回忆 OSENM 听起来有多接近“possum”或“awesome”来记住这个框架。

然而，OSEMN 是一个首字母缩写词，代表着**O**b ain、 **S** crub、 **E** xplore、 **M** odel 和 i **N** terpret。很可能大多数数据科学家对每个术语都很熟悉，并且在研究每个术语时应该会感到舒适。虽然数据科学将需要 OSEMN 的组合元素，但作者确定“O”和“S”任务需要更多的数据黑客技能，而“E”和“M”需要更多的机器学习技能，因此没有数据科学家会在每个步骤上都是专家。

由于没有图像说明，我将分解每个步骤。

**#1 获取**

为了执行数据科学，我们需要数据，因此该框架将“O”(获取)定义为工作流中的第一步。作者不赞成手动数据收集过程(即使用鼠标点击、从文档中复制和粘贴等)。)效率低下且不可扩展。

根据作者的说法，获取数据的最有效方法是采用一系列工具，这些工具将允许您针对您的问题自动获取数据。这些工具包括 unix 命令行工具、SQL、web 抓取和 shell 脚本。

**注意:** *另外，作者提到 API 对于访问数据至关重要*

**#2 擦洗**

更常见的情况是，获得的数据会非常混乱——我们可以认为混乱的数据是被破坏的(即缺少值)。数据科学家经常在工作流程的这一部分花费大量时间，许多人会同意这可能是 21 世纪最性感工作中最不性感的部分。

对干净数据的简单分析可能比对嘈杂和不规则数据的复杂分析更有成效–数据科学的分类法

尽管如此，确保数据的干净、统一和一致可以在数据项目中获得极大的回报，因为你总是会得到你输入的内容——如果你输入垃圾，那么你可能会得到垃圾结果。

**#3 探索**

数据科学家通常将“探索”阶段称为探索性数据分析(或 EDA)。EDA 背后的想法是收集关于手边数据的情报。有了这种智能，数据科学家就可以收集想法，当我们进入数据科学工作流程的下一个阶段时，可以进一步测试这些想法。

在 Python 中执行 EDA 的一些有用工具有:

**熊猫**进行数据操作

*   **Matplotlib** 用于数据可视化
*   **Jupyter 笔记本电脑**易于使用的交互式数据科学环境
*   **降维**PCA 等方法
*   **聚类算法**寻找数据中的自然分组
*   **#4 型号**

模型阶段是我们实施各种机器学习算法来预测某个结果的阶段。我们将最佳模型定义为最适合我们预定义的评估度量的模型。

*“通常‘最佳’模型是最具预测性的模型”——数据科学的一种分类法*

**#5 解释**

为了确定模型的表现如何，我们通常在训练期间模型不可用的新的看不见的实例上验证它的表现——想法是模型将很好地推广到新的看不见的实例。

根据作者的说法，模型的解释能力定义了模型建议接下来进行的最有用/最有趣的实验的能力(这是因为模型输出的解释应该提供对正在解决的问题的更多洞察)。

一个数据科学家读到这里可能会觉得工作流似乎是显而易见的，但是在开始一个项目之前定义一个工作流仍然有巨大的价值。因此，团队的所有成员都会确切地了解正在发生什么，需要做什么，并且让团队的其他成员相互负责。

组织您的数据科学项目

## 数据科学项目通常适合作为更大系统的一部分。我们为项目编写的实际代码只是整个系统的一小部分，例如，当创建假新闻分类器时，与同一系统中的其他元素(即数据收集、特征提取、配置等)相比，机器学习代码非常小。).正是基于这一点，我倾向于将这样的项目视为软件项目。

**注**:好奇的读者可能想读读 [*隐藏在机器学习系统*](https://web.archive.org/web/20221201180359/https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf) *中的技术债。*

“分层方法被认为是比整体实施协议更好的做法，因为分别实施概念上不同的问题有几个好处”Buschmann 等人(1996 年)

结构描述了数据科学项目开发中元素的预定义排列和组织。开始时可能会很耗时，但决定记录并遵循一个结构对数据科学团队来说非常有用——这往往会在整个项目过程中产生回报。

为什么结构很重要？

### 对于那些还没有被说服的人，这里有两个原因可以解释为什么在项目开始时花点时间来建立一个结构是非常有益的。

**你未来的自己会感谢你**

我们在本文前面探索的所有工作流本质上都是非常迭代的——仅此一点就足以证明可再现性的重要性。在将来的某个时候，我们可能需要重现我们在某个实验中做过的事情，但是作为一个从业者，这可能是你最不想去的地方。我们更关心下一步可以尝试什么实验，我们从当前的实验中学到了什么，什么有效，什么无效。

数据科学家参与快速实验、可视化和分析并不罕见，但是，这种类型的工作状态不适合结构性思维——实验是混乱的！

没有一个正式的结构，回到过去的实验/项目本身就是一个全新的挑战。作为脚本的创建者，如果不清楚最终的训练脚本是 train.py、new_train.py 还是 last_train.py，那么您现在必须花时间浏览每个脚本，以确定哪个脚本实际上执行了代码——时间花得不值得。

当你告诉某人你能做某事，而他们告诉你去证明它，而当你尝试时，它却不起作用，这有多尴尬？没错。当你无法重现自己的实验时，这是同样的感觉。当你不得不执行不可避免的任务，重温你过去做过的工作时，保持一个良好的结构将会节省你很多时间。

**别人会感谢你**

数据科学是一个团队的事情，成为一个好的团队成员的一部分包括通过尽自己的力量来减轻队友的负担；由于数据科学可能被视为一项团队工作，因此在某个时候可能会有人想要重温您过去完成的某些东西(例如，他们可能想要扩展它、验证它、展示它、学习它等等。).

这个人可能是一个当前的团队成员，一个刚刚被雇佣的人，或者是一个对组织内的项目完全陌生的人。通过保持良好的结构，你可以为将来可能需要使用你的作品的人节省大量时间。

拟议布局

## 像数据科学工作流一样，在定义要遵循的结构时，没有一个尺寸适合所有人，但有一些关键组件往往会在大多数项目中重复出现。一旦我们创建了项目目录，第一步就是分离代码、数据、笔记本和模型。为了更好地从概念上理解，请看下图。

数据

![](img/d90cf9cf0faecd99b5f0a93878bfb14e.png)

*Image by Author*

### 首先是数据目录，它存储了项目所需的所有数据。在整个项目过程中，我们会处理不同类型的数据，因此我们最好将它们分开，以避免在进行实验时出现任何复杂情况。下面是我们如何在父数据目录下对不同的数据目录进行排序:

**Raw** :存储数据的本地副本可以确保数据的稳定性——如果数据库每天更新，可能会干扰您对当前状态的数据执行的任务。该目录中的数据是不可变的，这意味着存储在该目录中的数据将始终保持其原始状态。

*   **外部**:从第三方来源提取的数据。存放在这个目录中的数据应该是不可变的(也就是说，您不应该将您以任何方式转换的数据保存到这个目录中)。
*   **过渡**:任何转换(即连接、合并等。)取得的原始数据和外部数据(如果有的话)将被存储在临时目录中。该目录中的数据是任何特征工程的数据。
*   **已处理**:当我们对数据执行了特征工程后，我们可以继续将数据存储到已处理的目录中。这里，数据处于准备由机器学习算法消费的状态(如果需要)。
*   模型

### 模型目录是我们在训练后存储序列化机器学习算法的地方。我们在训练后存储我们的模型，因为我们需要恢复它们用于推断。但是，存储模型也是一个好主意，以防我们将来决定构建一个合奏，并比较模型，因为我们很可能会在项目过程中构建许多模型。

笔记本电脑

### 笔记本是制作原型、探索和交流见解的更好方式。笔记本电脑缺乏的是可扩展性和可再现性。虽然上图中没有说明，但将笔记本电脑分成单独的子目录也是一个好主意，例如；探索、原型等。

***注意*** *:使用好的命名约定将有助于将每台笔记本区分开来*

科学研究委员会

### Src 是源代码的缩写，所有的源代码都保存在这里。很难确切地知道我们需要什么样的脚本，但是作为一个通用的经验法则，您总是可以期望检索数据、执行特性工程和建模数据。

最后的想法

## 确定您团队的工作流程以及如何着手构建项目的最佳方式是评估团队成员，然后提出一套最适合团队价值观和目标的实践——评估现有框架并看看可以从它们那里获得什么也是一个好主意。

The best way to determine your team’s workflow and how to go about structuring projects is to assess the members of the team then come up with a set of practices that best fit the values and goals of the team – it may also serve as a good idea to assess existing frameworks and see what could be taken from them.