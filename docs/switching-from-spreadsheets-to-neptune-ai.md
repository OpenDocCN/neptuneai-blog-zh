# 从电子表格切换到 Neptune.ai，以及它如何将我的模型构建过程推向下一个层次

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/switching-from-spreadsheets-to-neptune-ai>

许多 ML 项目，包括 Kaggle 竞赛，都有类似的工作流程。您从一个带有基准模型的简单管道开始。

接下来，您开始合并改进:添加特性、扩充数据、调整模型……在每次迭代中，您评估您的解决方案并保留改进目标度量的更改。

![Iterative improvement process ML](img/0e9811e52b8912d75cba7a3ec7256542.png)

*The figure illustrates the iterative improvement process in ML projects. *
*Green lines indicate an improvement, red lines – a decrease in the score.*

这个工作流程包括运行大量的实验。随着时间的推移，很难跟踪进展和积极的变化。

你不去想新点子，而是花时间去思考:

*   “我已经试过这个东西了吗？”,
*   "上周运行良好的超参数值是多少？"

你最终会多次运行相同的东西。如果你还没有跟踪你的实验，我强烈建议你开始！

在我之前的 Kaggle 项目中，我曾经依赖电子表格进行跟踪。一开始它工作得很好，但是很快我意识到用实验元数据建立和管理电子表格需要大量的额外工作。我厌倦了每次实验后手动填写模型参数和性能值，非常想切换到自动化解决方案。

> [Neptune.ai]让我节省了大量时间，并专注于建模决策，这帮助我在 Kaggle 比赛中赢得了三枚奖牌。

这是我发现 Neptune.ai 的时候，这个工具让我节省了很多时间，专注于建模决策，帮助我在 Kaggle 比赛中获得了三枚奖牌。

在这篇文章中，我将分享我从电子表格切换到海王星进行[实验跟踪](/web/20221117203610/https://neptune.ai/experiment-tracking)的故事。我将描述电子表格的一些缺点，解释 Neptune 如何帮助解决这些缺点，并给出一些使用 Neptune 进行 Kaggle 的技巧。

## 电子表格用于实验跟踪有什么问题？

电子表格有很多用途。要跟踪实验，您可以简单地设置一个电子表格，其中包含不同的列，包含管道的相关参数和性能。与队友分享这个电子表格也很容易。

![ML experiment tracking with spreadsheets](img/b937de60746fe4c77d6bc35ab9c124f1.png)

*The figure illustrates ML experiment tracking with spreadsheets.*

听起来很棒，对吧？

不幸的是，这有一些问题。

### 手工作业

做了一段时间后，你会注意到维护一个电子表格开始消耗太多时间。您需要为每个新实验手动填充一行元数据，并为每个新参数添加一列。一旦你的管道变得更加复杂，这种**将会失去控制。**

也很容易**出现错别字**，从而导致糟糕的决策。

在一次深度学习竞赛中，我在一次实验中错误地输入了学习率。看着电子表格，我得出结论，高学习率降低了准确性，并继续从事其他工作。直到几天后，我才意识到有一个错别字，糟糕的表现实际上来自于学习率低。基于一个错误的结论，我花了两天时间在错误的方向上投资。

### 没有实时跟踪

使用电子表格，你需要等到实验完成才能记录性能。

除了每次都要手动完成而感到沮丧之外，这也不允许您比较实验的中间结果，这有助于查看新的运行是否有希望。

当然，您可以在每个时期后登录模型性能，但是为每个实验手动进行需要更多的时间和精力。我从来没有足够的勤奋来定期做这件事，结果花费了一些计算资源。

### 附件限制

电子表格的另一个问题是它们**只支持可以在单元格**中输入的文本元数据。

如果您想要附加其他元数据，比如:

*   模型重量，
*   源代码，
*   带有模型预测的图，
*   输入数据版本？

您**需要手动将这些内容存储在电子表格之外的项目文件夹**中。

在实践中，在本地机器、Google Colab、Kaggle 笔记本和您的队友可能使用的其他环境之间组织和同步实验输出变得很复杂。将这样的元数据附加到跟踪电子表格看起来很有用，但是很难做到。

## 从电子表格切换到 Neptune

几个月前，我们的团队正在进行木薯叶疾病竞赛，并使用谷歌电子表格进行实验跟踪。挑战开始一个月后，我们的电子表格已经很混乱了:

*   有些跑步表现不佳，因为我们中的一个人忘记了登录，并且不再有结果。
*   带有损失曲线的 pdf 散布在 Google Drive 和 Kaggle 笔记本上。
*   有些参数可能输入不正确，但是恢复和仔细检查旧的脚本版本太费时间了。

基于我们的电子表格很难做出好的数据驱动型决策。

尽管只剩下四周时间，我们还是决定转到海王星。我惊讶地发现，我们实际上花了很少的力气就设置好了。简而言之，有三个主要步骤:

*   注册一个 Neptune 帐户并创建一个项目，
*   在您的环境中安装 neptune 包，
*   包括流水线中的几行，以便能够记录相关的元数据。

你可以在这里阅读更多关于开始使用 Neptune [的步骤。当然，浏览文档和熟悉平台可能需要几个小时。但是记住这只是一次性投资。在学习了一次这个工具之后，我能够自动完成大部分的跟踪，并且在接下来的 Kaggle 比赛中依靠 Neptune，只需要很少的额外努力。](https://web.archive.org/web/20221117203610/https://docs.neptune.ai/)

查看文档，了解如何在 Neptune 组织人工智能实验，或者跳转到[的示例项目](https://web.archive.org/web/20221117203610/https://app.neptune.ai/common/quickstarts/experiments?split=bth&dash=leaderboard&viewId=64a274e0-25bc-49e0-b1ca-f093fb2b8f7b)并探索应用程序(不需要注册)。

## 海王星有什么好的？

![Spreadsheets vs Neptune](img/68b3784b4e55d3f4e952b9b59452bb68.png)

*The figure illustrates ML experiment tracking with Neptune.*

### 更少的手工工作

与电子表格相比，Neptune 的一个关键优势是**它为您节省了大量手工工作**。使用 Neptune，您可以使用管道中的 API 在代码运行时自动上传和存储元数据。

```py
import neptune.new as neptune

run = neptune.init(project='#', api_token='#') 

config = {
    "batch_size": 64,
    "learning_rate": 0.001,
    "optimizer": "Adam"
    }
run["parameters"] =  config

for epoch in range(100):
    run["train/accuracy"].log(epoch * 0.6)

run["f1_score"] = 0.66

```

您不必手动将它放入结果表中，并且您也**避免了打错字。**因为元数据是直接从代码发送到 Neptune 的，所以无论数字有多少位，你都会得到正确的数字。

> …在每次实验中登录节省的时间积累得非常快，并带来切实的收益…这给了你一个机会…更好地专注于建模决策。

这听起来可能是一件小事，但是从每次实验中节省下来的时间积累得非常快，并在项目结束时带来切实的收益。这给你一个机会，不要过多考虑实际的跟踪过程，更好地关注建模决策。在某种程度上，这就像雇用一名助理来处理一些无聊(但非常有用)的日志任务，以便您可以更专注于创造性的工作。

### 实时跟踪

我喜欢 Neptune 的一点是，它允许你进行实时跟踪。如果您使用的是神经网络或梯度增强等模型，这些模型在收敛之前需要进行大量迭代，那么您知道及早查看损失动态对于检测问题和比较模型非常有用。

在电子表格中跟踪中间结果太令人沮丧了。Neptune API 可以在每个时期甚至每个批次之后记录性能，这样您就可以在实验仍在运行时开始比较学习曲线。

> …很多 ML 实验的结果都是否定的…用 Neptune dashboard 对比中间的图和前几个性能值，可能就足以意识到你需要停止实验，改变一些东西了。

这证明是非常有用的。正如你所料，许多 ML 实验都有负面结果(抱歉，但你花了几天时间研究的这个伟大想法实际上降低了准确性)。

这完全没问题，因为这就是 ML 的工作方式。

不好的是，您可能需要等待很长时间，直到从您的管道中获得负面信号。用 Neptune dashboard 对比中间的图和前几个性能值，可能就足以意识到你需要停止实验，改变一些东西了。

### 附加输出

海王星的另一个优势是能够在每次实验中附加几乎任何东西。这确实有助于将模型权重和预测等重要输出保存在一个地方，并且可以从您的实验表中轻松访问它们。

如果您和您的同事在不同的环境中工作并且必须手动上传输出以同步文件，这**尤其有用。**

我还喜欢将源代码附加到每次运行的能力，以确保您有产生相应结果的笔记本版本。如果您想恢复一些没有提高性能的更改，并想回到以前的最佳版本，这将非常有用。

## 使用 Neptune 提高 Kaggle 性能的技巧

当在 Kaggle 比赛中工作时，我可以给你一些提示来进一步改善你的跟踪体验。

### 在 Kaggle 笔记本或 Google Colab 中使用 Neptune

首先， **Neptune 对于在使用 GPU/TPU 时有会话时间限制的 Kaggle 笔记本或 Google Colab** 中工作非常有帮助。我记不清有多少次，当培训时间只比允许的 9 小时限制多几分钟时，由于笔记本电脑崩溃，我丢失了所有的实验结果！

为了避免这种情况，我强烈建议设置 Neptune，以便在每个时期后存储模型权重和损失度量。这样，即使你的 Kaggle 笔记本超时，你也会有一个检查点上传到 Neptune 服务器来继续你的训练。您还将有机会将您在会话崩溃之前的中间结果与其他实验进行比较，以判断它们的潜力。

### 用 Kaggle 排行榜分数更新跑步记录

第二，Kaggle 项目中需要跟踪的一个重要指标是排行榜分数。有了 Neptune，你可以自动跟踪你的交叉验证分数，但在代码中获取排行榜分数是不可能的，因为它需要你通过 Kaggle 网站提交预测。

将您的实验的排行榜分数添加到 Neptune 跟踪表的最便捷方式是使用[“恢复跑步”功能](https://web.archive.org/web/20221117203610/https://docs.neptune.ai/you-should-know/logging-and-managing-runs-results/resume-run#how-to-resume-run)。**它允许你用几行代码用一个新的指标更新任何完成的实验。**这个特性也有助于恢复跟踪崩溃的会话，我们在上一段中已经讨论过了。

```py
import neptune.new as neptune

run = neptune.init(project=’Your-Kaggle-Project’, run="SUN-123")

run[“LB_score”] = 0.5

model = run["train/model_weights"].download()

```

### 下载实验元数据

最后，我知道许多 Kagglers 喜欢对他们的提交进行复杂的分析，比如估计 CV 和 LB 分数之间的相关性，或者绘制最佳分数相对于时间的动态图。

虽然在网站上做这些事情还不可行，但是 Neptune 允许你使用一个简单的 API 调用将所有实验的元数据直接下载到你的笔记本 [中。这使得更深入地研究结果或导出元数据表并在外部与使用不同跟踪工具或不依赖任何实验跟踪的人共享变得容易。](https://web.archive.org/web/20221117203610/https://docs.neptune.ai/you-should-know/logging-and-managing-runs-results/downloading-runs-data)

```py
import neptune.new as neptune

my_project = neptune.get_project('Your-Workspace/Your-Kaggle-Project')

sophia_df = my_project.fetch_runs_table(owner='sophia').to_pandas()
sophia_df.head()

```

## 最后的想法

在这篇文章中，我分享了我从电子表格切换到海王星来跟踪 ML 实验的故事，并强调了海王星的一些优势。我想再次强调，在基础设施工具上投入时间——无论是实验跟踪、代码版本控制，还是其他任何东西——总是一个好的决定，并且可能会随着生产力的提高而得到回报。

用电子表格跟踪实验元数据比不做任何跟踪要好得多。它将帮助您更好地看到您的进展，了解哪些修改改进了您的解决方案，并帮助您做出建模决策。用电子表格做同样会花费你额外的时间和精力。**像 Neptune 这样的工具将实验跟踪提升到了一个新的水平**，允许您自动记录元数据并专注于建模决策。

希望你觉得我的故事有用。祝你未来的 ML 项目好运！