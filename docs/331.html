<html>
<head>
<title>Image Segmentation: Architectures, Losses, Datasets, and Frameworks </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>图像分割:架构、损失、数据集和框架</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/image-segmentation#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/image-segmentation#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>在这篇文章中，我们将使用深度学习深入研究图像分割的世界。</p>



<p>让我们开始吧。</p>



<h2 id="what-it-is">什么是图像分割？</h2>



<p>顾名思义，这是将图像分割成多个片段的过程。在这个过程中，图像中的每个像素都与一个对象类型相关联。有两种主要类型的图像分割——语义分割和实例分割。</p>



<p>在语义分割中，相同类型的所有对象使用一个类别标签来标记，而在实例分割中，相似的对象得到它们自己的单独标签。</p>


<div class="wp-block-image wp-image-19061 size-full">
<figure class="aligncenter"><img decoding="async" src="../Images/33ad428c294c2ad7e9a986231f1e5585.png" alt="image segmentation" class="wp-image-19061" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230224202420im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/semantic_vs_instance.png?ssl=1"/><figcaption class="wp-element-caption"><em>Anurag Arnab, Shuai Zheng et. al 2018 “Conditional Random Fields Meet Deep Neural Networks for Semantic Segmentation” | <a href="https://web.archive.org/web/20230224202420/https://ieeexplore.ieee.org/document/8254255" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>






<h2 id="architectures">图像分割架构</h2>



<p>图像分割的基本架构由编码器和解码器组成。</p>


<div class="wp-block-image">
<figure class="aligncenter"><img decoding="async" src="../Images/b04db0a1a568043655448dbe210fb597.png" alt="" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230224202420im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Image-segmentation-architectures.png?ssl=1"/><figcaption class="wp-element-caption"><em>Vijay Badrinarayanan et. al 2017 “SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation” | <a href="https://web.archive.org/web/20230224202420/https://arxiv.org/abs/1511.00561" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<p>编码器通过过滤器从图像中提取特征。解码器负责生成最终输出，该输出通常是包含对象轮廓的分割掩模。大多数架构都有这种架构或其变体。</p>



<p>让我们来看看其中的几个。</p>



<h3>优信网</h3>



<p>U-Net是一种卷积神经网络，最初开发用于分割生物医学图像。它的结构看起来像字母U，因此得名U-Net。它的架构由两部分组成，左边部分是收缩路径，右边部分是扩张路径。收缩路径的目的是捕获上下文，而扩展路径的作用是帮助精确定位。</p>


<div class="wp-block-image">
<figure class="aligncenter"><img decoding="async" src="../Images/81b85b688008a8b6ffa160dd9a3ed15a.png" alt="U-net architecture image segmentation" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230224202420im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/U-net-architecture.png?ssl=1"/><figcaption class="wp-element-caption"><em>Olaf Ronneberger et. al 2015 “U-net architecture image segmentation” | <a href="https://web.archive.org/web/20230224202420/https://arxiv.org/abs/1505.04597" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<p>收缩路径由两个三乘三的回旋组成。卷积之后是校正的线性单元和用于下采样的2乘2最大池计算。</p>



<p>U-Net的完整实现可以在<a href="https://web.archive.org/web/20230224202420/https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" target="_blank" rel="noreferrer noopener nofollow">这里</a>找到。</p>



<h3>Fast fcn—快速全卷积网络</h3>



<p>在这种架构中，联合金字塔上采样(JPU)模块用于取代<a href="https://web.archive.org/web/20230224202420/https://arxiv.org/pdf/1808.08931.pdf" target="_blank" rel="noreferrer noopener nofollow">扩张卷积，因为它们消耗大量内存和时间</a>。它的核心使用全连接网络，同时应用JPU进行上采样。JPU将低分辨率要素地图上采样为高分辨率要素地图。</p>


<div class="wp-block-image">
<figure class="aligncenter"><img decoding="async" src="../Images/099810cb4a432eaf2fdc14ca79c80166.png" alt="" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230224202420im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/FastFCN.png?ssl=1"/><figcaption class="wp-element-caption"><em>Huikai Wu et.al 2019 “FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation” | <a href="https://web.archive.org/web/20230224202420/https://arxiv.org/abs/1903.11816" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<p>如果你想尝试一些代码实现，<a href="https://web.archive.org/web/20230224202420/https://github.com/wuhuikai/FastFCN" target="_blank" rel="noreferrer noopener nofollow">这就是你要做的</a>。</p>



<h3>门控SCNN</h3>



<p>这种架构由双流CNN架构组成。在这个模型中，一个单独的分支用于处理图像形状信息。形状流用于处理边界信息。</p>


<div class="wp-block-image">
<figure class="aligncenter"><img decoding="async" src="../Images/8216cf07b76b729d2e7f575cb28d65fd.png" alt="" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230224202420im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Gated-SCNN.png?ssl=1"/><figcaption class="wp-element-caption"><em>Towaki Takikawa et. al 2019 “Gated-SCNN: Gated Shape CNNs for Semantic Segmentation” | <a href="https://web.archive.org/web/20230224202420/https://arxiv.org/abs/1907.05740" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<p>你可以通过检查<a href="https://web.archive.org/web/20230224202420/https://github.com/nv-tlabs/gscnn" target="_blank" rel="noreferrer noopener nofollow">这里的代码</a>来实现它。</p>



<h3>DeepLab</h3>



<p>在这种架构中，带有上采样滤波器的卷积用于涉及密集预测的任务。通过atrous空间金字塔池实现多尺度的对象分割。最后，使用DCNNs来改进目标边界的定位。阿特鲁卷积是通过插入零对滤波器进行上采样或者对输入特征图进行稀疏采样来实现的。</p>


<div class="wp-block-image">
<figure class="aligncenter"><img decoding="async" src="../Images/584aff13dd7f20be3ac28f9b5c5022e0.png" alt="" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230224202420im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/DeepLab.png?ssl=1"/><figcaption class="wp-element-caption"><em>Liang-Chieh Chen et. al 2016 “DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs” | <a href="https://web.archive.org/web/20230224202420/https://arxiv.org/abs/1606.00915" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<p>你可以在<a href="https://web.archive.org/web/20230224202420/https://github.com/fregu856/deeplabv3" target="_blank" rel="noreferrer noopener nofollow"> PyTorch </a>或者<a href="https://web.archive.org/web/20230224202420/https://github.com/sthalles/deeplab_v3" target="_blank" rel="noreferrer noopener nofollow"> TensorFlow </a>上尝试它的实现。</p>



<h3>屏蔽R-CNN</h3>



<p>在这个<a href="https://web.archive.org/web/20230224202420/https://github.com/facebookresearch/Detectron" target="_blank" rel="noreferrer noopener nofollow">架构</a>中，使用边界框和语义分割对对象进行分类和定位，将每个像素分类到一组类别中。每个感兴趣的区域得到一个分割掩模。产生类别标签和边界框作为最终输出。该架构是更快的R-CNN的扩展。更快的R-CNN由提出区域的深度卷积网络和利用区域的检测器组成。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/5dae29c5fa5f97f9b5e5b2bccdbfe0cc.png" alt="" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230224202420im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Mask-R-CNN.png?resize=580%2C300&amp;ssl=1"/><figcaption class="wp-element-caption"><em>Kaiming He et. al 2017 “Mask R-CNN” | <a href="https://web.archive.org/web/20230224202420/https://arxiv.org/abs/1703.06870" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<p>下面是在COCO测试集上获得的结果图像。</p>


<div class="wp-block-image">
<figure class="aligncenter"><img decoding="async" src="../Images/ad75d77e2d8504cb1342e109abe7c975.png" alt="Mask R-CNN results on the COCO test set" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230224202420im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Mask-R-CNN-results-on-the-COCO-test-set.png?ssl=1"/><figcaption class="wp-element-caption"><em>Kaiming He et. al 2017 “Mask R-CNN” | <a href="https://web.archive.org/web/20230224202420/https://arxiv.org/abs/1703.06870" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<p>现在，让我们研究几个Mask R-CNN用例，为图像中的每个对象自动分割和构建像素级遮罩。</p>



<h4 id="use-cases">屏蔽R-CNN用例</h4>



<p>对于今天的介绍性用例演示，我们将重点关注用于图像分割的Mask R-CNN框架。具体地，我们将利用在前述COCO数据集上预先训练的掩模R-CNN模型的权重来构建推理类型的模型。</p>



<p>在模型构建过程中，我们还将设置Neptune实验来跟踪和比较不同超参数调整的预测性能。</p>



<p>现在，让我们开始吧！</p>



<p>\ u 003 cimg class = \ u 0022 lazy load block-blog-intext-CTA _ _ arrow-image \ u 0022 src = \ u 0022 https://Neptune . ai/WP-content/themes/Neptune/img/image-ratio-holder . SVG \ u 0022 alt = \ u 0022 \ u 0022 width = \ u 002212 \ u 0022 height = \ u 002222 \ u 022</p>



<p>首先，我们需要安装所需的包并设置我们的环境。在本练习中，将使用由<a href="https://web.archive.org/web/20230224202420/https://github.com/matterport/Mask_RCNN" target="_blank" rel="noreferrer noopener nofollow"> Matterport </a>实现的算法。因为到目前为止还没有这个包的发行版，所以我把从Github repo克隆来安装它的几个步骤放在一起:</p>



<p>这里需要注意的一点是，最初的Matterport代码没有更新为与Tensorflow 2+兼容。因此，对于包括我在内的所有Tensorflow 2+用户来说，让它工作变得非常具有挑战性，因为这需要对源代码进行重大修改。如果您不想自定义您的代码，Tensorflow 2+的更新版本也可以从<a href="https://web.archive.org/web/20230224202420/https://github.com/akTwelve/Mask_RCNN">这里</a>获得。因此，请确保根据您的Tensorflow版本克隆正确的回购。</p>



<p><strong>第一步:</strong>克隆屏蔽R-CNN GitHub repo</p>



<ul>
<li>2.2.4之前的Tensorflow 1+和keras:</li>
</ul>



<pre class="hljs">git clone https://github.com/matterport/Mask_RCNN.git</pre>







<pre class="hljs">git clone https://github.com/akTwelve/Mask_RCNN.git updated_mask_rcnn</pre>



<p>这将创建一个名为“updated_mask_rcnn”的新文件夹，以区分更新版本和原始版本。</p>



<p><strong>第二步:</strong></p>



<ul>
<li>检查并安装软件包相关性</li>



<li>导航到包含回购的文件夹</li>



<li>运行:pip install -r requirements.txt</li>
</ul>



<p><strong>第三步:</strong></p>



<ul>
<li>运行安装程序来安装软件包</li>



<li>运行:python setup.py clean -all install</li>
</ul>



<p>需要思考的几点:</p>



<ol>
<li>如果您遇到此错误消息:<code>ZipImportError: bad local file header: mask_rcnn-2.1-py3.7.egg.</code>，请升级您的setuptools。</li>



<li>对于windows用户，如果要求您安装pycocotools，请确保使用pip install pycocotools-windows，而不是pycocotools，因为它可能与Windows存在兼容性问题。</li>
</ol>



<p><strong>\ u 003 cimg class = \ u 0022 lazy load block-blog-intext-CTA _ _ arrow-image \ u 0022 src = \ u 0022 https://Neptune . ai/WP-content/themes/Neptune/img/image-ratio-holder . SVG \ u 0022 alt = \ u 0022 \ u 0022 width = \ u 002212 \ u 022 height = \ u 0022212 \ u 002</strong></p>



<p>接下来，从Mask_RCNN项目Github中，我们将模型权重下载到当前工作目录:<a href="https://web.archive.org/web/20230224202420/https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5" target="_blank" rel="noreferrer noopener nofollow"> mask_rcnn_coco.h5 </a></p>



<p><strong>\ u 003 cimg class = \ u 0022 lazy load block-blog-intext-CTA _ _ arrow-image \ u 0022 src = \ u 0022 https://Neptune . ai/WP-content/themes/Neptune/img/image-ratio-holder . SVG \ u 0022 alt = \ u 0022 \ u 0022 width = \ u 002212 \ u 022 height = \ u 0022212 \ u 002</strong></p>



<p>当涉及到模型训练过程时，Neptune提供了一种有效且易于使用的方法来跟踪和记录几乎所有与模型相关的内容，从超参数规范到最佳模型保存，再到绘图记录的结果等等。使用Neptune跟踪的<a href="/web/20230224202420/https://neptune.ai/experiment-tracking" target="_blank" rel="noreferrer noopener">实验的酷之处在于，它会自动生成性能图表，供从业者比较不同的运行，从而选择一个最优的运行。</a></p>



<p>关于配置你的海王星环境和设置你的实验的更详细的解释，请查看这个<a href="https://web.archive.org/web/20230224202420/https://docs.neptune.ai/getting-started/installation" target="_blank" rel="noreferrer noopener">完整指南</a>和我的另一个关于<a href="/web/20230224202420/https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras" target="_blank" rel="noreferrer noopener">在</a> Keras中实现宏F1分数的博客。</p>



<p>在这篇博客中，我还将演示如何在图像分割实现过程中利用Neptune。是的，海王星可以很好地用于跟踪图像处理模型！</p>



<p>导入所有必需的包:</p>



<pre class="hljs">
<span class="hljs-keyword">import</span> neptune.new <span class="hljs-keyword">as</span> neptune

<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> skimage.io
<span class="hljs-keyword">import</span> matplotlib
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

ROOT_DIR = os.path.abspath(PATH_TO_YOUR_WORK_DIRECTORY) 


sys.path.append(ROOT_DIR)  
<span class="hljs-keyword">from</span> mrcnn <span class="hljs-keyword">import</span> utils
<span class="hljs-keyword">import</span> mrcnn.model <span class="hljs-keyword">as</span> modellib
<span class="hljs-keyword">from</span> mrcnn <span class="hljs-keyword">import</span> visualize
<span class="hljs-keyword">from</span> mrcnn.config <span class="hljs-keyword">import</span> Config
<span class="hljs-keyword">from</span> mrcnn.model <span class="hljs-keyword">import</span> MaskRCNN
<span class="hljs-keyword">from</span> mrcnn.visualize <span class="hljs-keyword">import</span> display_instances
<span class="hljs-keyword">from</span> mrcnn.model <span class="hljs-keyword">import</span> log

<span class="hljs-keyword">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> load_img
<span class="hljs-keyword">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> img_to_array

sys.path.append(os.path.join(ROOT_DIR, <span class="hljs-string">"samples/coco/"</span>))  
<span class="hljs-keyword">import</span> coco


MODEL_DIR = os.path.join(ROOT_DIR, <span class="hljs-string">"logs"</span>)


COCO_MODEL_PATH = os.path.join(ROOT_DIR, <span class="hljs-string">"mask_rcnn_coco.h5"</span>)

</pre>



<p>现在，让我们用Neptune专门为这个图像分割附加工作创建一个项目:</p>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20230224202420im_/https://neptune.ai/wp-content/uploads/1.Neptune_CreateProject-1.mp4"/></figure>



<p>接下来，在Python中，创建一个连接到我们的图像分割项目的Neptune实验，以便我们可以记录和监视模型信息并输出到Neptune:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune
<span class="hljs-keyword">import</span> os

myProject = <span class="hljs-string">'YourUserName/YourProjectName'</span>
project = neptune.init(api_token=os.getenv(<span class="hljs-string">'NEPTUNE_API_TOKEN'</span>),
                       project=myProject)
project.stop()


npt_exp = neptune.init(
        api_token=os.getenv(<span class="hljs-string">'NEPTUNE_API_TOKEN'</span>),
        project=myProject,
        name=<span class="hljs-string">'implement-MaskRCNN-Neptune'</span>,
        tags=[<span class="hljs-string">'image segmentation'</span>, <span class="hljs-string">'mask rcnn'</span>, <span class="hljs-string">'keras'</span>, <span class="hljs-string">'neptune'</span>])

</pre>



<p>几个音符:</p>



<ol>
<li>neptune.init()中的api_token arg获取从配置步骤中生成的Neptune API</li>



<li>project.create_experiment()中的标记arg是可选的，但是为给定的项目指定标记以便于共享和跟踪是很好的。</li>
</ol>



<p>在我的演示中有了ImageSegmentationProject，以及成功设置的初始实验，我们可以进入建模部分了。</p>



<p><strong>\ u 003 cimg class = \ u 0022 lazy load block-blog-intext-CTA _ _ arrow-image \ u 0022 src = \ u 0022 https://Neptune . ai/WP-content/themes/Neptune/img/image-ratio-holder . SVG \ u 0022 alt = \ u 0022 \ u 0022 width = \ u 002212 \ u 022 height = \ u 0022212 \ u 002</strong></p>



<p>为了运行图像分割和推断，我们需要将我们的模型定义为Mask R-CNN类的一个实例，并构造一个config对象作为输入该类的一个参数。这个配置对象的目的是指定如何利用我们的模型进行训练和预测。</p>



<p>为了预热，让我们只为最简单的实现指定批量大小。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">InferenceConfig</span><span class="hljs-params">(coco.CocoConfig)</span>:</span>
    GPU_COUNT = <span class="hljs-number">1</span>
    IMAGES_PER_GPU = <span class="hljs-number">1</span>

config = InferenceConfig()


npt_exp[<span class="hljs-string">'Model Config Pars'</span>] = str(config.to_dict())</pre>



<p>这里，batch size = GPU _ COUNT * IMAGES _ PER _ GPU，其中两个值都设置为1，因为我们将一次对一个图像进行分割。我们还将配置信息发送到Neptune，这样我们就可以跟踪我们的实验。</p>



<p>这个视频剪辑显示了我们将在海王星项目中看到的东西，我将它放大以显示细节。</p>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20230224202420im_/https://neptune.ai/wp-content/uploads/2.Neptune_ModelConfig.mp4"/></figure>



<p><strong>\ u 003 cimg class = \ u 0022 lazy load block-blog-intext-CTA _ _ arrow-image \ u 0022 src = \ u 0022 https://Neptune . ai/WP-content/themes/Neptune/img/image-ratio-holder . SVG \ u 0022 alt = \ u 0022 \ u 0022 width = \ u 002212 \ u 022 height = \ u 0022212 \ u 002</strong></p>



<p>随着所有准备工作的完成，我们继续最令人兴奋的部分——在真实图像上进行推理，看看模型做得如何。</p>



<p>对于任务#1，我们将使用这张图片，它可以从这里免费下载。</p>





<p>下面演示如何定义我们的Mask R-CNN模型实例:</p>



<pre class="hljs">
model = modellib.MaskRCNN(mode=<span class="hljs-string">"inference"</span>, model_dir=MODEL_DIR, config=config)


model.load_weights(COCO_MODEL_PATH, by_name=<span class="hljs-keyword">True</span>)


image_path = path_to_image_monks
img = load_img(image_path)
img = img_to_array(img)


results = model.detect([img], verbose=<span class="hljs-number">1</span>)</pre>



<p>思考要点:</p>



<ol>
<li>我们将当前模型的类型指定为“推断”，表明我们正在进行图像预测/推断。</li>



<li>对于要进行预测的掩模R-CNN模型，必须将图像转换为Numpy数组。</li>



<li>我们调用model.detect()函数，而不是像对Keras模型预测那样使用model.predict()。</li>
</ol>



<p>太棒了。现在我们有了分割结果，但是我们应该如何检查结果并从中获得相应的图像呢？嗯，模型输出是一个包含多个组件的字典，</p>



<ul>
<li><strong>ROI</strong>:分割对象的感兴趣区域(ROI)。</li>



<li><strong>遮罩</strong>:被分割对象的遮罩。</li>



<li><strong> class_ids </strong>:被分割对象的类ID整数。</li>



<li><strong>分数</strong>:每个片段属于一个类别的预测概率。</li>
</ul>



<p>为了可视化输出，我们可以使用下面的代码。</p>



<pre class="hljs">
image_results = results[<span class="hljs-number">0</span>]

box, mask, classID, score = image_results[<span class="hljs-string">'rois'</span>], image_results[<span class="hljs-string">'masks'</span>], image_results[<span class="hljs-string">'class_ids'</span>], image_results[<span class="hljs-string">'scores'</span>]


fig_images, cur_ax = plt.subplots(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">15</span>))
display_instances(img, box, mask, classID, class_names, score, ax=cur_ax)


npt_exp[<span class="hljs-string">'Predicted Image'</span>].upload(neptune.types.File.as_image(fig_images))</pre>



<p>这里，class_names指的是COCO数据集中80个对象标签/类别的列表。可以从<a href="https://web.archive.org/web/20230224202420/https://github.com/YiLi225/NeptuneBlogs/blob/main/Image_Segmentation_MaskRCNN.py" target="_blank" rel="noreferrer noopener nofollow">我的Github </a>复制粘贴。</p>



<p>运行上面的代码会返回Neptune实验中的预测输出图像，</p>





<p>令人印象深刻不是吗！我们的模型成功地分割了僧侣/人类和狗。更令人印象深刻的是，该模型为每个细分分配了非常高的概率/置信度得分(即接近1)！</p>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20230224202420im_/https://neptune.ai/wp-content/uploads/3.Neptune_Task1_RunModel.mp4"/></figure>



<p><strong>\ u 003 cimg class = \ u 0022 lazy load block-blog-intext-CTA _ _ arrow-image \ u 0022 src = \ u 0022 https://Neptune . ai/WP-content/themes/Neptune/img/image-ratio-holder . SVG \ u 0022 alt = \ u 0022 \ u 0022 width = \ u 002212 \ u 022 height = \ u 0022212 \ u 002</strong></p>



<p>现在你可能会认为我们的模型在最后一张图像上做得很好，可能是因为每个对象都在焦点上，这使得分割任务更容易，因为没有太多混杂的背景对象。背景模糊的图像怎么样？该模型能达到同等水平的性能吗？</p>



<p>我们一起实验吧。</p>



<p>下图是一只可爱的泰迪熊，背景是模糊的蛋糕。</p>





<p>为了更好地组织代码，我们可以将前面提到的模型推理步骤编译成函数runMaskRCNN，它接受两个主要参数。:模型配置和图像路径:</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">runMaskRCNN</span><span class="hljs-params">(modelConfig, imagePath, MODEL_DIR=MODEL_DIR, COCO_MODEL_PATH=COCO_MODEL_PATH)</span>:</span>
    <span class="hljs-string">'''
    Args:
        modelConfig: config object
        imagePath: full path to the image
   '''</span>

    model = modellib.MaskRCNN(mode=<span class="hljs-string">"inference"</span>, model_dir=MODEL_DIR, config=modelConfig)
    model.load_weights(COCO_MODEL_PATH, by_name=<span class="hljs-keyword">True</span>)

    image_path = imagePath
    img = load_img(image_path)
    img = img_to_array(img)

    results = model.detect([img], verbose=<span class="hljs-number">1</span>)
    modelOutput = results[<span class="hljs-number">0</span>]

    <span class="hljs-keyword">return</span> modelOutput, img</pre>



<p>在试验第一个模型时，我们使用与任务#1相同的配置。这些信息将被发送到海王星进行跟踪和比较。</p>



<pre class="hljs">cur_image_path = path_to_image_teddybear
image_results, img = runMaskRCNN(modelConfig=config, imagePath=cur_image_path)


npt_exp[<span class="hljs-string">'Model Config Pars'</span>] = str(config.to_dict())
fig_images, cur_ax = plt.subplots(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">15</span>))
display_instances(img, image_results[<span class="hljs-string">'rois'</span>], image_results[<span class="hljs-string">'masks'</span>], image_results[<span class="hljs-string">'class_ids'</span>], class_names, image_results[<span class="hljs-string">'scores'</span>], ax=cur_ax)


npt_exp[<span class="hljs-string">'Predicted Image'</span>].upload(neptune.types.File.as_image(fig_images))</pre>



<p>根据这个模型的预测，下面的输出图像应该出现在我们的Neptune实验日志中。</p>





<p>正如我们所看到的，该模型成功地分割了背景中的泰迪熊和纸杯蛋糕。就纸杯蛋糕封面而言，模型以相当高的概率/置信度将它标记为“瓶子”，这同样适用于下面的纸杯蛋糕托盘，它被标识为“碗”。两者都有道理！</p>



<p>总的来说，我们的模型很好地识别了每个物体。然而，我们也注意到蛋糕的一部分被错误地标注为“泰迪熊”，概率得分为0.702(即中间的绿色方框)。</p>



<p>我们如何解决这个问题？</p>



<p><strong>\ u 003 cimg class = \ u 0022 lazy load block-blog-intext-CTA _ _ arrow-image \ u 0022 src = \ u 0022 https://Neptune . ai/WP-content/themes/Neptune/img/image-ratio-holder . SVG \ u 0022 alt = \ u 0022 \ u 0022 width = \ u 002212 \ u 022 height = \ u 0022212 \ u 002</strong></p>



<p>我们可以构建一个定制的模型配置来覆盖基本配置类中的超参数。因此，要专门为这个泰迪熊图像定制建模过程:</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CustomConfig</span><span class="hljs-params">(coco.CocoConfig)</span>:</span>
    <span class="hljs-string">"""Configuration for inference on the teddybear image.
    Derives from the base Config class and overrides values specific
    to the teddybear image.
    """</span>
    
    NAME = <span class="hljs-string">"customized"</span>
    
    NUM_CLASSES = <span class="hljs-number">1</span> + <span class="hljs-number">80</span>

    
    GPU_COUNT = <span class="hljs-number">1</span>
    IMAGES_PER_GPU = <span class="hljs-number">1</span>

    
    STEPS_PER_EPOCH = <span class="hljs-number">500</span>
    
    DETECTION_MIN_CONFIDENCE = <span class="hljs-number">0.71</span>

    
    LEARNING_RATE = <span class="hljs-number">0.06</span>
    LEARNING_MOMENTUM = <span class="hljs-number">0.7</span>
    WEIGHT_DECAY = <span class="hljs-number">0.0002</span>

    VALIDATION_STEPS = <span class="hljs-number">30</span>

config = CustomConfig()


npt_exp.send_text(<span class="hljs-string">'Model Config Pars'</span>, str(config.to_dict()))</pre>



<p>在使用这个新配置运行模型之后，我们将在Neptune项目日志中看到这个具有正确分段的图像，</p>





<p>在我们的自定义配置类中，我们指定了类的数量、每个时期的步数、学习速率、权重衰减等等。有关超参数的完整列表，请参考软件包中的config.py文件。</p>



<p>我们鼓励您尝试不同的(超参数)组合，并设置您的Neptune项目来跟踪和比较它们的性能。下面的视频剪辑展示了我们刚刚建立的两个模型以及它们在海王星的预测结果。</p>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20230224202420im_/https://neptune.ai/wp-content/uploads/4.Neptune_Task2_CompareModels.mp4"/></figure>



<p><strong>\ u 003 cimg class = \ u 0022 lazy load block-blog-intext-CTA _ _ arrow-image \ u 0022 src = \ u 0022 https://Neptune . ai/WP-content/themes/Neptune/img/image-ratio-holder . SVG \ u 0022 alt = \ u 0022 \ u 0022 width = \ u 002212 \ u 022 height = \ u 0022212 \ u 002</strong></p>



<p>对于那些想用我们的模型深入杂草的极客观众来说，我们也可以收集并可视化这个CNN模型每一层的权重和偏差。下面的代码片段演示了如何对前5个卷积层执行此操作。</p>



<pre class="hljs">
LAYER_TYPES = [<span class="hljs-string">'Conv2D'</span>]

layers = model.get_trainable_layers()
layers = list(filter(<span class="hljs-keyword">lambda</span> l: l.__class__.__name__ <span class="hljs-keyword">in</span> LAYER_TYPES, layers))
print(f<span class="hljs-string">'Total layers = {len(layers)}'</span>)


layers = layers[:<span class="hljs-number">5</span>]


fig, ax = plt.subplots(len(layers), <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>*len(layers)+<span class="hljs-number">10</span>),
                       gridspec_kw={<span class="hljs-string">"hspace"</span>:<span class="hljs-number">1</span>})

<span class="hljs-keyword">for</span> l, layer <span class="hljs-keyword">in</span> enumerate(layers):
    weights = layer.get_weights()
    <span class="hljs-keyword">for</span> w, weight <span class="hljs-keyword">in</span> enumerate(weights):
        tensor = layer.weights[w]
        ax[l, w].set_title(tensor.name)
        _ = ax[l, w].hist(weight[w].flatten(), <span class="hljs-number">50</span>)


npt_exp[<span class="hljs-string">'Model_Weights'</span>].upload(neptune.types.File.as_image(fig))

npt_exp.stop()
</pre>



<p>这是海王星显示的截图，显示了层权重的直方图，</p>





<h2 id="loss-functions">图像分割损失函数</h2>



<p>语义分割模型通常在训练期间使用简单的交叉分类熵损失函数。然而，如果你对获得图像的粒度信息感兴趣，那么你必须回到稍微高级一点的损失函数。</p>



<p>让我们来看几个例子。</p>



<h3>焦点损失</h3>



<p>这种损失是对标准交叉熵准则的改进。这是通过改变它的形状来实现的，使得分配给分类良好的例子的损失是向下加权的。最终，这确保了没有阶级不平衡。在该损失函数中，随着正确类别的置信度增加，交叉熵损失随着缩放因子在零处衰减而缩放。比例因子在训练时自动降低简单示例的权重，并关注困难的示例。</p>





<h3>骰子损失</h3>



<p>这个损失是通过计算平滑的<a href="https://web.archive.org/web/20230224202420/https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient" target="_blank" rel="noreferrer noopener nofollow">骰子系数</a>函数得到的。这种损失是最常用的损失是分割问题。</p>





<h3>并集上的交集(IoU)-平衡损耗</h3>



<p>IoU平衡的分类损失旨在增加具有高IoU的样本的梯度，并降低具有低IoU的样本的梯度。这样，提高了机器学习模型的定位精度。</p>





<h3>边界损失</h3>



<p>边界损失的一个变体应用于具有高度不平衡分割的任务。这种损失的形式是空间轮廓上的距离度量，而不是区域。以这种方式，它解决了由高度不平衡的分割任务的区域损失引起的问题。</p>





<h3>加权交叉熵</h3>



<p>在交叉熵的一个变体中，所有的正例都被某个系数加权。它用于涉及阶级不平衡的情况。</p>





<h3>Lovász-Softmax损失</h3>



<p>该损失基于子模块损失的凸Lovasz扩展，对神经网络中的平均交并损失进行直接优化。</p>





<p>值得一提的其他损失有:</p>



<ul>
<li>其目的是确保网络在训练过程中专注于硬样本。</li>



<li><strong>距离惩罚ce损失</strong>将网络导向难以分割的边界区域。</li>



<li><strong>敏感性-特异性(SS)损失</strong>，计算特异性和敏感性的均方差的加权和。</li>



<li><strong>豪斯多夫距离(HD)损失</strong>估计卷积神经网络的豪斯多夫距离。</li>
</ul>



<p>这些只是图像分割中使用的几个损失函数。要探索更多，请查看这个<a href="https://web.archive.org/web/20230224202420/https://github.com/JunMa11/SegLoss" target="_blank" rel="noreferrer noopener nofollow">回购</a>。</p>



<h2 id="datasets">图像分割数据集</h2>



<p>如果你还在这里，你可能会问自己从哪里可以得到一些数据集。</p>



<p>我们来看几个。</p>



<h3>1.上下文中的常见对象— Coco数据集</h3>



<p>COCO是一个大规模的对象检测、分割和字幕数据集。<a href="https://web.archive.org/web/20230224202420/https://www.tensorflow.org/datasets/catalog/coco" target="_blank" rel="noreferrer noopener nofollow">数据集</a>包含91个类。它有25万人的关键点。其下载大小为37.57 GiB。它包含80个对象类别。它在<a href="https://web.archive.org/web/20230224202420/https://www.apache.org/licenses/LICENSE-2.0" target="_blank" rel="noreferrer noopener nofollow"> Apache 2.0许可</a>下可用，并且可以从<a href="https://web.archive.org/web/20230224202420/http://cocodataset.org/#download" target="_blank" rel="noreferrer noopener nofollow">这里</a>下载。</p>



<h3>2.PASCAL可视对象类(PASCAL VOC)</h3>



<p>PASCAL有20个不同类别的9963个图像。训练/验证集是一个2GB的tar文件。数据集可以从<a href="https://web.archive.org/web/20230224202420/http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" target="_blank" rel="noreferrer noopener nofollow">官网下载。</a></p>



<h3>3.Cityscapes数据集</h3>



<p>该数据集包含城市场景的图像。它可用于评估视觉算法在城市场景中的性能。数据集可以从<a href="https://web.archive.org/web/20230224202420/https://www.cityscapes-dataset.com/downloads/" target="_blank" rel="noreferrer noopener nofollow">这里</a>下载。</p>



<h3>4.剑桥驾驶标记视频数据库——CamVid</h3>



<p>这是一个基于运动的分割和识别数据集。它包含32个语义类。这个<a href="https://web.archive.org/web/20230224202420/http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/" target="_blank" rel="noreferrer noopener nofollow">链接</a>包含进一步的解释和数据集的下载链接。</p>



<h2 id="frameworks">图像分割框架</h2>



<p>现在你已经有了可能的数据集，让我们来介绍几个你可以用来开始的工具/框架。</p>



<ul>
<li><a href="https://web.archive.org/web/20230224202420/https://docs.fast.ai/" target="_blank" rel="noreferrer noopener nofollow"> <em> FastAI库</em> </a> —给定一幅图像，该库能够创建图像中对象的遮罩。</li>



<li><a href="https://web.archive.org/web/20230224202420/http://www.fexovi.com/sefexa.html" target="_blank" rel="noreferrer noopener nofollow"> <em> Sefexa图像分割工具</em> </a> — Sefexa是一款免费工具，可用于半自动图像分割、图像分析和创建地面真相</li>



<li><a href="https://web.archive.org/web/20230224202420/https://github.com/facebookresearch/deepmask" target="_blank" rel="noreferrer noopener nofollow"><em>deep mask</em></a><em>—deep mask</em>由脸书研究是<a href="https://web.archive.org/web/20230224202420/http://arxiv.org/abs/1506.06204"> DeepMask </a>和<a href="https://web.archive.org/web/20230224202420/http://arxiv.org/abs/1603.08695"> SharpMask </a>的火炬实现</li>



<li><a href="https://web.archive.org/web/20230224202420/https://github.com/facebookresearch/multipathnet" target="_blank" rel="noreferrer noopener nofollow"> <em>多路径</em> </a> —这一火炬实现的物体检测网络来自于<a href="https://web.archive.org/web/20230224202420/https://arxiv.org/abs/1604.02135" target="_blank" rel="noreferrer noopener nofollow">一多路径物体检测网络</a>。</li>



<li><a href="https://web.archive.org/web/20230224202420/https://opencv.org/about/" target="_blank" rel="noreferrer noopener nofollow"> <em> OpenCV </em> </a> —这是一个开源的计算机视觉库，拥有超过2500种优化算法。</li>



<li><a href="https://web.archive.org/web/20230224202420/https://github.com/frankkramer-lab/MIScnn/wiki" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>——</em>是一个医学图像分割开源库。它允许在几行代码中建立具有最先进的卷积神经网络和深度学习模型的管道。</li>



<li><a href="https://web.archive.org/web/20230224202420/https://www.fritz.ai/image-segmentation/" target="_blank" rel="noreferrer noopener nofollow"><em>Fritz</em></a><em>:</em>Fritz为移动设备提供了包括图像分割工具在内的多种计算机视觉工具。</li>
</ul>



<h2 id="h-final-thoughts">最后的想法</h2>



<p>希望这篇文章给你一些图像分割的背景知识，以及一些工具和框架。我们还希望用例演示能够激发您开始探索深度神经网络这一迷人领域的兴趣</p>



<p>我们涵盖了:</p>



<ul>
<li>什么是图像分割，</li>



<li>一些图像分割架构，</li>



<li>一些图像分割损失，</li>



<li>图像分割工具和框架，</li>



<li>掩模R-CNN算法的用例实现。</li>
</ul>



<p>要了解更多信息，请查看每个架构和框架的链接。此外，Neptune项目在这里<a href="https://web.archive.org/web/20230224202420/https://app.neptune.ai/katyl/ImageSegmentationProject/experiments?compare=IwGlyA&amp;split=bth&amp;dash=charts&amp;viewId=standard-view" target="_blank" rel="noreferrer noopener">可用</a>，完整代码可以在这个Github repo <a href="https://web.archive.org/web/20230224202420/https://github.com/YiLi225/NeptuneBlogs/blob/main/Image_Segmentation_MaskRCNN_NewVersion.py" target="_blank" rel="noreferrer noopener">这里</a>访问。</p>



<p>快乐分段！</p>
        </div>
        
    </div>    
</body>
</html>