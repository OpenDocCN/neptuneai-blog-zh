<html>
<head>
<title>Object Detection with YOLO: Hands-on Tutorial </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>YOLO物体检测:实践教程</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/object-detection-with-yolo-hands-on-tutorial#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/object-detection-with-yolo-hands-on-tutorial#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            <div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img decoding="async" src="../Images/4d722b6d7731526e30611110693d69eb.png" alt="" class="wp-image-51623" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203617im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/image1.jpg?resize=600%2C399&amp;ssl=1"/><figcaption><em>Object detection in action. <br/><a href="https://web.archive.org/web/20221117203617/https://ai.googleblog.com/2017/06/supercharge-your-computer-vision-models.html" target="_blank" rel="noreferrer noopener nofollow">Source</a>: Supercharge your Computer Vision models with the TensorFlow Object Detection API, <br/>Jonathan Huang, Research Scientist and Vivek Rathod, Software Engineer,<br/>Google AI Blog</em></figcaption></figure></div>


<h2 id="h-object-detection-as-a-task-in-computer-vision">作为计算机视觉任务的目标检测</h2>



<p>我们生活中每天都会遇到物体。环顾四周，你会发现多个物体围绕着你。作为一个人，你可以很容易地发现和识别你看到的每一个物体。这很自然，不需要太多努力。</p>



<p>然而，对于计算机来说，检测物体是一项需要复杂解决方案的任务。对计算机来说,“检测物体”意味着处理输入图像(或视频中的单个帧),并以图像上的物体及其位置的信息做出响应。用计算机视觉的术语来说，我们把这两个任务叫做<em>分类</em>和<em>定位</em>。我们希望计算机能说出给定图像上呈现的是哪种物体，以及它们的确切位置。</p>



<p>已经开发了多种解决方案来帮助计算机检测物体。今天，我们将探索一种叫做YOLO的最先进的算法，它能以实时速度实现高精度。特别是，我们将学习如何在TensorFlow / Keras中的自定义数据集上训练该算法。</p>



<p>首先，让我们看看YOLO到底是什么，它以什么而闻名。</p>



<h2 id="yolo">作为实时物体探测器的YOLO</h2>



<h3>什么是YOLO？</h3>



<p>YOLO是“你只看一次”的首字母缩写(不要和《辛普森一家》中的“你只活一次”混淆)。顾名思义，单个“外观”就足以找到图像上的所有对象并识别它们。</p>



<p>用机器学习的术语来说，我们可以说所有的物体都是通过单次算法运行检测到的。这是通过将图像划分为网格并预测网格中每个单元的边界框和分类概率来实现的。如果我们想使用YOLO进行汽车检测，下面是网格和预测边界框的样子:</p>


<div class="wp-block-image">
<figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/c838f97d4b0647711780ba4d44cd64b7.png" alt="" class="wp-image-51626" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203617im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/0_h_3FEWPgMDf6TBUo.jpg?resize=600%2C687&amp;ssl=1"/><figcaption><em>Grid that YOLO builds (black cells).<br/>Bounding box that YOLO predicts for the first car is in red.<br/>Bounding box that YOLO predicts for the second car is yellow.</em><br/><em><a href="https://web.archive.org/web/20221117203617/https://heartbeat.fritz.ai/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2-65fe59ac12d" target="_blank" rel="noreferrer noopener nofollow">Source</a></em> <em> of the image.</em> </figcaption></figure></div>


<p>上面的图像只包含过滤后得到的最后一组盒子。值得注意的是，YOLO的原始输出包含同一个对象的许多边界框。这些盒子的形状和大小不同。正如您在下图中所看到的，一些盒子在捕捉目标对象方面表现得更好，而算法提供的其他盒子则表现不佳。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/53293579bd9121f3492da63a62e85e13.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh5.googleusercontent.com/Ys_zo4WytCaTOhvDY1jLQtBbiRwcPBMB9kJHaHCSb-5zJuMYmWljon4JfmIIMN5JD_sFm5bTpf5uMW6tmEtV5-iqdNajFdjfPc_flGFCMf7ByBpIf7112R5ApJmzkURxvV67aFZV=s0"/><figcaption><em>All bounding boxes in red are predicted by YOLO to capture the first car.<br/>All yellow boxes are for the second car.<br/>The bold red and yellow boxes are the best for car detection.<br/><a href="https://web.archive.org/web/20221117203617/https://heartbeat.fritz.ai/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2-65fe59ac12d" target="_blank" rel="noreferrer noopener nofollow">Source</a></em> <em> of the image.</em> </figcaption></figure></div>


<p>为了选择给定对象的最佳边界框，应用了<a href="https://web.archive.org/web/20221117203617/https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c" target="_blank" rel="noreferrer noopener nofollow">非最大抑制(NMS) </a>算法。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/833ee10b7383841ebb375f4e1999bdef.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh5.googleusercontent.com/6qQuk3r8RMlw2_G98xRew1_09VvCLNmIyVSXcTwwe7YemALUbqshhQLLzd9TFnEnXpIMo0OXOI1cjlL4YVV9o9XvUXJ5M4apza9GjGP7LOjUbjXzls8z9VRMDE2jz1sRXkfF_Ezk=s0"/><figcaption><em>Non-maximum suppression will handle the multiple bounding <br/>boxes predicted for the cars to keep only those that best capture objects.<br/><a href="https://web.archive.org/web/20221117203617/http://datahacker.rs/deep-learning-non-max-suppression/" target="_blank" rel="noreferrer noopener nofollow">Source</a> of the image.</em></figcaption></figure></div>


<p>YOLO预测的所有盒子都有与之相关联的置信度。NMS使用这些置信度值来去除预测可信度低的框。通常，这些都是预测置信度低于0.5的框。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/c2f9b7fa3c29e903e10d21d499b05136.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh3.googleusercontent.com/CMFAaCPn8T7gH140bzSuHSft1Fwi7gocDaChfeX2EBvYUTPKfmkAVs-OD7Y8LB19wcaoEkOJvPHIS0mhilX3dneP71DRHkXXdkdCLUT87OnX2SkHuie3xyKCrbLiIqhnqau66e1e=s0"/><figcaption><em>Each bounding box is predicted with a confidence level.<br/>You can see the confidence scores in the top-left corner of each box, next to the object name.<br/><a href="https://web.archive.org/web/20221117203617/https://sandipanweb.wordpress.com/2018/03/11/autonomous-driving-car-detection-with-yolo-in-python/" target="_blank" rel="noreferrer noopener nofollow">Source</a> of the image.</em></figcaption></figure></div>


<p>当所有不确定的边界框都被移除时，只留下具有高置信度的框。为了从表现最好的候选中选出最好的一个，NMS选择了可信度最高的盒子，并计算它与周围其他盒子的交集。如果一个<a href="https://web.archive.org/web/20221117203617/https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" target="_blank" rel="noreferrer noopener nofollow">交点</a>高于特定的阈值水平，则具有较低置信度的边界框被移除。如果NMS比较两个具有低于选定阈值的交集的框，则两个框都保留在最终预测中。</p>



<h3>YOLO与其他探测器相比</h3>



<p>虽然卷积神经网络(CNN)在YOLO的引擎盖下使用，但它仍然能够实时检测对象。这是可能的，因为YOLO有能力用一步到位的方法同时进行预测。</p>



<p>其他较慢的对象检测算法(如<a href="https://web.archive.org/web/20221117203617/https://arxiv.org/abs/1506.01497" target="_blank" rel="noreferrer noopener nofollow">更快的R-CNN </a>)通常使用两阶段方法:</p>



<ul><li>在第一阶段，选择感兴趣的图像区域。这些是图像中可能包含任何对象的部分；</li><li>在第二阶段，使用卷积神经网络对这些区域中的每一个进行分类。</li></ul>



<p>通常，在一幅图像上有许多带有物体的区域。所有这些区域都被送去分类。分类是一项耗时的操作，这就是两阶段对象检测方法比一阶段检测方法执行速度慢的原因。</p>



<p>YOLO不会选择图像中有趣的部分，这是没有必要的。取而代之的是，它在一次正向网络传递中预测整个图像的边界框和类。</p>



<p>下面你可以看到与其他流行的探测器相比，YOLO的速度有多快。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/f0d49471392d79f878fb6accdb235a0d.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh5.googleusercontent.com/UMdf0t7xY2h-1Zw4CgqWnmiI7u_l0qzzIPBJzTZiiKOeaA1MqDpqZ1MKnmqpUceQ4S85YpDp98DKncHg0edH_yPhVjFN1-WaWoZUQqXPHQeCcxvW4E7lbjy2pMP56XvmaaPRU03x=s0"/><figcaption><em>Frames per second (FPS) is a metric that lets us compare the speed of different object detectors.<br/>SSD and YOLO are one stage object detectors whereas Faster-RCNN <br/>and R-FCN are two-stage object detectors.<br/><a href="https://web.archive.org/web/20221117203617/https://www.researchgate.net/figure/Comparison-of-frames-processed-per-second-FPS-implementing-the-Faster-R-CNN-R-FCN-SSD_fig6_342570032" target="_blank" rel="noreferrer noopener nofollow">Source</a> of the image.</em></figcaption></figure></div>


<h3>YOLO的版本</h3>



<p>2015年，Joseph Redmon在其名为“你只看一次:统一、实时的物体检测”的研究论文中首次介绍了YOLO。</p>



<p>自那以后，YOLO发生了很大的变化。2016年，Joseph Redmon <a href="https://web.archive.org/web/20221117203617/https://arxiv.org/abs/1612.08242" target="_blank" rel="noreferrer noopener nofollow">在《YOLO9000:更好、更快、更强》中描述了第二个YOLO版本</a>。</p>



<p>大约在第二次YOLO升级两年后，Joseph提出了另一个网络升级。他的论文名为“<a href="https://web.archive.org/web/20221117203617/https://arxiv.org/abs/1804.02767" target="_blank" rel="noreferrer noopener nofollow"> YOLOv3:增量改进</a>”，引起了许多计算机工程师的注意，并在机器学习社区中流行起来。</p>



<p>2020年，Joseph Redmon决定停止研究计算机视觉，但这并没有阻止YOLO被其他人开发。同年，一个由三名工程师组成的团队(阿列克谢·博奇科夫斯基、钱和廖宏远)设计了第四版的YOLO，比以前更快更精确。他们在2020年4月23日发表的“<a href="https://web.archive.org/web/20221117203617/https://arxiv.org/abs/2004.10934" target="_blank" rel="noreferrer noopener nofollow"> YOLOv4:物体探测的最佳速度和精度</a>”论文中描述了他们的发现。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/7a2a15d676f81d4cd4402c584eb93a53.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh6.googleusercontent.com/YKjt_fr0rIar7ZJOZrulaTiZ9nN1SKyKVpF_ZZqCRumYqdrwmHiGMIsO5dbGqE1d5fMAKt-TY2AzIVLnOgYa-S0mTmotlEijfTqLDYEOR1LKFJJyOHAWwJfRoWYJ4hChpw1Jh1bH=s0"/><figcaption><em>YOLOv4 compared to other detectors, including YOLOv3.<br/>AP on the Y-axis is a metric called “average precision”. It describes the accuracy of the net.<br/>FPS (frames per second) on the X-axis is a metric that describes speed.<br/><a href="https://web.archive.org/web/20221117203617/https://neurohive.io/en/news/yolo-v4-is-the-new-state-of-the-art-object-detector/" target="_blank" rel="noreferrer noopener nofollow">Source</a> of the image.</em></figcaption></figure></div>


<p class="has-text-align-center"><a href="https://web.archive.org/web/20221117203617/https://docs.google.com/document/d/1_tvAY9ZogD-JtyhpPrEbMhDogMSaxQQrS7bEncxaE-I/edit#heading=h.f81isuazfv25"/></p>



<p>第四版发布两个月后，独立开发者Glenn Jocher发布了YOLO的第五版。这一次，没有发表研究论文。这个网络作为PyTorch实现出现在Jocher的GitHub页面上。第五版的精确度和第四版差不多，但速度更快。</p>



<p>最后，在2020年7月，我们得到了另一个大的YOLO更新。在一篇名为“<a href="https://web.archive.org/web/20221117203617/https://arxiv.org/abs/2007.12099" target="_blank" rel="noreferrer noopener nofollow"> PP-YOLO:物体探测器</a>的有效和高效实现”的论文中，项龙和他的团队提出了一个新版本的YOLO。YOLO的这一迭代是基于第三个模型版本，并超过了YOLO v4的性能。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/144f586bea7e20c64f8609d5c9e46417.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh5.googleusercontent.com/moHTLhHxHf4pUS8iVipcIYD5VsID_7EOs1jHWTw2j8p9Hc7U9hew15QEE8yWbWYeGUgJ6b7Xek0BDSdgSoAuaEunP4sVW1zQ814UsmulxqjuexWJ5wWU5QbF0gtsw3vmcRtkyNCh=s0"/><figcaption><em>PP-YOLO compared to other detectors, including YOLOv4.<br/>The mAP on the Y-axis is a metric called “ mean average precision”. It describes the accuracy of the net.<br/>FPS (frames per second) on the X-axis is a metric that describes speed.<br/><a href="https://web.archive.org/web/20221117203617/https://neurohive.io/en/news/yolo-v4-is-the-new-state-of-the-art-object-detector/" target="_blank" rel="noreferrer noopener nofollow">Source</a> of the image.</em></figcaption></figure></div>


<p>在本教程中，我们将仔细研究YOLOv4及其实现。为什么是YOLOv4？三个原因:</p>



<ul><li>它在机器学习社区得到了广泛的认可；</li><li>这个版本已经在广泛的探测任务中证明了它的高性能；</li><li>YOLOv4已经在多个流行的框架中实现，包括我们将要合作的TensorFlow和Keras。</li></ul>



<h3>YOLO应用示例</h3>



<p>在我们进入本文的实际部分，实现我们自定义的基于YOLO的对象检测器之前，我想向您展示几个很酷的YOLOv4实现，然后我们将进行我们的实现。</p>



<p>注意预测有多快多准！</p>



<p>这是YOLOv4可以做的第一个令人印象深刻的例子，从不同的游戏和电影场景中检测多个物体。</p>



<p>或者，您可以从现实生活的摄像机视角查看<a href="https://web.archive.org/web/20221117203617/https://www.youtube.com/watch?v=-d6-thAu9dc" target="_blank" rel="noreferrer noopener nofollow">这个物体检测演示</a>。</p>



<h2 id="tensorflow">YOLO作为TensorFlow &amp; Keras中的对象检测器</h2>



<h3>机器学习中的TensorFlow和Keras框架</h3>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/fc7d73caf662012aba982c3cc7f2b7ed.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh4.googleusercontent.com/z185d1dPGtW_sfsVPYFNBZzXvbBGSoT8UTDNLePFHPXogOZvT46exEO_JOef5ja4aPppRHhOxEgLAf0eLPf-7dq4Jm-wpAgB8vM9vEwZ8TlML5UswhWUvZzqrRb9HenBi4NedZ-i=s0"/><figcaption><em>TensorFlow &amp; Keras logos.<br/><a href="https://web.archive.org/web/20221117203617/https://habr.com/ru/post/482126/" target="_blank" rel="noreferrer noopener nofollow">Source</a> of the image.</em></figcaption></figure></div>


<p>框架在每个信息技术领域都是必不可少的。机器学习也不例外。在ML市场上有几个成熟的玩家帮助我们简化整体的编程体验。PyTorch、scikit-learn、TensorFlow、Keras、MXNet和Caffe只是值得一提的几个。</p>







<p>今天，我们将与TensorFlow/Keras密切合作。毫不奇怪，这两个是机器学习领域中最受欢迎的框架。这主要是因为TensorFlow和Keras都提供了丰富的开发能力。这两个框架非常相似。无需深究细节，需要记住的关键事情是Keras只是TensorFlow框架的一个包装器。</p>



<h3>TensorFlow &amp; Keras中的YOLO实现</h3>



<p>在撰写本文时，<a href="https://web.archive.org/web/20221117203617/https://github.com/search?q=yolo+keras" target="_blank" rel="noreferrer noopener nofollow">有808个存储库</a>在TensorFlow / Keras后端实现了YOLO。YOLO版本4是我们将要实现的。将搜索限制在YOLO v4，我得到了<a href="https://web.archive.org/web/20221117203617/https://github.com/search?q=yolov4+keras" target="_blank" rel="noreferrer noopener nofollow"> 55个存储库</a>。</p>



<p>仔细浏览所有这些，我发现<a href="https://web.archive.org/web/20221117203617/https://github.com/taipingeric/yolo-v4-tf.keras" target="_blank" rel="noreferrer noopener nofollow">是一个有趣的候选人</a>继续下去。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/63156359654f513e63565d12f7e1295b.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh5.googleusercontent.com/qDxAW5aC8uhI03Vz_g9xr4atiMFNUIRh2n4IHlBYKRNW_DLaH3f23t_vHJE8SbcSCXmXnB5gNNAbi4SyZ__dCSxgkkrLULJW4uH3LnbJoMDNEm_IeNsr6bSbTgZEHUghJjMovZWF=s0"/><figcaption><em>Repo with the YOLO implementation that we’re going to work with.<br/><a href="https://web.archive.org/web/20221117203617/https://github.com/taipingeric/yolo-v4-tf.keras" target="_blank" rel="noreferrer noopener nofollow">Source</a> of the image.</em></figcaption></figure></div>


<p>这个实现是由<a href="https://web.archive.org/web/20221117203617/https://github.com/taipingeric" target="_blank" rel="noreferrer noopener nofollow"> taipingeric </a>和<a href="https://web.archive.org/web/20221117203617/https://github.com/jimmyaspire" target="_blank" rel="noreferrer noopener nofollow"> jimmyaspire </a>开发的。如果你以前使用过TensorFlow和Keras，这非常简单和直观。</p>



<p>要开始使用这个实现，只需将repo克隆到您的本地机器上。接下来，我将向您展示如何使用YOLO开箱即用，以及如何训练您自己的自定义对象检测器。</p>



<h3>如何开箱即用地运行预先培训的YOLO并获得结果</h3>



<p>查看回购的<a href="https://web.archive.org/web/20221117203617/https://github.com/taipingeric/yolo-v4-tf.keras" target="_blank" rel="noreferrer noopener nofollow">“快速启动”部分</a>，您可以看到，要启动并运行一个模型，我们只需导入YOLO作为一个类对象，并加载模型权重:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> models <span class="hljs-keyword">import</span> Yolov4
model = Yolov4(weight_path=<span class="hljs-string">'yolov4.weights'</span>,
               class_name_path=<span class="hljs-string">'class_names/coco_classes.txt'</span>)</pre>



<p>请注意，您需要提前手动下载模型权重。YOLO附带的模型权重文件来自COCO数据集，可以在GitHub 的<a href="https://web.archive.org/web/20221117203617/https://github.com/AlexeyAB/darknet" target="_blank" rel="noreferrer noopener nofollow"> AlexeyAB官方darknet项目页面获得。</a></p>



<p>紧接着，模型完全准备好在推理模式下处理图像。只需对您选择的图像使用predict()方法。该方法是TensorFlow和Keras框架的标准。</p>



<pre class="hljs">pred = model.predict(<span class="hljs-string">'input.jpg'</span>)

</pre>



<p>例如，对于此输入图像:</p>





<p>我得到了以下模型输出:</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/3e4d915a436869721dc9912ba5b1a790.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh5.googleusercontent.com/auT6jp8D2RKi4pKb-wEiD-WbZXlRJoSxkxaBAeDczHVo_FM8d0yA24kbZCqEXgsUX7lySnw0v9XOCH-Re9Vk3mhMNB4YDZoR4wV7O1KdTsXKXVwuiRCc0FCidKs5j_XYdmHfExpy=s0"/><figcaption><em>Prediction made by pre-trained YOLOv4</em></figcaption></figure></div>


<p>模型做出的预测以熊猫数据框的形式方便地返回。我们得到每个检测到的对象的类名、盒子大小和坐标:</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/6ff41cc42d83c3729ee08a37eadfb586.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh3.googleusercontent.com/zT6ylwqNLdF5J6A_2aliSmt8zo5G3sFCdQfKBmfgnjYJ7bK-UX5DJzSkCbmKf0i4kAuxeFuncCPEVVCa6CZCoSg8BsR94TOmfJQgzeQSlMZ6gVi-cK5eJaKM4AcaPL20WDvn4RM8=s0"/><figcaption><em>DataFrame returned as a result of prediction. <br/>Lots of useful information about the detected objects</em></figcaption></figure></div>


<p>predict()方法中有多个参数，让我们指定是否要用预测的边界框、每个对象的文本名称等来绘制图像。查看predict()方法附带的docstring，以熟悉我们可以使用的内容:</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/57d4941b33605adad5e2ae5731cd6df2.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh3.googleusercontent.com/nyRDij2LHB97Dp_bVfq9QWtxnm1Md2tnH2ocv4GYKsFHZ5LE0JtsOej_k6G1SXyqmmOXVxR3k9aStSwmzOx8ivWVCM1aBM-ckLrOJx3qOCzr-GDgke26ZENuA1xU5DC7onUCDuWJ=s0"/><figcaption><em>Parameters available within the predict method</em></figcaption></figure></div>


<p>您应该预料到您的模型将只能检测严格限于COCO数据集的对象类型。要了解预训练的YOLO模型能够检测哪些对象类型，请查看…/yolo-v4-tf.kers/class_names/中的coco_classes.txt文件。那里有80种对象类型。</p>



<h2 id="custom">如何训练您的自定义YOLO对象检测模型</h2>



<h3>任务陈述</h3>





<p>要设计一个对象检测模型，你需要知道你想要检测什么对象类型。这应该是您想要为其创建检测器的有限数量的对象类型。当我们转移到实际的模型开发时，准备一个对象类型的列表是很好的。</p>



<p>理想情况下，您还应该有一个包含您感兴趣的对象的带注释的数据集。该数据集将用于训练检测器并对其进行验证。如果您还没有数据集或注释，不要担心，我将向您展示在哪里以及如何获得它。</p>



<h3>数据集和注释</h3>



<h4>从哪里获取数据</h4>



<p>如果你有一个带注释的数据集，跳过这一部分，进入下一章。但是，如果您的项目需要数据集，我们现在将探索您可以获取数据的在线资源。</p>



<p>你在哪个领域工作并不重要，很有可能已经有一个开源数据集可以用于你的项目。</p>



<p>我推荐的第一个资源是由<a href="https://web.archive.org/web/20221117203617/https://medium.com/towards-artificial-intelligence/50-object-detection-datasets-from-different-industry-domains-1a53342ae13d?source=rss----98111c9905da---4" target="_blank" rel="noreferrer noopener nofollow"> Abhishek Annamraju </a>撰写的“<a href="https://web.archive.org/web/20221117203617/https://towardsai.net/p/computer-vision/50-object-detection-datasets-from-different-industry-domains" target="_blank" rel="noreferrer noopener nofollow">来自不同行业领域的50多个对象检测数据集</a>”文章，他收集了时尚、零售、体育、医药等行业的精彩注释数据集。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/44d411d20d95af79b28eae1b03363a8d.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh4.googleusercontent.com/xKKOADhD8Wlhi9bgYYDfzp-dBHX2T6ASuBdf_M5dDmnoxHTpPVwaH29HyS3yspd_7_gxL9M5e2qo8_wHDW_flvLaRxhHs2Je97jngX-5CSSYzrUxHz_erLZC30j7asMMwejMwLdi=s0"/><figcaption><em>Example of an annotated dataset for the driving car systems.<br/><a href="https://web.archive.org/web/20221117203617/https://towardsai.net/p/computer-vision/50-object-detection-datasets-from-different-industry-domains" target="_blank" rel="noreferrer noopener nofollow">Source</a> of the image.</em></figcaption></figure></div>


<p>另外两个寻找数据的好地方是<a href="https://web.archive.org/web/20221117203617/https://paperswithcode.com/datasets?task=object-detection&amp;page=1" target="_blank" rel="noreferrer noopener nofollow">paperswithcode.com</a>和<a href="https://web.archive.org/web/20221117203617/https://public.roboflow.com/object-detection" target="_blank" rel="noreferrer noopener nofollow">roboflow.com</a>，这两个地方提供高质量的对象检测数据集。</p>



<p>查看以上资产，收集您需要的数据或丰富您已经拥有的数据集。</p>



<h4>如何为YOLO标注数据</h4>



<p>如果您的图像数据集不带注释，您必须自己完成注释工作。这个手动操作是相当耗时的，一定要保证自己有足够的时间去做。</p>



<p>作为一个注释工具，你可以考虑<a href="https://web.archive.org/web/20221117203617/https://neptune.ai/blog/annotation-tool-comparison-deep-learning-data-annotation" target="_blank" rel="noreferrer noopener">多个选项</a>。我个人会推荐使用<a href="https://web.archive.org/web/20221117203617/https://github.com/tzutalin/labelImg" target="_blank" rel="noreferrer noopener nofollow">Labe</a>T4】lImg。这是一个轻量级和易于使用的图像注释工具，可以直接输出YOLO模型的注释。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/ee19905111e60a2b16df7beb84feca1a.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh4.googleusercontent.com/g8mvSCPJy8m0kVgmmFM5gpSqfmL8wzvZToaxEeGrvEZ14M1f7-xFGi3PV-_V9XX_ttAo8Qj99StuuvrP8XeTs2cqEd5XKRXo3az4JZup2Tp35CCg2rCeJmrRMdMA_FoK4nUX-Vxj=s0"/><figcaption><em>Annotation work in LabeIIimg shown.<br/><a href="https://web.archive.org/web/20221117203617/https://github.com/tzutalin/labelImg" target="_blank" rel="noreferrer noopener nofollow">Source</a> of the image.</em></figcaption></figure></div>


<h3>如何将数据从其他格式转换到YOLO</h3>



<p>YOLO的注释是txt文件的形式。YOLO的txt文件中的每一行必须具有以下格式:</p>



<pre class="hljs">image1.jpg <span class="hljs-number">10,15,345</span>,<span class="hljs-number">284</span>,<span class="hljs-number">0</span>
image2.jpg <span class="hljs-number">100,94,613</span>,<span class="hljs-number">814,0 31</span>,<span class="hljs-number">420,220</span>,<span class="hljs-number">540</span>,<span class="hljs-number">1</span></pre>



<p>我们可以分解txt文件中的每一行，看看它由什么组成:</p>



<ul><li>一行的第一部分指定了图像的基名:<em>image1.jpg</em>，【image2.jpg】T2</li></ul>



<ul><li>线的第二部分定义边界框坐标和类标签。例如，<em> 10，15，345，284，0 </em>表示xmin，ymin，xmax，ymax，class_id</li></ul>



<ul><li>如果一个给定的图像上有多个对象，那么在图像基本名称旁边会有多个框和类标签，用空格分隔。</li></ul>



<p>边界框坐标是一个清晰的概念，但是指定类标签的<em> class_id </em>数字呢？每个<em> class_id </em>都与另一个txt文件中的特定类相链接。例如，预先训练好的YOLO带有<em> coco_classes.txt </em>文件，看起来像这样:</p>



<pre class="hljs">person
bicycle
car
motorbike
aeroplane
bus
...</pre>



<p>类文件中的行数必须与您的检测器将要检测的类数相匹配。编号从零开始，这意味着classes文件中第一个类的<em> class_id </em>号将为0。位于类txt文件第二行的类的编号为1。</p>



<p>现在你知道YOLO的注释是什么样子了。要继续创建自定义对象检测器，我建议您现在做两件事:</p>



<ol><li>创建一个classes txt文件，在其中存放您希望检测器检测的类。记住上课顺序很重要。</li></ol>



<ol start="2"><li>创建带注释的txt文件。如果你已经有了注释，但是是VOC格式的。XMLs)，你可以用<a href="https://web.archive.org/web/20221117203617/https://github.com/taipingeric/yolo-v4-tf.keras/blob/master/xml_to_txt.py" target="_blank" rel="noreferrer noopener nofollow">这个文件</a>把XML转换成YOLO。</li></ol>



<h4>将数据分割成子集</h4>



<p>和往常一样，我们希望将数据集分成两个子集:用于训练和验证。这可以简单地做到:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> read_annotation_lines

train_lines, val_lines = read_annotation_lines(<span class="hljs-string">'../path2annotations/annot.txt'</span>, test_size=<span class="hljs-number">0.1</span>)</pre>



<h4>创建数据生成器</h4>



<p>当数据被分割时，我们可以进行数据生成器初始化。我们将为每个数据文件准备一个数据生成器。在我们的例子中，我们将有一个用于训练子集和验证子集的生成器。</p>



<p>以下是数据生成器的创建方式:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> DataGenerator

FOLDER_PATH = <span class="hljs-string">'../dataset/img'</span>
class_name_path = <span class="hljs-string">'../class_names/bccd_classes.txt'</span>

data_gen_train = DataGenerator(train_lines, class_name_path, FOLDER_PATH)
data_gen_val = DataGenerator(val_lines, class_name_path, FOLDER_PATH)</pre>



<p>总而言之，下面是数据分割和生成器创建的完整代码:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> read_annotation_lines, DataGenerator

train_lines, val_lines = read_annotation_lines(<span class="hljs-string">'../path2annotations/annot.txt'</span>, test_size=<span class="hljs-number">0.1</span>)

FOLDER_PATH = <span class="hljs-string">'../dataset/img'</span>
class_name_path = <span class="hljs-string">'../class_names/bccd_classes.txt'</span>

data_gen_train = DataGenerator(train_lines, class_name_path, FOLDER_PATH)
data_gen_val = DataGenerator(val_lines, class_name_path, FOLDER_PATH)</pre>



<h3>模型培训所需的安装和设置</h3>



<p>让我们来谈谈创建您自己的对象检测器所必需的先决条件:</p>







<ul><li>如果你的计算机有一个支持CUDA的GPU(NVIDIA制造的GPU)，那么需要一些相关的库来支持基于GPU的训练。如果你需要启用GPU支持，请查看NVIDIA网站上的指南。您的目标是为您的操作系统安装CUDA工具包和cuDNN的最新版本；</li></ul>



<ul><li>你可能想组织一个独立的虚拟环境来工作。此项目需要安装TensorFlow 2。所有其他的库将在后面介绍；</li></ul>



<ul><li>至于我，我在Jupyter笔记本开发环境中构建和训练我的YOLOv4模型。尽管Jupyter Notebook似乎是一个合理的选择，但是如果您愿意，可以考虑在您选择的IDE中进行开发。</li></ul>



<h3>模特培训</h3>





<h4>先决条件</h4>



<p>到目前为止，您应该已经:</p>



<ul><li>数据集的拆分；</li></ul>



<ul><li>初始化两个数据生成器；</li></ul>



<ul><li>包含类的txt文件。</li></ul>



<h4>模型对象初始化</h4>



<p>要为培训作业做好准备，请初始化YOLOv4模型对象。确保使用<em> None </em>作为<em> weight_path </em>参数的值。在这一步，您还应该提供类txt文件的路径。以下是我在项目中使用的初始化代码:</p>



<pre class="hljs">class_name_path = <span class="hljs-string">'path2project_folder/model_data/scans_file.txt'</span>

model = Yolov4(weight_path=<span class="hljs-keyword">None</span>,
               class_name_path=class_name_path)</pre>



<p>上述模型初始化导致创建具有默认参数集的模型对象。考虑通过将字典作为值传递给<em> config </em>模型参数来更改模型的配置。</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img decoding="async" loading="lazy" src="../Images/291a2e86802f0cdff56bf25f02f264d3.png" alt="" data-original-src="https://web.archive.org/web/20221117203617im_/https://lh5.googleusercontent.com/UtKa5pi2Icg0m9TTd-rBvCo-OQdqpdagk5Qwnn7j2sTtdtNC2gRqJo1j92O7-ukei2o8HCZ38YBn9hg-iI3wyYqMISmjUVCKH3w2iQCr4HQz-JWZVh_TJRW8oT8px0lm7h-fM6G-=s0"/><figcaption><em>Model object initialization parameter displayed.</em></figcaption></figure></div>


<p><em> Config </em>指定YOLOv4模型的一组参数。</p>



<p>默认模型配置是一个很好的起点，但是您可能想要尝试其他配置以获得更好的模型质量。</p>



<p>我特别强烈推荐尝试<em>主播</em>和<em> img_size </em>。<em>锚点</em>指定将用于捕捉对象的锚点的几何图形。锚点的形状越符合对象的形状，模型的性能就越高。</p>



<p>增加<em> img_size </em>在某些情况下也是有用的。请记住，图像越高，模型进行推理的时间就越长。</p>



<p>如果您想使用Neptune作为跟踪工具，您还应该初始化一次实验运行，如下所示:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune.new <span class="hljs-keyword">as</span> neptune

run = neptune.init(project=<span class="hljs-string">'projects/my_project'</span>,
                   api_token=my_token)</pre>



<h4>定义回调</h4>



<p>TensorFlow &amp; Keras让我们使用回调来监控训练进度、设置检查点和管理训练参数(例如学习率)。</p>



<p>在拟合您的模型之前，定义对您的目的有用的回调。确保指定存储模型检查点和相关日志的路径。以下是我在我的一个项目中是如何做到的:</p>



<pre class="hljs"> 

dir4saving = <span class="hljs-string">'path2checkpoint/checkpoints'</span>
os.makedirs(dir4saving, exist_ok = <span class="hljs-keyword">True</span>)

logdir = <span class="hljs-string">'path4logdir/logs'</span>
os.makedirs(logdir, exist_ok = <span class="hljs-keyword">True</span>)

name4saving = <span class="hljs-string">'epoch_{epoch:02d}-val_loss-{val_loss:.4f}.hdf5'</span>

filepath = os.path.join(dir4saving, name4saving)

rLrCallBack = keras.callbacks.ReduceLROnPlateau(monitor = <span class="hljs-string">'val_loss'</span>,
                                             factor = <span class="hljs-number">0.1</span>,
                                             patience = <span class="hljs-number">5</span>,
                                             verbose = <span class="hljs-number">1</span>)

tbCallBack = keras.callbacks.TensorBoard(log_dir = logdir,
                                         histogram_freq = <span class="hljs-number">0</span>,
                                         write_graph = <span class="hljs-keyword">False</span>,
                                         write_images = <span class="hljs-keyword">False</span>)

mcCallBack_loss = keras.callbacks.ModelCheckpoint(filepath,
                                            monitor = <span class="hljs-string">'val_loss'</span>,
                                            verbose = <span class="hljs-number">1</span>,
                                            save_best_only = <span class="hljs-keyword">True</span>,
                                            save_weights_only = <span class="hljs-keyword">False</span>,
                                            mode = <span class="hljs-string">'auto'</span>,
                                            period = <span class="hljs-number">1</span>)

esCallBack = keras.callbacks.EarlyStopping(monitor = <span class="hljs-string">'val_loss'</span>,
                                          mode = <span class="hljs-string">'min'</span>,
                                          verbose = <span class="hljs-number">1</span>,
                                          patience = <span class="hljs-number">10</span>)</pre>



<p>你可能已经注意到，在上面的回调中，set TensorBoard被用作跟踪工具。考虑使用海王星作为一个更先进的实验跟踪工具。如果是这样的话，不要忘记初始化另一个回调来支持与Neptune的集成:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> neptune.new.integrations.tensorflow_keras <span class="hljs-keyword">import</span> NeptuneCallback

neptune_cbk = NeptuneCallback(run=run, base_namespace=<span class="hljs-string">'metrics'</span>)</pre>



<h4>拟合模型</h4>



<p>要开始训练工作，只需使用TensorFlow / Keras中的标准<em> fit() </em>方法来拟合模型对象。我是这样开始训练我的模型的:</p>



<pre class="hljs">model.fit(data_gen_train,
          initial_epoch=<span class="hljs-number">0</span>,
          epochs=<span class="hljs-number">10000</span>,
          val_data_gen=data_gen_val,
          callbacks=[rLrCallBack,
                     tbCallBack,
                     mcCallBack_loss,
                     esCallBack,
                     neptune_cbk]
         )</pre>



<p>当培训开始时，您会看到一个标准的进度条。</p>





<p>训练过程将在每个时期结束时评估模型。如果您使用一组类似于我在拟合时初始化并传入的回调，那些显示模型在较低损失方面有所改进的检查点将被保存到指定的目录中。</p>



<p>如果没有错误发生并且训练过程顺利进行，则训练作业将会停止，这或者是因为训练时期数结束，或者是因为早期停止回调没有检测到进一步的模型改进并停止整个过程。</p>



<p>在任何情况下，您都应该有多个模型检查点。我们希望从所有可用的选项中选择最好的一个，并使用它进行推理。</p>



<h3>推理模式下的定型自定义模型</h3>



<p>在推理模式下运行已训练模型类似于开箱即用地运行预训练模型。</p>





<p>您初始化一个模型对象，传递到最佳检查点的路径以及带有类的txt文件的路径。下面是我的项目的模型初始化的样子:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> models <span class="hljs-keyword">import</span> Yolov4
model = Yolov4(weight_path=<span class="hljs-string">'path2checkpoint/checkpoints/epoch_48-val_loss-0.061.hdf5'</span>,
               class_name_path=<span class="hljs-string">'path2classes_file/my_yolo_classes.txt'</span>)</pre>



<p>当模型初始化后，只需对您选择的图像使用predict()方法来获得预测。概括地说，该模型所做的检测以熊猫数据帧的便利形式返回。我们得到每个检测到的对象的类名、盒子大小和坐标。</p>



<h2 id="h-conclusions">结论</h2>



<p>您刚刚学习了如何创建一个定制的YOLOv4对象检测器。我们已经讨论了端到端的过程，从数据收集、注释和转换开始。你对第四代YOLO探测器及其与其他探测器的不同已经有了足够的了解。</p>



<p>现在没有什么能阻止你在TensorFlow和Keras中训练自己的模型。你知道从哪里得到一个预训练的模型，以及如何开始训练工作。</p>



<p>在我即将发表的文章中，我将向您展示一些有助于提高最终模型质量的最佳实践和生活窍门。和我们在一起！</p>


        </div>
        
    </div>    
</body>
</html>