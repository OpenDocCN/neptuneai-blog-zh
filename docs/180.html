<html>
<head>
<title>In-Depth ETL in Machine Learning Tutorial - Case Study With Neptune </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>机器学习教程中的深度ETL——Neptune案例研究</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/etl-in-machine-learning#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/etl-in-machine-learning#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>大多数时候，作为数据科学家，我们认为我们的核心价值是我们找出解决任务的机器学习算法的能力。事实上，模型训练只是大量工作的最后一部分，主要是数据，这是开始构建模型所需要的。</p>



<p>在ML解决方案或高度复杂的神经网络出现之前，一个完整的数据基础设施应该已经就位、经过测试并准备就绪。不幸的是，我们常常认为整个数据管道上游部分所需的大量工作是理所当然的。</p>



<p>数据应该是我们的主要关注点，同时找到将其转化为可操作见解的方法。如今，对于任何寻求在快速发展的世界中领先的公司来说，数据都被视为高价值资源。</p>



<p>在本文中，我将介绍一组构建数据基础设施的概念和过程，以及成功的数据管道的不同构件。我将重点介绍:</p>



<ul><li><strong>解释通用数据管道</strong></li><li><strong>涉及ML实践的ETL(提取、转换、加载)过程</strong></li><li><strong>使用Apache Airflow实现数据工作流程自动化</strong></li><li><strong>在Neptune培训和管理不同的ML模型解决方案</strong></li></ul>



<p>为了让这篇文章更加实用和有指导意义，我将使用一个<strong>信用评分</strong>数据集来运行所有的实验。我坚信边做边学，所以让我们马上开始吧！</p>



<p><strong> <em>注</em> </strong> : <em>完整的代码可以在我的<a href="https://web.archive.org/web/20221117203556/https://github.com/aymanehachcham/ETL-Processing-ML" target="_blank" rel="noreferrer noopener nofollow"> Github repo </a>中找到。不要犹豫，克隆它，自己尝试。</em></p>



<h2 id="data">数据管道和工作流</h2>



<p>通常在商业中，我们经常为了各种各样的目的处理大量的数据。构建健壮的管道可能会变得相当复杂，尤其是在数据稀缺、转换过程涉及大量技术细节的时候。</p>



<p>高效的数据传输依赖于三个重要的模块:</p>



<ol><li><strong>数据生产者</strong></li></ol>



<p>数据源指向原始数据准备好被获取的地方。</p>



<ol start="2"><li><strong>转换和运输工作流程</strong></li></ol>



<p>ETL子流程，涉及一堆<em>提取、转换和数据加载</em>层，将数据路由到相应的端点。</p>



<ol start="3"><li><strong>数据消费者</strong></li></ol>



<p>利用干净和预处理信息执行高端taks的最终端点。</p>







<p>管道是非常通用的，根据商业计划有不同的用途。它们通常共享一般的概念，但是具体的实现会有所不同。</p>



<p>在我们的例子中，数据集已经准备好了。我们需要的是设计一个ETL过程，根据我们假装要做的事情来转换我们的数据。</p>



<h2 id="credit-scoring-dataset">探索信用评分数据集</h2>



<p>在这一部分，我们将彻底分析信用评分数据集。数据集将帮助我们实现和测试我们的ETL工作流。</p>



<p>银行使用信用评分模型为客户分配信用评分。这些分数代表了客户的总体信用度。信用评分起源于20世纪50年代初的美国，供债权人评估有信用历史的客户的财务实力。今天，这项技术已经成为一个真正的金融机构。这一评级适用于所有拥有社会保障号码的美国人。</p>







<p>我们将使用的数据集“给我一些信任”，来自<a href="https://web.archive.org/web/20221117203556/https://www.kaggle.com/c/GiveMeSomeCredit" target="_blank" rel="noreferrer noopener nofollow"> Kaggle </a>。我将简要概述数据结构，解释每个特性的本质，并展示一组通用统计数据。</p>



<h3>数据特征</h3>



<ul><li><strong>无担保额度的循环使用</strong>:信用卡和个人信用额度的总余额，除了房地产，没有分期付款债务，如汽车贷款，除以信用额度的总和</li><li><strong>年龄</strong>:借款人的年龄，以年为单位</li><li><strong>逾期30-59天未恶化的次数</strong>:在过去2年中，借款人逾期30-59天未恶化的次数</li><li><strong>债务比率</strong>:每月的债务支付、赡养费、生活费用除以每月总收入</li><li><strong>月收入</strong>:人均月收入</li><li><strong>未结清的信用额度和贷款数量</strong>:未结清的贷款(汽车贷款或抵押贷款等分期付款)和信用额度(如信用卡)数量</li><li><strong>逾期90天的次数</strong>:借款人逾期90天或以上的次数</li><li><strong>房地产贷款数量或额度</strong>:包括房屋净值信用额度在内的抵押和房地产贷款数量</li><li><strong>逾期60-89天的次数没有恶化</strong>:在过去2年中，借款人逾期60-89天的次数没有恶化</li><li><strong>被赡养人数</strong>:家庭中除自己以外的被赡养人数(配偶、子女等)。)</li></ul>



<h3>目标</h3>



<ul><li><strong>两年内严重拖欠</strong>:逾期拖欠90天或更长时间的人</li></ul>



<p>该目标表明在两年的时间窗内衡量的债务人的拖欠情况。值为1表示借款人拖欠贷款，并且在过去2年中一直拖欠贷款。值为0表示借款人是一个好客户，并且在最近两年中按时偿还了债务。</p>



<p>通常，大多数金融行业数据包含缺失值，或者对于特定特征没有意义的值。这个特殊的数据集也不例外，在债务和信贷余额比率方面存在不一致，价值远远超出了应该承认的范围。</p>



<p>因此，我们将应用数据转换来消除所有会改变建模和训练阶段结果的差异。</p>



<p><strong>现在，让我们专注于描述性分析</strong>:</p>



<p>不出所料，目标特性高度不平衡，会导致模型训练出现严重问题。将<strong> 86.3 </strong> %的债务人归类为不良付款人将导致模型过度适合这一特定类别，而完全忽略其他类别。</p>



<p>通过查看<strong>年龄</strong>分布，我们清楚地观察到，40岁到50岁之间的年龄组包含大多数样本，其他剩余的组或多或少是平衡的。</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/fa233fe81a1f82f29cd0f960d1246ec7.png" alt="Age distribution" class="wp-image-43916" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Age-Distribution-regarding-Frequency-and-Cumulative-frequency.png?ssl=1"/><figcaption><em>Age distribution regarding Frequency and Cumulative Frequency</em></figcaption></figure>



<p>年龄分析表明，人口中负债最多的部分是35至70岁的人。最重要的是，月收入最低的人群的债务总是更高。</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/a21578e2ba274c3cfcd1cc4460c03d6a.png" alt="Debt ratio" class="wp-image-43917" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Debt-Ratio-distribution-regarding-Age-and-Monthly-Income.png?ssl=1"/><figcaption><em>Debt ratio distribution regarding Age and Monthly Income</em></figcaption></figure>



<p>积累金融资源最多的人口段在45岁到60岁之间，这是相当符合逻辑和似是而非的。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/6e86c2d2421a77f705e7431d96adc0a4.png" alt="Monthly Income distribution by Age" class="wp-image-43918" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Monthly-Income-Distribution-by-age.png?ssl=1"/><figcaption><em>Monthly Income distribution by Age</em></figcaption></figure></div>



<p>循环债务的趋势与长期债务非常相似，人口非常年轻，负债总是越来越多。数据中最明显的相关性是短期和长期债务、年龄组和工资之间的相关性。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/ef3801fc29ac07e27585c9fdf51afe1f.png" alt="" class="wp-image-43920" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Revolving-credits-ratio-by-Age-and-Monthly-Income.png?ssl=1"/><figcaption><em>Revolving credits ratio by Age and Monthly Income</em></figcaption></figure></div>



<p>现在我们对数据集有了更多的了解和理解，我们可以继续进行数据提取和转换的ETL过程。</p>



<h2 id="etl-process">构建并自动化您的ETL过程</h2>



<p>在这一节中，我们将实现一个ETL工作流，该工作流将数据从csv文件提取到一个可用的pandas数据框架中，我们将应用所有需要的转换来为ML模型准备一个干净的数据集。为了提高效率，我们将尝试模拟一个自动化循环来运行该流程。</p>



<p>本节将介绍一些概念，如用于过程自动化的气流定向非循环图，以及实现数据转换的因素分析程序。</p>



<h3>数据析取</h3>



<p>我们希望从<em> csv </em>文件中提取数据，并将其用于我们的实验目的。为此，首先我们创建一个小的<em> Python数据管理器</em>类，它将负责解析csv、提取和格式化任何相关数据以供我们分析。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DataETLManager</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, root_dir: str, csv_file: str)</span>:</span>
        <span class="hljs-keyword">if</span> os.path.exists(root_dir):
            <span class="hljs-keyword">if</span> csv_file.endswith(<span class="hljs-string">'.csv'</span>):
                self.csv_file = os.path.join(root_dir, csv_file)
            <span class="hljs-keyword">else</span>:
                logging.error(<span class="hljs-string">'The file is not in csv format'</span>)
                exit(<span class="hljs-number">1</span>)
        <span class="hljs-keyword">else</span>:
            logging.error(<span class="hljs-string">'The root dir path does not exist'</span>)
            exit(<span class="hljs-number">1</span>)

        self.credit_scoring_df = pd.read_csv(self.csv_file, sep=<span class="hljs-string">','</span>, encoding=<span class="hljs-string">'ISO-8859-1'</span>)
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extract_data</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.credit_scoring_df

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fetch_columns</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.credit_scoring_df.columns.tolist()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">data_description</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.credit_scoring_df.describe()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fetch_categorical</span><span class="hljs-params">(self, categorical=False)</span>:</span>
        <span class="hljs-keyword">if</span> categorical:
            categorical_columns = list(set(self.credit_scoring_df.columns) - set(self.credit_scoring_df._get_numerical_data().columns))
            categorical_df = self.credit_scoring_df[categorical_columns]
            <span class="hljs-keyword">return</span> categorical_df
        <span class="hljs-keyword">else</span>:
            non_categorical = list(set(self.credit_scoring_df._get_numerical_data().columns))
            <span class="hljs-keyword">return</span> self.credit_scoring_df[non_categorical]</pre>



<p>需要仔细研究的三种重要方法:</p>



<ul><li>extract_data():返回我们刚刚创建的信用评分数据框架</li><li>fetch_columns():返回数据中的所有列</li><li>data_description():如果我们想快速浏览一下数据的结构，这很有用</li><li>fetch _ categorical():返回数据框中的分类值</li></ul>



<p>我们将把列名改得更短:</p>



<pre class="hljs">credit_df=credit_df.drop(<span class="hljs-string">'Unnamed: 0'</span>, axis=<span class="hljs-number">1</span>)
credit_df.columns = [<span class="hljs-string">'Target'</span>, <span class="hljs-string">'Revolving'</span>, <span class="hljs-string">'Age'</span>, <span class="hljs-string">'30-59PastDue'</span>, <span class="hljs-string">'DbtRatio'</span>, <span class="hljs-string">'Income'</span>, <span class="hljs-string">'NumOpenLines'</span>, <span class="hljs-string">'Num90DayLate'</span>, <span class="hljs-string">'NumRealEstLines'</span>, <span class="hljs-string">'60-89PastDueNoW'</span>, <span class="hljs-string">'FamMemb'</span>]</pre>



<p>结果看起来像这样:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/d07c193025eb5d627298b6e06c26bc85.png" alt="Credit Scoring datatset" class="wp-image-43924" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Credit-Scoring-datatset.png?ssl=1"/><figcaption><em>Credit Scoring datatset</em></figcaption></figure></div>



<h3>数据转换</h3>



<p>我们将关注两个转型阶段:</p>



<ul><li>预处理转换</li><li>分析转换</li></ul>



<p>这个想法是，我们绝对需要预处理传入的原始数据，消除重复，丢弃空值和缺失值。此外，进行单变量分析时，我们很快会发现许多样本的比率变量超出了范围。通常，我们需要检测和删除异常值。</p>



<p>有了可用的数据，我们将开始实施因子分析，以提取最能解释方差和相关性的深刻特征。</p>



<h4>预处理转换</h4>



<p><strong>删除重复值和缺失值:</strong></p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transform_data</span><span class="hljs-params">(self)</span>:</span>
        
        self.credit_scoring_df.drop_duplicates(keep=<span class="hljs-string">'last'</span>, inplace=<span class="hljs-keyword">True</span>)

        
        self.credit_scoring_df.dropna(how=<span class="hljs-string">'all'</span>, inplace=<span class="hljs-keyword">True</span>)</pre>



<p><strong>去除异常值:</strong></p>



<ul><li>年龄特征是从0到最大值100的连续变量。有某些记录，价值为零，没有意义，不能作为借款人的资格，该人必须是18岁的成年人。</li><li>检查值大于1的债务和周转比率的上限和顶部编码方法，这意味着所有高于上限的值都将被删除。</li><li>从模型中排除具有显著(超过50%)缺失值的特征或记录，尤其是当缺失的程度对于数据不平衡率(相当高)来说足够重要时。</li></ul>



<pre class="hljs">clean_credit = self.credit_scoring_df.loc[self.credit_scoring_df[<span class="hljs-string">'Revolving'</span>] &lt;= <span class="hljs-number">1</span>]
clean_credit = clean_credit.loc[clean_credit[<span class="hljs-string">'DbtRatio'</span>] &lt;= <span class="hljs-number">1</span>]
clean_credit = clean_credit.loc[clean_credit[<span class="hljs-string">'Age'</span>] &lt;= <span class="hljs-number">100</span>]
clean_credit = clean_credit.loc[clean_credit[<span class="hljs-string">'Age'</span>] &gt;= <span class="hljs-number">18</span>]
clean_credit = clean_credit.loc[clean_credit[<span class="hljs-string">'FamMemb'</span>] &lt; <span class="hljs-number">20</span>]
</pre>



<h4>分析转换</h4>



<p>清理完数据后，我们将用<strong>均值=0 </strong>和<strong>标准差=1 </strong>对数据进行标准化，除了作为目标的二元因变量。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">normalize</span><span class="hljs-params">(dataset)</span>:</span>
    dataNorm=((dataset-dataset.min())/(dataset.max()-dataset.min()))
    dataNorm[<span class="hljs-string">"Target"</span>]=dataset[<span class="hljs-string">"Target"</span>]
    <span class="hljs-keyword">return</span> dataNorm

clean_scaled_df = normalize(clean_credit)</pre>



<p>数据值范围从0到1:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/b95bc2c46b048e73bb42a4341cbf905b.png" alt="Normalized data values" class="wp-image-43923" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Normalized-data-values.png?ssl=1"/><figcaption><em>Normalized data values</em></figcaption></figure></div>



<h3>从因子分析开始</h3>



<p>在分离训练阶段的数据之前，我们希望检查所有变量之间的相关性，以了解变量将如何组合。</p>



<p>因子分析是一种线性统计模型。它用来解释观察变量之间的方差，将一组观察变量浓缩成<em>未观察变量，</em>称为因子。观察变量被建模为因子和误差项的线性组合。</p>



<p>首先，让我们运行一个充分性测试来检查数据集是否适合于因子分析。我们将进行<a href="https://web.archive.org/web/20221117203556/https://www.statology.org/bartletts-test-of-sphericity/#:~:text=Bartlett's%20Test%20of%20Sphericity%20compares,are%20orthogonal%2C%20i.e.%20not%20correlated." target="_blank" rel="noreferrer noopener nofollow">巴莱特球形度测试</a>。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> chi2, pearsonr
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">barlett_test</span><span class="hljs-params">(frame: pd.DataFrame)</span>:</span>
    
    frame.info()

    col, row = frame.shape
    x_corr = frame.corr()

    corr_det = np.linalg.det(x_corr)
    chi_measure = -np.log(corr_det) * (col - <span class="hljs-number">1</span> - (<span class="hljs-number">2</span> * row + <span class="hljs-number">5</span>) / <span class="hljs-number">6</span>)
    degrees_of_freedom = row * (row - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>
    p_value = chi2.sf(statistic, degrees_of_freedom)
    <span class="hljs-keyword">return</span> chi_measure, p_value</pre>



<pre class="hljs">chi_square, p_value = barlett_test(clean_scaled_df)
(<span class="hljs-number">1003666.113272278</span>, <span class="hljs-number">0.0</span>)</pre>



<p><strong>高度适合</strong> : <em>观察到的相关性不同于单位矩阵，H0和H1假设得到验证。</em></p>



<p>我们将使用factor_analyzer python包，通过以下命令安装它:</p>



<pre class="hljs">pip install factor_analyzer</pre>



<p>将数据拟合到FactorAnalyzer类，我们将运行Kaiser criterion内部统计以得出数据中的特征值。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> factor_analyzer <span class="hljs-keyword">import</span> FactorAnalyzer

fa = FactorAnalyzer()
fa.fit(clean_scaled_df)


ev, v = fa.get_eigenvalues()
eigen_values = pd.DataFrame(ev)
eigen_values</pre>



<p><strong>原始特征值:</strong></p>



<p>有四个特征值大于1的主分量:</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/871b8715a587707b2883fe503a1c2a50.png" alt="Eigen Values" class="wp-image-43926" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/EigenValues-after-running-Factor-Analysis.png?resize=219%2C407&amp;ssl=1"/><figcaption><em>Eigen Values after Running Factor Analysis</em></figcaption></figure></div>



<p>绘制碎石图我们可以很容易地想象出我们需要的四个相关因素:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

plt.scatter(range(<span class="hljs-number">1</span>,clean_scaled_df.shape[<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>),ev)
plt.plot(range(<span class="hljs-number">1</span>,clean_scaled_df.shape[<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>),ev)
plt.title(<span class="hljs-string">'Scree Plot'</span>)
plt.xlabel(<span class="hljs-string">'Factors'</span>)
plt.ylabel(<span class="hljs-string">'Eigenvalue'</span>)
plt.grid()
plt.show()</pre>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/ae4dd76a079c54bf8bcfffe8502c6cb6.png" alt="Scree plot for the 4 factors" class="wp-image-43927" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Scree-plot-for-the-4-factors.png?ssl=1"/><figcaption><em>Scree plot for the 4 factors</em></figcaption></figure></div>



<section id="blog-intext-cta-block_60748796b6d85" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>检查如何将在<a href="https://web.archive.org/web/20221117203556/https://docs.neptune.ai/essentials/integrations/visualization-libraries/matplotlib" target="_blank" rel="noopener"> matplotlib </a>中生成的图表记录到Neptune。</p>
    
    </section>



<p>让我们对这4个因素进行因素分析轮换，以获得更好的解释。旋转可以是正交的或倾斜的。它有助于在观察到的变量之间重新分配<a href="https://web.archive.org/web/20221117203556/https://www.datacamp.com/community/tutorials/introduction-factor-analysis" target="_blank" rel="noreferrer noopener nofollow">公度</a>，具有清晰的载荷模式。</p>



<pre class="hljs">fac_rotation = FactorAnalyzer(n_factors=<span class="hljs-number">4</span>, rotation=<span class="hljs-string">'varimax'</span>)
fac_rotation.fit(clean_scaled_df)

fac_rotation.get_factor_variance()</pre>



<p id="separator-block_61af1b29427fb" class="block-separator block-separator--10"> </p>



<div id="medium-table-block_61af27f88cb1f" class="block-medium-table ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">特征</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">因素1</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">因素2</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">因素3</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">因素4</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>



<p id="separator-block_61af1b50427fe" class="block-separator block-separator--25"> </p>



<p>我们得出这四个分量解释的方差为<strong> 51.4% </strong>，是令人满意的。基于此，我们决定用四个因素进行进一步分析。</p>



<p>显示4个因素及其各自的可观察特征，我们看到:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/f3b8c16fc4b9974b459fb50444817554.png" alt="Factors and Observable Features " class="wp-image-43928" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Factors-and-Observable-Features-side-by-side.png?ssl=1"/><figcaption><em>Factors and Observable Features side by side</em></figcaption></figure></div>



<p><strong>我的观察</strong></p>



<ul><li>变量<em>30-59过期</em>、<em>60-89过期现在</em>和<em>num 90延迟</em>在<strong>工厂1 </strong>上加载高。这意味着这一因素导致借款人越来越拖欠。因此我们可以把这个因素命名为<strong> <em>财务斗争</em> </strong>。</li></ul>



<p><strong>二观察</strong></p>



<ul><li>变量<em> DbtRatio </em>、<em> NumOpenLines </em>和<em> NumRealEstLines </em>在<strong> Factor2 </strong>上加载高。这意味着这一因素导致借款人承担更多的债务和更多的信贷额度。我们可以将其命名为<strong> <em>财务要求</em> </strong>。</li></ul>



<p><strong>三观察</strong></p>



<ul><li>变量<em>年龄</em>、<em>家庭成员</em>和<em>旋转</em>在<strong>因子3 </strong>上负载高，年龄与因子3成间接比例。这个因素被命名为<strong> <em>消耗性收入</em> </strong>，因为随着年龄的增长，消耗性收入也会增加。</li></ul>



<p><strong>四观察</strong></p>



<ul><li>变量<em>收入</em>和<em>家庭成员</em>加载到<strong>工厂4 </strong>上。所以我们可以很容易地将其命名为<strong> <em>行为生活方式</em> </strong>因为随着收入的增加，我们的生活方式和我们家庭成员的生活方式也会增加。</li></ul>



<p>现在我们有了ML分析的四个因素:</p>



<ul><li>财务斗争</li><li>财务要求</li><li>消耗性收入</li><li>行为生活方式</li></ul>



<h3>使用Dag自动化ETL流程</h3>



<p>一旦提取和转换管道形成并准备好进行部署，我们就可以开始考虑将以前的数据存储在数据库中的方法。</p>



<p>为了简化和便于说明，我们将使用Sql_Alchemy python包将之前转换的数据框加载到本地MySQL数据库中。</p>



<pre class="hljs">pip install SQLAlchemy</pre>



<pre class="hljs"><span class="hljs-keyword">from</span> sqlalchemy.engine <span class="hljs-keyword">import</span> create_engine

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_data</span><span class="hljs-params">(self)</span>:</span>
        database_config = {
            <span class="hljs-string">'username'</span>: <span class="hljs-string">'your_username'</span>,
            <span class="hljs-string">'password'</span>: <span class="hljs-string">'your_pass'</span>,
            <span class="hljs-string">'host'</span>: <span class="hljs-string">'127.0.0.1'</span>,
            <span class="hljs-string">'port'</span>:<span class="hljs-string">'3306'</span>,
            <span class="hljs-string">'database'</span>:<span class="hljs-string">'db_name'</span>
        }

        
        engine = create_engine(<span class="hljs-string">'mysql+mysqlconnector://{}:{}@{}:{}/{}'</span>.format(
            database_config[<span class="hljs-string">'username'</span>],
            database_config[<span class="hljs-string">'password'</span>],
            database_config[<span class="hljs-string">'host'</span>],
            database_config[<span class="hljs-string">'port'</span>],
            database_config[<span class="hljs-string">'database'</span>]
        ))

        data_to_load = type(pd.DataFrame())(self.credit_scoring_df)
        <span class="hljs-keyword">try</span>:
            data_to_load.to_sql(<span class="hljs-string">'Credit Scoring'</span>, con=engine, if_exists=<span class="hljs-string">'append'</span>, index=<span class="hljs-keyword">False</span>)
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> err:
            print(err)</pre>



<h4>气流有向图:有向无环图</h4>



<p>Airflow计划自动化的数据工作流，包括共享特定依赖关系的多个任务或流程。气流文件对Dag的定义如下:</p>



<p>在Airflow中，DAG(或有向无环图)是您想要运行的所有任务的集合，以反映它们的关系和依赖性的方式组织。在Python脚本中定义了一个DAG，它将DAG结构(任务及其依赖项)表示为代码</p>



<p>要安装并开始使用Airflow，可以查看网站，网站对每一步都解释的很透彻:<a href="https://web.archive.org/web/20221117203556/https://airflow.apache.org/docs/apache-airflow/stable/installation.html" target="_blank" rel="noreferrer noopener nofollow"> AirFlow Doc </a>。</p>



<p>在我们的例子中，我们将编写一个小的DAG文件来模拟ETL的自动化。我们将安排DAG从2021年3月25日开始每天运行。DAG将有三个python运算符，分别代表提取、转换和加载功能。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> timedelta, datetime
<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.python <span class="hljs-keyword">import</span> PythonOperator
</pre>



<pre class="hljs"><span class="hljs-keyword">from</span> etl_process <span class="hljs-keyword">import</span> DataETLManager, DATA_PATH

default_dag_args = {
    <span class="hljs-string">'owner'</span>: <span class="hljs-string">'airflow'</span>,
    <span class="hljs-string">'depends_on_past'</span>: <span class="hljs-keyword">False</span>,
    <span class="hljs-string">'start_date'</span>: datetime(<span class="hljs-number">2021</span>, <span class="hljs-number">3</span>, <span class="hljs-number">9</span>),
    <span class="hljs-string">'email'</span>: [<span class="hljs-string">'airflow@example.com'</span>],
    <span class="hljs-string">'email_on_failure'</span>: <span class="hljs-keyword">False</span>,
    <span class="hljs-string">'email_on_retry'</span>: <span class="hljs-keyword">False</span>,
    <span class="hljs-string">'retries'</span>: <span class="hljs-number">2</span>,
    <span class="hljs-string">'retry_delay'</span>: timedelta(minutes=<span class="hljs-number">1</span>),
}

etl_dag = DAG(
    <span class="hljs-string">'etl_retail'</span>,
    default_args=default_dag_args,
    description=<span class="hljs-string">'DAG for ETL retail process'</span>,
    schedule_interval=timedelta(days=<span class="hljs-number">1</span>),
    tags=[<span class="hljs-string">'retail'</span>]
)</pre>



<p>负责运行每个ETL过程的PyhtonOperators:</p>



<pre class="hljs">etl_manager = DataETLManager(DATA_PATH, <span class="hljs-string">'OnlineRetail.csv'</span>)

extract = PythonOperator(
    task_id=<span class="hljs-string">'extract_data'</span>,
    python_callable=etl_manager.extract_data,
    dag=etl_dag
)

transform = PythonOperator(
    task_id=<span class="hljs-string">'transform_data'</span>,
    python_callable=etl_manager.transform_data,
    dag=etl_dag
)

load = PythonOperator(
    task_id=<span class="hljs-string">'load_data'</span>,
    python_callable=etl_manager.load_data,
    dag=etl_dag
)</pre>



<p>最后，我们定义任务相关性:提取，然后转换，然后加载到数据库中。</p>



<pre class="hljs">extract &gt;&gt; transform &gt;gt; load</pre>



<h2 id="predictions-benchmarks">运行预测和基准测试结果</h2>



<p>在这一部分中，我们将把我们获得的经过转换和简化的数据分成训练集和测试集，并使用三种集成算法，使用我们之前提取的4个因素来预测严重的拖欠行为。</p>



<p>我们将把我们所有的ML开发与Neptune集成在一起，以跟踪和比较我们一路上得到的所有指标。</p>



<hr class="wp-block-separator"/>



<p class="c-box">请注意，由于最近的<a href="/web/20221117203556/https://neptune.ai/blog/neptune-new" target="_blank" rel="noreferrer noopener"> API更新</a>，这篇文章也需要一些改变——我们正在努力！与此同时，请检查<a href="https://web.archive.org/web/20221117203556/https://docs.neptune.ai/" target="_blank" rel="noreferrer noopener">海王星文档</a>，那里的一切都是最新的！</p>



<hr class="wp-block-separator"/>



<p>使用三个ML模型，我们将能够比较结果，并看到每种方法在哪里可以表现得更好。我们将为此特定任务测试三个ML模型:</p>



<ul><li>逻辑回归</li><li>决策树</li><li>极端梯度推进</li></ul>



<h3>为发展设定海王星</h3>



<p>安装所需的neptune客户端库，开始将您的代码与Neptune集成:</p>



<p>安装neptune库:</p>



<pre class="hljs">pip  install neptune-client</pre>



<p>安装Neptune笔记本，这样可以将我们所有的工作保存到Neptune网站</p>



<pre class="hljs">pip install -U neptune-notebooks</pre>



<p>通过安装以下扩展来启用jupiter集成</p>



<pre class="hljs">jupyter nbextension enable --py neptune-notebooks</pre>



<p>获取您的api密钥，将您的笔记本与您的Neptune会话连接起来:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/fbebb4c833672af60262b6d9cd07ba05.png" alt="ML-development-API" class="wp-image-43537" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/ML-development-API.png?ssl=1"/><figcaption><em>Set up your Neptune Token from the website </em></figcaption></figure></div>



<p>要完成设置，请在笔记本中导入neptune客户端库，并调用neptune.init()方法初始化连接:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune
neptune.init(project_qualified_name=<span class="hljs-string">'aymane.hachcham/CreditScoring'</span>)</pre>



<h3>逻辑回归</h3>



<p>回归模型的主要目标是根据特定的数学标准找到最适合一组数据的预测值组合。</p>



<p>在分类反应数据问题中最常用的回归模型是逻辑回归。通常，逻辑回归使用逻辑函数的基本形式，根据各种预测值对二元因变量进行建模。</p>



<p>该模型的训练非常简单。我们使用著名的机器学习库<em> SciKit-Learn </em>来实现逻辑模型。之前在数据处理方面所做的努力将在这一部分对我们有很大帮助。</p>



<p>包含先前定义的因子的表格如下所示:</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/d2181a816a555649f03c9524d163d54f.png" alt="ETL final_table" class="wp-image-43932" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/ETL-final_table.png?resize=767%2C431&amp;ssl=1"/><figcaption><em>Table that aggregates the values for the 4 factors</em></figcaption></figure></div>



<pre class="hljs">credit_scoring_final = pd.DataFrame(new_data_frame, columns=[<span class="hljs-string">'Financial_Struggle'</span>, <span class="hljs-string">'Finance_Requirements'</span>, <span class="hljs-string">'Expendable_Income'</span>, <span class="hljs-string">'Behavioral_LifeStyle'</span>])
credit_scoring_final</pre>



<p><strong>分离训练集和测试集中的数据:</strong></p>



<pre class="hljs">X = credit_scoring_final
x_train, x_test, y_train, y_test = train_test_split(X, target, test_size=<span class="hljs-number">0.25</span>, random_state=<span class="hljs-number">56</span>)</pre>



<p><strong>用逻辑回归训练:</strong></p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression

model = LogisticRegression(C=<span class="hljs-number">0.00026366508987303583</span>, class_weight=<span class="hljs-keyword">None</span>, dual=<span class="hljs-keyword">False</span>, max_iter=<span class="hljs-number">100</span>, multi_class=<span class="hljs-string">'auto'</span>, n_jobs=<span class="hljs-keyword">None</span>, penalty=<span class="hljs-string">'l1'</span>,
random_state=<span class="hljs-keyword">None</span>, solver=<span class="hljs-string">'saga'</span>)
model.fit(x_train, y_train)</pre>



<p><strong>测试结果:</strong></p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, classification_report

predictions = model.predict(x_test)
test_accuracy = accuracy_score(y_test, predictions)</pre>



<p id="separator-block_61af171d427bd" class="block-separator block-separator--10"> </p>



<div class="is-layout-flex wp-container-7 wp-block-columns">
<div class="is-layout-flow wp-block-column">
<div id="medium-table-block_61af1747427c0" class="block-medium-table ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">准确(性)</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">回忆</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">精确</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">f1-分数</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">因素4</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>
</div>



<p class="is-layout-flow wp-block-column"/>
</div>



<p id="separator-block_61af17d5427cd" class="block-separator block-separator--25"> </p>



<p><strong> ROC曲线和混淆矩阵:</strong></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/475bd039be647ec88f16c26a114b4b92.png" alt="ROC Curve and Confusion Matrix" class="wp-image-43933" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/ROC-Curve-and-Confusion-Matrix-for-the-Logistics-Regression.png?ssl=1"/><figcaption><em>ROC Curve and Confusion Matrix for the Logistic Regression</em></figcaption></figure></div>



<p>由于坏账在数据中所占的比例，该模型总是能预测出比好债务人更好的坏账。这是不可避免的，回归模型无法超越这一限制，无论涉及的工程水平如何。</p>



<h3>XGBoost</h3>



<p><strong>EXtreme Gradient Boosting</strong>(XGBoost)是华盛顿大学博士生陈天琦(Tianqi Chen)创建的梯度增强的优化和并行化开源实现。</p>



<p>XGBoost使用决策树(像random forest)来解决分类(二进制和多类)、排序和回归问题。所以，我们在这里的监督学习算法领域。</p>



<p>从初始化海王星实验开始:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> neptune
<span class="hljs-keyword">from</span> neptunecontrib.monitoring.xgboost <span class="hljs-keyword">import</span> neptune_callback

params_attempt3 = {
    <span class="hljs-string">'max_depth'</span>:<span class="hljs-number">10</span>,
    <span class="hljs-string">'learning_rate'</span>:<span class="hljs-number">0.001</span>,
    <span class="hljs-string">'colsample_bytree'</span>: <span class="hljs-number">0.7</span>,
    <span class="hljs-string">'subsample'</span>:<span class="hljs-number">0.8</span>,
    <span class="hljs-string">'gamma'</span>: <span class="hljs-number">0.3</span>,
    <span class="hljs-string">'alpha'</span>:<span class="hljs-number">0.35</span>,
    <span class="hljs-string">'n_estimator'</span>: <span class="hljs-number">100</span>,
    <span class="hljs-string">'objective'</span>: <span class="hljs-string">'binary:logistic'</span>,
    <span class="hljs-string">'eval_metric'</span>: <span class="hljs-string">'error'</span>
}

neptune.create_experiment(
    name=<span class="hljs-string">'CreditScoring XGB'</span>,
    tags=[<span class="hljs-string">'XGBoost'</span>, <span class="hljs-string">'Credit Scoring'</span>],
    params=params
)</pre>



<p>拆分数据并实例化DMatrix数据加载器:</p>



<pre class="hljs">x_train, x_test, y_train, y_test = train_test_split(X, target, test_size=<span class="hljs-number">0.25</span>, random_state=<span class="hljs-number">56</span>)

dtrain = xgb.DMatrix(x_train, label=y_train)
dtest = xgb.DMatrix(x_test, label=y_test)</pre>



<p>让我们开始训练模型，并使用<a href="https://web.archive.org/web/20221117203556/https://docs.neptune.ai/essentials/integrations/machine-learning-frameworks/xgboost" target="_blank" rel="noreferrer noopener">Neptune XGBoost integration</a>跟踪每个指标。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb
<span class="hljs-keyword">import</span> neptune
<span class="hljs-keyword">from</span> neptunecontrib.monitoring.xgboost <span class="hljs-keyword">import</span> neptune_callback

xgb_classifer = xgb.XGBClassifier(**params_attempt3)
xgb_classifer.fit(
    x_train,
    y_train,
    eval_set=[(x_test, y_test)],
    callbacks=[neptune_callback(log_tree=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])])

neptune.stop()</pre>



<p>回到Neptune查看损失指标和特性重要性图:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/53aab21fb9043ed4430c21f3b8ada323.png" alt="Charts of train and eval loss" class="wp-image-43934" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Charts-of-Train-and-Eval-Loss_-Neptune-Web-UI.png?ssl=1"/><figcaption><em>Charts of train and eval loss: Neptune web UI</em></figcaption></figure></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/41ebf272918226cc3d83d49558514f58.png" alt="Feature importance graph " class="wp-image-43935" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Feature-Importance-Graph-for-the-4-factors.png?resize=591%2C367&amp;ssl=1"/><figcaption><em>Feature importance graph for the 4 factors</em></figcaption></figure></div>



<p>我们甚至可以看看模型的内部估计器:XGBoost内部使用的树的图形。</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><a href="https://web.archive.org/web/20221117203556/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Internal-XGBoost-estimators.png?ssl=1" target="_blank" rel="noopener"><img decoding="async" loading="lazy" src="../Images/37637cd366efa264f208a0655c8288b4.png" alt="Internal XGBoost estimators" class="wp-image-43937" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Internal-XGBoost-estimators.png?resize=900%2C76&amp;ssl=1"/></a><figcaption><em>Internal XGBoost estimators (click to enlarge)</em></figcaption></figure></div>



<p>XGBoost结果:</p>



<p id="separator-block_61af18d7427ce" class="block-separator block-separator--10">准确(性)</p>



<div class="is-layout-flex wp-container-10 wp-block-columns">
<div class="is-layout-flow wp-block-column">
<div id="medium-table-block_61af18ee427d1" class="block-medium-table ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">回忆</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">精确</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">f1-分数</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">                                                      </p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">决策图表</td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>
</div>



<p class="is-layout-flow wp-block-column">决策树的优点是解释简单、训练快速、非参数化，并且只需要很少的数据预处理。它们可以通过监督学习算法自动计算，该算法能够在非结构化和潜在的大数据中自动选择区别变量。</p>
</div>



<p id="separator-block_61af18eb427d0" class="block-separator block-separator--25">我们将为这个准备好的数据集测试决策树的一个轻量级实现，然后对三个模型进行基准测试，看看哪个性能更好。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/2647dab2832f9818ce5ec01eb29166d3.png" alt="" class="wp-image-43940" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/ROC-Curve-and-Confusion-Matrix-for-the-XGBoost-performer.png?ssl=1"/><figcaption><em>ROC Curve and Confusion Matrix for the XGBoost performer</em></figcaption></figure></div>



<h3>训练模型</h3>



<p>结果:</p>



<p>准确(性)</p>



<h3>回忆</h3>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier

classifier = DecisionTreeClassifier()
classifier.fit(x_train, y_train)</pre>



<p>精确</p>



<pre class="hljs">preds = classifier.predict(x_test)
print(<span class="hljs-string">'Accuracy Score: '</span>, metrics.accuracy_score(y_test, preds))</pre>



<p id="separator-block_61af1987427dd" class="block-separator block-separator--10">f1-分数</p>



<div class="is-layout-flex wp-container-13 wp-block-columns">
<div class="is-layout-flow wp-block-column">
<div id="medium-table-block_61af1998427de" class="block-medium-table ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">                                                      </p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">                                                      </p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">                                                      </p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">                                                      </p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">三个模型的基准</td>

                    
                        <td class="c-ceil">在多次训练逻辑回归(LR)、决策树(DT)和极端梯度推进(XGBoost)以避免单一结果偏差之后，将使用每个模型的最佳执行迭代来进行比较。</td>

                    
                        <td class="c-ceil"> </td>

                    
                        <td class="c-ceil"> </td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>
</div>



<p class="is-layout-flow wp-block-column">我们看到所有的模型在准确性方面都非常接近，尽管总的来说<strong> XGBoost </strong>做得更好。最有趣的度量是F1分数，它更好地评估了模型在识别正确类别方面的混乱程度。<strong> XGBoost </strong>在预测正确的类别和区分“<em>坏</em>”和“<em>好</em>”债务人方面做得更好。</p>
</div>



<p id="separator-block_61af199c427df" class="block-separator block-separator--25">结论</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/3b55c843ba1c377bbd80d43ee6292f62.png" alt="" class="wp-image-43938" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203556im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/ROC-Curve-and-Confusion-Matrix-for-Decision-Tree.png?ssl=1"/><figcaption><em>ROC Curve and Confusion Matrix for the Decision Tree model</em></figcaption></figure></div>



<h3>在本教程中，我们对数据科学工作中涉及的不同方面进行了全面考察，例如:</h3>



<p>数据预处理和转换，</p>



<p id="separator-block_61af1a19427e9" class="block-separator block-separator--10">数据管道自动化，</p>







<p id="separator-block_61af1a2d427ea" class="block-separator block-separator--15">统计分析和降维，</p>



<p>ML开发和培训，</p>



<h2 id="conclusion">对多个模型变量进行基准测试并选择正确的一个，</h2>



<p>使用Neptune的ML工作流。</p>



<ul><li>我希望这篇教程对你有用，因为我已经把它设计成完全覆盖真实数据科学用例的不同方面。如果你觉得你对知识的渴望仍然需要冷却，请查看下面的参考资料。玩得开心！</li><li>参考</li><li>Statistical Analysis and Dimensionality reduction,</li><li>ML development and training,</li><li>Benchmarking multiple model variants and picking the right one,</li><li>ML workflow using Neptune.</li></ul>



<p>I hope this tutorial was useful to you, as I’ve designed it to fully cover different aspects of real data science use-cases.  If you feel that your thirst for knowledge still needs quenching go ahead check the references below. Have fun!</p>



<h3>References</h3>




        </div>
        
    </div>    
</body>
</html>