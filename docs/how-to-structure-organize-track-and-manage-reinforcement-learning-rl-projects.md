# 如何构建、组织、跟踪和管理强化学习(RL)项目

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/how-to-structure-organize-track-and-manage-reinforcement-learning-rl-projects>

构建和管理机器学习项目可能是一件棘手的事情。

当您投入到一个项目中时，您可能会很快意识到您淹没在 Python 脚本、数据、算法、函数、更新等等的海洋中。在某些时候，你会忘记你的实验，甚至说不出哪个脚本或更新产生了最好的结果。

因此，**组织你的项目和[跟踪实验](/web/20220926084128/https://neptune.ai/experiment-tracking)** 是成功的关键部分。

从这个角度来看，从事一个 **ML** 项目总体来说可能具有挑战性，但是有些领域比其他领域更复杂。**强化学习** ( **RL** )就是其中比较复杂的一种**。**

本文致力于**构建和管理 RL 项目**。我会尽量精确，并提供一个全面的分步指南和一些有用的提示。

我们将涵盖:

*   **一般提示**—项目目录结构， **Cookiecutter** ，使用 **Neptune** 跟踪实验，适当评估
*   **将问题定义为 RL 问题**–强化学习、监督学习、优化问题、最大化和最小化
*   **挑选 RL 环境**–open ai 健身房
*   **选择 RL 库和算法**–RL _ 蔻驰、张量力、稳定基线、RL _ 蔻驰准则
*   **测试代理的性能**
*   **准备发布**–自述文件、需求、可读代码、可视化

让我们跳进来。

## 一般提示

首先，你必须回到基础，记住 **可以应用于任何 ML 项目**的**提示。这些是:**

1.  **项目目录结构**
2.  **跟踪实验**
3.  **适当的模型评估**

### 项目目录结构

总的来说，**保持工作目录结构化并易于浏览**的能力是一项了不起的技能。

当谈到数据科学工作流时，我们面临多种因素，例如:

*   数据
*   模型
*   日志
*   培训和测试脚本
*   超参数文件
*   其他的

有各种各样的[实践和方法](https://web.archive.org/web/20220926084128/https://arxiv.org/pdf/1210.0530v3.pdf)来构建你的工作目录。根据我个人的经验，最好、最快、最简单的方法是使用 [Cookiecutter](https://web.archive.org/web/20220926084128/https://drivendata.github.io/cookiecutter-data-science/) 模板。

Cookiecutter 是一个有着自己理念的强大工具。它提供了易于浏览的完整文档。

尝试使用 **Cookiecutter** 模板来构建您下一个 RL 项目的工作目录，您会发现这是多么方便。

### 跟踪实验

从我的角度来看，最重要的一般提示是**跟踪实验**。您可能想要跟踪许多关键值:

*   超参数
*   推理时间
*   超过基线的增益
*   任何注释(例如，实验的文本描述)
*   其他的

即使你的目录结构很差，适当的**实验跟踪**也是一个必须具备的特性。它将使你免于失去工作节奏和有价值的结果。

对于强化学习，由于有了 [Neptune](https://web.archive.org/web/20220926084128/https://neptune.ai/) ，实验跟踪并不是一项具有挑战性的任务，它可以与大多数 [RL 库](/web/20220926084128/https://neptune.ai/blog/the-best-tools-for-reinforcement-learning-in-python)一起使用。

还有其他的跟踪工具，但是它们不太适合 RL 项目。在选择你的 RL 库的时候记住这一点，并且总是**仔细检查你是否可以使用你最喜欢的跟踪工具来跟踪一个特定的库**。

尽管如此，海王星是一个稳定和伟大的工具来跟踪你的实验。它有大量有价值的例子和教程，既可以用于你自己的项目，也可以用于团队项目。

你绝对应该在下一个 RL 项目中尝试一下。

### 恰当的评价

最后但同样重要的是，在评估算法的性能时，您需要既小心又精确。在强化学习领域，**评估指标和流程在很大程度上取决于您的问题和您使用的环境**。

我建议先检查一下比赛和论坛，因为你可能会发现和你相似的问题，以及有价值的想法和建议。**永远不要低估社区的力量**，并且总是关注是否有新的有趣的东西。

请记住，在 RL 项目中工作时，如果可能的话，您应该总是**查看您的代理执行**的视频。当然，这不是一个合适的评估或实验跟踪技术，但它是对其他工具和度量标准的一个很好的补充。

我们已经讨论了基础知识，所以让我们继续讨论解决 RL 问题的所有主要步骤:

1.  **将问题定义为 RL 问题**
2.  **选择一个 RL 环境**
3.  **选择一个 RL 库和算法**
4.  **测试代理的性能**
5.  **准备发布您的项目**

## 将问题定义为 RL 问题

首先，我们需要决定**强化学习是否适合我们的问题**。这是非常重要的一步，因为我们不想通过使用不合适的学习模型或不相关的算法来使任务过于复杂。

RL 是关于**探索和开发，以及它们之间的权衡**。这是 RL 和许多其他类型学习的主要区别，比如监督学习。

RL 代理通过**与环境互动来学习，尝试不同的行动，并为这些行动获得不同的奖励值，同时目标是在结束时最大化整体奖励**。

这是一个与监督学习完全不同的概念，在监督学习中，代理通过将他们的预测与现有标签进行比较来学习，并在之后更新他们的策略。

这就是为什么你需要确定 RL 是否可以用来解决你的问题。

幸运的是，这很容易做到。

想想你的任务是不是一个**优化问题**。接下来，您必须弄清楚是否有您希望您的 RL 代理学会最大化或最小化的任何**指标。**

如果你的两个答案都是肯定的，那么强化学习**可能是解决这个问题的一个很好的选择**，你应该开始考虑一个 RL 环境。

## 选择一个 RL 环境

如果强化学习很好地解决了你的问题，那么是时候**选择或构建运行 RL 算法的基础设施**了。

这个基础设施被称为**环境**。它是用来训练特工的。基本上，环境是一个模拟真实环境的**模拟，代理将在真实环境中部署。**

有很多不同的 RL 环境:

这也是为什么，如果你不想自己搭建环境，我建议**用最流行最常用的——open ai Gym**。

如果您确实想构建自己的环境，您将面临一系列挑战。你需要考虑:

1.  **环境结构**–您想要构建什么类型的环境
2.  **环境接口**–将环境连接到 RL 算法的接口
3.  **测试环境**–确保您的实现完美运行，您的 RL 代理将正确学习

说实话，**这些都不是在**上工作的超级明显的事情。这就是为什么我建议使用有价值的[文章](https://web.archive.org/web/20220926084128/https://towardsdatascience.com/create-your-own-reinforcement-learning-environment-beb12f4151ef)和[帖子](https://web.archive.org/web/20220926084128/https://anisdismail.com/RL-article-2.html#RL-article-2)和[视频教程](https://web.archive.org/web/20220926084128/https://www.youtube.com/watch?v=G92TF4xYQcU)来打磨这个话题。希望这足以让你建立自己的 RL 环境。

然而，请记住不要让任务过于复杂，所以只有在必要时才设置自己的环境。使用预装的也没什么不好。

## 挑选一个 RL 库和算法

在这一点上，你有**来选择一个 RL 库和你将用来解决问题的算法**。

有很多 RL 库，选择正确的库是项目成功的关键。我推荐阅读“[你实际想尝试的 Python 中强化学习的最佳工具](/web/20220926084128/https://neptune.ai/blog/the-best-tools-for-reinforcement-learning-in-python)”。这篇文章将帮助你做出选择。

总体来说，我强烈推荐 **Tensorforce，稳定基线，或者 RL _ 蔻驰**。它们似乎是最新的，实现了一组很好的算法，并提供了有价值的教程和完整的文档。此外，它们可以在多种环境下工作，因此设置应该不成问题。

至于 RL 算法的选择，我觉得你已经投入到任务中并选择了算法。如果是这样，那太好了，你应该开始训练你的经纪人了。

如果没有，请检查[RL _ 蔻驰文档](https://web.archive.org/web/20220926084128/https://intellabs.github.io/coach/selecting_an_algorithm.html)。在我看来，**为如何为你的任务**选择正确的算法提供了完美的指导。有这个问题应该对你有很大帮助。

## 测试代理的性能

现在，当你的 RL 代理被训练后，是时候评估它了。正如我之前提到的，这可能是一个棘手的过程，取决于您的问题和您使用的环境。

尽管如此，我还是想在这里提一些一般性的建议。

如果你的目标是最优控制，你应该使用一些**奖励**的综合措施。例如，每集的总报酬，或每个时间步长的平均报酬。这将有助于您了解代理在任务中的表现。

如果你正在处理一个视频游戏问题，或者一个设计得很容易识别的问题，使用**一个基于奖励的衡量标准的最大界限**。之后，您可以将您的代理与这个已知值进行比较。有理由期待一个好的代理会接近最大值。

在实践中，**许多有趣的问题没有一个已知的奖励总数或平均值的上限**。对于这些问题，通常您能做的最好的事情就是在代理之间进行比较。您可以比较:

*   随机行动的代理人–这通常只是一个基线，表明代理人已经学到了一些东西
*   自动化代理–代理使用简单的行动选择启发式，这可能是给定问题中自然或明显的东西
*   一个或多个人在同一项任务上
*   **其他受过 ML 训练的代理**包括同一代理的先前实例

如果政策或环境是随机的，你可能想要**运行多个测试并平均结果**，以尽可能多地评估具有期望值的代理。

在测试期间关闭任何探索也是非常重要的，如果你使用任何不符合政策的技术，比如 DQN，就可以公平地衡量训练有素的代理表现如何。

如果您的代理设计为不断学习和探索，和/或使用政策方法，您可以使用培训期间的结果来评估它。例如，你可以对过去 N 集的总奖励进行滚动平均，或者类似的方法。

此外，这对于**监控培训**来说是一个不错的指标，甚至对于非政策方法也是如此。尽管对于不符合策略的情况，与单独的测试运行相比，您可能会低估性能。

有**其他方法和其他指标**来评估代理。例如，代理需要学习多少经验或多少计算才能达到某一水平通常是感兴趣的。

如果你想断定代理人对于最优控制任务的训练是好是坏，这种对总报酬的评估可能就是你所需要的。

但是，您也可以查看任何神经网络内部的损失指标——您不会为了将代理分为更好或更差而这样做，但您可以这样做来识别问题。

这些损失度量通常与监督学习等同物相同。例如，在 DQN，或者对于 PPO 的批评部分，您会对**任何状态的预测值是否与最终值**匹配感兴趣，并使用 MSE 损失。

## 准备发布

这是最后一步，在这里你可以发挥创造力，展示你的个性。

不过，请记住，如果你计划向全世界发布你的项目，比如在 **Github** 上，你可能需要遵循一些简单的规则:

1.  **自述文件**–请在您的存储库中准备好一份自述文件，它将帮助那些不熟悉您的项目的人建立项目
2.  **requirements . txt**–一个包含您用来制作这个项目的库和库版本的文件
3.  **易于定制的可读代码**——这对你和潜在用户都有好处
4.  **一些有价值的可视化效果**–如果是 RL 项目，这可以是你的代理工作的 gif

遵守这些规则是值得的，因为在数据科学社区，它们被认为是基本的礼仪。

就这样，你的项目完成了。恭喜你！

## 最后的想法

我希望你能为下一个强化学习项目找到新的思路。

总之，我们从构建和管理任何 ML 项目的一些通用技巧开始，并逐步指导如何在强化学习项目中构建您的工作。最后，我们讨论了项目发布的一些想法。

如果你喜欢这篇文章，那么下一步就是开始用所有相关的工具构建你自己的 RL 项目结构。查看工具，如:

感谢阅读，祝训练愉快！

## 资源

### 弗拉基米尔·利亚申科

年轻的人工智能爱好者，对医学中的教育技术和计算机视觉充满热情。我想通过帮助其他人学习，探索新的机会，并通过先进的技术跟踪他们的健康状况，让世界变得更美好。

* * *

**阅读下一篇**

## ML 元数据存储:它是什么，为什么重要，以及如何实现它

13 分钟阅读|作者 Jakub Czakon |年 8 月 13 日更新

大多数找到这个页面的人都想改进他们的建模过程。

但是他们在存储和管理 ML 模型元数据方面的问题是不同的。

对一些人来说，问题在于杂乱的实验。

其他人已经将第一批模型部署到生产中，但是他们不知道这些模型是如何创建的，也不知道使用了哪些数据。

有些人已经在生产中有了许多模型，但是编排模型 A/B 测试，切换挑战者和冠军，或者触发、测试和监控再培训管道并不是很好。

如果你认为自己属于这些群体中的一员，或者介于两者之间，我可以告诉你，ML 元数据存储可以帮助你完成所有这些事情，甚至更多。

您可能需要将其连接到其他 [MLOps 工具](/web/20220926084128/https://neptune.ai/blog/best-mlops-tools)或您的 CI/CD 管道，但它将简化大多数工作流程中的模型管理。

…但是[实验跟踪](/web/20220926084128/https://neptune.ai/experiment-tracking)、模型注册、模型存储、模型目录和其他与模型相关的动物也是如此。

那么 ML 元数据存储到底是什么，它与其他模型有什么不同，它如何帮助您更自信地构建和部署模型？

这就是这篇文章的内容。

另外，如果你是那种喜欢摆弄东西来看看它们是什么的人，你可以在 Neptune ML 元数据存储库中查看这个[示例项目。](https://web.archive.org/web/20220926084128/https://app.neptune.ai/common/example-project-tensorflow-keras/e/TFKERAS-14/dashboard/summary-6f234f52-6b77-476a-9486-63037655b3be)

但是首先…

## 元数据管理和什么是 ML 元数据？

在我们深入 ML 元数据存储之前，我可能应该告诉你我所说的“机器学习元数据”是什么意思。

当你做机器学习时，总会涉及到一个模型。这就是机器学习。

它可能是一个经典的监督模型，如 lightGBM 分类器、强化学习代理、贝叶斯优化算法或其他任何东西。

但它需要一些数据，通过一些数字运行，并输出一个决定。

…将它投入生产需要大量的工作。

[Continue reading ->](/web/20220926084128/https://neptune.ai/blog/ml-metadata-store)

* * *