<html>
<head>
<title>Deploying ML Models on GPU With Kyle Morris </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>使用Kyle Morris在GPU上部署ML模型</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/deploying-ml-models-on-gpu-with-kyle-morris#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/deploying-ml-models-on-gpu-with-kyle-morris#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>这篇文章最初是MLOps Live(T1)的一集，这是一个互动的Q(T2)环节，在这里，ML从业者回答来自其他ML从业者的问题。</p>



<p>每集都专注于一个特定的ML主题，在这一集里，我们和来自<a href="https://web.archive.org/web/20230103154737/https://www.banana.dev/" target="_blank" rel="noreferrer noopener nofollow"> Banana </a>的Kyle Morris谈论了关于<strong>在GPU上部署模型的事情。</strong></p>



<p>你可以在YouTube上观看:</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><p class="wp-block-embed__wrapper"><iframe title="Deploying Models on GPU With Kyle Morris" src="https://web.archive.org/web/20230103154737if_/https://www.youtube.com/embed/LchUTiF50xE?list=PLKePQLVx9tOczB07_oyDkdQqdNiqLV-zX" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p></figure>



<p>或者作为播客在以下位置收听:</p>







<p>但如果你更喜欢书面版本，这里有！</p>



<p>您将了解到:</p>



<div id="case-study-numbered-list-block_1855c1e1dbc5dd94dcf99ce9caf4d7f1" class="block-case-study-numbered-list ">

    
    <h2 id="h-"><span class="c-list__counter"> 1 </span>开始部署GPU之前需要知道的事情</h2>

    <ul class="c-list">
                    <li class="c-list__item"><span class="c-list__counter"> 2 </span> GPU工具及其实用程序</li>
                    <li class="c-list__item"><span class="c-list__counter"> 3 </span>围绕GPU的常见误解</li>
                    <li class="c-list__item"><span class="c-list__counter"> 4 </span>充分利用GPU</li>
                    <li class="c-list__item"><span class="c-list__counter"> 5 </span>最佳GPU用例</li>
                    <li class="c-list__item"><span class="c-list__counter"> 6 </span>使用GPU管理权衡</li>
                    <li class="c-list__item"><span class="c-list__counter">7</span>…等等。</li>
                    <li class="c-list__item"><strong> Sabine: </strong>大家好，欢迎来到MLOps Live。这是与我们今天的嘉宾<a href="https://web.archive.org/web/20230103154737/https://www.linkedin.com/in/kylejohnmorris/" target="_blank" rel="noreferrer noopener nofollow">凯尔·莫瑞斯</a>的互动问答环节&amp;。谁是今天这个话题的专家，就是在GPU上部署模型。凯尔，让你暖和一下。你如何解释在一分钟内在GPU上部署模型？</li>
            </ul>
</div>



<p>凯尔·莫里斯:是的，好问题。</p>



<p>我会说这感觉类似于在21世纪初部署一个网站。当我开始我的职业生涯时，没有基础设施来做这件事。我们已经建立了许多工具来使网站上线，并能够处理诸如可扩展性和平衡流量之类的事情，部署在不同的国家。一旦你用GPU做事情，基础设施就不存在了。我会称之为非常相似的底层基础设施，直到你深入了解并意识到它还没有建成。</p>



<blockquote class="wp-block-quote">
<p>当你问自己为什么会意识到有很多独特的延迟边缘情况。我认为GPU的游戏名称是延迟。缓慢的启动时间，如果你在做机器学习，缓慢的推理，诸如此类的事情是阻碍市场的原因。生产环境中的延迟越快，托管成本越低，世界就越容易访问它。成本和速度是阻碍生产基于GPU的系统的两个因素。我认为这是与传统系统的主要区别。</p>
</blockquote>



<p>部署GPU:开始之前要知道的事情</p>



<h2 id="h-deploying-gpus-things-to-know-before-starting-out">斯蒂芬:再问一个问题，你希望自己在创业时知道的三件事是什么？关于部署GPU的事情，通常只是使用基于GPU的模型？</h2>



<p>凯尔:有两件事我希望一开始就知道。一个是集装箱化，在这方面非常精通。使用Docker，Kubernetes，Pulumi，设置并能够容器化依赖GPU的应用程序。知道了如何设置CUDA驱动程序，你会花很多时间去做。假设你是一个早期的初创公司，你正试图部署一个生产应用程序，你不熟悉那些工具，你对GPU几乎没有希望。</p>



<p>第二件我觉得很不直观但很有趣的事情是，你用来做机器学习的应用程序主要是为训练而构建的。<a href="https://web.archive.org/web/20230103154737/https://pytorch.org/" target="_blank" rel="noreferrer noopener nofollow"> Pytorch </a>、<a href="https://web.archive.org/web/20230103154737/https://www.tensorflow.org/?gclid=Cj0KCQiAtICdBhCLARIsALUBFcFL3cbSQw-TAnQqvYVDrw4a-RqwrZa3YoBySiqYAB4iZj65zjP2WfYaAuY2EALw_wcB" target="_blank" rel="noreferrer noopener nofollow"> TensorFlow </a>，这些都是研发工具，还有<a href="https://web.archive.org/web/20230103154737/https://flask.palletsprojects.com/en/2.2.x/" target="_blank" rel="noreferrer noopener nofollow"> Flask </a>，你要设置的很多服务器，你都将处于开发模式。人们有一个很大的误解，就是他们带着开发工具去生产，他们没有意识到…</p>



<blockquote class="wp-block-quote">
<p>举一个具体的例子，我们有多少次客户试图给我们生产应用，他们没有启用GPU。他们已经在GPU上启动了，但他们实际上只是在使用CPU，他们甚至没有意识到他们没有这样做。然后，当您走到引擎盖下时，您意识到您的Pytorch脚本正在CPU上部署。诸如此类的小事，但当它深入下去，你会意识到这些工具并不是为快速运行这些事情而构建的。这就是我一直以来的观点，意识到我们需要一套全新的工具来部署生产GPU应用程序。</p>
</blockquote>



<p><strong> Stephen: </strong>你谈到了这些工具中的大部分都是为训练自己而优化的。在我们深入探讨其他领域之前，我只想简单介绍一下这些工具。例如，您用于部署的工具是什么？它们有多相关？考虑在GPU上部署时，我是否需要学习一些原则？</p>







<p>Kyle: 我认为从任何编程的角度来说，理解你正在运行的硬件都是非常重要的。我的职业是机器人工程师。在此之前，我从事自动驾驶汽车的工作，如果你不了解硬件是如何执行的，你就无法利用速度优化的优势。这与人们一开始没有意识到他们部署在错误的硬件上有关。</p>



<p>GCP上的GPU机器，或AWS上有一个CPU。人们没有意识到他们甚至没有利用这个系统。一旦你做到了，你需要意识到GPU有一种不同的读、写和使用内存的方式。您可以做很多事情来大大提高模型的速度，比在训练环境中要快得多。我认为对GPU内存与CPU内存的工作原理有一个基本的了解，只是对操作系统有一个基本的了解，会给你一个新的视角。意识到这不仅仅是引擎盖下的计算，如果你认真尝试对生产机器学习进行压力测试，你会想要挖掘不同的提取，如GPU应用程序。</p>



<p>生产中的GPU与CPU</p>



<h2 id="h-gpus-vs-cpus-in-production"><strong>斯蒂芬:</strong>对。在计算方面，你认为团队在部署GPU和CPU时应该了解的最大区别是什么？</h2>



<p>凯尔:是的。我认为最重要的是你正在使用的GPU驱动程序，并了解它们是否针对ML设置进行了优化。你真的在快速地加载和卸载一个GPU吗？例如，在我所做的工作中，我采用了需要30分钟加载的模型，并且我将它们优化为在10秒内加载。然后人们会说，“这不可能。”这就像，“不，你还没有打开引擎盖，意识到你正在像在CPU上一样按顺序加载它，而GPU不是这样工作的。你可以做这些新的事情。”我真的认为对内存接口的理解是一个很大的区别。</p>



<p>当你看一台计算机时，你有处理，就像一个单元，你有从内存读取的操作系统，但然后是GPU和CPU，根本的区别是指令中的并行性。了解您是否真正充分利用了并行性。我建议你深入研究那些可以让你可视化GPU使用的工具，比如监控软件，基本上是为了真正看到你正在利用你正在使用的这个昂贵的硬件。</p>



<p>同样，为了将堆栈附加到它上面，我已经…</p>



<p>我在ML主机领域，我和几十个人一起工作过，90%的人没有利用GPU——超过一半，他们没有意识到这一点。即使专家喜欢来自特斯拉、克鲁斯和汽车公司的人，他们仍然没有利用这一点。大多数CPU工具都是为了利用我们已经拥有多年的CPU而构建的。有了GPU，还没有大量的生产机器学习。这只是，这只是开始成为一个大事情，所以这就是大优势所在。</p>



<blockquote class="wp-block-quote">
<p>围绕GPU的常见误解</p>
</blockquote>



<h2 id="h-common-misconceptions-around-gpus"><strong> Stephen: </strong>您还发现人们会犯其他错误吗，尤其是在理解他们如何利用或不利用他们在GPU上的部署时？</h2>



<p>Kyle: 我之前说的不实际使用GPU是最大、最昂贵的错误。人们将自动扩展到10个GPU来处理流量。他们会过来对我说，“嘿，我需要更快地做出推论。”然后，我会意识到您正在使用一个CPU，但他们已经部署在一个GPU上，碰巧他们使用的CPU更快，所以他们获得了20%的速度提升。他们认为这就是GPU的力量。然后我们会说，“不，实际上在GPU上正确地运行这个并启用并行”，就像一个较低的级别。</p>



<p>如果你不熟悉Python全局解释器锁，我认为另一个模块是……它确实很具体，但这阻碍了许多ML应用程序，因为在Python语言中，大多数机器学习应用程序现在都是用Python编写的，它禁用了真正的并发或真正的并行。如果你知道如何把一些东西移植到C++，或者另一个就像……我没有和Golang在机器学习或infra方面合作太多，你可以拥有真正的并行性。我说的是30倍的速度提升，不是2倍，3倍。我的意思是，比如10x，30x，32x，具体来说，基于GPU的大小。</p>



<p>你可以做很多很酷的事情。这需要努力，但没错，这是一个大障碍，人们通常不会意识到。那他们就不会堕落到承认这一点。他们只是认为，“哦，硬件一定很慢”。我认为一个常见的错误是人们过早地指责硬件。</p>



<blockquote class="wp-block-quote">
<p>他们没有意识到，如果你知道如何破解，GPU硬件是非常强大的。如果你是一个新的工程师，我会看到的典型情况是，他们会尝试CPU，就像这样，“我需要更快。扔在一个GPU上。”</p>
</blockquote>



<p>他们意识到他们甚至没有使用它，或者他们没有意识到，然后他们会说，“我需要更快，使用TPU”。他们只是使用这种每月3000美元的硬件，并在其上投入开箱即用的东西。我想重点是，如果你深入研究，你可以获得10到30倍的改进。这需要工作，这就是为什么我试图民主化，我想开始建立那里的工具。那是我的焦点。我在做，这样别人就不用做了。如果你在前沿领域遥遥领先，比方说，你在一家大的ML公司工作，但那里什么都没有，那就是你能创造很多价值的地方。</p>



<p>充分利用GPU</p>



<h2 id="h-getting-the-most-out-of-gpus">Stephen: 在回答社区问题之前，我会稍微深入一点。使用GPU的主要目的当然是为了加快推理时间。您首先会做什么，尤其是考虑到您希望优化正在进行的任何部署时？无论是从软件端还是硬件端？</h2>



<p>凯尔:是的。第一件事是了解你在什么硬件上运行，以及该硬件的基本限制是什么。其次，了解你使用了多少，并知道如何使用调试器。对于CPU，对于大多数代码，如果你在做Python，你可以做第三步调试。一旦你开始使用CUDA，在这个级别上，许多人不再看引擎盖下面，他们说，“哦，那是CUDA。不是我的事。”可能这个社区有点不一样。</p>



<p>给自己一个周末的时间，就像这样，我要成为一名GPU工程师，一头扎进CUDA。不要把它和一个结果联系在一起，只是强迫自己进入一点点的杂草中。了解CUDA如何分配内存，以便能够使用…即使最基本的命令行工具是NVIDIA-SMI。然后能够理解如何在GPU上进行相当于步骤3的调试，并查看使用了多少内存。</p>



<blockquote class="wp-block-quote">
<p>一个好的第一步是什么是我的…假设你正在运行一个ML推理。这是一个很常见的方法，如果我把这个给每个人，我会说，“试试这个”。你们中的一半人可能会找到更好地使用GPU的方法。这个东西将运行你的应用程序。假设是一个ML服务器，看看有多少GPU内存。然后，只需对自己思考“我首先使用的是完整的GPU吗？”然后意识到可以并行加载东西。这是一个巨大的优势。</p>
</blockquote>



<p>然后你会开始意识到有很多基础设施阻碍了内存虚拟化。其实很难做到。还有这些深层的技术问题。我并不是说这些都是容易的，快速的胜利，但是如果你真的想在这些系统上推动大的改变，那是你应该开始的地方。只是理解，我是在使用所有的GPU能力，还是有更多的可用？然后你可以开始做批量推理，这样你就可以调整你的模型。希望我使用的是以ML为中心的。我假设那是观众，但是如果你正在做ML工作负载，你可以开始批量处理输入，然后你可以说，开始。</p>



<p>到目前为止，我有需要3GB内存的型号。我开始批量处理。我基本上是并行进行推理，现在我使用12GB的内存。还是一个GPU，除了做更高的吞吐量。这意味着推理之类的成本降低了4倍。这一切都始于理解如何调试你正在做的事情。如何真正打开它，看看发生了什么，然后问问题。</p>



<p>你调试得越好，你调试的工具就越好，这些显而易见的东西就会越快出现。人们是聪明的，你会看到什么是错的。但如果没有调试工具，一切都只是黑匣子。那么GPU就是这个用数字做事的无定形盒子。你只是无法利用他们提供的性能。</p>



<p>浏览当前的ML框架</p>







<h2 id="h-navigating-through-current-ml-frameworks"><strong>斯蒂芬:</strong>对。稍后我们将回到理解和工具上来。我想我们有几个来自社区的问题。</h2>



<p><strong>萨宾:</strong>是的。凯尔，你肯定已经提到这个了。Pietra在chat中还提到，在TensorFlow这样的ML框架出现之前，你必须在底层用原生CUDA编码。开发起来更困难，需要更多的时间，但是你有很多的控制力和能力来优化你的代码。他想知道，在当前的框架下，你真的能做些什么来使推论更快？什么是典型的瓶颈，你能做些什么？</p>



<p>是的，我可以说你肯定可以。我正在做一个名为banana.dev的项目，主要是为了生产而优化ML。一个例子是，我们让GPTJ(一个非常大的变压器模型负载)快了近100倍。仅仅是通过开放库，在引擎盖下破解一些东西，并开始问一些简单的问题，比如“我们如何分配内存？这是我们能做的最好的吗？这个操作系统允许我们做什么？”所以，你肯定可以。是的，我同意。刚开始编程的时候，我是用C++从头开始写渐变下降的。我知道这些新的抽象已经消除了这一点。</p>



<p>所以你不再问自己这些问题，但是从Python的角度你可以做很多事情。在一定程度上，你必须开始降低到C++的全局解释器锁之下，因为要实现真正的并行，你肯定可以做到。我认为区别在于训练优化和生产推理。这些是非常不同类别的优化模型。训练更多的是关于机器之间的并行，你如何更快地收敛到一组重量？而生产推理就像，你如何向前传递？</p>



<p>首先，培训是向后传递，生产是向前传递。非常相似的事情，除了在生产中，你试图最小化你使用的计算量。在训练中，你会问，我如何最大限度地提高计算并行性，以便更快地进行训练？你只需要同时做一堆这样的事情。更多的是关于培训的吞吐量优化。在生产中，它是关于单个呼叫延迟和每次呼叫的价格。有一整套的东西，只要想想大多数人。您就培训优化与生产进行了多少次对话。就像人们还没有真正探索它一样。我看到还有第二个问题，但是…</p>



<p>GPU适合实时推理吗？</p>



<h2 id="h-are-gpus-suitable-for-real-time-inference">萨宾:是的，没错。这第二个问题正是关于在生产中使用GPU进行推理。问题是，它只是用于批量预测，还是真的可以用于近实时？例如，在引擎盖下使用GPU的REST API端点。</h2>



<p><strong> Kyle: </strong>是的，我再次假设，这意味着机器学习推理，并考虑使用GPU生产。这个问题取决于您的用例。通常，我可以说，我已经从CPU到GPU完成了大约50-100个项目。根据推理，你将看到的平均加速是通过GPU运行的3-8倍，就像现有的框架一样。您将更快地看到端到端延迟，并且您可以使用类似REST API的东西。</p>



<p>我多次构建的基本上是一个调用后端的SDK，它只是一个rest API。获取一个任务ID，提取它，然后跟踪该任务。你把它作为一个长期的东西登记在一个数据库里。然后检查该任务的状态，并从推理服务器更新该任务。你有一个中间层。</p>



<p>用GPU肯定更快。最重要的是价格。如果你在做类似聊天机器人的事情，大多数人最终需要GPU推理，因为你不能…如果客户发送一条消息，需要20秒钟来响应，这与三秒钟、五秒钟的记忆有很大不同。这里的问题是，应该考虑用GPU做生产吗？是的。你需要考虑的最重要的事情是你的成本。您是否有一个支持成本的应用程序？</p>



<p>我不想无耻地插太多，但我在Banana做的大事，我正在做的项目，就是让GPU主机更便宜，完全公开。我能说的是，如果你有一个GPU应用程序，即使你有无服务器的GPU，你也可以在不使用时关闭它们。如果使用它们的成本超过了客户交互的成本，你就不会想使用GPU。</p>



<p>例如，有人进来，他们正在与你的聊天机器人聊天，你正在使用GPU，他们的聊天时间是10分钟。如果这10分钟的GPU成本超过了客户为您的业务创造的成本或支付的成本，那么您就有负单位经济。我认为主要的障碍不是GPU很贵，而是人们总是开着它。主要的障碍是永远在线的图形处理器让你的单位经济变得很糟糕。你需要无服务器的…你可能会减少很多。</p>



<p>我会考虑这些不同的权衡。应该考虑GPU吗？当然，这样更快。客户喜欢更快的体验。也许你可以给出一个你可能不想使用GPU的边缘案例。我认为，如果您正在处理客户不需要响应的批量工作负载，比如说，在周五晚上，您将运行一个cron作业或他们现在的名称，这是一个基本的自动化作业，可以处理大量数据。CPU可以便宜很多。这对你来说可能是一个不错的选择，但是一旦速度成为一个因素，GPU肯定应该被考虑用于ML。</p>



<p><strong>观看与凯尔·莫里斯的另一次精彩谈话:</strong></p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><iframe loading="lazy" title="Don't Listen Unless You Are Going to Do ML in Production // Kyle Morris // MLOps Coffee Sessions #87" src="https://web.archive.org/web/20230103154737if_/https://www.youtube.com/embed/eDFCZNZnN-Q?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">视频</iframe></p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><p class="wp-block-embed__wrapper">实时GPU使用技巧</p></figure>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 id="h-tips-for-real-time-gpu-usage">斯蒂芬:是的，我会跟进这个问题。一名社区成员向我们提出了一个问题。因此，这个特殊的团队过去常常通过他们的批处理工作流来运行GPU推断。现在，他们正在转向更实时的工作流程，因为这是业务发展的方向。这是更好地为他们的用户服务的东西。团队现在询问您在使用GPU实时节省成本以及优化资源方面有什么建议。我是说，这就是我们目前为止所说的。也许是更具体的东西。</h2>



<p>凯尔:对不起。首先，我想澄清一下，当你说批处理时，你的意思是有两种类型的批处理。一种是批处理，你的电话在一周的某个时间打进来，你可以一次处理所有的电话。那么第二次批处理就是并行推理。你实际上是在向你的神经网络发送更大的有效载荷。我假设这需要后者，也就是你同时处理多个入口。就像你发送一个更大的有效载荷，然后问题是，如果你想把它像单独的调用一样分散，你不需要做这个庞大笨重的调用。你如何使它实时，这是我对这个问题的理解。</p>



<p>如果你想加快速度，我要考虑的主要事情是为你的用例重写内核，所以思考GPU所做的底层主要操作就像是网络的内核乘法，很多时候，我发现这一点。同样，有一种误解，认为你使用的工具是完美优化的，一旦你进入生产推理，你需要挑战这个假设。</p>



<p>你需要意识到，如果你深入了解——例如，我已经进入了最先进的图书馆，查看了他们的GPU所使用的内核。我会说，“嘿，这里有一个更好的方法来做矩阵乘法，”然后建立像2-3x这样的推理。我得说这是一个立竿见影的好方法。是的，它是这样的，因为你试图成为一个单一的调用速度内核，像乘法是最好的方式，或者尝试具有更快时钟周期的硬件，诸如此类，因为如果你不做批处理，那么这些都是你的选择。</p>



<p>如果你愿意沿着JAX的道路走下去，开始进行即时编译并查看你使用的硬件，即使是CPU也是值得考虑的，但我认为如果你想要2倍的速度，内核重写将是最容易实现的。那也许能让你实时。</p>



<p>最佳GPU用例</p>



<h2 id="h-best-gpu-use-cases"><strong> Sabine: </strong>是的，我们确实有一个关于GPU用例的问题。你在哪里看到最好的用例？显然，ML训练和推理，尤其是神经网络，但是对于一些数据处理工作流或类似的东西是否也有好处？约书亚在问。有哪些最好的用例？</h2>



<p>在我看来，我还是不知道真相。我希望你去发现你认为最好的，因为有很多东西要学。我所看到的是，视频处理是仅次于banana.dev的客户。这是他们来找我们进行视频处理的地方，他们没有使用机器学习，他们只是试图做基本上，我不能进入太多的细节，但基本上处理大型视频和做搜索之类的事情，没有神经网络，这只是传统的处理，但在GPU上你可以做得很好，因为你可以开始抨击帧，你可以比CPU快10-30倍。</p>



<p>我在想还有什么。机器人感知，我的意思是，这是我以前在克鲁斯时研究的。处理现场。同样，很多都是在引擎盖下进行的机器学习，但基本上，快速处理复杂丰富的场景就像是GPU的事情。视频最终会成为机器人感知，因为如果你在建造一辆自动驾驶汽车，你会不断地拍摄世界，你基本上是在创建一个视频，但你也可以有不同帧的视频，如点云和不同的数据类型。</p>



<p>任何需要像非ML那样的高帧率的地方。ML是预测输出的东西，比如在bash过程和传统方法中进行推理。这些是我在实践中看到的主要事情。</p>



<p>无服务器GPU的问题</p>



<h2 id="h-problems-with-serverless-gpus"><strong> Stephen: </strong>说到基础设施，我们有一个问题，实际上是来自社区提交的。这个人问你怎么看待无服务器GPU？您在与他们合作时发现了什么问题，他们正计划将其工作流程迁移到无服务器。<strong> </strong>问题是，他们在考虑无服务器的低延迟容忍度和冷启动问题。</h2>



<p>凯尔:我的意思是，这是我的激情问题，我要试着回答正确。基本上，这里的理解是为什么？</p>



<p><strong> Stephen: </strong>你在使用无服务器GPU时发现了哪些问题？</p>



<p>凯尔:是的。第一个问题是它们没有被提供，所以这就是为什么我试图为它们建立一个解决方案，像一些可靠的东西。他们不存在的主要原因，从我的理解，是冷启动一个模型的能力。我可以给你一个例子，如果你要去建立无服务器的GPU，我可以解释你接下来的三个月基本上会怎样。</p>



<p>这就像如果你从一个模型开始，你想设置自动缩放或什么的。你会发现的第一件事是，配置GPU可能需要几分钟时间来访问某些东西。一旦你有一个可用的GPU，使用Torch或TensorFlow，任何传统的库，加载一个典型的模型可能需要5到20分钟才能放入GPU内存。您需要进入并真正优化模型启动时间。在Banana，这是我们唯一的焦点。这就是我们如何将启动一个模型的时间从20分钟缩短到几秒钟。基本上，如果你能做到这一点，你的推理速度是相同的速度，那么你就可以关闭电脑。</p>



<p>我认为这种类比就像，想象你有一辆车，你知道你可能要去做一次非常重要的旅行，比如说，去医院或者去朋友家什么的，你不知道什么时候，你只知道，在某个时候，你不得不这样做，那辆车需要30分钟才能熄火，然后你可能会一直开着车。这就是我们对GPU的问题，因为机器学习工作负载需要很长时间，人们只是让他们的GPU开着，因为这是保证客户拥有良好体验的唯一方法。</p>



<p>无服务器GPU的主要问题是启用它们，延迟，并消除这一点，因为这样你就有了一台永远在线的机器。他们擅长客户体验，除了你节省90%或更多。我们正在朝这个方向努力。我们已经将它降低了95%到99%，但在许多应用程序中仍然不够，如果你有，比如说，一个客户进来，你可以通知GPU打开，你有5，10秒的时间，但如果你的应用程序需要亚秒，它还不存在，这是非常困难的。</p>



<p>我可以解释为什么这很难，但如果你想在一秒钟内启动一台机器，你需要能够在一秒钟内将内存加载到GPU上，最先进的固态硬盘的加载速率为3-4GB/s，所以这就像是将内存加载到硬件上的能力有一个理论界限，足够快，可以在不到5秒钟内将20GB的型号加载到内存中。我们正试图寻找解决方法。这是一个限制，我们可能正在试验分片加载，一次跨多个GPU加载，我们正在进行分片推理，这样一来，每个GPU只需加载一秒钟的工作，因此在网络中展开各层，这基本上是我们试图解决的问题。</p>



<p>使用GPU进行模型维护</p>



<h2 id="h-model-maintenance-with-gpus"><strong> Sabine: </strong>然后，Piatra向我们提出了更多的模型维护问题。让我们假设我们有一个生产模型，由数据科学家手动重新训练，可能每三个月改变一次网络架构、额外功能等。这是否意味着生产代码必须由ML工程师手动重写，以便在每次更新时针对GPU进行优化？</h2>



<p>凯尔:这真是个好问题。只是说，只是权重在变化，还是有其他东西，因为ML模型有不同的组件，有代码和权重，基本上，想想看，你只是用某种语言定义了这个网络，然后你就有一堆权重加载到网络上。通常，如果权重发生变化，您不必更改底层代码。权重是最常见的，如果你微调模型，你不必改变底层代码，因此你不必重新优化。</p>



<p>第二件事是，即使权重或架构发生变化，通常情况下，你会希望编写抽象来捕获一组广泛的神经网络，你不会希望为一个网络硬编码你的优化，这通常不是它的工作方式。例如，如果你正在做像Keras这样的东西，这是一个非常高级的抽象，他们有什么，他们有一些层API，如果你在优化它，你希望它能确保任何类型的层通过该API，被编译成优化的GPU代码，或解释什么取决于你在使用什么。通常情况下，您不必对其进行太多更改。</p>



<p>我正试图想出一个例外，因为我认为这比只是说你不必这样做更有价值。我想一个例外是，如果你正在做—在Banana，我们做了很多关于启动的事情，快速启动机器。一个很酷的例子是，如果您不断改变重量，或者您有许多不同的重量，您需要考虑如何快速存储和加载这些重量，因此存储成为一个实际的瓶颈，这成为您更多问题的来源，就是我在哪里放置所有这些优化的重量，我如何跟踪哪些需要进入瓶子，然后缓存它，使这个过程变得更快？这是花费时间最多的地方，而不是你的代码。</p>



<p>使用GPU管理权衡</p>



<h2 id="h-managing-trade-offs-with-gpus">萨宾:谢谢。马塞洛还有一个问题。正如您所说，在进行推理时拥有低延迟和降低成本之间似乎存在矛盾。如果你想能够足够快地进行推理，内存必须被加载到内存中，并且一直在为它的成本付费，即使是在不使用的时候。有没有解决这个问题的办法，基本上是让GPU的延迟和使用费用都很低？</h2>



<p>凯尔:是的。我认为这取决于最终客户的体验。我假设每个人都在为客户构建，也许这是一个不正确的假设，但有人在消费你的模型的输出，这是我的假设。如果您的客户需要真正快速的响应，您可以做一些事情，如GPU的预测可用性。基本上，速度和成本是相互权衡的，我发现解决这个问题的唯一方法是在需要时让GPU可用，即无服务器GPU，但有两个阵营，无服务器GPU仍然需要几秒钟才能启动，所以如果可以的话，对于您的应用程序，我会使用无服务器GPU，就个人而言，这是节省大量成本的主要方式。</p>



<p>如果您的应用程序需要实时响应，而它何时出现是完全不可预测的，这是另一个未解决的问题，您只需要在成本不变高的情况下更快地做出推断。再说一遍，核仁是一个东西。沿着ASIC的道路，创建一个定制电路，在硬编码的网络架构上执行操作，这绝对是另一个角度。</p>



<p>我认为一个简单的方法是尝试预测流量，例如，您的应用程序需要快速响应，并且您有五个始终运行的GPU，您能找到一种方法让其中一个始终运行，然后在需要时让其他四个自动扩展吗？然后，你可以处理99%的流量高峰，除了你不用为五台永不停机的机器付费，这是你最省钱的地方。</p>



<p>我想我应该换一种说法，如何在不牺牲延迟的情况下省钱？这是我迄今为止想到的最好的办法。如果你想换一种方式，那就是真正的深度R&amp;D，解决这些硬件能源瓶颈，这是一样的。在Cruise，我们正在进入这个领域，进入非常非常低的层次，深入到硬件之下。大多数团队可能不会这样做。如果这里有人对无服务器GPU感兴趣，显然，我不知道如何联系，但DM我，这是我正在努力做的事情。我很乐意帮忙。</p>



<p>云与本地GPU</p>



<h2 id="h-cloud-vs-on-premises-gpus"><strong>Sabine:</strong>Sri Kant向我们提出了一个关于本地与云的问题。例如，您如何看待由NVIDIA AI企业软件套件结合Red Hat OpenShift或VMware Tanzu管理的内部GPU集群，而不是由EKS管理的相同GPU集群的AWS堆栈或Azure堆栈？</h2>



<p>凯尔:我想确定我收到了。他们特别询问现场体验。在VPC，一个虚拟的私有云，可以访问您的资源，我假设存在隐私问题或其他问题，并且您正在询问多个不同的云。我没有和所有的软件都进行过深入的合作，我使用过GCP和AWS，并对其他一些软件进行过实验。我认为，当你开始使用Kubernetes时，需要优化的是，如果你在所有东西上都有一个容器抽象，你基本上可以相当容易地改变云提供商。</p>



<p>我发现有一些令人头痛的问题，但我正在寻找一个提供最容易部署您的应用程序的抽象概念作为第一步，因为我认为大多数人在公司早期遇到的瓶颈还不是延迟或成本，而是他们是否为人们建立了有价值的东西，特别是如果你是一家初创公司，所以我会优化部署快速学习。我注意到的是，在很大程度上，许多提供商平衡了他们的GCP在一个领域的成本，但这将为您在其他产品上节省资金。</p>



<p>例如，我不能引用Azure的确切数字，但我知道AWS上有一组EC2实例要贵得多，而GCP上没有。这是一个立竿见影的效果，但当你进入后，你会看到网络成本或更多，然后开始正常化。我真的会优化，比如，我会问这样的问题，你的团队熟悉哪一个？您是否使用了使云不可知论者变得容易的抽象？如果不是，那就更成问题了。</p>



<p>如果你不使用像Pulumi这样的东西，我会认真考虑的。Pulumi是我去年发现的最喜欢的工具之一，因为基本上，你可以使用相同的Python代码在GCP的AWS上部署。你不必进入控制台，这样会减少很多决策变量。它专注于在全球范围内为您节省最多时间的决策，比如如何使您的代码云不可知？那么仅仅选择决定也是高风险的。对吗？因为你真正要问的是，哦，所有这些供应商锁定了我，所以我必须做出这个重大决定，因为我知道这很难改变。</p>



<p>如果你去掉这个假设，你就很容易改变。你可以两全其美。你可以做一个多云部署，你可以说对于高GPU工作负载，使用GCP，我认为这是我推荐的一个非常好的选择，不确定Azure，然后如果你做一些高网络带宽或隐私中心，AWS有很多好的产品，如EKS。</p>



<p>削减GPU使用成本</p>



<h2 id="h-cutting-gpu-usage-costs"><strong> Stephen: </strong>好的，在我们进入社区之前，当然，我们一直在说的一件事是成本节约，特别是在GPU方面。当然，使用GPU的一个关键障碍是价格。你不想有一天醒来，你有几千美元，因为你只是简单地让一个EC2实例和你的GPU等一切都开着。你节约成本的最佳秘诀是什么？我们谈到了无服务器，这是一方面，或者我可以通过使用GPU或部署模型和GPU来节省成本吗？</h2>



<p>凯尔:这是个好问题。最简单的？我是根据做起来有多难以及能为你节省多少来衡量的。最简单的就是我前面说的，就是看你的模型占用多少内存，放在内存少的GPU上。如果可以的话，像这样，你只需要在内部削减成本。这真是太神奇了。</p>



<p>很多推理服务器在Torch中都有内存泄漏。很多Torch代码都有内存泄漏，比如超过50%。事实是它一直在你的记忆痕迹中蔓延。如果你能够修复这样的东西，你可以把它保存在一台GPU内存更少的机器上。另一个问题是，如何将多个模型打包到一个GPU上？这是我们目前正在努力解决的问题。我们正在努力，因为在引擎盖下，这些工具，你可以自然地做到这一点。GPU内存没有一个可访问的虚拟化层，因此这变得令人头痛，但这将是第二个赌注，就像自然的事情一样。就像，怎么把多个内存的东西放到同一个GPU上？</p>



<p>第三，我认为这是如果你愿意牺牲一点点性能，这在我的经验中是大多数团队都不愿意的，但是如果你愿意放弃1%到5%的性能量化，那么就像通过将浮点64、浮点32改为8位浮点来减少模型权重的大小，你可以将模型大小减少2-4倍，并且你牺牲了性能，但是这看起来像是一个非常容易的胜利，我推荐你。我推荐一种混合的方法，所以如果你，再次，我在和种子阶段的A轮公司说话，就像早期阶段的产品，但是看看你正在做一个演示应用。</p>



<p>你需要速度。你能对模型进行量化吗？所以好像便宜了四倍。你只需要让它一直开着，你可以把它作为一个演示来提供，然后给那些需要真正高质量的超级用户。向你付费的人，你给他们一个不同的模型，没有那个，有额外的5%，我会看到很多团队这样做，他们基本上根据客户类型在他们使用的模型之间切换。</p>



<p>我认为，如果你在这方面很聪明，如果你将产品思维和工程思维结合起来，而不是一切都只是工程问题，这就像，你的客户需要什么？你可以开始变聪明了。你可以说，这是前端人员的廉价模型，然后真正掏钱的人，他们得到这个模型。我认为你可以很容易地做到这些事情，你可以削减一半的成本。</p>



<p>使用GPU的分布式处理</p>



<h2 id="h-distributed-processing-with-gpus">萨宾:好吧。Ricardo还有一个问题，您会使用哪些工具或框架来构建一个模型组合，这是一个需要由许多模型处理的推理请求，可能会在不同机器的分布式环境中运行。它们中的每一个都可能具有单独的资源约束、自动缩放策略等等。</h2>



<p>Kyle: 有意思，模型链，我试着不要太偏向，因为我有一种方法来做我正在构建的产品，但我试着挑战自己，想想还有什么其他方法可以做到这一点。最简单的抽象是将每个模型视为一个单独的实体，您可以通过抽象掉几行代码来调用它。如果延迟–</p>



<p>让我们来看看目标是一个真正干净的架构的例子。然后你可能想要做的是有一个编排服务，能够发送一个请求给每一个模型，它基本上做一个调用管道。如果假设这些都是大型模型，它们可能运行在不同的服务器上，你基本上需要一个传送带式的过程，像调用一样，服务器将响应插入下一个服务器。</p>



<p>这是一个非常干净的设计，因为这样你就可以在管道的每一步进行日志记录。你将避免GPU内存耗尽的混乱，然后优化就会出现，就像，你会有网络延迟，对不对？如果你在所有这些之间做一个网络调用，并把它们链接在一起，那就很容易扩展，因为这样你就可以根据瓶颈自动扩展这个链的各个部分。短期内会容易些。一旦需要长期节省，那么瓶颈就是网络延迟。您可能想要相同的架构，但是如何在同一台机器上打包模型呢？然后，您可以在同一个服务器上调用它们。</p>



<blockquote class="wp-block-quote">
<p>香蕉就像是一个中间层。你有一个Python SDK，你有一个类似banana.run或你正在使用的任何SDK的函数，它调用这个中间层，然后将它路由到一台机器，如果你有一个模型链，你只需调用六个不同的运行命令，你将它们绑定在一起，然后在幕后，你可以将它们打包在同一台机器上作为性能优化，但最终用户不应该改变他们的代码。</p>
</blockquote>



<p>我认为最令人头痛的是，你必须不断地修改应用程序代码，以便与这些新的链式推理服务器一起工作。在用户和这个模型网格之间建立一个抽象将为您省去很多麻烦，因为每次您必须更改应用程序代码时，那都是一个新的轮询请求。这是新的测试，它真的增加了，人们低估了它。</p>



<p>使用GPU进行模型推理</p>



<h2 id="h-model-inference-using-gpus"><strong>萨宾:</strong>爽。我们对推论有一些疑问。Gagan想知道像TFSerb、ONNX Runtime等推理服务器有多好。帮助我们对付Q2。例如，将REST API包装在一个巨大的模型上，比如拥抱面部变形金刚。</h2>



<p><strong> Kyle: </strong>为了澄清，相对于仅仅做那件事或者仅仅是除此之外，这些工具有多少帮助——如果你想为一个模型服务，基本上，这些工具有多少帮助。</p>



<p>萨宾:是的，我想是的。</p>



<p><strong> Kyle: </strong>是的，我认为他们帮助很大的领域是，如果你在一个生态系统中，你试图快速部署一个开源模型，就像它是一个已经拥有所有这些服务基础架构设置的大型存储库的一部分。你可以做得很快。我使用一个叫做<a href="https://web.archive.org/web/20230103154737/https://docs.python-zeep.org/en/master/" target="_blank" rel="noreferrer noopener nofollow"> Zeep </a>的工具。这不是我的，它只是一个很酷的东西，基本上可以让您获取这些报告并快速部署它们。我认为TFServe，很多这种类型的-他们熟悉其他部署软件，有一个标准接口，更容易设置它，而如果您在像Flask app这样的准系统中进行操作，您必须思考这样的问题，好吧，这可以处理负载平衡吗？诸如此类的事情。</p>



<p>那里有帮助。我想我有点偏见，因为我已经陷入了足够多的杂草。我用过Triton之类的东西。我们现在开始，至少在Banana，我们正在构建我们自己的东西，因为我们遇到了自然瓶颈，所以我想应该怎么说呢，如果您是部署基础架构、生产和所有方面的新手，我认为这真的会很有帮助，然后如果您试图比市场上的任何其他地方都更快地打包东西，您试图去别人不去的地方，您可以选择这些工具。</p>



<p>最终，你会在它们下面结束，它们都像Python，C++一样。在这一点上，它只是运行在GPU上的一些位，最终，你开始意识到所有这些框架都只是一些抽象，你失去了以同样的方式将它们视为工具的能力。基本上，作为一个初学者，我会从他们开始，直到，一般来说，我会这样，如果有一个工具可以解决你的问题，使用这个工具，而不是构建它，但如果你有任何真正的可伸缩性需求，你可能最终不得不去引擎盖下。然后，你最终必须在我所在的公司建立硬件。</p>



<p>Sabine: 好的，为了防止对这些推理框架有任何补充，我们有Ricardo，他有兴趣知道你对这些的看法。像比如说RayServe，TorchServe，KFserving，还有你说的那个NVIDIA Triton推理服务器，有用吗？它们只是增加了不必要的复杂性吗？他们中有谁开始超越其他人了吗？有什么要补充的吗？</p>



<p>Kyle: 是的，老实说，这些我都没用过。我还没有用过<a href="https://web.archive.org/web/20230103154737/https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noreferrer noopener nofollow"> RayServe </a>。我用过TorchServe、KF和NVIDIA，所以我可以在这些上面发言。就我个人而言，我是《T4》和《海卫一》的粉丝，我认为在制作环境中，我看到海卫一走得相当远。在中，我看到它扩展到数百人的团队和相当大的生产工作负载，我认为像这样的工具有助于它在NVIDIA生态系统中，你有内置的GPU兼容性，他们已经在内核优化的掩护下完成了一些工作。他们正在努力，所以越来越好了。如果你想要最好的GPU兼容性，我认为这将是我的默认选择。</p>



<p>老实说，TorchServe 已经有一段时间没用了，所以我可以根据我被吸引到的地方来谈论它。我不认为我有足够的全球视野来对哪个更好有一个坚定的看法，但我会说Triton是推理服务器的一个很好的起点。此外，我想我认为一件大事不仅仅是推理服务器，而是它所部署的基础设施，就像它是一件大事一样。</p>



<p>酷，你可能有一个准系统Flask应用程序或一个Sanic应用程序或基本上公开一个端口的东西，你可以发出post请求，我们正在进行推理。那是一个推理服务器。我认为更大的问题是您将在哪里部署它。这仅仅是在EC2实例上运行而没有自动重启吗？这在Docker上吗？在Kubernetes上吗？您是否正在进行与云无关的部署？这就是可靠性高于单个服务器的地方。</p>



<p>我几乎要缩小范围说，如果你过度专注于这些单独的服务器，你不会从它们那里获得胜利，那么它可能是不必要的复杂性。我在生产中部署的许多应用程序实际上都是Torch或TensorFlow，我们得到了模型，我喜欢将它打包在一个应用程序中。然后，我注意到最大的故障模式来自——想想看，这不是单个服务器。它让你的服务器运行了很多。现在问题变成了自动扩展、冷启动和负载平衡。所有这些都在单个服务器之外。让我们来看看。</p>



<p>如果你从中获得了巨大的回报，那就继续钻吧，说实话，我不会太在意。感觉GPU游戏真的变成了低级游戏。有两个阵营。如果你只是尝试部署一些东西，使用一个框架，使用一个工具，走出去，并有可靠性。如果你试图优化一个GPU生产工作负载，那么你将不得不放弃这些框架。您可能会向这些提交拉取请求。我们需要更多的人这样做。</p>



<p><strong>萨宾:</strong>没关系。我们之前确实跟进过这个问题，对于实时推理，你会推荐什么编排框架？</p>



<p><strong>凯尔:</strong>是的，我显然会选择<a href="https://web.archive.org/web/20230103154737/https://www.banana.dev/" target="_blank" rel="noreferrer noopener nofollow">香蕉</a>，因为我认为这是我们正在努力解决的问题之一。你会有永远在线的机器，但就像我们正在开发一个自助服务工具，就像尝试制作它，所以你只需上传你的模型，然后恢复调用它的能力，它将是实时的，因为它在GPU上。它永远在线，它会花费更多。如果您使用无服务器，基本上会有额外的延迟。我想跟进一下。</p>



<p>我还见过其他一些工具。再次，我会推荐Zeep，如果你想把一堆推理服务器连在一起，我发现这个工具真的很有帮助。它与云无关。您只需点击一个回购，部署它，然后将它们链接在一起，实时性更多地是关于您在什么硬件上运行，以及它是否支持您的使用情形？在我看来，这不是一个真正的框架级别的事情。真正的实时推理是当你的推理足够快时，比如每秒32帧或其他什么。你可以在许多不同的框架上这样做，比如你如何实时连接是你推理的速度，这是两个不同的问题。</p>



<p>其他问题</p>



<h2 id="h-other-questions">萨宾:好吧。我们可以从聊天中获取一些问题。我们有一个安德烈斯的。您对云中的加密货币挖掘有什么看法吗？在云中部署加密货币挖掘网格并根据哈希算法的复杂性调整GPU负载是否可行？</h2>



<p>凯尔:我不是密码专家。我很早就在现场，但后来我在过去的十年里一直在做ML。我可以给出一点背景。是的，你可以。通常，我所看到的是采矿最终没有最大的投资回报，例外是如果你在做与优步相同的模型，即你有一辆未使用的汽车，所以你把它租出去，如果你有一个未使用的GPU，你为采矿这样做，这可能很酷。</p>



<p>我认为一个非常有趣的使用案例，我很喜欢，如果这里有人构建它，我会成为客户，就是找到那些忘记从云中关闭GPU的人，过去，如果你忘记关闭GPU，用它挖掘一些东西，利用它，这就是一个例子。如果只是部署他们，我觉得是可行的。</p>



<p>我还没有钻够，以了解看哈希率和你的投资回报率是多少，但我已经尝试了采矿的场景。一般来说，你会做一些东西，但必须是非常专注的。从技术上讲，如果你想用香蕉之类的东西来做这件事，你可以部署一个比特币挖矿机。我不知道云提供商是怎么想的，有些人会检查服务条款，但我认为你可以像机器学习一样做这件事。</p>



<p><strong> Sabine: </strong>我们也有一个来自Marcello的问题，很多时候，ML模型可以作为PyTorch或TensorFlow模型使用，但是当将它们转换为TensorRT时，它们的性能会提高。这种情况总是发生吗，还是只适用于某些类型的神经网络？</p>



<p>Kyle: 我不知道，老实说，我对底层工作的了解还不足以胜任，我会鼓励你去经历，我不想说，我基本上知道这一点，但我会假设像循环网络这样的东西，会有更好的优化，因为你可以用循环数来兑现任何东西的权重，但是的，我真的不够了解。我对这些工具的底层还不够了解。我只是猜测，我不想浪费任何人的时间去猜测，如果我知道，我宁愿回答。</p>



<p><strong> Sabine: </strong>很好，我想我们还有时间回答另一个问题，这个问题是Piotra在聊天中提出的。当您通过降低浮点精度来缩小模型时，是否遇到过任何数值稳定性问题？</p>



<p>凯尔:这个问题很复杂。我想确保我理解了这个问题。你说的数值不稳定性是指爆炸梯度吗？基本上，你改变了权重，然后突然，输出变成了垃圾，因为通过改变的权重传播计算？你会得到这个的。这是相对于精度损失的渐近行为。我会注意到，从64位浮点到32位浮点，几乎没有不稳定性，32到16趋于正常，16到8，你会开始看到它，然后8更少，它只是很快下降，成为垃圾。我不知道它背后的所有数学原理，但在实践中，我会把它视为一个渐近位，一旦接近8位精度，它就会有一堵墙。</p>



<p>我认为这也取决于网络大小，就像网络越长，乘法越多，误差通过精度传播的机会就越大。这尤其是一个给你上下文的东西，尤其是在循环网络中的问题。也许我在这里没有正确地使用递归，网络中有反馈回路，如果你在做重复的计算，你在把东西传递给下一层之前，你要多次运行它。基本上，乘法次数越多，误差就会越大。</p>



<p>仔细想想，64位和32位之间的差异非常大，与8位到7位或8位到4位相比，这是一个巨大的差距，相对于另一种类型，这是一个巨大的信息损失，所以你会更快地看到这一点。这是非常特别的。我肯定有论文显示了这方面的实际渐近下降。我记得看到过一些。我不认为我有足够深刻的数学理解或数字来让你确切地知道它何时发生。</p>



<p>把它包起来！</p>



<h2 id="h-wrapping-it-all-up">斯蒂芬:我们今天已经谈论了很多关于工具的话题。我认为我们应该把它们都集中起来。可能把它们都绑在一起。什么是您部署GPU模型的关键工具，您随身携带并觉得有用的工具？</h2>



<p><strong>凯尔:</strong>基本上，就是<a href="https://web.archive.org/web/20230103154737/https://www.docker.com/" target="_blank" rel="noreferrer noopener nofollow">码头工人</a>，<a href="https://web.archive.org/web/20230103154737/https://kubernetes.io/" target="_blank" rel="noreferrer noopener nofollow">库伯内特</a>，<a href="https://web.archive.org/web/20230103154737/https://www.pulumi.com/" target="_blank" rel="noreferrer noopener nofollow">普鲁米</a>。那应该是三个。如果你在一个岛上有三个工具，你必须建立一个生产栈，而你只有这三个工具，那就是我会选择的三个。我知道可能会有一些特定的容器运行时与Kubernetes一起工作，如Containerd，我相信您可能会想探索其他一些容器，但如果您试图做一个生产工作负载，您需要容器编排，因为您需要能够将它连接到前端，像重新启动逻辑等等。这是我最简单的，那三个。</p>



<p>我知道我们的时间不多了，但我还有最后一个问题来总结这一切，把一切都确定下来。我来找你，我在试着优化我的GPU模型来进行like推理。我来找你，你带我经历你会带我经历的过程，从拥有我的模型到能够在GPU上运行实时推理等等。</p>



<p>凯尔:我们有两条路，对吧？我们有自助服务，这是我们现在真正的早期MVP。你可以尝试一下。我可以跟进一个git-repo。基本上，你按照一些指示，然后你可以上传你的模型到我们的服务器。你会得到两行你可以调用它的代码。你得到一个API模型密钥，这将给你无服务器的GPU速度。第二种会帮到你。对于那些不想为计算这些东西而头疼的人，我们可以建立一个Slack频道，我们的团队会和你一起工作。</p>



<p>我们仍处于起步阶段，所以我们希望有亲身体验来了解单个客户的痛点。如果后面有分享链接的方法，就像香蕉ML和GitHub一样。我们有回购协议。这是一条完全自动化的管道。前端UX还不多，主要是我们团队不是前端团队。</p>



<p>我们是非常深入的R&amp;D工程师，因此如果您遇到任何问题，我们很乐意提供帮助，但我可以说，我们目前提供的通常是5到10倍的成本降低，因为您不必一直开着机器。我们有像自动驾驶汽车领域的人，我们有音频应用程序，像GANs这样的图像生成，以及许多不同类型的应用程序正在运行。很想听听你在做什么。</p>



<p>有没有让人伸手的方法？我不知道，我可以放弃一个LinkedIn，或者如果你们有它，但基本上这里的任何人，也是非正式的，我喜欢帮助人们解决GPU问题。你还有什么问题吗？跟我一拍即合。我很高兴成为一个接触点。这可能是我们最擅长的一件事。我喜欢用它来帮助这个行业。我认为它现在在生产方面非常落后，所以如果它还没有在这次对话中传达出来，我可能会给你几个星期甚至几个月的时间。</p>



<p><strong>萨宾:</strong>优秀。所以，LinkedIn是和你联系的地方？是这样吗？也是通过香蕉和松脆？</p>



<p>Kyle: 是的，你可以这么做——我需要仔细检查一下我的LinkedIn是什么，但我可以把它放在这里。我认为，就像<a href="https://web.archive.org/web/20230103154737/https://www.linkedin.com/in/kylejohnmorris/" target="_blank" rel="noreferrer noopener nofollow">凯尔·j·莫里斯一样，</a>你可以随心所欲地分享。</p>



<p>Sabine: 是时候结束了。非常感谢你，凯尔，和我们在一起，有你在真是太好了。</p>



<p><strong>Sabine: </strong>It is indeed time to wrap it up. Thank you so much, Kyle, for being with us here, it was wonderful to have you.</p>
        </div>
        
    </div>    
</body>
</html>