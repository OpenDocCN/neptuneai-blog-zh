<html>
<head>
<title>How to Code BERT Using PyTorch - Tutorial With Examples </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>如何使用PyTorch编写BERT代码</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>如果你是一个NLP爱好者，那么你可能听说过伯特。在本文中，我们将探讨BERT:它是什么？以及它是如何工作的？，并学习如何使用PyTorch对其进行编码。</p>



<p>2018年，Google发表了一篇名为“<a href="https://web.archive.org/web/20230124065338/https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noreferrer noopener nofollow">深度双向变压器语言理解预训练</a>”的论文。在这篇论文中，他们引入了一个名为<strong> BERT(使用变压器的双向编码器表示)</strong>的语言模型，该模型在类似<em>问答</em>、<em>自然语言推理、分类和一般语言理解评估(GLUE) </em>等任务中实现了最先进的性能。</p>



<p>BERT版本是在三个架构发布之后发布的，这三个架构也实现了最先进的性能。这些模型是:</p>



<ul><li>乌尔姆-菲特(1月)</li><li>埃尔莫(二月)，</li><li>GPT公开赛(6月)</li><li>伯特(十月)。</li></ul>



<p>OpenAI GPT和BERT使用不使用递归神经网络的<a href="https://web.archive.org/web/20230124065338/https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noreferrer noopener nofollow"> <strong>变压器</strong> </a>架构；这使得该架构能够通过<strong>自我关注机制</strong>考虑长期依赖性，这种机制从本质上改变了我们对顺序数据建模的方式。它引入了一种<strong>编码器-解码器</strong>架构，这种架构出现在计算机视觉应用中，例如通过可变自动编码器编码器生成图像。</p>



<p>那么BERT与2018年发布的所有车型有什么不同呢？</p>



<p>要回答这个问题，我们需要了解什么是BERT，以及它是如何工作的。</p>



<p>那么，我们开始吧。</p>



<h2 id="h-what-is-bert">伯特是什么？</h2>



<p>BERT代表“使用变压器的双向编码器表示”。简而言之，BERT通过编码器从数据或单词嵌入中提取模式或表示。编码器本身是一个堆叠在一起的转换器架构。它是一个双向转换器，这意味着在训练过程中，它会考虑词汇左侧和右侧的上下文来提取模式或表示。</p>







<p>BERT使用两种训练范式:<strong>预训练</strong>和<strong>微调</strong>。</p>



<p>在<strong>预训练期间，</strong>在大数据集上训练模型以提取模式。这通常是一个<strong>无监督学习</strong>任务，其中模型在一个未标记的数据集上训练，如来自维基百科等大型语料库的数据。</p>



<p>在<strong>微调</strong>期间，模型被训练用于下游任务，如分类、文本生成、语言翻译、问答等等。本质上，你可以下载一个预先训练好的模型，然后根据你的数据转换学习这个模型。</p>







<h3>BERT的核心组件</h3>



<p>伯特借用了以前发布的SOTA模型的想法。让我们详细阐述一下那句话。</p>



<h4>变形金刚</h4>



<p>BERT的主要组件是变压器架构。变压器由两部分组成:<strong>编码器</strong>和<strong>解码器</strong>。编码器本身包含两个组件:T4自关注层T5和T6前馈神经网络T7。</p>



<p>自我注意层接受输入，将每个单词编码成中间编码表示，然后通过前馈神经网络。前馈网络将这些表示传递给解码器，解码器本身由三个组件组成:<strong>自关注层、编码器-解码器</strong>、<strong>关注、</strong>和<strong>前馈神经网络</strong>。</p>







<p>transformer架构的好处在于，它有助于模型保留无限长的序列，这在传统的RNNs、LSTMs和GRU中是不可能的。但即使从它可以实现长期依赖的事实来看，它仍然<strong>缺乏上下文理解</strong>。</p>



<p><em> Jay Alammar在他的文章</em> <a href="https://web.archive.org/web/20230124065338/https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noreferrer noopener nofollow"> <em>中深入解释了变形金刚，图文并茂的变形金刚</em> </a> <em>，值得一探究竟。</em></p>



<h4>工程与后勤管理局</h4>



<p>BERT借用了ELMo的另一个概念，即语言模型嵌入。埃尔莫是由<a href="https://web.archive.org/web/20230124065338/https://arxiv.org/abs/1802.05365" target="_blank" rel="noreferrer noopener nofollow">彼得斯等人介绍的。艾尔。</a>2017年，涉及语境理解的思想。ELMo的工作方式是使用<strong>双向</strong> LSTM来理解上下文。由于它从两个方向考虑单词，它可以为拼写相似但含义不同的单词分配不同的单词嵌入。</p>



<p>例如，“你们这些孩子应该<strong>在黑暗中把</strong>粘在一起”和“把那根<strong>棍子</strong>递给我”是完全不同的。尽管两个句子中使用了同一个词，但根据上下文的不同，意思是不同的。</p>



<p>因此，ELMo通过考虑来自右和左两个方向的单词来分配嵌入，而之前开发的模型只考虑来自左的单词。这些模型是单向的，如RNNs、LSTMs等。</p>



<p>这使得ELMo能够从序列中捕获上下文信息，但是因为ELMo使用LTSM，所以与变形金刚相比，它不具有长期依赖性。</p>



<p>到目前为止，我们已经看到，由于transformers中存在的注意机制，BERT可以访问文档中的序列，即使它比序列中的当前单词落后“n”个单词，即，它可以保持长期依赖性，并且由于ELMo中存在的双向机制，它还可以实现对句子的上下文理解。</p>



<h4>乌尔姆拟合</h4>



<p>2018年，杰瑞米·霍华德和塞巴斯蒂安·鲁德发布了一篇名为<a href="https://web.archive.org/web/20230124065338/https://arxiv.org/pdf/1801.06146.pdf" target="_blank" rel="noreferrer noopener nofollow">通用语言模型微调或ULM-FiT </a>的论文，他们在论文中指出，迁移学习可以用于自然语言处理，就像它用于计算机视觉一样。</p>



<p>以前，我们使用预训练的单词嵌入模型，该模型仅针对整个模型的第一层，即嵌入层，并且整个模型是从头开始训练的，这很耗时，并且在该领域没有发现很多成功。然而，Howard和Ruder提出了3种文本分类方法:</p>



<ul><li>第一步包括在更大的数据集上训练模型，以便模型学习表示。</li><li>第二步包括用特定于任务的数据集对模型进行微调以进行分类，在此期间，他们引入了另外两种方法:判别微调和倾斜三角学习率(STLR)。前一种方法试图在网络的传输层期间微调或优化每个参数，而后一种方法控制每个优化步骤中的学习速率。</li><li>第三步是在特定于任务的数据集上微调分类器以进行分类。</li></ul>







<p>随着ULM-FiT的发布，NLP实践者现在可以在他们的NLP问题中实践迁移学习方法。但是，ULM-FiT迁移学习方法的唯一问题是，它包括微调网络中的所有层，这是一项繁重的工作。</p>



<h4>GPT开放大学</h4>



<p>生成式预训练变压器或GPT是由OpenAI的团队引入的:拉德福德、纳拉辛汉、萨利曼斯和苏茨基弗。他们提出了一个模型，该模型在单向方法中仅使用来自转换器的解码器而不是编码器。因此，它在各种任务中的表现优于所有以前的型号，例如:</p>



<ul><li>分类</li><li>自然语言推理</li><li>语义相似度</li><li>问题回答</li><li>多项选择。</li></ul>



<p>即使GPT只使用解码器，它仍然可以保持长期的依赖性。此外，与我们在ULM-FiT中看到的相比，它将微调减少到了最低限度。</p>



<p>下表根据预培训、下游任务以及最重要的微调对不同模型进行了比较。</p>







<p>GPT <a href="https://web.archive.org/web/20230124065338/https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noreferrer noopener nofollow">论文</a>的一段摘录写道“<em>与循环网络等替代方案相比，这种模型选择为我们提供了一种更结构化的记忆，用于处理文本中的长期依赖性，从而在不同任务之间实现稳健的传输性能。在转换过程中，我们利用来自遍历式方法的特定于任务的输入适应，这种方法将结构化文本输入作为单个连续的标记序列来处理。正如我们在实验中所展示的，这些适应使我们能够有效地进行微调，对预训练模型的架构进行最小的改变。</em></p>



<p>让我们将所有模型与BERT进行比较，看它们能够执行的任务:</p>



<p id="separator-block_61af4ee581c93" class="block-separator block-separator--10"> </p>



<div id="medium-table-block_61af4eee81c94" class="block-medium-table c-table__outer-wrapper ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">                                                      </p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">变压器</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">工程与后勤管理局</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">乌尔姆拟合</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">GPT开放大学</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">伯特</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p> <strong>自然语言推理</strong></p>T5】</div></td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p> <strong>分类或者</strong> <br/> <strong>情绪分析</strong> </p> </div></td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>



<p id="separator-block_60a291a8ac116" class="block-separator block-separator--20"> </p>



<p>你可以检查<a href="https://web.archive.org/web/20230124065338/https://huggingface.co/models" target="_blank" rel="noreferrer noopener nofollow">拥抱脸模型</a>来检查模型在每个任务上的表现。</p>



<h2 id="h-why-bert">为什么是伯特？</h2>



<p>伯特陷入了<a href="/web/20230124065338/https://neptune.ai/blog/self-supervised-learning" target="_blank" rel="noreferrer noopener">自我监督</a>模式。这意味着，它可以从原始语料库中生成输入和标签，而无需人类显式编程。请记住，它所训练的数据是非结构化的。</p>



<p>伯特接受了两项特定任务的预训练:掩蔽语言模型和下一句预测。前者使用类似“该男子[面具]到商店”的屏蔽输入，而不是“该男子去了商店”。这限制了BERT看到它旁边的单词，这允许它尽可能多地学习双向表示，使它对几个下游任务更加灵活和可靠。后者预测这两个句子是否在上下文中相互分配。</p>







<p>例如，如果句子A是“[CLS]那个人[面具]去商店”，句子B是“企鹅[面具]是不会飞的鸟[SEP]”，那么BERT将能够区分这两个句子是否是连续的。</p>



<p>在训练过程中，BERT使用特殊类型的标记，如[CLS]、[屏蔽]、[分离]等，这些标记允许BERT区分一个句子何时开始，哪个单词被屏蔽，以及两个句子何时分开。我已经在<strong>预处理</strong>部分以表格的形式解释了这些令牌。</p>



<p><strong>由于我们之前讨论过的特性，BERT也可用于特征提取</strong>,并将这些提取提供给现有模型。</p>







<p>在最初的BERT论文中，它与GPT在<a href="https://web.archive.org/web/20230124065338/https://gluebenchmark.com/" target="_blank" rel="noreferrer noopener nofollow">通用语言理解评估基准</a>上进行了比较，下面是结果。</p>







<p>正如你所看到的，伯特在所有的任务中都超过了GPT，平均比GPT高出7%。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/db006a24520b0179c5c5f5098d41f06d.png" alt="BERT tasks" class="wp-image-45976" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230124065338im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/BERT-tasks.png?ssl=1"/><figcaption><em>The image above shows the different tasks that BERT can be used for. | <a href="https://web.archive.org/web/20230124065338/https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<h2 id="h-coding-bert-with-pytorch">用Pytorch编写BERT代码</h2>



<p>让我们用代码来理解如何用PyTorch构建BERT。</p>



<p>我们将整个计划分为4个部分:</p>



<ol><li>预处理</li><li>建筑模型</li><li>损失和优化</li><li>培养</li></ol>







<h3>预处理</h3>



<p>在预处理中，我们将结构化数据，以便神经网络可以处理它。我们首先指定一个原始文本进行训练。</p>



<pre class="hljs">text = (
       <span class="hljs-string">'Hello, how are you? I am Romeo.n'</span>
       <span class="hljs-string">'Hello, Romeo My name is Juliet. Nice to meet you.n'</span>
       <span class="hljs-string">'Nice meet you too. How are you today?n'</span>
       <span class="hljs-string">'Great. My baseball team won the competition.n'</span>
       <span class="hljs-string">'Oh Congratulations, Julietn'</span>
       <span class="hljs-string">'Thanks you Romeo'</span>
   )</pre>



<p>然后，我们将通过以下方式清理数据:</p>



<ul><li>把句子变成小写。</li><li>创造词汇。<strong>词汇表</strong>是文档中唯一单词的列表。</li></ul>



<pre class="hljs">  sentences = re.sub(<span class="hljs-string">"[.,!?-]"</span>, <span class="hljs-string">''</span>, text.lower()).split(<span class="hljs-string">'n'</span>)  
   word_list = list(set(<span class="hljs-string">" "</span>.join(sentences).split()))
</pre>



<p>现在，在接下来的步骤中，重要的是要记住伯特在训练中使用特殊的代币。下表解释了各种令牌的用途:</p>



<p id="separator-block_61af510f81cd3" class="block-separator block-separator--10"> </p>



<div id="medium-table-block_61af511e81cd4" class="block-medium-table c-table__outer-wrapper ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">代币</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">目的</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>第一个令牌总是分类</p> </div></td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>用等长截断句子。</p>T3】</div></td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>用于通过替换原来的单词来创建一个遮罩。</p>T3】</div></td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>



<p id="separator-block_60a29279ac117" class="block-separator block-separator--20"> </p>



<p>这些标记应该包含在单词字典中，其中词汇表中的每个标记和单词都分配有一个索引号。</p>



<pre class="hljs">word_dict = {<span class="hljs-string">'[PAD]'</span>: <span class="hljs-number">0</span>, <span class="hljs-string">'[CLS]'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'[SEP]'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'[MASK]'</span>: <span class="hljs-number">3</span>}
<span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> enumerate(word_list):
   word_dict[w] = i + <span class="hljs-number">4</span>
   number_dict = {i: w <span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> enumerate(word_dict)}
   vocab_size = len(word_dict)</pre>



<p>考虑到这一点，我们需要创建一个函数，为三种类型的嵌入格式化输入序列:<strong>标记嵌入</strong>、<strong>片段嵌入、</strong>和<strong>位置嵌入</strong>。</p>



<p>什么是令牌嵌入？</p>



<p>例如，如果句子是“猫在走路”。狗在叫”，那么该函数应该以如下方式创建一个序列:“[CLS]猫在走[SEP]狗在叫”。</p>



<p>之后，我们将所有内容转换为单词字典中的索引。所以前面的句子看起来就像“[1，5，7，9，10，2，5，6，9，11]”。请记住，1和2分别是[CLS]和[九月]。</p>



<p><strong>什么是片段嵌入？</strong></p>



<p>片段嵌入将两个句子彼此分开，并且它们通常被定义为0和1。</p>



<p><strong>什么是位置嵌入？</strong></p>



<p>位置嵌入给出了序列中每个嵌入的位置。</p>



<p>稍后我们将创建一个位置嵌入函数。</p>







<p>现在下一步将是创建<strong>屏蔽</strong>。</p>



<p>正如在原始论文中提到的，BERT随机地将掩码分配给序列的15%。但是请记住，不要给特殊标记分配掩码。为此，我们将使用条件语句。</p>



<p>一旦我们将15%的单词替换为[MASK]标记，我们将添加填充。填充通常是为了确保所有的句子长度相等。例如，如果我们拿这个句子来说:</p>



<p><em> <strong>“猫在走路。</strong>狗对着树叫</em></p>



<p>然后加上填充，看起来会是这样的:</p>



<p><em><strong>【CLS】猫在走[PAD] [PAD] [PAD]。[CLS]狗在对着树叫。”</strong>T3】</em></p>



<p>第一句话的长度等于第二句话的长度。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_batch</span><span class="hljs-params">()</span>:</span>
   batch = []
   positive = negative = <span class="hljs-number">0</span>
   <span class="hljs-keyword">while</span> positive != batch_size/<span class="hljs-number">2</span> <span class="hljs-keyword">or</span> negative != batch_size/<span class="hljs-number">2</span>:
       tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))

       tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]

       input_ids = [word_dict[<span class="hljs-string">'[CLS]'</span>]] + tokens_a + [word_dict[<span class="hljs-string">'[SEP]'</span>]] + tokens_b + [word_dict[<span class="hljs-string">'[SEP]'</span>]]
       segment_ids = [<span class="hljs-number">0</span>] * (<span class="hljs-number">1</span> + len(tokens_a) + <span class="hljs-number">1</span>) + [<span class="hljs-number">1</span>] * (len(tokens_b) + <span class="hljs-number">1</span>)

       
       n_pred =  min(max_pred, max(<span class="hljs-number">1</span>, int(round(len(input_ids) * <span class="hljs-number">0.15</span>)))) 
       cand_maked_pos = [i <span class="hljs-keyword">for</span> i, token <span class="hljs-keyword">in</span> enumerate(input_ids)
                         <span class="hljs-keyword">if</span> token != word_dict[<span class="hljs-string">'[CLS]'</span>] <span class="hljs-keyword">and</span> token != word_dict[<span class="hljs-string">'[SEP]'</span>]]
       shuffle(cand_maked_pos)
       masked_tokens, masked_pos = [], []
       <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> cand_maked_pos[:n_pred]:
           masked_pos.append(pos)
           masked_tokens.append(input_ids[pos])
           <span class="hljs-keyword">if</span> random() &lt; <span class="hljs-number">0.8</span>:  
               input_ids[pos] = word_dict[<span class="hljs-string">'[MASK]'</span>] 
           <span class="hljs-keyword">elif</span> random() &lt; <span class="hljs-number">0.5</span>:  
               index = randint(<span class="hljs-number">0</span>, vocab_size - <span class="hljs-number">1</span>) 
               input_ids[pos] = word_dict[number_dict[index]] 

       
       n_pad = maxlen - len(input_ids)
       input_ids.extend([<span class="hljs-number">0</span>] * n_pad)
       segment_ids.extend([<span class="hljs-number">0</span>] * n_pad)

       
       <span class="hljs-keyword">if</span> max_pred &gt; n_pred:
           n_pad = max_pred - n_pred
           masked_tokens.extend([<span class="hljs-number">0</span>] * n_pad)
           masked_pos.extend([<span class="hljs-number">0</span>] * n_pad)

       <span class="hljs-keyword">if</span> tokens_a_index + <span class="hljs-number">1</span> == tokens_b_index <span class="hljs-keyword">and</span> positive &lt; batch_size/<span class="hljs-number">2</span>:
           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, <span class="hljs-keyword">True</span>]) 
           positive += <span class="hljs-number">1</span>
       <span class="hljs-keyword">elif</span> tokens_a_index + <span class="hljs-number">1</span> != tokens_b_index <span class="hljs-keyword">and</span> negative &lt; batch_size/<span class="hljs-number">2</span>:
           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, <span class="hljs-keyword">False</span>]) 
           negative += <span class="hljs-number">1</span>
   <span class="hljs-keyword">return</span> batch</pre>



<p>由于我们正在处理下一个单词预测，我们必须创建一个标签来预测该句子是否有连续的句子，即IsNext或NotNext。所以我们为下一句之前的每一句赋值True，我们用一个条件语句来实现。</p>



<p>例如，如果在上下文中，文档中的两个句子通常是前后相接的。假设第一句是A，那么下一句应该是+1。直观地，我们编写代码，使得如果第一个句子位置，即tokens _ a _ index+1 = = tokens _ b _ index，即相同上下文中的第二个句子，那么我们可以将该输入的标签设置为True。</p>



<p>如果不满足上述条件，即如果token _ a _ index+1！= tokens_b_index然后我们将这个输入的标签设置为False。</p>



<h3>建筑模型</h3>



<p>BERT是一个复杂的模型，如果你慢慢理解它，你就会失去逻辑。所以解释它的组件和它们的功能是有意义的。</p>



<p>BERT具有以下组件:</p>



<ol><li>嵌入层</li><li>注意力屏蔽</li><li>编码器层<ol><li>多头注意力<ol><li>比例点产品关注度</li></ol></li><li>位置式前馈网络</li></ol></li><li>伯特(组装所有部件)</li></ol>



<p>为了便于学习，你可以随时参考这个图表。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/416c232fced237218ab2c880c034cd84.png" alt="BERT building model" class="wp-image-45979" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230124065338im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/BERT-building-model.png?ssl=1"/><figcaption><em>Source: Author</em></figcaption></figure></div>



<h4>嵌入层</h4>



<p>嵌入是BERT中的第一层，它接受输入并创建一个<em>查找表</em>。嵌入层的参数是可学习的，这意味着当学习过程结束时，嵌入层会将相似的单词聚集在一起。</p>



<p>嵌入层还保留单词之间的不同关系，例如:语义、句法、线性，并且由于BERT是双向的，它还将保留上下文关系。</p>



<p>在BERT的例子中，它为</p>



<ul><li>令牌，</li><li>分段和</li><li>位置。</li></ul>



<p>如果您还记得，我们还没有创建一个函数来接收输入并对其进行格式化以进行位置嵌入，但是标记和段的格式化已经完成。因此，我们将接受输入，并为序列中的每个单词创建一个位置。它看起来像这样:</p>



<pre class="hljs">print(torch.arange(<span class="hljs-number">30</span>, dtype=torch.long).expand_as(input_ids))</pre>



<pre class="hljs">Output:

tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>,
         <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">24</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26</span>, <span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>],
        [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>,
         <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">24</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26</span>, <span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>],
        [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>,
         <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">24</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26</span>, <span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>],
        [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>,
         <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">24</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26</span>, <span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>],
        [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>,
         <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">24</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26</span>, <span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>],
        [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>,
         <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">24</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26</span>, <span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>]])</pre>



<p>在前向函数中，我们总结了所有的嵌入，并对它们进行归一化。</p>







<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Embedding</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
       super(Embedding, self).__init__()
       self.tok_embed = nn.Embedding(vocab_size, d_model)  
       self.pos_embed = nn.Embedding(maxlen, d_model)  
       self.seg_embed = nn.Embedding(n_segments, d_model)  
       self.norm = nn.LayerNorm(d_model)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x, seg)</span>:</span>
       seq_len = x.size(<span class="hljs-number">1</span>)
       pos = torch.arange(seq_len, dtype=torch.long)
       pos = pos.unsqueeze(<span class="hljs-number">0</span>).expand_as(x)  
       embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)
       <span class="hljs-keyword">return</span> self.norm(embedding)</pre>







<h4>创建注意力屏蔽</h4>



<p>伯特需要注意力面具。并且这些应该是适当的格式。以下代码将帮助您创建遮罩。</p>



<p>它会将[PAD]转换为1，其他地方为0。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_attn_pad_mask</span><span class="hljs-params">(seq_q, seq_k)</span>:</span>
   batch_size, len_q = seq_q.size()
   batch_size, len_k = seq_k.size()
   
   pad_attn_mask = seq_k.data.eq(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">1</span>)  
   <span class="hljs-keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)  </pre>



<pre class="hljs">print(get_attn_pad_mask(input_ids, input_ids)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], input_ids[<span class="hljs-number">0</span>])</pre>



<pre class="hljs">Output:
(tensor([<span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>,
         <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,
          <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>]),
 tensor([ <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>, <span class="hljs-number">26</span>, <span class="hljs-number">21</span>, <span class="hljs-number">14</span>, <span class="hljs-number">16</span>, <span class="hljs-number">12</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">2</span>, <span class="hljs-number">27</span>,  <span class="hljs-number">3</span>, <span class="hljs-number">22</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,
          <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>]))</pre>



<h4>编码器</h4>



<p>编码器有两个主要组件:</p>



<ul><li>多头注意力</li><li>位置式前馈网络。</li></ul>



<p>编码器的工作是从输入和注意屏蔽中找到表示和模式。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EncoderLayer</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
       super(EncoderLayer, self).__init__()
       self.enc_self_attn = MultiHeadAttention()
       self.pos_ffn = PoswiseFeedForwardNet()

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, enc_inputs, enc_self_attn_mask)</span>:</span>
       enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) 
       enc_outputs = self.pos_ffn(enc_outputs) 
       <span class="hljs-keyword">return</span> enc_outputs, attn</pre>



<p><strong>多头关注</strong></p>



<p>这是编码器的第一个主要组件。</p>



<p>注意力模型有三个输入:<strong>查询</strong>、<strong>键、</strong>和<strong>值</strong>。</p>



<p>我强烈推荐你阅读杰伊·阿拉姆马的《变形金刚》<a href="https://web.archive.org/web/20230124065338/https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>，它深入地解释了注意力模型。</em></p>



<p>多头注意力需要四个输入:<strong>查询</strong>、<strong>键</strong>、<strong>值、</strong>和<strong>注意力屏蔽</strong>。嵌入作为输入提供给查询、键和值参数，而注意掩码作为输入提供给注意掩码参数。<br/>这三个输入和注意屏蔽用点积运算操作，产生两个输出:<strong>上下文向量</strong>和<strong>注意</strong>。然后上下文向量通过一个线性层，最后产生输出。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiHeadAttention</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
       super(MultiHeadAttention, self).__init__()
       self.W_Q = nn.Linear(d_model, d_k * n_heads)
       self.W_K = nn.Linear(d_model, d_k * n_heads)
       self.W_V = nn.Linear(d_model, d_v * n_heads)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, Q, K, V, attn_mask)</span>:</span>
       
       residual, batch_size = Q, Q.size(<span class="hljs-number">0</span>)
       
       q_s = self.W_Q(Q).view(batch_size, <span class="hljs-number">-1</span>, n_heads, d_k).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)  
       k_s = self.W_K(K).view(batch_size, <span class="hljs-number">-1</span>, n_heads, d_k).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)  
       v_s = self.W_V(V).view(batch_size, <span class="hljs-number">-1</span>, n_heads, d_v).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)  

       attn_mask = attn_mask.unsqueeze(<span class="hljs-number">1</span>).repeat(<span class="hljs-number">1</span>, n_heads, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>) 

       
       context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)
       context = context.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(batch_size, <span class="hljs-number">-1</span>, n_heads * d_v) 
       output = nn.Linear(n_heads * d_v, d_model)(context)


<span class="hljs-keyword">return</span> nn.LayerNorm(d_model)(output + residual), attn </pre>



<p>现在，让我们来探索这种成比例的点积注意力:</p>



<ul><li>缩放的点积注意类有四个参数:查询、键、值和注意掩码。本质上，前三个参数由单词嵌入提供，而注意力屏蔽参数由注意力屏蔽嵌入提供。</li><li>然后，它在<strong>查询</strong>和<strong>键</strong>之间进行矩阵乘法，以获得分数。</li></ul>



<p>接下来我们使用scores.masked_fill_(attn_mask，-1e9)。该属性用-1e9填充scores <strong> </strong>的元素，其中注意力掩码为<strong>真</strong>，而其余元素得到一个<strong>注意力分数</strong>，该分数然后通过softmax函数传递，该函数给出0到1之间的分数。最后，我们执行注意力和值之间的矩阵乘法，这给出了上下文向量。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ScaledDotProductAttention</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
       super(ScaledDotProductAttention, self).__init__()

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, Q, K, V, attn_mask)</span>:</span>
       scores = torch.matmul(Q, K.transpose(<span class="hljs-number">-1</span>, <span class="hljs-number">-2</span>)) / np.sqrt(d_k) 
       scores.masked_fill_(attn_mask, <span class="hljs-number">-1e9</span>) 
       attn = nn.Softmax(dim=<span class="hljs-number">-1</span>)(scores)
       context = torch.matmul(attn, V)
       <span class="hljs-keyword">return</span> score, context, attn</pre>



<pre class="hljs">emb = Embedding()
embeds = emb(input_ids, segment_ids)

attenM = get_attn_pad_mask(input_ids, input_ids)

SDPA= ScaledDotProductAttention()(embeds, embeds, embeds, attenM)

S, C, A = SDPA

print(<span class="hljs-string">'Masks'</span>,masks[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])
print()
print(<span class="hljs-string">'Scores: '</span>, S[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],<span class="hljs-string">'nnAttention Scores after softmax: '</span>, A[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])</pre>



<pre class="hljs">Output:

Masks tensor([<span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>,
        <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,
         <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>])

Scores:  tensor([ <span class="hljs-number">9.6000e+01</span>,  <span class="hljs-number">3.1570e+01</span>,  <span class="hljs-number">2.9415e+01</span>,  <span class="hljs-number">3.3990e+01</span>,  <span class="hljs-number">3.7752e+01</span>,
         <span class="hljs-number">3.7363e+01</span>,  <span class="hljs-number">3.1683e+01</span>,  <span class="hljs-number">3.2156e+01</span>,  <span class="hljs-number">3.5942e+01</span>, <span class="hljs-number">-2.4670e+00</span>,
        <span class="hljs-number">-2.2461e+00</span>, <span class="hljs-number">-8.1908e+00</span>, <span class="hljs-number">-2.1571e+00</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>,
        <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>,
        <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>,
        <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>, <span class="hljs-number">-1.0000e+09</span>],
       grad_fn=&lt;SelectBackward&gt;)

Attention Scores after softmax::  tensor([<span class="hljs-number">1.0000e+00</span>, <span class="hljs-number">1.0440e-28</span>, <span class="hljs-number">1.2090e-29</span>, <span class="hljs-number">1.1732e-27</span>, <span class="hljs-number">5.0495e-26</span>, <span class="hljs-number">3.4218e-26</span>,
        <span class="hljs-number">1.1689e-28</span>, <span class="hljs-number">1.8746e-28</span>, <span class="hljs-number">8.2677e-27</span>, <span class="hljs-number">1.7236e-43</span>, <span class="hljs-number">2.1440e-43</span>, <span class="hljs-number">0.0000e+00</span>,
        <span class="hljs-number">2.3542e-43</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>,
        <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>,
        <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">0.0000e+00</span>],
       grad_fn=&lt;SelectBackward&gt;)</pre>



<p><strong>位置式前馈网络</strong></p>



<p>multihead的输出进入前馈网络，构成编码器部分。</p>



<p>让我们喘口气，复习一下到目前为止我们所学的内容:</p>



<ul><li>输入进入嵌入和注意功能。两者都被馈送到具有多头功能和前馈网络的编码器中。</li><li>多头功能本身具有使用点积操作来操作嵌入和注意力屏蔽的功能。</li></ul>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/89ccf11359d767de90f165d081c201c1.png" alt="BERT building model" class="wp-image-45981" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230124065338im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/BERT-building-model-2.png?ssl=1"/><figcaption><em>Source: Author</em></figcaption></figure></div>



<h4>组装所有组件</h4>



<p>让我们从我们离开的地方继续，即编码器的输出。</p>



<p>编码器产生两个输出:</p>



<ul><li>一个来自前馈层，并且</li><li>注意力面具。</li></ul>



<p>请记住，BERT并没有明确使用解码器。相反，它使用输出和注意力屏蔽来获得想要的结果。</p>



<p>虽然变压器中的解码器部分被替换为浅网络，该浅网络可用于分类，如下面的代码所示。<br/>同样，BERT输出两个结果:一个是针对<strong>分类器</strong>的，另一个是针对<strong>屏蔽</strong>的。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BERT</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
       super(BERT, self).__init__()
       self.embedding = Embedding()
       self.layers = nn.ModuleList([EncoderLayer() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n_layers)])
       self.fc = nn.Linear(d_model, d_model)
       self.activ1 = nn.Tanh()
       self.linear = nn.Linear(d_model, d_model)
       self.activ2 = gelu
       self.norm = nn.LayerNorm(d_model)
       self.classifier = nn.Linear(d_model, <span class="hljs-number">2</span>)
       
       embed_weight = self.embedding.tok_embed.weight
       n_vocab, n_dim = embed_weight.size()
       self.decoder = nn.Linear(n_dim, n_vocab, bias=<span class="hljs-keyword">False</span>)
       self.decoder.weight = embed_weight
       self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, input_ids, segment_ids, masked_pos)</span>:</span>
       output = self.embedding(input_ids, segment_ids)
       enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)
       <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:
           output, enc_self_attn = layer(output, enc_self_attn_mask)
       
       
       h_pooled = self.activ1(self.fc(output[:, <span class="hljs-number">0</span>])) 
       logits_clsf = self.classifier(h_pooled) 

       masked_pos = masked_pos[:, :, <span class="hljs-keyword">None</span>].expand(<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, output.size(<span class="hljs-number">-1</span>)) 

       
       h_masked = torch.gather(output, <span class="hljs-number">1</span>, masked_pos) 
       h_masked = self.norm(self.activ2(self.linear(h_masked)))
       logits_lm = self.decoder(h_masked) + self.decoder_bias 

       <span class="hljs-keyword">return</span> logits_lm, logits_clsf</pre>



<p>需要记住的几件事:</p>



<ol><li>您可以指定编码器的数量。在原始论文中，基本模型有12个。</li><li>有两个激活函数:Tanh和GELU(<strong>G</strong>aussian<strong>E</strong>rror<strong>L</strong>linear<strong>U</strong>nit)。</li></ol>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gelu</span><span class="hljs-params">(x)</span>:</span>
   <span class="hljs-keyword">return</span> x * <span class="hljs-number">0.5</span> * (<span class="hljs-number">1.0</span> + torch.erf(x / math.sqrt(<span class="hljs-number">2.0</span>)))</pre>



<h3>损失和优化</h3>



<p>虽然原始论文计算了所有词汇的概率分布，但我们可以使用softmax近似值。但是一个简单的方法是使用<strong> <em>交叉熵损失</em> </strong>。它是<em> softmax </em>和<em>负对数似然</em>的组合。</p>



<p>因此，在构建模型时，您不必包括softmax，而是从没有softmax归一化的前馈神经网络获得清晰的输出。</p>



<p>说到优化，我们将使用<strong> Adam </strong>优化器。</p>



<pre class="hljs">criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=<span class="hljs-number">0.001</span>)</pre>







<h3>培养</h3>



<p>最后，我们将开始训练。</p>



<pre class="hljs">model = BERT()
batch = make_batch()
input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))

   <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):
       optimizer.zero_grad()
       logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)
       loss_lm = criterion(logits_lm.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), masked_tokens) 
       loss_lm = (loss_lm.float()).mean()
       loss_clsf = criterion(logits_clsf, isNext) 
       loss = loss_lm + loss_clsf
       <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
           print(<span class="hljs-string">'Epoch:'</span>, <span class="hljs-string">'%04d'</span> % (epoch + <span class="hljs-number">1</span>), <span class="hljs-string">'cost ='</span>, <span class="hljs-string">'{:.6f}'</span>.format(loss))
       loss.backward()
       optimizer.step()

   
   input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[<span class="hljs-number">0</span>]))
   print(text)
   print([number_dict[w.item()] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> input_ids[<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> number_dict[w.item()] != <span class="hljs-string">'[PAD]'</span>])

   logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)
   logits_lm = logits_lm.data.max(<span class="hljs-number">2</span>)[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>].data.numpy()
   print(<span class="hljs-string">'masked tokens list : '</span>,[pos.item() <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> masked_tokens[<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> pos.item() != <span class="hljs-number">0</span>])
   print(<span class="hljs-string">'predict masked tokens list : '</span>,[pos <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> logits_lm <span class="hljs-keyword">if</span> pos != <span class="hljs-number">0</span>])

   logits_clsf = logits_clsf.data.max(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].data.numpy()[<span class="hljs-number">0</span>]
   print(<span class="hljs-string">'isNext : '</span>, <span class="hljs-keyword">True</span> <span class="hljs-keyword">if</span> isNext <span class="hljs-keyword">else</span> <span class="hljs-keyword">False</span>)
   print(<span class="hljs-string">'predict isNext : '</span>,<span class="hljs-keyword">True</span> <span class="hljs-keyword">if</span> logits_clsf <span class="hljs-keyword">else</span> <span class="hljs-keyword">False</span>)</pre>



<pre class="hljs">Output:

Hello, how are you? I am Romeo.
Hello, Romeo My name <span class="hljs-keyword">is</span> Juliet. Nice to meet you.
Nice meet you too. How are you today?
Great. My baseball team won the competition.
Oh Congratulations, Juliet
Thanks you Romeo
[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'nice'</span>, <span class="hljs-string">'meet'</span>, <span class="hljs-string">'you'</span>, <span class="hljs-string">'too'</span>, <span class="hljs-string">'how'</span>, <span class="hljs-string">'are'</span>, <span class="hljs-string">'you'</span>, <span class="hljs-string">'today'</span>, <span class="hljs-string">'[SEP]'</span>, <span class="hljs-string">'[MASK]'</span>, <span class="hljs-string">'congratulations'</span>, <span class="hljs-string">'[MASK]'</span>, <span class="hljs-string">'[SEP]'</span>]
masked tokens list :  [<span class="hljs-number">27</span>, <span class="hljs-number">22</span>]
predict masked tokens list :  []
isNext :  <span class="hljs-keyword">False</span>
predict isNext :  <span class="hljs-keyword">True</span></pre>



<p>这就是伯特从头开始的编码。如果你在一个大的语料库上训练它，那么你可以使用相同的模型:</p>



<ol><li>预训练:使用任何语料库，但要使用前面提到的输入表示的精确格式。</li><li>微调:确保你使用监督学习数据。</li><li>不同任务的特征提取器，甚至主题建模。</li></ol>



<p>你可以在这里找到完整的笔记本<a href="https://web.archive.org/web/20230124065338/https://colab.research.google.com/drive/13FjI_uXaw8JJGjzjVX3qKSLyW9p3b6OV?usp=sharing" target="_blank" rel="noreferrer noopener nofollow">。</a></p>



<h2 id="h-is-there-a-way-to-get-a-pre-trained-model">有办法得到预训练的模型吗？</h2>



<p>在最初的论文中，发布了两个模型:BERT-base和BERT-large。在本文中，我展示了如何从头开始编写BERT代码。</p>



<p>一般可以下载预训练好的模型，这样就不用经历这些步骤了。Huggingface库提供了这个特性，你可以使用Huggingface for PyTorch的transformer库。过程保持不变。</p>



<p>我有一个笔记本，我用了一个来自Huggingface的预先训练过的BERT，你可以在这里查看一下。</p>



<p>当你使用一个预先训练好的模型时，你需要做的就是下载这个模型，然后在一个类中调用它，并使用一个向前的方法来提供你的输入和掩码。</p>



<p>例如:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> transformers

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BERTClassification</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span> <span class="hljs-params">(self)</span>:</span>
        super(BERTClassification, self).__init__()
        self.bert = transformers.BertModel.from_pretrained(<span class="hljs-string">'bert-base-cased'</span>)
        self.bert_drop = nn.Dropout(<span class="hljs-number">0.4</span>)
        self.out = nn.Linear(<span class="hljs-number">768</span>, <span class="hljs-number">1</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, ids, mask, token_type_ids)</span>:</span>
        _, pooledOut = self.bert(ids, attention_mask = mask,
                                token_type_ids=token_type_ids)
        bertOut = self.bert_drop(pooledOut)
        output = self.out(bertOut)

        <span class="hljs-keyword">return</span> output</pre>



<h2 id="h-final-thoughts">最后的想法</h2>



<p>BERT是一个非常强大的最先进的NLP模型。预训练模型是在大型语料库上训练的，您可以根据自己的需要并基于较小数据集上的任务对其进行微调。关于微调的最好的事情是，你不要做1000个时期，它甚至可以在3到10个时期内模仿SOTA性能，这取决于参数和数据集处理的好坏。</p>



<p>我希望这个教程是有趣的和有益的。我希望你能从中得到些什么。</p>



<h3>资源</h3>




        </div>
        
    </div>    
</body>
</html>