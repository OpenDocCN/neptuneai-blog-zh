<html>
<head>
<title>Exploring Clustering Algorithms: Explanation and Use Cases </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>探索聚类算法:解释和用例</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/clustering-algorithms#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/clustering-algorithms#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>聚类(<a href="https://web.archive.org/web/20221206013350/https://en.wikipedia.org/wiki/Cluster_analysis" target="_blank" rel="noreferrer noopener nofollow">聚类分析</a>)是根据相似性对对象进行分组。聚类可用于许多领域，包括机器学习、计算机图形学、模式识别、图像分析、信息检索、生物信息学和数据压缩。</p>



<p>聚类是一个棘手的概念，这就是为什么有这么多不同的聚类算法。采用不同的聚类模型，并且对于这些聚类模型中的每一个，可以给出不同的算法。由一种聚类算法发现的聚类肯定不同于由不同算法发现的聚类。</p>



<p>在机器学习系统中，我们通常将示例分组作为理解数据集的第一步。将一个未标记的例子分组称为<a href="https://web.archive.org/web/20221206013350/https://developers.google.com/machine-learning/glossary#clustering" target="_blank" rel="noreferrer noopener nofollow">聚类</a>。由于样本是未标记的，聚类依赖于无监督的机器学习。如果例子被标记，那么它就变成了<a href="https://web.archive.org/web/20221206013350/https://developers.google.com/machine-learning/glossary#classification_model" target="_blank" rel="noreferrer noopener nofollow">分类</a>。</p>



<p>如果您想了解各种集群算法之间的差异，那么集群模型的知识是基础，在本文中，我们将深入探讨这个主题。</p>



<h2 id="definition">什么是聚类算法？</h2>



<p>聚类算法用于根据某些相似性对数据点进行分组。好的聚类没有标准。聚类使用未标记的数据确定分组。主要看具体用户和场景。</p>



<p>典型的集群模型包括:</p>



<ul><li><strong>连接性模型</strong>–如基于距离连接性构建模型的分层聚类。</li><li><strong>质心模型—</strong>类似K均值聚类，用单个均值向量表示每个聚类。</li><li><strong>分布模型</strong>–这里，使用统计分布对集群进行建模。</li><li><strong>密度模型</strong>–如DBSCAN和OPTICS，它们将聚类定义为数据空间中的连通密集区域。</li><li><strong>分组模型</strong>–这些模型不提供精确的结果。他们只提供分组信息。</li><li><strong>基于图的模型</strong>–图中节点的子集，使得子集中每两个节点之间有一条边相连，这可以被视为集群的原型形式。</li><li><strong>神经模型</strong>–自组织映射是最常见的无监督神经网络(NN)之一，它们的特征类似于上面的一个或多个模型。</li></ul>



<p>请注意，有不同类型的集群:</p>



<ul><li><strong>硬聚类</strong>–数据点要么完全属于该聚类，要么不属于该聚类。例如，考虑具有四个组的客户细分。每个客户可以属于四个组中的任何一个。</li><li><strong>软聚类</strong>–概率分数被分配给那些聚类中的数据点。</li></ul>



<p>在本文中，我们将关注前四个模型(连接性、质心、分布和密度模型)。</p>



<h2 id="types">聚类算法的类型以及如何为您的用例选择一种算法</h2>



<p>让我们看看聚类算法的类型，以及如何为您的用例选择它们。</p>



<h3 id="hierarchical">分层聚类算法(基于连通性的聚类)</h3>



<p><strong>层次聚类</strong>的主要思想是基于附近的对象比较远的对象更相关的概念。让我们仔细看看这些算法的各个方面:</p>



<ul><li>该算法连接到“<strong>对象</strong>，以根据它们的距离形成“<strong>簇</strong>”。</li><li>一个集群可以通过连接到集群的各个部分所需的最大距离来定义。</li><li><strong>树状图</strong>可以代表在不同距离上形成的不同聚类，解释了“<strong>层次聚类</strong>名称的由来。这些算法提供了一个聚类层次结构，这些聚类在一定距离处被合并。</li><li>在树状图中，y轴标记聚类合并的距离。对象被放置在x轴旁边，这样簇就不会混合。</li></ul>



<p>层次聚类是一系列以不同方式计算距离的方法。常见的选择有<strong>单连锁聚类、完全连锁聚类、</strong>和<strong> UPGMA </strong>。此外，分层聚类可以是:</p>



<ol><li><strong>agglomerate</strong>–它从单个元素开始，然后将它们分组为单个簇。</li><li><strong>divisible</strong>–它从一个完整的数据集开始，并将其划分为多个分区。</li></ol>



<h4>凝聚层次聚类(AHC)</h4>



<p>在这一部分，我将解释AHC算法，它是最重要的层次聚类技术之一。做到这一点的步骤是:</p>



<ol><li>每个数据点被视为单个聚类。一开始我们有K个集群。开始时，数据点的数量也是k。</li><li>现在，我们需要在这一步中通过连接2个最近的数据点来形成一个大的集群。这将导致总共K-1个集群。</li><li>现在需要将两个最接近的集群连接起来，以形成更多的集群。这将导致总共K-2个集群。</li><li>重复以上三个步骤，直到K变为0，形成一个大的集群。没有更多要连接的数据点。</li><li>在最终形成一个大的集群之后，我们可以根据用例使用树状图将集群分成多个集群。</li></ol>



<p>下图给出了层次聚类方法的概念。</p>







<p><strong>AHC的优势:</strong></p>



<ul><li>AHC很容易实现，它还可以提供对象排序，这可以为显示提供信息。</li><li>我们不必预先指定集群的数量。通过在特定级别切割树状图，很容易决定聚类的数量。</li><li>在AHC方法中，将创建更小的集群，这可能会发现数据中的相似性。</li></ul>



<p><strong>AHC的缺点:</strong></p>



<ul><li>在开始的任何步骤中被错误分组的对象不能被撤销。</li></ul>



<ul><li>分层聚类算法不提供数据集的唯一分区，但是它们给出了可以从中选择聚类的层次结构。</li><li>他们<strong>没有很好地处理离群值</strong>。每当发现异常值时，它们将作为一个新的聚类结束，或者有时导致与其他聚类合并。</li></ul>



<p>凝聚层次聚类算法是一种自下而上的聚类形式，其中每个数据点都被分配到一个聚类中。这些集群然后连接在一起。相似的聚类在每次迭代中被合并，直到所有的数据点都是一个大的根聚类的一部分。</p>



<h4>聚类数据集</h4>



<p>通过<a href="https://web.archive.org/web/20221206013350/https://scikit-learn.org/stable/modules/clustering.html#clustering" target="_blank" rel="noreferrer noopener nofollow"> Scikit-learn </a>开始使用Python中的集群很简单。一旦安装了这个库，就可以选择多种聚类算法。</p>



<p>我们将使用`<strong><em>make _ class ification</em></strong>`函数从`<a href="https://web.archive.org/web/20221206013350/https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html"> <em> sklearn </em> </a>`库中生成一个数据集，以演示不同聚类算法的使用。`<em> make_classification </em>'函数接受以下参数:</p>



<ul><li>样本的数量。</li><li>特征的总数。</li><li>信息特征的数量。</li><li>冗余特征的数量。</li><li>从冗余要素和信息要素中随机抽取的重复要素的数量。</li><li>每个类的聚类数。</li></ul>



<pre class="hljs"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> where
<span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> unique
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_classification
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> AgglomerativeClustering
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plot


train_data, _ = make_classification(n_samples=<span class="hljs-number">1000</span>,
                                       n_features=<span class="hljs-number">2</span>,
                                       n_informative=<span class="hljs-number">2</span>,
                                       n_redundant=<span class="hljs-number">0</span>,
                                       n_clusters_per_class=<span class="hljs-number">1</span>,
                                       random_state=<span class="hljs-number">4</span>)

agg_mdl = AgglomerativeClustering(n_clusters=<span class="hljs-number">4</span>)


agg_result = agg_mdl.fit_predict(train_data)


agg_clusters = unique(agg_result)


<span class="hljs-keyword">for</span> agg_cluster <span class="hljs-keyword">in</span> agg_clusters:
    
    index = where(agg_result == agg_cluster)

    plot.scatter(train_data[index, <span class="hljs-number">0</span>], train_data[index,<span class="hljs-number">1</span>])


plot.show()</pre>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" src="../Images/ab79a94ad378c37d4e76124399a6f969.png" alt="Hierarchical clustering results" class="wp-image-52017" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013350im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Hierarchical-clustering-results.png?ssl=1"/><figcaption><em>Clusters obtained by Hierarchical Cluster Algorithm</em></figcaption></figure></div>



<p>分层聚类通常以描述性建模的形式使用，而不是预测性的。它不能很好地处理大型数据集，只能在某些情况下提供最佳结果。有时也很难从树状图上检测出正确的簇数。</p>



<h3 id="centroid">基于质心的聚类算法/分区聚类算法</h3>



<p>在质心/分区聚类中，<strong>个聚类由一个中心向量</strong>表示，该向量不一定是数据集的成员。即使在这种特定的聚类类型中，也需要选择K的值。这是一个优化问题:找到质心的数量或K的值，并将对象分配到附近的聚类中心。这些步骤需要以这样一种方式执行，使得离聚类的平方距离最大化。</p>



<p>使用最广泛的一种基于质心的聚类算法是K-Means，它的一个缺点是你需要预先选择一个K值。</p>



<h4>k-均值聚类算法</h4>



<p>K-Means算法使用特定的距离度量将给定的数据集分成预定义的(K)个聚类。每个簇/组的中心被称为<strong>质心</strong>。</p>



<p>K-Means算法是如何工作的？</p>



<p>让我们看看K-Means算法是如何工作的:</p>



<ul><li>最初，选择K个质心。选择正确的k值有不同的方法。</li><li>无序播放数据并初始化质心-随机选择K个数据点作为质心，无需替换。</li><li>通过计算分配给每个先前质心的所有样本的平均值来创建新质心。</li><li>随机初始化质心，直到质心没有变化，这样数据点对聚类的分配就不会改变。</li><li>K-Means聚类使用<a href="https://web.archive.org/web/20221206013350/https://en.wikipedia.org/wiki/Euclidean_distance#:~:text=In%20mathematics%2C%20the%20Euclidean%20distance,being%20called%20the%20Pythagorean%20distance." target="_blank" rel="noreferrer noopener nofollow">欧几里德距离</a>来找出点之间的距离。</li></ul>







<p><em>注意:K-Means聚类的一个例子将在下面的用例部分用客户细分的例子来解释。</em></p>



<p><strong>选择K的正确值有两种方法:肘法和剪影法。</strong></p>



<p><strong>手肘法</strong></p>



<p>Elbow方法选取值的范围，并从中选取最佳值。它计算不同k值的组内平方和(WCSS)。它计算平方点的和，并计算平均距离。</p>







<p>在上面的公式中，Yi是对进行观测的质心。K的值需要选择在WCSS开始减弱的地方。在WCSS对K的曲线图中，这显示为一个<strong>肘</strong>。</p>







<p><strong>剪影法</strong></p>



<p>使用平均聚类内距离(m)和每个样本的最近聚类距离(n)的平均值来计算轮廓得分/系数(SC)。</p>







<p>在上面的公式中，n是数据点和该数据点不属于的最近聚类之间的距离。我们可以计算所有样本的平均SC，并以此作为度量来决定聚类数。</p>



<p>SC值的范围在-1到1之间。1表示聚类被很好地分开并且是可区分的。如果值为-1，则错误地分配了聚类。</p>



<p>以下是K-Means姿势相对于其他算法的一些优势:</p>



<ul><li>实现起来很简单。</li><li>它可扩展到大规模数据集，对于大规模数据集也更快。</li><li>它非常频繁地适应新的例子。</li></ul>



<p><strong> K-Medians </strong>是相对于K-Means算法的另一种聚类算法，除了使用中值重新计算聚类中心。在K-Median算法中，对异常值的敏感度较低，因为需要排序来计算大型数据集的中值向量。</p>



<p>K-Means有一些缺点；该算法可以在不同的运行中提供不同的聚类结果，因为K-Means从聚类中心的随机初始化开始。获得的结果可能不会重复。</p>



<p>K-Means算法带来的其他缺点是:</p>



<ul><li>如果<strong>聚类具有类似球形的形状</strong>，K-Means聚类<strong>擅长捕捉数据的结构</strong>。它总是试图围绕质心构建一个漂亮的球形。这意味着当聚类具有不同的几何形状时，K-Means在聚类数据方面表现不佳。</li><li>即使数据点属于同一个聚类，K-Means也不允许数据点彼此远离，它们共享同一个聚类。</li><li>K-Means算法对异常值很敏感。</li><li>随着维度数量的增加，可伸缩性会降低。</li></ul>



<p>使用K-Means进行聚类时，需要记住以下几点:</p>



<ul><li><a href="https://web.archive.org/web/20221206013350/https://scikit-learn.org/stable/modules/preprocessing.html" target="_blank" rel="noreferrer noopener nofollow">在应用K-Means算法时将数据</a>标准化，因为这将帮助您获得高质量的聚类，并提高聚类算法的性能。由于K-Means使用基于距离的度量来查找数据点之间的相似性，因此最好将数据标准化为标准偏差为1，平均值为零。通常，任何数据集中的要素都有不同的测量单位，例如，收入与年龄。</li><li>K-Means赋予更大的聚类更多的权重。</li><li>用于选择聚类数量的肘方法不能很好地工作，因为对于所有的k，误差函数减小。</li><li>如果聚类之间有重叠，K-Means就没有内在的不确定性度量来确定属于重叠区域的样本分配给哪个聚类。</li><li>k-表示即使数据不能被聚类，也要对其进行聚类，例如来自均匀分布的数据。</li></ul>



<h4>小批量K均值聚类算法</h4>



<p>K-Means是流行的聚类算法之一，主要是因为它具有良好的时间性能。当数据集的大小增加时，K-Means会导致内存问题，因为它需要整个数据集。为此，为了降低算法的时间和空间复杂度，提出了一种叫做<strong>小批量K-Means </strong>的方法。</p>



<p>小批量K-Means算法试图以这样一种方式将数据放入主存储器中，即该算法使用随机选择的固定大小的小批量数据。关于小批量K均值算法，有几点需要注意:</p>



<ul><li>通过从数据集中获得新的任意样本，在每次迭代中更新聚类(取决于聚类质心的先前位置)，并且重复这些步骤直到收敛。</li><li>一些研究表明，这种方法节省了大量的计算时间，但代价是聚类质量略有下降。但是还没有进行深入的研究来量化影响聚类质量的聚类数量或它们的大小。</li></ul>







<ul><li>聚类的位置根据每批中的新点进行更新。</li><li>所做的更新是梯度下降更新，这明显比正常的批量K-均值更快。</li></ul>



<h3 id="density">基于密度的聚类算法</h3>



<p>基于密度的聚类将高示例密度的区域连接成聚类。这允许任意的形状分布，只要密集区域是相连的。对于高维数据和不同密度的数据，这些算法遇到了问题。按照设计，这些算法不会将离群值分配给聚类。</p>



<h4>基于密度的噪声应用空间聚类</h4>



<p>最流行的基于密度的方法是对有噪声的应用进行基于密度的空间聚类(<a href="https://web.archive.org/web/20221206013350/https://en.wikipedia.org/wiki/DBSCAN" target="_blank" rel="noreferrer noopener nofollow"> DBSCAN </a>)。它具有一个定义良好的集群模型，称为“<strong>密度可达性</strong>”。</p>



<p>这种类型的聚类技术连接满足特定密度标准(半径内最小数量的对象)的数据点。DBSCAN聚类完成后，有三种类型的点:<strong>核心、边界、噪声。</strong></p>







<p>如果你看上面的图，<strong>核心</strong>是一个点，它有一些(m)点在离它自己特定的(n)距离内。<strong>边界</strong>是在距离n处至少有一个核心点的点</p>



<p><strong>噪点</strong>既不是边界也不是核心的点。需要分离聚类的稀疏区域中的数据点被认为是噪声和更宽的点。</p>



<p>DBSCAN使用两个参数来确定如何定义集群:</p>



<ul><li><strong> minPts </strong>:一个区域被认为是密集的，所需的最小点数是`<strong> <em> minPts </em> </strong>`。</li><li><strong> eps </strong>:为了定位任意点附近的数据点，使用`<strong> eps (ε) 【T3 `作为距离度量。</strong></li></ul>



<p>下面是对DBSCAN算法的逐步解释:</p>



<ul><li>DBSCAN从一个随机数据点(未访问的点)开始。</li><li>使用距离ε提取该点的邻域。</li><li>如果在该区域内有足够的数据点，并且当前数据点成为最新聚类中的第一个点，则聚类过程开始，否则该点被标记为噪声并被访问。</li><li>其<strong>εε</strong>距离邻域内的点也成为新群集中第一个点的同一群的一部分。对于添加到上述聚类中的所有新数据点，重复使所有数据点属于同一聚类的过程。</li><li>重复上述两个步骤，直到确定了群集中的所有点。聚类的<strong> ε邻域</strong>内的所有点都被<strong>访问过</strong>并且<strong>被标记为</strong>。一旦我们完成了当前的聚类，就会检索和处理一个新的未访问点，从而进一步发现聚类或噪声。重复该过程，直到所有数据点都被标记为已访问。</li></ul>



<p>DBSCAN的一个令人兴奋的特性是它的低复杂度。它需要对数据库进行线性数量的范围查询。</p>



<p>DBSCAN的主要问题是:</p>



<ul><li>它希望通过某种密度下降来检测星团的边界。DBSCAN连接高示例密度的区域。在处理形状奇怪的数据时，该算法优于K-Means。</li></ul>



<p>DBSCAN算法的一个优点是:</p>



<ul><li>它不需要预定义的集群数量。它还识别噪声和异常值。此外，该算法可以很好地找到任意大小和形状的聚类。</li></ul>



<pre class="hljs"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> where
<span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> unique
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_classification
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCAN
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plot


train_data, _ = make_classification(n_samples=<span class="hljs-number">1000</span>,
                                       n_features=<span class="hljs-number">2</span>,
                                       n_informative=<span class="hljs-number">2</span>,
                                       n_redundant=<span class="hljs-number">0</span>,
                                       n_clusters_per_class=<span class="hljs-number">1</span>,
                                       random_state=<span class="hljs-number">4</span>)


dbscan_model = DBSCAN(eps=<span class="hljs-number">0.25</span>, min_samples=<span class="hljs-number">9</span>)


dbscan_model.fit(train_data)


dbscan_res = dbscan_model.fit_predict(train_data)


dbscan_clstrs = unique(dbscan_res)


<span class="hljs-keyword">for</span> dbscan_clstr <span class="hljs-keyword">in</span> dbscan_clstrs:
    
    index = where(dbscan_res == dbscan_clstr)
    
    plot.scatter(train_data[index, <span class="hljs-number">0</span>], train_data[index, <span class="hljs-number">1</span>])


plot.show()</pre>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" src="../Images/9d82bb11bde525c6d85acf3030ce9df0.png" alt="DBSCAN clustering results" class="wp-image-52023" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013350im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/DBSCAN-clustering-results.png?ssl=1"/><figcaption><em>Clusters obtained by DBSCAN Cluster Algorithm</em></figcaption></figure></div>



<h3 id="distribution">基于分布的聚类算法</h3>



<p>与统计学密切相关的聚类模型是基于分布模型的。集群<strong>可以被定义为属于相同分布</strong>的对象。这种方法非常类似于通过从分布中随机抽取对象来生成人工数据集。</p>



<p>虽然这些方法的理论方面相当不错，但这些模型都存在<a href="/web/20221206013350/https://neptune.ai/blog/overfitting-vs-underfitting-in-machine-learning" target="_blank" rel="noreferrer noopener">过度拟合</a>的问题。</p>



<h4>高斯混合模型</h4>



<p>高斯混合模型(<strong> GMM </strong>)是一种基于分布的聚类类型。这些聚类方法假设数据由分布组成，例如高斯分布。在下图中，基于分布的算法将数据聚类成三个<a href="https://web.archive.org/web/20221206013350/https://en.wikipedia.org/wiki/Normal_distribution" target="_blank" rel="noreferrer noopener nofollow">高斯分布</a>。随着与分布的距离增加，该点属于该分布的概率降低。</p>



<p>GMM可以像K-Means一样用来寻找聚类。一个点属于分布中心的概率随着离分布中心的距离增加而减小。下图中的条带显示概率降低。由于GMM包含了一个概率模型，我们也可以找到概率集群分配。当您不知道数据的分布类型时，您应该使用不同的算法。</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/b32c88ecc5e8dec8201ddb1a58910888.png" alt="Distribution-based clustering algorithm" class="wp-image-52029" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013350im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Distribution-based-clustering.png?resize=609%2C380&amp;ssl=1"/><figcaption><em>Example of distribution-based clustering | <a href="https://web.archive.org/web/20221206013350/https://developers.google.com/machine-learning/clustering/clustering-algorithms" target="_blank" rel="noreferrer noopener nofollow">Source</a> </em></figcaption></figure></div>



<p>让我们看看GMM是如何计算概率并将其分配给数据点的:</p>



<ul><li>一个GM是由几个Gaussianss组成的函数，每个Gaussian由k ∈ {1，…，K}标识，其中K是聚类的个数。混合物中的每个高斯K由以下参数组成:<ul><li>定义其中心的平均值μ。</li><li>定义其宽度的协方差σ。</li><li>定义高斯函数大小的混合概率。</li></ul></li></ul>



<p>这些参数可以在下图中看到:</p>







<p>为了找到协方差、均值、方差和聚类权重，GMM使用了期望最大化技术。</p>



<p>假设我们需要分配K个聚类，即K个高斯分布，平均值和协方差值分别为μ1，μ2，..μk和σ1，σ2，..σk，还有一个参数πI，代表分布密度。</p>



<p>为了定义高斯分布，我们需要找到这些参数的值。我们已经决定了聚类的数量，并指定了平均值、协方差和密度的值。接下来是期望步骤和最大化步骤，你可以在这个<a href="https://web.archive.org/web/20221206013350/https://www.linkedin.com/pulse/gaussian-mixture-models-clustering-machine-learning-cheruku/" target="_blank" rel="noreferrer noopener nofollow">帖子</a>中查看。</p>



<p><strong>GMM的优势</strong></p>



<ul><li>GMM相对于K-Means的一个优点是，K-Means不考虑<a href="https://web.archive.org/web/20221206013350/https://en.wikipedia.org/wiki/Variance">方差</a>(这里，方差是指钟形曲线的宽度)，GMM返回数据点属于K个聚类中每个聚类的概率。</li><li>在重叠聚类的情况下，所有上述聚类算法都不能将其识别为一个聚类。</li><li>GMM使用概率方法，并为属于聚类的每个数据点提供概率。</li></ul>



<p><strong>GMM的劣势</strong></p>



<ul><li>如果分布的数量很大或者数据集包含较少的观察数据点，混合模型在计算上是昂贵的。</li><li>它需要大型数据集，并且很难估计聚类的数量。</li></ul>



<p>现在让我们看看GMM是如何对数据进行聚类的。下面的代码帮助您:</p>



<ul><li>创建数据，</li><li>将数据拟合到“高斯混合”模型，</li><li>找到分配给聚类的数据点，</li><li>获取唯一的集群，以及</li><li>如下图所示绘制聚类图。</li></ul>



<pre class="hljs"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> where
<span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> unique
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_classification
<span class="hljs-keyword">from</span> sklearn.mixture <span class="hljs-keyword">import</span> GaussianMixture
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plot


train_data, _ = make_classification(n_samples=<span class="hljs-number">1200</span>,
                                       n_features=<span class="hljs-number">3</span>,
                                       n_informative=<span class="hljs-number">2</span>,
                                       n_redundant=<span class="hljs-number">0</span>,
                                       n_clusters_per_class=<span class="hljs-number">1</span>,
                                       random_state=<span class="hljs-number">4</span>)

gaussian_mdl = GaussianMixture(n_components=<span class="hljs-number">3</span>)


gaussian_mdl.fit(train_data)


gaussian_res = gaussian_mdl.fit_predict(train_data)


gaussian_clstr = unique(dbscan_res)


<span class="hljs-keyword">for</span> gaussian_cluser <span class="hljs-keyword">in</span> gaussian_clstr:

    index = where(gaussian_res == gaussian_cluser)
    
    plot.scatter(train_data[index, <span class="hljs-number">0</span>], train_data[index, <span class="hljs-number">1</span>])


plot.show()</pre>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" src="../Images/a2a71662bce729ed989e3a3c8cf6f1a3.png" alt="Gaussian mixture model results" class="wp-image-52031" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013350im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Gaussian-mixture-model-results.png?ssl=1"/><figcaption><em>Clusters obtained by Gaussian Mixture Model Algorithm</em></figcaption></figure></div>



<h2 id="applications">聚类在不同领域的应用</h2>



<p>可以应用集群的一些领域有:</p>



<ul><li><strong>营销</strong>:客户细分发现。</li><li><strong>图书馆</strong>:根据主题和信息对不同的书籍进行聚类。</li><li><strong>生物学</strong>:不同种类的植物和动物之间的分类。</li><li><strong>城市规划</strong>:根据区位分析房屋价值。</li><li><strong>文档分析</strong>:各种研究数据和文档可以按照一定的相似性进行分组。标注大数据真的很难。在这些情况下，聚类有助于对文本进行聚类&amp;并将其分成不同的类别。像LDA这样的无监督技术在这些情况下也有利于在大型语料库中发现隐藏的主题。</li></ul>



<h2 id="issues">无监督建模方法的问题</h2>



<p>以下是您在应用聚类技术时可能会遇到的一些问题:</p>



<ul><li>结果可能不太准确，因为数据没有提前标注，输入数据也未知。</li><li>该算法的学习阶段可能需要很长时间，因为它会计算和分析所有可能性。</li><li>在没有任何先验知识的情况下，模型从原始数据中学习。</li><li>随着功能数量的增加，复杂性也会增加。</li><li>一些涉及实时数据的项目可能需要持续向模型提供数据，从而导致耗时且不准确的结果。</li></ul>



<h2 id="factors">选择聚类算法时要考虑的因素</h2>



<p>现在让我们来看看选择聚类算法时要考虑的一些因素:</p>



<ul><li>选择聚类分析算法，使其能够很好地适应数据集。并非所有的聚类算法都可以有效地扩展。机器学习中的数据集可以有数百万个例子。</li><li>许多聚类算法通过计算所有样本对之间的相似性来工作。运行时间随着样本数<strong> n </strong>的增加而增加，在复杂度符号中表示为<strong> O(n^2) </strong>。<strong> O(n^2) </strong>当例子数以百万计时不实用。这里重点关注的是K-Means算法，其复杂度为O(n)，这意味着该算法与n 成线性比例关系。</li></ul>



<h2 id="use-cases">Python中集群的不同实际用例</h2>



<h3>1.客户细分</h3>



<p>我们将使用客户<a href="https://web.archive.org/web/20221206013350/https://github.com/AravindR7/Clustering-Algorithms/blob/main/Customers.csv" target="_blank" rel="noreferrer noopener nofollow">数据</a>来看看这个算法是如何工作的。此示例旨在将客户分成几个组，并决定如何将客户分组，以增加客户价值和公司收入。这个用例通常被称为<a href="https://web.archive.org/web/20221206013350/https://searchcustomerexperience.techtarget.com/definition/customer-segmentation" target="_blank" rel="noreferrer noopener nofollow">客户细分</a>。</p>



<p>数据中的一些特征是客户ID、性别、年龄、收入(以千美元为单位)以及基于消费行为和性质的客户消费分数。</p>







<p>安装依赖项:</p>



<pre class="hljs">!pip install numpy pandas plotly seaborn scikit-learn</pre>



<pre class="hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plot
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">import</span> plotly.express <span class="hljs-keyword">as</span> pxp
<span class="hljs-keyword">import</span> plotly.graph_objs <span class="hljs-keyword">as</span> gph
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> silhouette_score
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans

data = pd.read_csv(<span class="hljs-string">'Customers.csv'</span>)
data.head()</pre>



<figure class="wp-block-image"><img decoding="async" src="../Images/f94cd3d1c60904b94a7e5cf57330c4ea.png" alt="" data-original-src="https://web.archive.org/web/20221206013350im_/https://lh6.googleusercontent.com/2_RgjAveKV41BWvrWuGFPAPVZad08x6PF5uhkye-f4rjHitE1cVf-03LlLHiD2xg_mOnBDdXjU5XcAn7aLf46lLvv4VnXWUxqIapScDb-XidpLRYXUQEQ1uU7I-VgLaPQDWnAsi2=s0"/></figure>



<p>让我们从删除聚类过程中不需要的列开始。</p>



<pre class="hljs">
data.drop(<span class="hljs-string">'CustomerID'</span>, axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-keyword">True</span>)</pre>



<p>我们可以检查数据中列的分布，以了解数据在各个列中的分布情况。</p>



<pre class="hljs">plot.figure(figsize = (<span class="hljs-number">22</span>, <span class="hljs-number">10</span>))
plotnum = <span class="hljs-number">1</span>

<span class="hljs-keyword">for</span> cols <span class="hljs-keyword">in</span> [<span class="hljs-string">'Age'</span>, <span class="hljs-string">'AnnualIncome'</span>, <span class="hljs-string">'SpendingScore'</span>]:
    <span class="hljs-keyword">if</span> plotnum &lt;= <span class="hljs-number">3</span>:
        axs = plot.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, plotnum)
        sns.distplot(data[cols])

    plotnum += <span class="hljs-number">1</span>

plot.tight_layout()
plot.show()</pre>







<p>现在，让我们创建一个条形图来检查特定年龄组的客户分布。您还可以应用同样的方法来可视化客户数量与支出分数的关系，以及基于年收入的客户数量。</p>



<pre class="hljs">age_55above = data.Age[data.Age &gt;= <span class="hljs-number">55</span>]
age_46_55 = data.Age[(data.Age &gt;= <span class="hljs-number">46</span>) &amp; (data.Age &lt;= <span class="hljs-number">55</span>)]
age_36_45 = data.Age[(data.Age &gt;= <span class="hljs-number">36</span>) &amp; (data.Age &lt;= <span class="hljs-number">45</span>)]
age_26_35 = data.Age[(data.Age &gt;= <span class="hljs-number">26</span>) &amp; (data.Age &lt;= <span class="hljs-number">35</span>)]
age_18_25 = data.Age[(data.Age &gt;= <span class="hljs-number">18</span>) &amp; (data.Age &lt;= <span class="hljs-number">25</span>)]</pre>



<pre class="hljs">x_age_ax = [<span class="hljs-string">'18-25'</span>, <span class="hljs-string">'26-35'</span>, <span class="hljs-string">'36-45'</span>, <span class="hljs-string">'46-55'</span>, <span class="hljs-string">'55+'</span>]
y_age_ax = [len(age_18_25.values), len(age_26_35.values), len(age_36_45.values), len(age_46_55.values),
     len(age_55above.values)]

pxp.bar(data_frame = data, x = x_age_ax, y = y_age_ax, color = x_age_ax,
       title = <span class="hljs-string">'Count of customers per age group'</span>)</pre>



<div class="wp-block-image"><figure class="aligncenter size-full"><img decoding="async" src="../Images/21b988b67e72e56bb8a88e9229c1057f.png" alt="Clustering algorithms - customer segmentation" class="wp-image-52033" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013350im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Clustering-algorithms-customer-segmentation-2.png?ssl=1"/><figcaption><em>Result: Bar plot showing the distribution of customers in a particular age group</em></figcaption></figure></div>



<p>聚类最关键的一个方面是选择正确的K值。随机选择K可能不是一个好的选择。我们将使用肘法和轮廓评分来选择k值。</p>



<p>在我们的例子中，从下面的图表来看，通过肘方法找到的K的最佳值是4。我们希望最大化聚类的数量，并限制每个数据点成为其聚类质心的情况。</p>



<pre class="hljs">x_input = data.loc[:, [<span class="hljs-string">'Age'</span>, <span class="hljs-string">'SpendingScore'</span>]].values
wcss = []
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">12</span>):
    k_means = KMeans(n_clusters=k, init=<span class="hljs-string">'k-means++'</span>)
    k_means.fit(x_input)
    wcss.append(k_means.inertia_)

plot.figure(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">8</span>))

plot.plot(range(<span class="hljs-number">1</span>, <span class="hljs-number">12</span>), wcss, linewidth=<span class="hljs-number">2</span>, marker=<span class="hljs-string">'8'</span>)
plot.title(<span class="hljs-string">'Elbow methodn'</span>, fontsize=<span class="hljs-number">18</span>)
plot.xlabel(<span class="hljs-string">'K'</span>)
plot.ylabel(<span class="hljs-string">'WCSS'</span>)
plot.show()</pre>







<p>让我们检查一下这个特殊实现的轮廓系数是怎样的。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> silhouette_score
label = k_means.predict(x_input)

print(f<span class="hljs-string">' Silhouette Score(n=4): {silhouette_score(x_input,
label)}'</span>)</pre>



<figure class="wp-block-image"><img decoding="async" src="../Images/bb7fb0f887c108fd4db8a9005adceb82.png" alt="" data-original-src="https://web.archive.org/web/20221206013350im_/https://lh4.googleusercontent.com/lpY1OX3NP_k5NT8JlLouheCGoMgzj1KGdpNeDyafUKnT5_4qehj1HctICKKVyZ7_4NphkA1IMxMwnqmtDaUxlOLKgDuwNwBIWTXAA95P4QcnK05bYik1yoxjTGo7qbUghFzDlvUI=s0"/></figure>



<p>从下面的年龄与花费的对比图中，你可以看到一些集群没有很好的分离。簇之间的簇内距离几乎不显著，这就是为什么n=4的SC是0.40，这是更小的。尝试不同的K值以找到最佳的聚类数。</p>



<pre class="hljs">k_means=KMeans(n_clusters=<span class="hljs-number">4</span>)
labels=k_means.fit_predict(x_input)
print(k_means.cluster_centers_)</pre>



<figure class="wp-block-image"><img decoding="async" src="../Images/2a6dfb877ab018010cb5c2c05007e234.png" alt="" data-original-src="https://web.archive.org/web/20221206013350im_/https://lh3.googleusercontent.com/3wS71quLqaKpJUKti5r7a4OQivXHDyUh-pYGaOIuxlges6lz5MhmyV-mVJKNkQLueyZNwjD1KxDJoCaWHn7FclaKlJmK-wb-OoWie2Di6sBpTGUyfVY354mfT1v8Uq34LyPfN0kj=s0"/></figure>



<p>现在，让我们绘制一个图表来检查这些聚类是如何从数据中形成的。</p>



<pre class="hljs">plot.figure(figsize = (<span class="hljs-number">16</span>, <span class="hljs-number">10</span>))
plot.scatter(x_input[:, <span class="hljs-number">0</span>], x_input[:, <span class="hljs-number">1</span>], c =
k_means.labels_, s = <span class="hljs-number">105</span>)
plot.scatter(k_means.cluster_centers_[:, <span class="hljs-number">0</span>],k_means.cluster_centers_[:, <span class="hljs-number">1</span>], color = <span class="hljs-string">'red'</span>, s = <span class="hljs-number">250</span>)
plot.title(<span class="hljs-string">'Customers clustersn'</span>, fontsize = <span class="hljs-number">20</span>)
plot.xlabel(<span class="hljs-string">'Age'</span>)
plot.ylabel(<span class="hljs-string">'Spending Score'</span>)
plot.show()</pre>



<p>红点表示星团中心。</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/b5c10d00c130a3fda9c9b8980301babb.png" alt="Customer segmentation clusters" class="wp-image-52035" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013350im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Customer-segmentation-clusters-1.png?resize=732%2C475&amp;ssl=1"/><figcaption><em>Clusters formed from Age versus SpendingScore using K-Means</em></figcaption></figure></div>



<p>这些数据形成了四个不同的集群。蓝色聚类代表消费得分较高的年轻客户，紫色聚类代表消费得分较低的老年客户。</p>



<p>可以遵循上面实施的类似步骤来聚类“年龄”对“年度收入”以及“支出分数”对“年度收入”。所有这三个都可以结合起来，用一个可以在<a href="https://web.archive.org/web/20221206013350/https://github.com/AravindR7/Clustering-Algorithms/blob/main/customer_segmentation.ipynb" target="_blank" rel="noreferrer noopener nofollow"> Jupyter笔记本</a>中找到的3D绘图来绘制。它还对相同的数据实现了不同的聚类算法。</p>



<h3>2.图像压缩</h3>



<p>图像压缩是一种在不降低图片质量的情况下应用于图像的压缩技术。图像尺寸的减小有助于将它们存储在有限的驱动器空间中。</p>



<p>为什么需要图像压缩技术？图像压缩有多种需求。压缩在医疗保健中至关重要，因为医疗图像需要存档，而且数据量非常大。</p>



<p>当我们需要提取和存储图像中最有用的部分(表示为嵌入)时，图像压缩可能是存储更多数据的非常有益的方法。</p>



<p>有两种图像压缩技术:</p>



<ol><li><strong>无损压缩</strong>:这种方法用于减小文件的大小，同时保持与压缩前相同的质量。文件可以恢复到原始形式，因为这种技术不会损害数据质量。</li><li><strong>有损压缩</strong>:有损压缩是一种消除不明显数据的方法。它给照片一个更小的尺寸；有损压缩会丢弃图片中一些不太重要的部分。在这种类型的压缩技术中，被压缩的图像不能被恢复到它的原始图像，并且数据的大小改变。</li></ol>



<p>让我们使用K-Means聚类来解决这个问题。</p>



<p>您可能已经知道，一幅图像由3个通道组成，<strong>、RGB </strong>，每个通道的值都在<strong>【0，255】</strong>范围内。因此，一个特定的图像可能有255*255*255种不同的颜色。所以在我们输入图像之前，我们需要对它进行预处理。</p>



<p>我们将要处理的图像的大小是(1365，2048，3)。因此，对于每个像素位置，我们将有两个8位整数来指定红色、绿色和蓝色强度值。我们的目标是将它减少到25种颜色，并且只用这些颜色来表现照片。</p>



<p>下面导入了此任务所需的一些包:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> matplotlib.image <span class="hljs-keyword">import</span> imread
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans</pre>



<p>从<a href="https://web.archive.org/web/20221206013350/https://github.com/AravindR7/Clustering-Algorithms/blob/main/palace.jpg" target="_blank" rel="noreferrer noopener nofollow">这里</a>下载图片并读入。</p>



<pre class="hljs">img = imread(<span class="hljs-string">'palace.jpg'</span>)
img_size = img.shape</pre>



<figure class="wp-block-image"><img decoding="async" src="../Images/362d5f9882decbb6c7a3c2429c3a5afc.png" alt="" data-original-src="https://web.archive.org/web/20221206013350im_/https://lh6.googleusercontent.com/Dd3iurmdNfBVGoYaXGvI-VYtxgwnDxFUkeh2m9U4pNXSPfiq-_18GNNQkphJkTGi61JcwP0fruVxiKQzaJ0rOjiy9hO4gw5d7hHPr1y-wElJRqm_4q6X1Cn3PKbuPNVN0p39bv_v=s0"/></figure>



<p>重塑2D的形象。我们将K-Means算法应用于图片，并将每个像素视为一个数据点来选择使用什么颜色。这意味着将图像从高X宽X通道重塑为(高X宽)X通道；我们将有1365 x 2048 = 2795520个数据点。</p>



<p>遵循这种方法将有助于我们使用25个质心来表示图像，并减少图像的大小。当我们使用质心作为像素颜色的查找时，会有一个相当大的不同，这将把每个像素位置的大小减少到4位，而不是8位。</p>



<pre class="hljs">X = img.reshape(img_size[<span class="hljs-number">0</span>] * img_size[<span class="hljs-number">1</span>], img_size[<span class="hljs-number">2</span>])</pre>



<p><strong>运行K均值算法</strong></p>



<p>上一节给出了K-means算法的详细说明。在本例中，我们将重点关注压缩部分。</p>



<pre class="hljs">km = KMeans(n_clusters=<span class="hljs-number">25</span>)
km.fit(X)</pre>



<p>使用质心来压缩图像。</p>



<pre class="hljs">X_cmpresd = km.cluster_centers_[km.labels_]
X_cmpresd = np.clip(X_cmpresd.astype(<span class="hljs-string">'uint8'</span>), <span class="hljs-number">0</span>, <span class="hljs-number">255</span>)</pre>



<p>将X_cmpresd整形为与原始图像128 * 128 * 3相同的尺寸</p>



<pre class="hljs">X_cmpresd = X_cmpresd.reshape(img_size[<span class="hljs-number">0</span>], img_size[<span class="hljs-number">1</span>], img_size[<span class="hljs-number">2</span>])</pre>



<p>现在，将原始图像和压缩图像相邻绘制。</p>



<pre class="hljs">figre, axs = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, figsize = (<span class="hljs-number">14</span>, <span class="hljs-number">10</span>))
axs[<span class="hljs-number">1</span>].imshow(img)
axs[<span class="hljs-number">1</span>].set_title(<span class="hljs-string">'Initial image'</span>)
axs[<span class="hljs-number">0</span>].imshow(X_cmpresd)
axs[<span class="hljs-number">0</span>].set_title(<span class="hljs-string">'Compressed one (25 colors)'</span>)
<span class="hljs-keyword">for</span> axs <span class="hljs-keyword">in</span> figre.axes:
    axs.axis(<span class="hljs-string">'off'</span>)
plot.tight_layout();</pre>



<p>这里，我使用了25个质心。压缩图像看起来更接近原始图像(意味着真实图像的许多特征被保留)。使用更少的聚类数，我们会以牺牲图像质量为代价获得更高的压缩率。相似的颜色被分组到k个簇中(k = 25(不同的RGB值))。</p>







<h3>3.数字分类</h3>



<p>在这个实现中，我们将使用小批量K-Means来执行图像分类。聚类也可以用于图像分析。利用Scikit-learn和<a href="https://web.archive.org/web/20221206013350/http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noreferrer noopener nofollow"> MNIST </a>数据集，我们将研究小批量K均值聚类在计算机视觉中的应用。</p>



<p>安装依赖项:</p>



<pre class="hljs">pip install keras tensorflow</pre>



<p>导入库:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">import</span> sklearn
<span class="hljs-keyword">import</span> matplotlib
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline</pre>



<p>加载MNIST数据集。可以通过<a href="https://web.archive.org/web/20221206013350/https://keras.io/" target="_blank" rel="noreferrer noopener nofollow"> Keras </a>买到。</p>



<pre class="hljs">figur, axi = plt.subplots(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, figsize=(<span class="hljs-number">14</span>, <span class="hljs-number">14</span>))
plt.gray()

<span class="hljs-keyword">for</span> j, axs <span class="hljs-keyword">in</span> enumerate(axi.flat):
    axs.matshow(x_train[j])
    axs.axis(<span class="hljs-string">'off'</span>)
    axs.set_title(<span class="hljs-string">'number {}'</span>.format(y_train[j]))

figur.show()</pre>







<p><strong>图像预处理</strong></p>



<p>存储为Numpy数组的图像是二维数组。Scikit-learn提供的小批量K均值聚类算法吸收了1D数组。因此，我们需要重塑形象。MNIST包含28 x 28像素的图像；因此，一旦我们将它们成形为1D阵列，它们的长度将为784。</p>



<pre class="hljs">
X = x_train.reshape(len(x_train), <span class="hljs-number">-1</span>)
Y = y_train

X = X.astype(float) / <span class="hljs-number">255.</span>
print(X.shape)
print(X[<span class="hljs-number">0</span>].shape)</pre>



<p><strong>聚类</strong></p>



<p>由于数据集的大小，我们使用K-Means的小批量实现。该算法需要更少的时间来拟合数据。MNIST数据集包含整数0-9的图像。因此，让我们通过将聚类数设置为10来开始聚类。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> MiniBatchKMeans
n_digits = len(np.unique(y_test))
print(n_digits)

kmeans = MiniBatchKMeans(n_clusters=n_digits)

kmeans.fit(X)
kmeans.labels_</pre>



<p><strong>分配聚类标签</strong></p>



<p>Mini-Batch K-means是一种无监督的ML方法，这意味着由算法分配的标签指的是每个阵列被分配到的聚类，而不是实际的目标整数。为了解决这个问题，让我们定义一些函数来预测哪个整数对应于每个聚类。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cluster_labels_infer</span><span class="hljs-params">(kmeans, actual_lbls)</span>:</span>
    <span class="hljs-string">"""
    returns: dictionary(clusters assigned to labels)
    """</span>
    infrd_labels = {}
    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(kmeans.n_clusters):
        labels = []
        index = np.where(kmeans.labels_ == n)
        labels.append(actual_lbls[index])

        
        <span class="hljs-keyword">if</span> len(labels[<span class="hljs-number">0</span>]) == <span class="hljs-number">1</span>:
            counts = np.bincount(labels[<span class="hljs-number">0</span>])
        <span class="hljs-keyword">else</span>:
            counts = np.bincount(np.squeeze(labels))

        
        <span class="hljs-keyword">if</span> np.argmax(counts) <span class="hljs-keyword">in</span> infrd_labels:

            infrd_labels[np.argmax(counts)].append(n)
        <span class="hljs-keyword">else</span>:
            
            infrd_labels[np.argmax(counts)] = [n]

    <span class="hljs-keyword">return</span> infrd_labels</pre>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">data_labels_infer</span><span class="hljs-params">(X_labels, clstr_labels)</span>:</span>
    <span class="hljs-string">"""
    Depending on the cluster assignment find the label
    predicted labels are returned for each array
    """</span>
    pred_labels = np.zeros(len(X_labels)).astype(np.uint8)

    <span class="hljs-keyword">for</span> l, clstr <span class="hljs-keyword">in</span> enumerate(X_labels):
        <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> clstr_labels.items():
            <span class="hljs-keyword">if</span> clstr <span class="hljs-keyword">in</span> val:
                pred_labels[i] = key

    <span class="hljs-keyword">return</span> pred_labels</pre>



<p>让我们测试上面写的函数来预测哪个整数对应于每个集群。</p>



<pre class="hljs">
clstr_labels = cluster_labels_infer(kmeans, Y)
input_clusters = kmeans.predict(X)
pred_labels = data_labels_infer(input_clusters, clstr_labels)

print(pred_labels[:<span class="hljs-number">20</span>])
print(Y[:<span class="hljs-number">20</span>])</pre>



<figure class="wp-block-image"><img decoding="async" src="../Images/fabc262997effea0435ae5fba8de2eaf.png" alt="" data-original-src="https://web.archive.org/web/20221206013350im_/https://lh3.googleusercontent.com/NbwD9e82f7PPGTb3SJaL-jFb6vbqLhxoMXg6vIPUALqfX3AdJDp8NFjww_Os-homrWXq2SFLlX3n3h_ldRlVHUaJZ91e0ERongAEr60h6x1xAidQOB6vWoLV5dP8IWL2PDSDaz52=s0"/></figure>



<p><strong>评估和优化聚类算法</strong></p>



<p>利用上面返回的函数，我们可以确定算法的准确性。由于我们使用聚类算法进行分类，准确性成为一个重要的衡量标准。一些度量可以直接应用于集群，而不考虑相关联的标签。使用的指标有:</p>







<p>之前，我们在选择K的特定值时做了假设，但情况可能并不总是如此。让我们将小批量K-Means算法适用于不同的K值，并使用我们的指标评估性能。计算模型指标的函数定义如下。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_metrics</span><span class="hljs-params">(estimator, data, labels)</span>:</span>

    
    print(<span class="hljs-string">'Number of Clusters: {}'</span>.format(estimator.n_clusters))
    print(<span class="hljs-string">'Inertia: {}'</span>.format(estimator.inertia_))
    print(<span class="hljs-string">'Homogeneity: {}'</span>.format(metrics.homogeneity_score(labels, estimator.labels_)))</pre>



<p>现在我们已经定义了指标，让我们为不同数量的集群运行模型。</p>



<pre class="hljs">clusters = [<span class="hljs-number">10</span>, <span class="hljs-number">16</span>, <span class="hljs-number">36</span>, <span class="hljs-number">64</span>, <span class="hljs-number">144</span>, <span class="hljs-number">256</span>]


<span class="hljs-keyword">for</span> n_clusters <span class="hljs-keyword">in</span> clusters:
    estimator = MiniBatchKMeans(n_clusters = n_clusters)
    estimator.fit(X)

    
    calculate_metrics(estimator, X, Y)

    
    cluster_labels = cluster_labels_infer(estimator, Y)
    predicted_Y = data_labels_infer(estimator.labels_, cluster_labels)

    
    print(<span class="hljs-string">'Accuracy: {}n'</span>.format(metrics.accuracy_score(Y, predicted_Y)))</pre>



<figure class="wp-block-image"><img decoding="async" src="../Images/61fed0b76263504a08d2224810455336.png" alt="" data-original-src="https://web.archive.org/web/20221206013350im_/https://lh3.googleusercontent.com/teMsTbY7GWERJYHnpOpl7OZaEjMYs5p-LA0ROLnSaII2OVr9vngF5zYVr6KbYEi0h2DABAh4dEOHjL2lJ01MZ-InULO62dtMwkY8cUljB9DwK_WYeUz5yzZQ-qJgPTur2f6n45Pv=s0"/></figure>



<p>让我们使用256作为分类数在测试集中运行模型，因为它对于这个特定的数字有更高的准确性。</p>



<pre class="hljs">

X_test = x_test.reshape(len(x_test),<span class="hljs-number">-1</span>)

X_test = X_test.astype(float) / <span class="hljs-number">255.</span>

kmeans = MiniBatchKMeans(n_clusters = <span class="hljs-number">256</span>)
kmeans.fit(X)
cluster_labels = cluster_labels_infer(kmeans, Y)

test_clusters = kmeans.predict(X_test)
predicted_labels = data_labels_infer(kmeans.predict(X_test),
cluster_labels)

print(<span class="hljs-string">'Accuracy: {}n'</span>.format(metrics.accuracy_score(y_test,
predicted_labels)))</pre>



<figure class="wp-block-image"><img decoding="async" src="../Images/a90a81d6e24b2a346e689828f69f2cae.png" alt="" data-original-src="https://web.archive.org/web/20221206013350im_/https://lh6.googleusercontent.com/OYwKGjqjRu5A8Au08YAbA9oE3JZKE2vP3-GSR2Ju2khHtWbb-8yNKNKiUMq-kkhBwTcsaFnPaXdC7-1ZpMj4np_ZrW1VsWn9YcV3TRIcqtu1_vCJ0xSlNbUQFkiXLv5e312DQsm8=s0"/></figure>



<p><strong>可视化集群质心</strong></p>



<p>质心是每个聚类中有代表性的点。如果我们处理A，B点，质心就是图上的一个点。因为我们使用长度为784的数组，所以我们的质心也是长度为784的数组。我们可以将这个数组重新整形为28×28像素的图像，并绘制出来。</p>



<pre class="hljs">
kmeans = MiniBatchKMeans(n_clusters = <span class="hljs-number">36</span>)
kmeans.fit(X)


centroids = kmeans.cluster_centers_


images = centroids.reshape(<span class="hljs-number">36</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)
images *= <span class="hljs-number">255</span>
images = images.astype(np.uint8)


cluster_labels = cluster_labels_infer(kmeans, Y)


fig, axs = plt.subplots(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, figsize = (<span class="hljs-number">20</span>, <span class="hljs-number">20</span>))
plt.gray()


<span class="hljs-keyword">for</span> i, ax <span class="hljs-keyword">in</span> enumerate(axs.flat):

 
    <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> cluster_labels.items():
        <span class="hljs-keyword">if</span> i <span class="hljs-keyword">in</span> value:
            ax.set_title(<span class="hljs-string">'Inferred Label: {}'</span>.format(key))

    
    ax.matshow(images[i])
    ax.axis(<span class="hljs-string">'off'</span>)


fig.show()</pre>



<p>这些图形显示了该群集最具代表性的图像。</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/8058701892fd6c28e11d3f2facafc0d5.png" alt="Clustering algorithms - digit classification " class="wp-image-52038" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013350im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Clustering-algorithms-digit-classification-2.png?resize=509%2C512&amp;ssl=1"/><figcaption><em>Result: Most representative image for each cluster</em></figcaption></figure></div>



<p>随着聚类数量和数据点数量的增加，计算时间的相对节省也会增加。只有当集群数量巨大时，节省的时间才会更加明显。只有当聚类数较大时，批量大小对计算时间的影响才更明显。</p>



<p>聚类数的增加降低了小批量K-均值解与K-均值解的相似性。随着集群数量的增加，分区之间的一致性会降低。这意味着最终的分区不同，但质量更接近。</p>



<h2 id="metrics">聚类度量</h2>



<p>有一些评估标准可以用来检查通过您的聚类算法获得的聚类有多好。</p>



<h3>同质性得分</h3>



<p><a href="https://web.archive.org/web/20221206013350/https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score">同质性度量</a>:如果所有聚类只包含属于单一类别的数据点，则聚类结果满足同质性。该指标独立于标签的绝对值。它被定义为:</p>







<p>同质性得分介于0和1之间。低值表示同质性低，1代表完全同质的标签。</p>



<p>当Ypred的知识减少了Ytrue的不确定性时，变小(h → 1)，反之亦然。</p>







<p>完美的标记是同质的。进一步将类分成更多聚类的非完美标记可以是同质的。来自一个聚类中不同类别的样本不满足同质标记。</p>



<h3>完整性分数</h3>



<p>仅当给定类的数据点是同一聚类的一部分时，聚类结果才满足完整性。</p>



<p>这个度量是不对称的，所以从上面的等式中切换label_true和label_pred将返回不同的同质性得分。这同样适用于同质性得分；切换label_true和label_pred将返回完整性分数。</p>



<p>完美的标签是完整的。将所有类成员分配到相同聚类的非完美标记仍然是完整的。如果班级成员分散在不同的集群中，分配就无法完成。</p>



<h3>衡量分数</h3>



<p>V-measure聚类标记给出了一个基本事实。V测度是同质性和完备性之间的调和平均值。</p>







<p>这个度量是对称的。将' label_true '与' label_pred '切换将返回相同的值。当真实情况未知时，该度量有助于计算对同一数据集的两种独立标注分配技术的接受度。</p>



<p>分数范围在0-1之间。1代表完全完整的标签。</p>



<p>示例:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
true_labels = [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
pred_labels = [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]
metrics.homogeneity_score(true_labels, pred_labels)
Output: <span class="hljs-number">1.0</span>

metrics.completeness_score(true_labels, pred_labels)
Output: <span class="hljs-number">1.0</span>

(<span class="hljs-number">1</span> stands <span class="hljs-keyword">for</span> perfectly complete labeling)
metrics.v_measure_score(true_labels, pred_labels)
Output: <span class="hljs-number">1.0</span>
-------------------------------------------------------------------
true_labels = [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
pred_labels = [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]
metrics.homogeneity_score(true_labels, pred_labels)
Output: <span class="hljs-number">0.58688267143572</span>
metrics.completeness_score(true_labels, pred_labels)
Output: <span class="hljs-number">0.7715561736794712</span>
metrics.v_measure_score(true_labels, pred_labels)
Output: <span class="hljs-number">0.6666666666666667</span>
</pre>



<h3>调整后的rand分数</h3>



<p>可以使用Rand指数(RI)计算两个聚类之间的相似性，方法是对所有样本对进行计数，并对真实聚类和预测聚类中分配到不同或相同聚类中的样本对进行计数。然后使用以下方案将RI分数“根据机会调整”为ARI分数。</p>







<p>ARI是一种对称度量:</p>







<p>请参考<a href="https://web.archive.org/web/20221206013350/https://scikit-learn.org/stable/modules/clustering.html#adjusted-rand-score" target="_blank" rel="noreferrer noopener nofollow">链接</a>获取详细的用户指南</p>



<h3>调整后的相互信息分数</h3>



<p>调整后的互信息(AMI)分数是对互信息分数的调整，以考虑机会。这说明了这样一个事实，即无论是否有更多的信息共享，具有许多聚类的2个聚类的互信息得分通常更高。</p>



<p>对于两个集群U和V，AMI为:</p>







<p>有关用户指南，请参考<a href="https://web.archive.org/web/20221206013350/https://scikit-learn.org/stable/modules/clustering.html#mutual-info-score" target="_blank" rel="noreferrer noopener nofollow">链接</a>。<a href="https://web.archive.org/web/20221206013350/https://github.com/AravindR7/Clustering-Algorithms" target="_blank" rel="noreferrer noopener nofollow"> GitHub </a> repo有这篇文章的数据和所有笔记本。</p>



<h2 id="h-summary">摘要</h2>



<p>这篇博客涵盖了聚类、图像压缩、数字分类、客户细分、不同聚类算法的实现和评估指标等最关键的方面。希望你们在这里学到了新东西。</p>



<p>感谢阅读。继续学习！</p>



<h3>参考</h3>



<ol><li><a href="https://web.archive.org/web/20221206013350/https://scikit-learn.org/stable/modules/clustering.html" target="_blank" rel="noreferrer noopener nofollow">https://scikit-learn.org/stable/modules/clustering.html</a></li><li><a href="https://web.archive.org/web/20221206013350/https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68" target="_blank" rel="noreferrer noopener nofollow">https://towards data science . com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d 136 ef 68</a></li><li><a href="https://web.archive.org/web/20221206013350/https://medium.datadriveninvestor.com/k-means-clustering-for-imagery-analysis-56c9976f16b6" target="_blank" rel="noreferrer noopener nofollow">https://medium . datadriveninvestor . com/k-means-clustering-for-imagery-analysis-56c 9976 F16 b 6</a></li><li><a href="https://web.archive.org/web/20221206013350/https://imaddabbura.github.io/post/kmeans-clustering/" target="_blank" rel="noreferrer noopener nofollow">https://imaddabbura.github.io/post/kmeans-clustering/</a></li><li><a href="https://web.archive.org/web/20221206013350/https://www.geeksforgeeks.org/ml-mini-batch-k-means-clustering-algorithm/" target="_blank" rel="noreferrer noopener nofollow">https://www . geeks forgeeks . org/ml-mini-batch-k-means-clustering-algorithm/</a></li><li><a href="https://web.archive.org/web/20221206013350/https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" target="_blank" rel="noreferrer noopener nofollow">https://sci kit-learn . org/stable/modules/generated/sk learn . metrics . v _ measure _ score . html # sk learn . metrics . v _ measure _ score</a></li><li><a href="https://web.archive.org/web/20221206013350/https://www.kaggle.com/niteshyadav3103/customer-segmentation-using-kmeans-hc-dbscan" target="_blank" rel="noreferrer noopener nofollow">https://www . ka ggle . com/niteshyadav 3103/customer-segmentation-using-k means-HC-DBS can</a></li><li><a href="https://web.archive.org/web/20221206013350/https://www.linkedin.com/pulse/gaussian-mixture-models-clustering-machine-learning-cheruku/" target="_blank" rel="noreferrer noopener nofollow">https://www . LinkedIn . com/pulse/Gaussian-mixture-models-clustering-machine-learning-cheruku/</a></li></ol>
        </div>
        
    </div>    
</body>
</html>