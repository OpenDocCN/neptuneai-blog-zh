<html>
<head>
<title>PyTorch Loss Functions: The Ultimate Guide </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>PyTorch损失函数:最终指南</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/pytorch-loss-functions#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/pytorch-loss-functions#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>你的神经网络可以完成很多不同的任务。无论是对数据进行分类，如将动物图片分为猫和狗，回归任务，如预测月收入，还是其他任何事情。每个任务都有不同的输出，需要不同类型的损失函数。</p>



<p>您配置损失函数的方式可以决定算法的性能。通过<a href="https://web.archive.org/web/20230304142301/https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/" target="_blank" rel="noreferrer noopener nofollow">正确配置损失函数</a>，你可以确保你的模型按照你想要的方式工作。</p>



<p>幸运的是，我们可以使用损失函数来充分利用机器学习任务。</p>



<p>在本文中，我们将讨论PyTorch中流行的损失函数，以及如何构建自定义损失函数。一旦你读完了，你应该知道为你的项目选择哪一个。</p>



<section id="blog-intext-cta-block_8c994597d351c8856bd189f396b84253" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>检查如何通过Neptune + PyTorch集成来监控您的PyTorch模型训练并跟踪所有模型构建元数据。</p>
    
    </section>



<h2 id="loss-functions">损失函数有哪些？</h2>



<p>在我们进入PyTorch细节之前，让我们回忆一下损失函数是什么。</p>



<p>损失函数用于测量预测输出和提供的目标值之间的误差。损失函数告诉我们算法模型离实现预期结果有多远。“损失”一词意味着模型因未能产生预期结果而受到的惩罚。</p>



<p>例如，损失函数(姑且称之为<strong> J </strong>)可以采用以下两个参数:</p>



<ul>
<li>预测产量(<strong> y_pred </strong></li>



<li>目标值(<strong> y </strong>)</li>
</ul>



<figure class="wp-block-image aligncenter size-large is-resized"><img decoding="async" src="../Images/88cd419a31ac33228ee0d8e7eebe6a08.png" alt="neural network loss&#10;" class="wp-image-28346" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230304142301im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/neural-network-loss.jpg?resize=768%2C407&amp;ssl=1"/><figcaption class="wp-element-caption"><em>Illustration of a neural network loss</em></figcaption></figure>



<p>此函数将通过比较模型的预测输出和预期输出来确定模型的性能。如果<strong> y_pred </strong>和<strong> y </strong>之间的偏差很大，损失值会很高。</p>



<p>如果偏差很小或者值几乎相同，它将输出一个非常低的损耗值。因此，当模型在所提供的数据集上进行训练时，您需要使用一个损失函数来适当地惩罚模型。</p>



<p>损失函数根据算法试图解决的问题陈述而变化。</p>



<h2 id="add-pytorch-loss-functions">如何添加PyTorch损失函数？</h2>



<p>PyTorch的<strong> torch.nn </strong>模块有多个标准损失函数，你可以在你的项目中使用。</p>



<p>要添加它们，您需要首先导入库:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn</pre>



<p>接下来，定义您想要使用的损失类型。以下是定义平均绝对误差损失函数的方法:</p>



<pre class="hljs">loss = nn.L1Loss()</pre>



<p>添加函数后，您可以使用它来完成您的特定任务。</p>



<h2 id="pytorch-loss-functions">PyTorch中有哪些损失函数？</h2>



<p>广义来说，PyTorch 中的损失<a href="https://web.archive.org/web/20230304142301/https://cs230.stanford.edu/blog/pytorch/" target="_blank" rel="noreferrer noopener nofollow">函数分为两大类:</a><a href="https://web.archive.org/web/20230304142301/https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" target="_blank" rel="noreferrer noopener nofollow">回归损失和分类损失。</a></p>



<p><strong>回归损失函数</strong>在模型预测连续值时使用，如人的年龄。</p>



<p><strong>分类损失函数</strong>在模型预测离散值时使用，例如电子邮件是否为垃圾邮件。</p>



<p><strong>排名损失函数</strong>在模型预测输入之间的相对距离时使用，例如根据电子商务搜索页面上的相关性对产品进行排名。</p>



<p>现在我们将探索PyTorch中不同类型的损失函数，以及如何使用它们:</p>







<h3 id="Mean-Absolute-Error">1.PyTorch平均绝对误差(L1损失函数)</h3>



<pre class="hljs">torch.nn.L1Loss
</pre>



<p><a href="https://web.archive.org/web/20230304142301/https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss" target="_blank" rel="noreferrer noopener nofollow">平均绝对误差</a> (MAE)，也称为L1损失，计算实际值和预测值之间的绝对差的<strong>和的平均值。</strong></p>



<p>它检查一组预测值的误差大小，而不关心它们的正负方向。如果不使用误差的绝对值，那么负值会抵消正值。</p>



<p>Pytorch L1损失表示为:</p>





<p><strong> x </strong>代表实际值，<strong> y </strong>代表预测值。</p>



<p>什么时候可以使用？</p>



<ul>
<li>回归问题，特别是当目标变量的分布有异常值时，如与平均值相差很大的小值或大值。<strong>它被认为对异常值更稳健。</strong></li>
</ul>



<p><strong>例子</strong></p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

input = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-keyword">True</span>)
target = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)

mae_loss = nn.L1Loss()
output = mae_loss(input, target)
output.backward()

print(<span class="hljs-string">'input: '</span>, input)
print(<span class="hljs-string">'target: '</span>, target)
print(<span class="hljs-string">'output: '</span>, output)</pre>



<pre class="hljs">###################### OUTPUT ######################


input:  tensor(<span class="hljs-string">[[ 0.2423,  2.0117, -0.0648, -0.0672, -0.1567],
        [-0.2198, -1.4090,  1.3972, -0.7907, -1.0242],
        [ 0.6674, -0.2657, -0.9298,  1.0873,  1.6587]]</span>, requires_grad=True)
target:  tensor(<span class="hljs-string">[[-0.7271, -0.6048,  1.7069, -1.5939,  0.1023],
        [-0.7733, -0.7241,  0.3062,  0.9830,  0.4515],
        [-0.4787,  1.3675, -0.7110,  2.0257, -0.9578]]</span>)
output:  tensor(<span class="hljs-number">1.2850</span>, grad_fn=&lt;L1LossBackward&gt;)</pre>



<h3 id="Mean-Squared-Error">2.PyTorch均方误差损失函数</h3>



<pre class="hljs">torch.nn.MSELoss
</pre>



<p><a href="https://web.archive.org/web/20230304142301/https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss" target="_blank" rel="noreferrer noopener nofollow">均方误差(MSE) </a>，也称为L2损失，计算实际值和预测值之间的平方差的<strong>平均值。</strong></p>



<p>Pytorch MSE Loss总是输出正的结果，不管实际值和预测值的符号如何。为了提高模型的准确性，您应该尝试减少L2损失-一个完美的值是0.0。</p>



<p>平方意味着较大的错误比较小的错误产生更大的误差。如果分类器偏离100，则误差为10，000。如果相差0.1，误差为0.01。这个<strong>惩罚犯大错</strong>的模特，鼓励小错。</p>



<p>Pytorch L2损失表示为:</p>





<p><strong> x </strong>代表实际值，<strong> y </strong>代表预测值。</p>



<p>什么时候可以使用？</p>



<ul>
<li>MSE是大多数Pytorch回归问题的默认损失函数。</li>
</ul>



<p><strong>例子</strong></p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

input = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-keyword">True</span>)
target = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)
mse_loss = nn.MSELoss()
output = mse_loss(input, target)
output.backward()

print(<span class="hljs-string">'input: '</span>, input)
print(<span class="hljs-string">'target: '</span>, target)
print(<span class="hljs-string">'output: '</span>, output)
</pre>



<pre class="hljs">###################### OUTPUT ######################


input:  tensor(<span class="hljs-string">[[ 0.3177,  1.1312, -0.8966, -0.0772,  2.2488],
        [ 0.2391,  0.1840, -1.2232,  0.2017,  0.9083],
        [-0.0057, -3.0228,  0.0529,  0.4084, -0.0084]]</span>, requires_grad=True)
target:  tensor(<span class="hljs-string">[[ 0.2767,  0.0823,  1.0074,  0.6112, -0.1848],
        [ 2.6384, -1.4199,  1.2608,  1.8084,  0.6511],
        [ 0.2333, -0.9921,  1.5340,  0.3703, -0.5324]]</span>)
output:  tensor(<span class="hljs-number">2.3280</span>, grad_fn=&lt;MseLossBackward&gt;)</pre>



<h3 id="Negative-Log-Likelihood">3.PyTorch负对数似然损失函数</h3>



<pre class="hljs">torch.nn.NLLLoss
</pre>



<p><a href="https://web.archive.org/web/20230304142301/https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss" target="_blank" rel="noreferrer noopener nofollow">负对数似然损失函数</a> (NLL)仅适用于将softmax函数作为输出激活层的模型。<a href="https://web.archive.org/web/20230304142301/https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/" target="_blank" rel="noreferrer noopener nofollow"> Softmax </a>指的是计算层中每个单元的归一化指数函数的激活函数。</p>



<p>Softmax函数表示为:</p>





<p>该函数获取一个大小为<strong> N </strong>、<strong>T3】的输入向量，然后修改这些值，使每个值都在0和1之间。此外，它对输出进行归一化，使得向量的<strong> N </strong>值之和等于1。</strong></p>



<p>NLL使用否定的含义，因为概率(或可能性)在0和1之间变化，并且这个范围内的值的对数是负的。最终，损失值变为正值。</p>



<p>在NLL中，最小化损失函数有助于我们获得更好的输出。负对数似然是从近似最大似然估计(MLE)中检索的。这意味着我们试图最大化模型的对数似然，结果，<a href="https://web.archive.org/web/20230304142301/https://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/" target="_blank" rel="noreferrer noopener nofollow">最小化NLL </a>。</p>



<p>在NLL中，模型因以较小的概率做出正确预测而受到惩罚，因以较高的概率做出预测而受到鼓励。对数做惩罚。</p>



<p>NLL不仅关心预测是否正确，还关心模型是否确定预测得分高。</p>



<p>Pytorch NLL损失表示为:</p>





<p>其中x是输入，y是目标，w是重量，N是批量。</p>



<p>什么时候可以使用？</p>



<ul>
<li>多类分类问题</li>
</ul>



<p><strong>例子</strong></p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn


input = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-keyword">True</span>)

target = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>])

m = nn.LogSoftmax(dim=<span class="hljs-number">1</span>)
nll_loss = nn.NLLLoss()
output = nll_loss(m(input), target)
output.backward()

print(<span class="hljs-string">'input: '</span>, input)
print(<span class="hljs-string">'target: '</span>, target)
print(<span class="hljs-string">'output: '</span>, output)</pre>



<pre class="hljs">

input:  tensor([[ <span class="hljs-number">1.6430</span>, <span class="hljs-number">-1.1819</span>,  <span class="hljs-number">0.8667</span>, <span class="hljs-number">-0.5352</span>,  <span class="hljs-number">0.2585</span>],
        [ <span class="hljs-number">0.8617</span>, <span class="hljs-number">-0.1880</span>, <span class="hljs-number">-0.3865</span>,  <span class="hljs-number">0.7368</span>, <span class="hljs-number">-0.5482</span>],
        [<span class="hljs-number">-0.9189</span>, <span class="hljs-number">-0.1265</span>,  <span class="hljs-number">1.1291</span>,  <span class="hljs-number">0.0155</span>, <span class="hljs-number">-2.6702</span>]], requires_grad=True)
target:  tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>])
output:  tensor(<span class="hljs-number">2.9472</span>, grad_fn=&lt;NllLossBackward&gt;)</pre>



<h3 id="Cross-Entropy">4.PyTorch交叉熵损失函数</h3>



<pre class="hljs">torch.nn.CrossEntropyLoss
</pre>



<p><a href="https://web.archive.org/web/20230304142301/https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noreferrer noopener nofollow">该损失函数</a>计算一组给定事件或随机变量的两个概率分布之间的差异。</p>



<p>它用于计算一个分数，该分数总结了预测值和实际值之间的平均差异。为了增强模型的准确性，您应该尝试最小化得分—交叉熵得分在0到1之间，一个完美的值是0。</p>



<p>其他损失函数，如平方损失，惩罚不正确的预测。<a href="https://web.archive.org/web/20230304142301/https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noreferrer noopener nofollow"> <strong>交叉熵</strong> </a> <strong>对非常自信和错误的人大加惩罚。</strong></p>



<p>与负对数似然损失不同，负对数似然损失不会基于预测置信度进行惩罚，交叉熵会惩罚不正确但有把握的预测，以及正确但不太有把握的预测。</p>



<p>交叉熵函数有很多种变体，其中最常见的类型是<strong>二元交叉熵(BCE) </strong>。BCE损失主要用于二元分类模型；即只有两个类别的模型。</p>



<p>Pytorch交叉熵损失表示为:</p>





<p>其中x是输入，y是目标，w是重量，C是类别数，N是小批量维度。</p>



<p>什么时候可以使用？</p>



<ul>
<li>二进制分类任务，这是Pytorch中的默认损失函数。</li>



<li>创建有信心的模型-预测将是准确的，并且具有更高的概率。</li>
</ul>



<p><strong>例子</strong></p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

input = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-keyword">True</span>)
target = torch.empty(<span class="hljs-number">3</span>, dtype=torch.long).random_(<span class="hljs-number">5</span>)

cross_entropy_loss = nn.CrossEntropyLoss()
output = cross_entropy_loss(input, target)
output.backward()

print(<span class="hljs-string">'input: '</span>, input)
print(<span class="hljs-string">'target: '</span>, target)
print(<span class="hljs-string">'output: '</span>, output)</pre>



<pre class="hljs">

input:  tensor([[ <span class="hljs-number">0.1639</span>, <span class="hljs-number">-1.2095</span>,  <span class="hljs-number">0.0496</span>,  <span class="hljs-number">1.1746</span>,  <span class="hljs-number">0.9474</span>],
        [ <span class="hljs-number">1.0429</span>,  <span class="hljs-number">1.3255</span>, <span class="hljs-number">-1.2967</span>,  <span class="hljs-number">0.2183</span>,  <span class="hljs-number">0.3562</span>],
        [<span class="hljs-number">-0.1680</span>,  <span class="hljs-number">0.2891</span>,  <span class="hljs-number">1.9272</span>,  <span class="hljs-number">2.2542</span>,  <span class="hljs-number">0.1844</span>]], requires_grad=True)
target:  tensor([<span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])
output:  tensor(<span class="hljs-number">1.0393</span>, grad_fn=&lt;NllLossBackward&gt;)</pre>



<h3 id="Hinge-Embedding">5.PyTorch铰链嵌入损失函数</h3>



<pre class="hljs">torch.nn.HingeEmbeddingLoss
</pre>



<p><a href="https://web.archive.org/web/20230304142301/https://pytorch.org/docs/stable/generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss" target="_blank" rel="noreferrer noopener nofollow">铰链嵌入损失</a>用于计算存在输入张量<strong> x </strong>和标签张量<strong> y </strong>时的损失。目标值介于{1，-1}之间，这有利于二进制分类任务。</p>



<p>使用铰链损失函数，只要实际类值和预测类值之间的符号存在差异，就可以给出更大的误差。这激励例子有正确的标志。</p>



<p>铰链嵌入损耗表示为:</p>





<p>什么时候可以使用？</p>



<ul>
<li>分类问题，尤其是在确定两个输入是相似还是不相似时。</li>



<li>学习非线性嵌入或半监督学习任务。</li>
</ul>



<p><strong>例子</strong></p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

input = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-keyword">True</span>)
target = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)

hinge_loss = nn.HingeEmbeddingLoss()
output = hinge_loss(input, target)
output.backward()

print(<span class="hljs-string">'input: '</span>, input)
print(<span class="hljs-string">'target: '</span>, target)
print(<span class="hljs-string">'output: '</span>, output)</pre>



<pre class="hljs">###################### OUTPUT ######################

input:  tensor(<span class="hljs-string">[[ 0.1054, -0.4323, -0.0156,  0.8425,  0.1335],
        [ 1.0882, -0.9221,  1.9434,  1.8930, -1.9206],
        [ 1.5480, -1.9243, -0.8666,  0.1467,  1.8022]]</span>, requires_grad=True)
target:  tensor(<span class="hljs-string">[[-1.0748,  0.1622, -0.4852, -0.7273,  0.4342],
        [-1.0646, -0.7334,  1.9260, -0.6870, -1.5155],
        [-0.3828, -0.4476, -0.3003,  0.6489, -2.7488]]</span>)
output:  tensor(<span class="hljs-number">1.2183</span>, grad_fn=&lt;MeanBackward0&gt;)</pre>



<h3 id="Margin-Ranking">6.PyTorch边际排序损失函数</h3>



<pre class="hljs">torch.nn.MarginRankingLoss
</pre>



<p><a href="https://web.archive.org/web/20230304142301/https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss" target="_blank" rel="noreferrer noopener nofollow">边际排名损失</a>计算一个标准来预测输入之间的相对距离。这不同于其他损失函数，如MSE或交叉熵，它们学习从给定的输入集直接预测。</p>



<p>利用边际排序损失，只要有输入<strong> x1 </strong>、<strong> x2 </strong>，以及标签张量<strong> y </strong>(包含1或-1)，就可以计算损失。</p>



<p>当<strong> y </strong> == 1时，第一次输入将被假定为一个较大的值。它的排名会高于第二个输入。如果<strong> y </strong> == -1，则第二个输入的排名会更高。</p>



<p>Pytorch利润排名损失表示为:</p>





<p>什么时候可以使用？</p>







<p><strong>例子</strong></p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

input_one = torch.randn(<span class="hljs-number">3</span>, requires_grad=<span class="hljs-keyword">True</span>)
input_two = torch.randn(<span class="hljs-number">3</span>, requires_grad=<span class="hljs-keyword">True</span>)
target = torch.randn(<span class="hljs-number">3</span>).sign()

ranking_loss = nn.MarginRankingLoss()
output = ranking_loss(input_one, input_two, target)
output.backward()

print(<span class="hljs-string">'input one: '</span>, input_one)
print(<span class="hljs-string">'input two: '</span>, input_two)
print(<span class="hljs-string">'target: '</span>, target)
print(<span class="hljs-string">'output: '</span>, output)</pre>



<pre class="hljs">


input one:  tensor([<span class="hljs-number">1.7669</span>, <span class="hljs-number">0.5297</span>, <span class="hljs-number">1.6898</span>], requires_grad=True)
input two:  tensor([ <span class="hljs-number">0.1008</span>, <span class="hljs-number">-0.2517</span>,  <span class="hljs-number">0.1402</span>], requires_grad=True)
target:  tensor([<span class="hljs-number">-1.</span>, <span class="hljs-number">-1.</span>, <span class="hljs-number">-1.</span>])
output:  tensor(<span class="hljs-number">1.3324</span>, grad_fn=&lt;MeanBackward0&gt;)</pre>



<h3 id="Triplet-Margin">7.PyTorch三重边界损失函数</h3>



<pre class="hljs">torch.nn.TripletMarginLoss
</pre>



<p><a href="https://web.archive.org/web/20230304142301/https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss" target="_blank" rel="noreferrer noopener nofollow">三重余量损失</a>计算模型中测量三重损失的标准。通过这个损失函数，可以计算出有输入张量、<strong> x1 </strong>、<strong> x2 </strong>、<strong> x3 </strong>以及大于零的裕量时的损失。</p>



<p>一个三联体由<strong> a </strong>(主播)<strong> p </strong>(正例)<strong> n </strong>(反例)组成。</p>



<p>Pytorch三线态余量损失表示为:</p>





<p>什么时候可以使用？</p>







<p><strong>例子</strong></p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
anchor = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>, requires_grad=<span class="hljs-keyword">True</span>)
positive = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>, requires_grad=<span class="hljs-keyword">True</span>)
negative = torch.randn(<span class="hljs-number">100</span>, <span class="hljs-number">128</span>, requires_grad=<span class="hljs-keyword">True</span>)

triplet_margin_loss = nn.TripletMarginLoss(margin=<span class="hljs-number">1.0</span>, p=<span class="hljs-number">2</span>)
output = triplet_margin_loss(anchor, positive, negative)
output.backward()

print(<span class="hljs-string">'anchor: '</span>, anchor)
print(<span class="hljs-string">'positive: '</span>, positive)
print(<span class="hljs-string">'negative: '</span>, negative)
print(<span class="hljs-string">'output: '</span>, output)
</pre>



<pre class="hljs">

anchor:  tensor([[ <span class="hljs-number">0.6152</span>, -<span class="hljs-number">0.2224</span>,  <span class="hljs-number">2.2029</span>,  <span class="hljs-keyword">...</span>, -<span class="hljs-number">0.6894</span>,  <span class="hljs-number">0.1641</span>,  <span class="hljs-number">1.7254</span>],
        [ <span class="hljs-number">1.3034</span>, -<span class="hljs-number">1.0999</span>,  <span class="hljs-number">0.1705</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">0.4506</span>, -<span class="hljs-number">0.2095</span>, -<span class="hljs-number">0.8019</span>],
        [-<span class="hljs-number">0.1638</span>, -<span class="hljs-number">0.2643</span>,  <span class="hljs-number">1.5279</span>,  <span class="hljs-keyword">...</span>, -<span class="hljs-number">0.3873</span>,  <span class="hljs-number">0.9648</span>, -<span class="hljs-number">0.2975</span>],
        <span class="hljs-keyword">...</span>,
        [-<span class="hljs-number">1.5240</span>,  <span class="hljs-number">0.4353</span>,  <span class="hljs-number">0.3575</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">0.3086</span>, -<span class="hljs-number">0.8936</span>,  <span class="hljs-number">1.7542</span>],
        [-<span class="hljs-number">1.8443</span>, -<span class="hljs-number">2.0940</span>, -<span class="hljs-number">0.1264</span>,  <span class="hljs-keyword">...</span>, -<span class="hljs-number">0.6701</span>, -<span class="hljs-number">1.7227</span>,  <span class="hljs-number">0.6539</span>],
        [-<span class="hljs-number">3.3725</span>, -<span class="hljs-number">0.4695</span>, -<span class="hljs-number">0.2689</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">2.6315</span>, -<span class="hljs-number">1.3222</span>, -<span class="hljs-number">0.9542</span>]],
       requires_grad=True)
positive:  tensor([[-<span class="hljs-number">0.4267</span>, -<span class="hljs-number">0.1484</span>, -<span class="hljs-number">0.9081</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">0.3615</span>,  <span class="hljs-number">0.6648</span>,  <span class="hljs-number">0.3271</span>],
        [-<span class="hljs-number">0.0404</span>,  <span class="hljs-number">1.2644</span>, -<span class="hljs-number">1.0385</span>,  <span class="hljs-keyword">...</span>, -<span class="hljs-number">0.1272</span>,  <span class="hljs-number">0.8937</span>,  <span class="hljs-number">1.9377</span>],
        [-<span class="hljs-number">1.2159</span>, -<span class="hljs-number">0.7165</span>, -<span class="hljs-number">0.0301</span>,  <span class="hljs-keyword">...</span>, -<span class="hljs-number">0.3568</span>, -<span class="hljs-number">0.9472</span>,  <span class="hljs-number">0.0750</span>],
        <span class="hljs-keyword">...</span>,
        [ <span class="hljs-number">0.2893</span>,  <span class="hljs-number">1.7894</span>, -<span class="hljs-number">0.0040</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">2.0052</span>, -<span class="hljs-number">3.3667</span>,  <span class="hljs-number">0.5894</span>],
        [-<span class="hljs-number">1.5308</span>,  <span class="hljs-number">0.5288</span>,  <span class="hljs-number">0.5351</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">0.8661</span>, -<span class="hljs-number">0.9393</span>, -<span class="hljs-number">0.5939</span>],
        [ <span class="hljs-number">0.0709</span>, -<span class="hljs-number">0.4492</span>, -<span class="hljs-number">0.9036</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">0.2101</span>, -<span class="hljs-number">0.8306</span>, -<span class="hljs-number">0.6935</span>]],
       requires_grad=True)
negative:  tensor([[-<span class="hljs-number">1.8089</span>, -<span class="hljs-number">1.3162</span>, -<span class="hljs-number">1.7045</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">1.7220</span>,  <span class="hljs-number">1.6008</span>,  <span class="hljs-number">0.5585</span>],
        [-<span class="hljs-number">0.4567</span>,  <span class="hljs-number">0.3363</span>, -<span class="hljs-number">1.2184</span>,  <span class="hljs-keyword">...</span>, -<span class="hljs-number">2.3124</span>,  <span class="hljs-number">0.7193</span>,  <span class="hljs-number">0.2762</span>],
        [-<span class="hljs-number">0.8471</span>,  <span class="hljs-number">0.7779</span>,  <span class="hljs-number">0.1627</span>,  <span class="hljs-keyword">...</span>, -<span class="hljs-number">0.8704</span>,  <span class="hljs-number">1.4201</span>,  <span class="hljs-number">1.2366</span>],
        <span class="hljs-keyword">...</span>,
        [-<span class="hljs-number">1.9165</span>,  <span class="hljs-number">1.7768</span>, -<span class="hljs-number">1.9975</span>,  <span class="hljs-keyword">...</span>, -<span class="hljs-number">0.2091</span>, -<span class="hljs-number">0.7073</span>,  <span class="hljs-number">2.4570</span>],
        [-<span class="hljs-number">1.7506</span>,  <span class="hljs-number">0.4662</span>,  <span class="hljs-number">0.9482</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">0.0916</span>, -<span class="hljs-number">0.2020</span>, -<span class="hljs-number">0.5102</span>],
        [-<span class="hljs-number">0.7463</span>, -<span class="hljs-number">1.9737</span>,  <span class="hljs-number">1.3279</span>,  <span class="hljs-keyword">...</span>,  <span class="hljs-number">0.1629</span>, -<span class="hljs-number">0.3693</span>, -<span class="hljs-number">0.6008</span>]],
       requires_grad=True)
output:  tensor(<span class="hljs-number">1.0755</span>, grad_fn=&lt;MeanBackward0&gt;)
</pre>



<h3 id="Kullback-Leibler-Divergence">8.PyTorch Kullback-Leibler散度损失函数</h3>



<pre class="hljs">torch.nn.KLDivLoss
</pre>



<p><a href="https://web.archive.org/web/20230304142301/https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss" target="_blank" rel="noreferrer noopener nofollow">kull back-lei bler散度</a>，简称KL散度，计算两个概率分布之间的差异。</p>



<p>使用此损失函数，您可以计算在预测概率分布用于估计预期目标概率分布的情况下损失的信息量(以位表示)。</p>



<p>它的输出告诉你两个概率分布的<strong>接近度。如果预测的概率分布与真实的概率分布相差甚远，就会导致巨大的损失。如果KL散度的值为零，则意味着概率分布是相同的。</strong></p>



<p>KL散度的表现就像交叉熵损失一样，在如何处理预测和实际概率方面有一个关键的区别。交叉熵根据预测的置信度惩罚模型，而KL散度不会。KL散度仅评估概率分布预测如何不同于基本事实的分布。</p>



<p>KL发散损失表示为:</p>





<p><strong> x </strong>代表真实标签的概率，<strong> y </strong>代表预测标签的概率。</p>



<p>什么时候可以使用？</p>



<ul>
<li>逼近复杂函数</li>



<li>多类分类任务</li>



<li>如果您希望确保预测的分布类似于定型数据的分布</li>
</ul>



<p><strong>例子</strong></p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

input = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, requires_grad=<span class="hljs-keyword">True</span>)
target = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)

kl_loss = nn.KLDivLoss(reduction = <span class="hljs-string">'batchmean'</span>)
output = kl_loss(input, target)
output.backward()

print(<span class="hljs-string">'input: '</span>, input)
print(<span class="hljs-string">'target: '</span>, target)
print(<span class="hljs-string">'output: '</span>, output)</pre>



<pre class="hljs">###################### OUTPUT ######################

input:  tensor(<span class="hljs-string">[[ 1.4676, -1.5014, -1.5201],
        [ 1.8420, -0.8228, -0.3931]]</span>, requires_grad=True)
target:  tensor(<span class="hljs-string">[[ 0.0300, -1.7714,  0.8712],
        [-1.7118,  0.9312, -1.9843]]</span>)
output:  tensor(<span class="hljs-number">0.8774</span>, grad_fn=&lt;DivBackward0&gt;)</pre>



<h2 id="custom-pytorch-loss-function">如何在PyTorch中创建自定义损失函数？</h2>



<p>PyTorch允许您创建自己的自定义损失函数，并在项目中实现。</p>



<p>以下是你如何创建自己简单的交叉熵损失函数。</p>



<h3>将自定义损失函数创建为python函数</h3>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">myCustomLoss</span><span class="hljs-params">(my_outputs, my_labels)</span>:</span>
    
    my_batch_size = my_outputs.size()[<span class="hljs-number">0</span>]
    
    my_outputs = F.log_softmax(my_outputs, dim=<span class="hljs-number">1</span>)
    
    my_outputs = my_outputs[range(my_batch_size), my_labels]
    
    <span class="hljs-keyword">return</span> -torch.sum(my_outputs)/number_examples</pre>



<p>还可以创建其他<a href="https://web.archive.org/web/20230304142301/https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/comments" target="_blank" rel="noreferrer noopener nofollow">高级PyTorch自定义损耗函数</a>。</p>



<h3>使用类定义创建自定义损失函数</h3>



<p>让我们修改计算两个样本之间相似性的Dice系数，作为二元分类问题的损失函数:</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DiceLoss</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, weight=None, size_average=True)</span>:</span>
        super(DiceLoss, self).__init__()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs, targets, smooth=<span class="hljs-number">1</span>)</span>:</span>

        inputs = F.sigmoid(inputs)

        inputs = inputs.view(<span class="hljs-number">-1</span>)
        targets = targets.view(<span class="hljs-number">-1</span>)

        intersection = (inputs * targets).sum()
        dice = (<span class="hljs-number">2.</span>*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)

        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> - dice</pre>



<h2 id="monitoring-pytorch-loss-functions">如何监控PyTorch损失函数？</h2>



<p>很明显，在训练模型时，需要关注损失函数值，以跟踪模型的性能。随着损失值不断降低，模型不断变好。我们有很多方法可以做到这一点。让我们来看看它们。</p>



<p>为此，我们将训练一个在PyTorch中创建的简单神经网络，该网络将对著名的<a href="https://web.archive.org/web/20230304142301/https://www.kaggle.com/datasets/uciml/iris">虹膜数据集</a>执行分类。</p>



<p>为获取数据集进行必要的导入。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
</pre>



<p>正在加载数据集。</p>



<pre class="hljs">iris = load_iris()
X = iris[<span class="hljs-string">'data'</span>]
y = iris[<span class="hljs-string">'target'</span>]
names = iris[<span class="hljs-string">'target_names'</span>]
feature_names = iris[<span class="hljs-string">'feature_names'</span>]
</pre>



<p>对数据集进行缩放，使均值=0，方差=1，可以快速收敛模型。</p>



<pre class="hljs">scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
</pre>



<p>将数据集以80:20的比例分成训练和测试。</p>



<pre class="hljs">X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">2</span>)
</pre>



<p>为我们的神经网络及其训练做必要的导入。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
plt.style.use(<span class="hljs-string">'ggplot'</span>)
</pre>



<p>定义我们的网络。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PyTorch_NN</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, input_dim, output_dim)</span>:</span>
        super(PyTorch_NN, self).__init__()
        self.input_layer = nn.Linear(input_dim, <span class="hljs-number">128</span>)
        self.hidden_layer = nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>)
        self.output_layer = nn.Linear(<span class="hljs-number">64</span>, output_dim)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = F.relu(self.input_layer(x))
        x = F.relu(self.hidden_layer(x))
        x = F.softmax(self.output_layer(x), dim=<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> x
</pre>



<p>定义用于获得精确度和训练网络的函数。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_accuracy</span><span class="hljs-params">(pred_arr,original_arr)</span>:</span>
    pred_arr = pred_arr.detach().numpy()
    original_arr = original_arr.numpy()
    final_pred= []

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(pred_arr)):
        final_pred.append(np.argmax(pred_arr[i]))
    final_pred = np.array(final_pred)
    count = <span class="hljs-number">0</span>

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(original_arr)):
        <span class="hljs-keyword">if</span> final_pred[i] == original_arr[i]:
            count+=<span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> count/len(final_pred)*<span class="hljs-number">100</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_network</span><span class="hljs-params">(model, optimizer, criterion, X_train, y_train, X_test, y_test, num_epochs)</span>:</span>
    train_loss=[]
    train_accuracy=[]
    test_accuracy=[]

    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):

        
        output_train = model(X_train)

        train_accuracy.append(get_accuracy(output_train, y_train))

        
        loss = criterion(output_train, y_train)
        train_loss.append(loss.item())

        
        optimizer.zero_grad()

        
        loss.backward()

        
        optimizer.step()

        <span class="hljs-keyword">with</span> torch.no_grad():
            output_test = model(X_test)
            test_accuracy.append(get_accuracy(output_test, y_test))

        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:
            print(f<span class="hljs-string">"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.4f}, Train Accuracy: {sum(train_accuracy)/len(train_accuracy):.2f}, Test Accuracy: {sum(test_accuracy)/len(test_accuracy):.2f}"</span>)

    <span class="hljs-keyword">return</span> train_loss, train_accuracy, test_accuracy
</pre>



<p>创建模型、优化器和损失函数对象。</p>



<pre class="hljs">input_dim  = <span class="hljs-number">4</span>
output_dim = <span class="hljs-number">3</span>
learning_rate = <span class="hljs-number">0.01</span>

model = PyTorch_NN(input_dim, output_dim)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
</pre>



<h3>1.监视笔记本电脑中PyTorch的丢失</h3>



<p>现在，您一定注意到了train_network函数中的打印语句，用于监控损失和准确性。这是做这件事的一种方法。</p>



<pre class="hljs">X_train = torch.FloatTensor(X_train)
X_test = torch.FloatTensor(X_test)
y_train = torch.LongTensor(y_train)
y_test = torch.LongTensor(y_test)

train_loss, train_accuracy, test_accuracy = train_network(model=model, optimizer=optimizer, criterion=criterion, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, num_epochs=<span class="hljs-number">100</span>)
</pre>



<p>我们得到这样的输出。</p>





<p>如果需要，我们也可以使用Matplotlib绘制这些值。</p>



<pre class="hljs">fig, (ax1, ax2, ax3) = plt.subplots(<span class="hljs-number">3</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>), sharex=<span class="hljs-keyword">True</span>)

ax1.plot(train_accuracy)
ax1.set_ylabel(<span class="hljs-string">"training accuracy"</span>)

ax2.plot(train_loss)
ax2.set_ylabel(<span class="hljs-string">"training loss"</span>)

ax3.plot(test_accuracy)
ax3.set_ylabel(<span class="hljs-string">"test accuracy"</span>)

ax3.set_xlabel(<span class="hljs-string">"epochs"</span>)
</pre>



<p>我们会看到这样一个图表，表明损失和准确性之间的相关性。</p>





<p>这个方法不错，而且行之有效。但是我们必须记住，我们的问题陈述和模型越复杂，就需要越复杂的监控技术。</p>



<h3>2.使用neptune.ai监控PyTorch损失</h3>



<p>监控度量标准的一个更简单的方法是将它们记录在一个类似Neptune的服务中，并专注于更重要的任务，比如构建和训练模型。</p>



<p>为此，我们只需要遵循几个小步骤。</p>



<p><em>注:最新的代码示例请参考<a href="https://web.archive.org/web/20230304142301/https://docs.neptune.ai/integrations/pytorch/" target="_blank" rel="noreferrer noopener"> Neptune-PyTorch集成文档</a>。</em></p>



<p>首先，让我们<a href="https://web.archive.org/web/20230304142301/https://docs.neptune.ai/setup/installation/" target="_blank" rel="noreferrer noopener">安装需要的东西</a>。</p>




<div class="block-code-snippet  l-padding__top--0 l-padding__bottom--0 block-code-snippet--regular language-py line-numbers">
    <pre><code>pip install neptune-client</code></pre>
</div>





<p>现在让我们<a href="https://web.archive.org/web/20230304142301/https://docs.neptune.ai/logging/new_run/" target="_blank" rel="noreferrer noopener">初始化一次海王星运行</a>。</p>




<div class="block-code-snippet  l-padding__top--0 l-padding__bottom--0 block-code-snippet--regular language-py line-numbers">
    <pre><code>import neptune.new as neptune

run = neptune.init_run()</code></pre>
</div>





<p>我们还可以分配配置变量，例如:</p>




<div class="block-code-snippet  l-padding__top--0 l-padding__bottom--0 block-code-snippet--regular language-py line-numbers">
    <pre><code>run["config/model"] = type(model).__name__
run["config/criterion"] = type(criterion).__name__
run["config/optimizer"] = type(optimizer).__name__</code></pre>
</div>





<p>这是它在用户界面上的样子。</p>





<p>最后，我们可以通过在train_network函数中添加几行来记录我们的损失。请注意与“运行”相关的行。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_network</span><span class="hljs-params">(model, optimizer, criterion, X_train, y_train, X_test, y_test, num_epochs)</span>:</span>
    train_loss=[]
    train_accuracy=[]
    test_accuracy=[]

    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):

        
        output_train = model(X_train)

        acc = get_accuracy(output_train, y_train)
        train_accuracy.append(acc)
        run[<span class="hljs-string">"training/epoch/accuracy"</span>].log(acc)

        
        loss = criterion(output_train, y_train)
        run[<span class="hljs-string">"training/epoch/loss"</span>].log(loss)

        train_loss.append(loss.item())

        
        optimizer.zero_grad()

        
        loss.backward()

        
        optimizer.step()

        <span class="hljs-keyword">with</span> torch.no_grad():
            output_test = model(X_test)
            test_acc = get_accuracy(output_test, y_test)
            test_accuracy.append(test_acc)

            run[<span class="hljs-string">"test/epoch/accuracy"</span>].log(test_acc)

        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:
            print(f<span class="hljs-string">"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.4f}, Train Accuracy: {sum(train_accuracy)/len(train_accuracy):.2f}, Test Accuracy: {sum(test_accuracy)/len(test_accuracy):.2f}"</span>)

    <span class="hljs-keyword">return</span> train_loss, train_accuracy, test_accuracy
</pre>



<p>这是我们在仪表盘上看到的。绝对无缝。</p>





<p>你可以在Neptune应用程序中查看这次运行<a href="https://web.archive.org/web/20230304142301/https://app.neptune.ai/jhabhishek3797/PyTorch-loss-functions/experiments?split=bth&amp;dash=charts&amp;viewId=standard-view" target="_blank" rel="noreferrer noopener nofollow">。不用说，你可以用任何损失函数做到这一点。</a></p>



<p><em>注:最新的代码示例请参考<a href="https://web.archive.org/web/20230304142301/https://docs.neptune.ai/integrations/pytorch/" target="_blank" rel="noreferrer noopener"> Neptune-PyTorch集成文档</a>。</em></p>



<h2 id="h-final-thoughts">最后的想法</h2>



<p>我们研究了PyTorch中最常见的损失函数。您可以选择适合您项目的任何函数，或者创建您自己的自定义函数。</p>



<p>希望这篇文章可以作为你在机器学习任务中使用PyTorch损失函数的快速入门指南。</p>



<p>如果你想更深入地了解这个主题或了解其他损失函数，可以访问<a href="https://web.archive.org/web/20230304142301/https://pytorch.org/docs/stable/nn.html#loss-functions" target="_blank" rel="noreferrer noopener nofollow"> PyTorch官方文档</a>。</p>
        </div>
        
    </div>    
</body>
</html>