# MLflow vs Kubeflow vs neptune.ai:有什么区别？

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/mlflow-vs-kubeflow-vs-neptune-differences>

作为一名数据科学家、ML/DL 研究人员或工程师，您可能会遇到或听说过 MLflow、Kubeflow 和 neptune.ai。由于 ML 和 DL 的大量采用，围绕部署、可伸缩性和可再现性出现了许多问题。因此，MLOps 作为数据工程、DevOps 和机器学习的混合体而诞生。

我们必须为 ML 想出这种新的方法，因为 ML 开发是复杂的。

很自然的问题是为什么？

很自然的，你可能会认为是因为数学，算法，需要的资源(GPU，TPUs，CPUs)，数据，API，库，框架。嗯，有一部分是真的，但不完全是，因为现在大部分都被我们抽象化了。如果我们以拥抱脸或 fast.ai 为例，你只需调用一个特定类的实例，框架/库就会帮你完成所有繁重的工作。再者，随着 **[迁移学习](/web/20230304141727/https://neptune.ai/blog/transfer-learning-guide-examples-for-images-and-text-in-keras)** 的发展，我们不再需要海量的数据来训练一个模型。

那么复杂性从何而来？

复杂性来自几个方面:

1.  ML 本质上是实验性的
2.  它有更多的部分需要考虑，例如:数据(收集、标记、版本化)、模型(培训、评估、版本化和部署)和配置(超参数等等)。
3.  我们如何进行传统软件开发(DevOps)的[范式](/web/20230304141727/https://neptune.ai/blog/data-science-project-management-in-2021-the-new-guide-for-ml-teams)不同于我们如何进行 ML (MLOps)。

随着 [MLOps](/web/20230304141727/https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective) 的成熟，许多工具已经出现并正在出现，以解决工作流的不同部分，这三个工具在 MLOps 工作流中发挥着关键作用，以降低复杂性并解决我们将在后面的章节中讨论的问题。

现在，他们到底是做什么的，他们是如何相互比较的？

在这篇文章中，我们将回答这些问题，甚至更多。以下是我们正在解决的问题:

*   工具
    *   MLflow
    *   库贝弗洛
    *   neptune.ai
*   你应该在什么时候使用哪一个？
*   高级功能对照表

让我们开始吧！

## MLflow

这是一个**开源** MLOps 平台，它诞生于学习大技术的标准，专注于创建可转移的知识、易用性、模块化以及与流行的 ML 库和框架的兼容性。它是为 1 人或 1000 多人的组织设计的。

MLFlow 允许您在本地或远程开发、跟踪(和比较实验)、打包和部署。它处理从数据版本化、模型管理、[实验跟踪](/web/20230304141727/https://neptune.ai/experiment-tracking)到部署的所有事情，除了数据源、标签和流水线。

它几乎是 MLOps 工作流程的万金油和/或瑞士刀。

该平台由 4 个组件组成:

*   物流跟踪
*   MLflow 项目
*   ml 流程模型
*   只是模型注册表

让我们更深入地了解这些组件中每一个的重要性以及它们是如何工作的。

### 物流跟踪

MLflow 跟踪组件是一个 API 和 UI，用于在运行机器学习代码时记录参数、代码版本、指标和输出文件，并在以后可视化和比较结果。MLflow Tracking 允许您使用 Python、REST、R API 和 Java API APIs 来记录和查询实验。

如前所述，MLFlow 允许本地或远程开发，因此实体和工件存储都是可定制的，这意味着您可以在本地或云上保存(AWS s3，GCP 等)

**跟踪中的关键概念**

*   参数:代码的键值输入
*   指标:数值(可以随时更新)
*   标签和注释:关于跑步的信息
*   工件:文件、数据和模型
*   来源:运行了什么代码？
*   版本:代码运行的版本是什么？
*   Run:由 MLFlow 运行的代码实例，其中将记录度量和参数

**跟踪 API**

*   流畅的 MLFlow APIs(高级)
*   MLFlow 客户端(低级)

### MLflow 项目

MLflow 项目是一个独立的执行单元，包含以下内容:

*   密码
*   配置
*   属国
*   数据

将其部署在本地或远程服务器上。

这种格式有助于再现性，并允许创建具有单独项目(或同一项目中的入口点)作为单个步骤的多步骤工作流。

换句话说，MLflow 项目只是一个组织和描述代码的约定，以便让其他数据科学家(或自动化工具)运行它。每个项目只是一个文件目录，或者一个 Git 存储库，包含您的代码。MLflow 可以根据在此目录中放置文件的惯例运行一些项目(例如，conda.yaml 文件被视为 conda 环境)，但您可以通过添加 MLproject 文件来更详细地描述您的项目，该文件基本上是 yaml 格式的文本文件。

### ml 流程模型

MLflow 模型是打包机器学习模型的标准格式，可用于各种下游工具，例如，通过 REST API 或 Apache Spark 上的批处理推理进行实时服务。该格式定义了一个约定，允许您以不同的“风格”保存模型，这些风格可以被不同的下游工具所理解。

口味是使 MLFlow 模型强大的关键概念:它们是部署工具可以用来理解模型的约定。基本上，我们通过创建一种中间格式来抽象模型，该格式将您想要部署到各种环境中的模型打包，就像模型的 docker 文件或 lambda 函数一样，您可以将它部署到所需的环境中，并调用其名为 predict 的评分函数。

### 模型注册表

MLflow 模型注册组件是一个集中式模型存储、一组 API 和 UI，用于协作管理 MLflow 模型的整个生命周期。它提供了模型沿袭(MLflow 实验和运行产生了模型)、模型版本化、阶段转换(例如从阶段转换到生产)和注释。

## Kubeflow

Kubeflow 是一个**开源**项目，它利用 Kubernetes 构建可扩展的 MLOps 管道并编排复杂的工作流。您可以将其视为 Kubernetes 的机器学习(ML)工具包。

***注*** *: Kubernetes(或简称 K8s)是一个容器编排工具。*

现在，出现了两个问题:

1.  为什么要容器化你的 ML 应用？
2.  为什么 K8s 上的 ML？

### 为什么要将 ML 应用容器化

通常团队中不同的人所处的环境是不同的，这些差异可以延伸到:

*   依赖性(库、框架和版本)
*   代码(辅助功能、培训和评估)
*   配置(数据转换、网络架构、批量大小等)
*   软件和硬件

如果两个或两个以上的成员要合作或继承某人的工作并做出改进，这会导致各种各样的问题。

但是通过容器，你可以简单地发送一个 docker 镜像，只要对方在本地或者他的云环境中安装了 docker。他可以轻而易举地重现同样的环境、实验和结果。

#### 集装箱的好处

*   包装:
    *   密码
    *   属国
    *   配置

*   帮助创建 ML envs:
    *   轻量级选手
    *   轻便的
    *   可攀登的

### 为什么 K8s 上的 ML？

正如我之前提到的，K8s 是一个容器编排工具。它实现了容器化应用程序的自动化部署、扩展和管理。但问题在于 k8s 本身的管理，它可能是 heptic。但是现在有不同的 k8s 即服务提供商，例如:AWS EKS、Google GKE 和 Azure AKS。

使用托管 k8s 作为服务，允许 ML 从业者充分利用 k8s 带来的好处，例如:

*   可组合性
*   轻便
*   可量测性
*   或者它已经是公司或团队工作流程的一部分

既然我们已经解决了这个问题，让我们更详细地看看 Kubeflow。

### Kubeflow 组件

**Kubeflow** 由各种项目/工具组成，但这里我们将重点关注 4 个主要项目:

*   笔记本电脑
*   管道
*   培养
*   服务

**笔记本**

Kubeflow 包括创建和管理交互式 Jupyter 笔记本的服务。您可以自定义笔记本电脑部署和计算资源，以满足您的数据科学需求。在本地试验您的工作流，然后在准备就绪时将它们部署到云中。

**管道**

这可能是最著名的项目，也是很多团队选择 kubeflow 的原因。简而言之，kubeflow pipelines 是一个基于 Docker 容器构建和部署可移植、可扩展的机器学习(ML)工作流的平台——它可作为 kubeflow 组件或独立安装使用。

该项目的核心是两个部分:

*   **Pipeline**–是对 ML 工作流的描述，包括工作流中的所有组件以及它们如何以图表的形式组合在一起。管道包括运行管道所需的输入(参数)以及每个管道组件的输入和输出的定义。
*   **管道组件**–是一组独立的用户代码，打包成 Docker 映像，执行管道中的一个步骤。例如，一个组件可以负责数据预处理、数据转换、模型训练等等。

**管道特征**

*   用于管理和跟踪实验、作业和运行的用户界面(UI)。
*   用于安排多步骤 ML 工作流的引擎。
*   用于定义和操作管道和组件的 SDK。
*   使用 SDK 与系统交互的笔记本电脑。
*   可重用性:使您能够重用组件和管道，而不必每次都重新构建。

**训练**

该项目为您提供了不同的培训 ML 模型框架，例如:

*   链式训练
*   MPI 培训
*   MXNet 培训
*   PyTorch 培训
*   作业调度
*   张量流训练(TFJob)

在这里，您可以执行培训工作，监控培训和更多。其中一个很酷的特性实际上是能够容易地定义和利用 kubernetes 副本，它允许您旋转容器映像的多个相同版本。因此，如果一个或多个副本在培训作业期间失败，您的进度不会完全丢失，因为您有另一个版本在并行运行。

**上菜**

说到服务模型，kubeflow 提供了很大的支持。

Kubeflow 有一个名为 KFServing 的组件，它支持 Kubernetes 上的无服务器推理，并为 TensorFlow、XGBoost、scikit-learn、PyTorch 和 ONNX 等常见机器学习(ML)框架提供高性能、高抽象的接口，以解决生产模型服务用例。

KFServing 可用于以下目的:

*   为在任意框架上服务 ML 模型提供 Kubernetes 定制资源定义。
*   封装自动扩展、网络、健康检查和服务器配置的复杂性，为您的 ML 部署带来尖端的服务功能，如 GPU 自动扩展、零扩展和 canary 部署。
*   通过提供现成的预测、预处理、后处理和可解释性，为您的生产 ML 推理服务器提供一个简单、可插入和完整的故事。

此外，除了 KFserving，Kubeflow 还支持 TensorFlow 服务容器将训练好的 TensorFlow 模型导出到 Kubernetes。它还与 Seldon Core(一个用于在 Kubernetes 上部署机器学习模型的开源平台)和 NVIDIA Triton Inference Server 集成，以便在大规模部署 ML/DL 模型时最大化 GPU 利用率。最后，它还支持 [BentoML](https://web.archive.org/web/20230304141727/https://www.bentoml.com/) ，这是一个用于高性能 ML 模型服务的开源平台。它使得为您的 ML 模型构建生产 API 端点变得容易，并支持所有主要的机器学习培训框架，包括 Tensorflow、Keras、PyTorch、XGBoost、scikit-learn 等

但这并没有结束，除此之外，你还可以在 Kubernetes Engine 和 AWS、GCP 或 Azure 上运行 Kubeflow。以 AWS 为例，Kubeflow 与 AWS Sagemaker 进行了集成，允许您充分利用这种托管服务带来的规模优势。

在我看来，我不认为端到端的 ML 平台是一条出路。更多细节你可以稍后阅读这篇 [*文章*](https://web.archive.org/web/20230304141727/https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective) *我会详细解释，一旦你完成这篇文章。*

我相信微服务可以让您更加灵活地将任何新服务插入到您的管道中，或者替换损坏的服务/组件或工具，但 kubeflow 和这些不同的云提供商等集成可以让您构建更强大的解决方案。

## neptune.ai

[![ML Metadata Store](img/aa671c30d75c451bfca8e52a415735f7.png)](https://web.archive.org/web/20230304141727/https://i0.wp.com/neptune.ai/wp-content/uploads/2023/01/Metadata-store.png?ssl=1)

neptune.ai 是 MLOps 的元数据存储，为运行大量实验的研究和生产团队而构建。

它为您提供了一个中心位置来记录、存储、显示、组织、比较和查询机器学习生命周期中生成的所有元数据。

成千上万的 ML 工程师和研究人员使用 Neptune 进行实验跟踪和模型注册，无论是作为个人还是在大型组织的团队内部。

现在，可能会出现一个问题:为什么是元数据存储？

### 为什么是元数据存储？

与 notes、组织协议或开源工具不同，正如我之前提到的，元数据存储是一个集中的地方，但它也是轻量级的、自动的，并由组织(在这种情况下是 Neptune)或社区维护，因此人们可以专注于实际做 ML 而不是元数据簿记。

此外，元数据存储是作为 MLOps 工作流的不同部分/阶段/工具之间的连接器的工具。

#### 元数据存储的好处

*   记录和显示所有元数据类型，包括参数、图像、HTML、音频、视频
*   在仪表板中组织和比较实验
*   观看模型训练直播
*   让别人(不是你)来维护和备份它(元数据存储)
*   无需额外努力即可调试和比较实验和模型
*   数据库和仪表板都可以通过数千次实验进行扩展
*   帮助简化从研究到生产的过渡
*   在其基础上轻松构建定制库/工具

既然我们已经解决了这个问题，让我们更详细地看看海王星。

### 海王星组件

**海王星**由 3 个主要部件组成:

*   数据版本化
*   实验跟踪
*   模型注册表

#### 数据版本化

版本控制系统帮助开发人员管理源代码的变更。而数据版本控制是一组工具和过程，其试图使版本控制过程适应数据世界，以管理与数据集相关的模型的改变，反之亦然。换句话说，该功能有助于跟踪我们用来训练模型的特定版本的数据集或数据集的子集，从而实现并促进实验的可重复性。

借助 Neptun e 中的[数据版本功能，您可以:](https://web.archive.org/web/20230304141727/https://docs.neptune.ai/how-to-guides/data-versioning)

*   在使用工件的模型训练运行中跟踪数据集版本
*   查询以前运行的数据集版本，以确保您在相同的数据集版本上进行训练
*   根据训练的数据集版本对 Neptune 跑步进行分组

#### 实验跟踪

海王星的这个特性帮助你在一个地方组织你的 ML 实验:

*   记录和显示度量、参数、图像和其他 ML 元数据
*   无需额外努力即可搜索、分组和比较实验
*   在实验运行时实时可视化和调试实验
*   通过发送持久链接来共享结果
*   以编程方式查询实验元数据

#### 模型注册表

这个特性允许您通过在[中央模型注册中心](https://web.archive.org/web/20230304141727/https://docs.neptune.ai/how-to-guides/model-registry)中组织您的模型来控制您的模型开发，使它们可重复和可追踪。

这意味着您可以在模型开发到部署的过程中对模型进行版本化、存储、组织和查询。保存的元数据包括:

*   数据集、代码、环境配置版本
*   参数和评估指标
*   模型二进制文件、描述和其他细节
*   测试集预测预览和模型解释

此外，它还使地理位置接近或远离的团队能够在实验中进行协作，因为您的团队记录到 Neptune 的所有内容都可以被每个团队成员自动访问。所以再现性不再是问题。

您可以通过 API 访问模型训练运行信息，如代码、参数、模型二进制文件或其他对象。

使用 Neptune，您可以用一个真实的来源来代替文件夹结构、电子表格和命名约定，在这个来源中，您的所有模型构建元数据都是有组织的，易于查找、共享和查询。

该工具通过记录模型开发过程中发生的所有事情，让您能够控制模型和实验。

这相当于减少了寻找配置和文件、上下文切换、非生产性会议所花费的时间，并将更多时间用于高质量的 ML 工作。有了 Neptune，您不必实现记录器、维护数据库或仪表板，或者教人们如何使用它们。

通过跟踪你已经尝试过的所有想法以及你使用了多少资源，你可以充分利用你的计算资源。实时监控您的 ML 运行，并在运行失败或模型停止收敛时快速做出反应。

最后，Neptune 允许您通过对所有的模型训练运行进行版本化来构建可再现的、兼容的和可追踪的模型，并且还允许您知道谁构建了生产模型，使用了哪些数据集和参数，以及它在任何时候是如何执行的。

## 现在，只要告诉我哪一个和什么时候使用它

### MLflow

如果您想要一个由**开源**社区支持的 MLOps 平台，它允许您:

*   跟踪、可视化和比较实验元数据
*   允许您可视化和比较实验结果的用户界面
*   开发(打包和部署)模型
*   允许您创建多步骤工作流的平台(很像 Kubeflow 管道，但不使用容器)

以及一种抽象模型的方法，从而可以轻松地将其部署到各种环境中，那么 MLflow 就是一种方法。

### Kubeflow

如果您想要一个端到端**开源**平台，让您能够:

*   管理和设置不同团队的资源配额，以及在本地或云中编码、运行和跟踪实验元数据
*   能够使用跨越整个 ML 生命周期(从数据收集一直到模型构建和部署)的组件构建可重复的管道，那么 kubeflow 就是一条路
*   允许您可视化管道和实验元数据以及比较实验结果的 UI。
*   内置笔记本服务器服务

最后，您的 K8s 环境可能资源有限，但 K8s 和 kubeflow 都与 AWS Sagemaker 集成，支持从 Kubernetes 或 Kubeflow 本机跨 ML 工作流使用完全托管的 Sagemaker ML 工具，这意味着您可以利用它的功能来扩展资源(即 GPU 实例)和服务(即 Sagemaker 地面真相、模型监视器等)。

这消除了您手动管理和优化基于 Kubernetes 的 ML 基础设施的需要，同时仍然保持对编排和灵活性的控制。

### neptune.ai

如果你想要集中的地方:

*   存储所有元数据(数据版本、实验跟踪和模型注册)
*   它具有直观且可定制的用户界面，允许您可视化和比较实验结果，并按照您的意愿排列显示的数据
*   拥有一个项目 wiki，有助于分享关于项目进度、运行和数据探索笔记本的报告、见解和评论
*   笔记本检查点(用于 Jupyter)
*   它可以与业界大多数最佳工具以及 MLOps 平台轻松无缝地集成
    *   例如，Neptune 集成了 MLflow 和许多其他库、工具和 ML/DL 框架。
    *   如果集成不可用，您可以将其添加到笔记本中。py 项目或容器化的 ML 项目(如果您使用的是 Kubernetes 或 Kubeflow ),由您喜欢的库、工具和框架提供支持，例如使用 python 客户端的 Pytorch。

最后，如果您想要一个完全托管的服务，或者如果您想要更多的控制，有服务器版本，那么 Neptune 是一个不错的选择。

## 高级功能对照表

|  | MLflow | Kubeflow | neptune.ai |
| --- | --- | --- | --- |
|  |  |  |  |
|  |  |  | 

对个人免费，非营利和教育研究
[对团队付费](/web/20230304141727/https://neptune.ai/pricing)

 |
|  |  |  |  |
|  |  | 

有一条学习曲线

 |  |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |

**结论**

## 最终，选择权在你手中，这取决于你的要求和需求，但我想让你知道，这不是一个非此即彼的情况。这些工具并不互相排斥，您可以根据自己的需求和愿望混合搭配它们。

它可以是 Kubeflow 与 MLflow 或 Kubeflow 与 neptune.ai 以及 MLflow 与 neptune.ai。

让我详细说明一下，例如 Kubeflow 和 MLflow 或 Kubeflow 和 Neptune，在这两种情况下，Kubeflow 可能没有直接集成，但您可以将 MLflow 或 Neptune 添加到管道组件(也称为容器化应用程序)。

现在当涉及到 MLflow 和海王星的时候就容易多了，因为海王星和 MLflow 是一体的。

因此，您不会只使用一种工具。

我们已经兜了一圈，下面是一大堆参考资料供你查阅和消化。玩得开心！

谢谢大家！

**参考文献**

## MLflow(流动)

### 忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈

### 海王星啊

### neptune.ai