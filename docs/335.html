<html>
<head>
<title>Exploratory Data Analysis for Natural Language Processing: A Complete Guide to Python Tools </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>自然语言处理的探索性数据分析:Python工具完全指南</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>探索性数据分析是任何机器学习工作流中最重要的部分之一，自然语言处理也不例外。但是<strong>你应该选择哪些工具</strong>来高效地探索和可视化文本数据呢？</p>



<p>在这篇文章中，我们将<strong>讨论和实现几乎所有的主要技术</strong>，你可以用它们来理解你的文本数据，并给你一个完成工作的Python工具的完整之旅。</p>



<h2>开始之前:数据集和依赖项</h2>



<p>在本文中，我们将使用来自Kaggle的百万新闻标题数据集。如果您想一步一步地进行分析，您可能需要安装以下库:</p>



<pre class="hljs">pip install \
   pandas matplotlib numpy \
   nltk seaborn sklearn gensim pyldavis \
   wordcloud textblob spacy textstat</pre>



<p>现在，我们可以看看数据。</p>



<pre class="hljs">news= pd.read_csv(<span class="hljs-string">'data/abcnews-date-text.csv'</span>,nrows=<span class="hljs-number">10000</span>)
news.head(<span class="hljs-number">3</span>)</pre>



<figure class="wp-block-image"><img src="../Images/64f70a78d45e20621f9bd6e659981a31.png" alt="jupyter output" data-lazy-src="https://web.archive.org/web/20220928195456/https://i2.wp.com/neptune.ai/wp-content/uploads/output1.png?fit=979%2C146&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220928195456im_/https://i2.wp.com/neptune.ai/wp-content/uploads/output1.png?fit=979%2C146&amp;ssl=1"/><noscript><img data-lazy-fallback="1" src="../Images/64f70a78d45e20621f9bd6e659981a31.png" alt="jupyter output" data-original-src="https://web.archive.org/web/20220928195456im_/https://i2.wp.com/neptune.ai/wp-content/uploads/output1.png?fit=979%2C146&amp;ssl=1"/></noscript></figure>



<p>数据集只包含两列，发布日期和新闻标题。</p>



<p>为了简单起见，我将探索这个数据集中的前<strong> 10000行</strong>。因为标题是按<em>发布日期</em>排序的，所以实际上从2003年2月19日<em>到2003年4月7日</em>有<strong>两个月。</strong></p>



<p>好了，我想我们已经准备好开始我们的数据探索了！</p>





<h2 id="text-statistics">分析文本统计</h2>



<p>文本统计可视化是简单但非常有洞察力的技术。</p>



<p>它们包括:</p>


<div class="custom-point-list">
<ul><li>词频分析，</li><li>句子长度分析，</li><li>平均字长分析，</li><li>等等。</li></ul>
</div>


<p>这些确实有助于<strong>探索文本数据的基本特征</strong>。</p>



<p>为此，我们将主要使用<strong>直方图</strong>(连续数据)和<strong>条形图</strong>(分类数据)。</p>



<p>首先，我将看看每个句子中出现的字符数。这可以让我们大致了解新闻标题的长度。</p>



<pre class="hljs">news[<span class="hljs-string">'headline_text'</span>].str.len().hist()
</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/1-4-top-non-stopwords-barchart-36267acc-a418-4a5f-a3ba-67a3b51dde12/b57bc536-8cec-46a7-918c-60fba6f2c83d" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_604b40f537327" class="separator separator-10"/>



<p>直方图显示，新闻标题的长度在10到70个字符之间，通常在25到55个字符之间。</p>



<p>现在，我们将转到单词级别的数据探索。让我们画出每个新闻标题中出现的字数。</p>



<pre class="hljs">text.str.split().\
    map(<span class="hljs-keyword">lambda</span> x: len(x)).\
    hist()</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/1-1-word-number-histogram-aff0bde6-6ad1-45cf-a8f8-68a2ad7da521/e4cee3db-8d07-4dc6-8584-063b11e76809" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d385a3ff9b" class="separator separator-10"/>



<p>很明显，新闻标题的字数在2到12之间，大部分在5到7之间。</p>



<p>接下来，让我们检查一下每个句子的平均单词长度。</p>



<pre class="hljs">news[<span class="hljs-string">'headline_text'</span>].str.split().\
   apply(<span class="hljs-keyword">lambda</span> x : [len(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x]). \
   map(<span class="hljs-keyword">lambda</span> x: np.mean(x)).hist()</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/1-2-word-length-histogram-6204616c-6314-4ddd-9398-fe73415c09ff/e5c67525-6a16-4751-b4c5-4c64c1ad2730" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d385e3ff9c" class="separator separator-10"/>



<p>平均单词长度在3到9之间，5是最常见的长度。这是否意味着人们在新闻标题中使用了非常短的单词？让我们找出答案。</p>



<p>这可能不正确的一个原因是停用词。停用词是在任何语言中最常用的词，如<em>、【the】、【a】、</em>等。由于这些单词的长度可能很小，这些单词可能会导致上图向左倾斜。</p>



<p>分析停用词的数量和类型可以让我们更好地了解数据。</p>



<p>要获得包含停用词的语料库，您可以使用<a href="https://web.archive.org/web/20220928195456/https://www.nltk.org/" target="_blank" rel="noreferrer noopener nofollow"> nltk库</a>。Nltk包含许多语言的停用词。因为我们只处理英语新闻，所以我将从语料库中过滤掉英语停用词。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> nltk
nltk.download(<span class="hljs-string">'stopwords'</span>)
stop=set(stopwords.words(<span class="hljs-string">'english'</span>))</pre>



<p>现在，我们将创建语料库。</p>



<pre class="hljs">corpus=[]
new= news[<span class="hljs-string">'headline_text'</span>].str.split()
new=new.values.tolist()
corpus=[word <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> new <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> i]

<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict
dic=defaultdict(int)
<span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> corpus:
    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> stop:
        dic[word]+=<span class="hljs-number">1</span></pre>



<p>并绘制顶部停用词。</p>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/1-3-top-stopwords-barchart-b953763c-3fea-4331-bff0-429411793e5f/5c0fca05-ba07-4564-a02e-c44b08bfb8cb" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38623ff9d" class="separator separator-10">我们可以清楚地看到，像“to”、“in”和“for”这样的停用词在新闻标题中占主导地位。</p>



<p>现在我们知道了哪些停用词在我们的文章中频繁出现，让我们检查一下除了这些停用词之外还有哪些词频繁出现。</p>



<p>我们将使用集合库中的<a href="https://web.archive.org/web/20220928195456/https://pymotw.com/2/collections/counter.html" target="_blank" rel="noreferrer noopener">计数器函数</a>来计数并存储元组列表中每个单词的出现次数。这是一个<strong>非常有用的函数，当我们在自然语言处理中处理单词级分析</strong>时。</p>



<p>We will use the <a href="https://web.archive.org/web/20220928195456/https://pymotw.com/2/collections/counter.html" target="_blank" rel="noreferrer noopener">counter function</a> from the collections library to count and store the occurrences of each word in a list of tuples. This is a <strong>very useful function when we deal with word-level analysis</strong> in natural language processing.</p>



<pre class="hljs">counter=Counter(corpus)
most=counter.most_common()

x, y= [], []
<span class="hljs-keyword">for</span> word,count <span class="hljs-keyword">in</span> most[:<span class="hljs-number">40</span>]:
    <span class="hljs-keyword">if</span> (word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop):
        x.append(word)
        y.append(count)
        
sns.barplot(x=y,y=x)</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/1-4-top-non-stopwords-barchart-36267acc-a418-4a5f-a3ba-67a3b51dde12/b57bc536-8cec-46a7-918c-60fba6f2c83d" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38673ff9e" class="separator separator-10">哇！“美国”、“伊拉克”和“战争”占据了过去15年的头条新闻。</p>



<p>这里的“我们”可以指美国或我们(你和我)。us不是一个停用词，但是当我们观察图表中的其他单词时，它们都与美国有关——伊拉克战争，这里的“US”可能指的是美国。</p>



<p>Ngram探索</p>



<h2 id="ngram">n元语法就是n个单词的连续序列。例如《河岸》、《三个火枪手》等。如果字数为二，则称之为bigram。对于3个单词，它被称为三元模型等等。</h2>



<p>查看最常见的n元语法可以让你更好地理解使用该单词的上下文。</p>



<p>为了实现n元语法，我们将使用来自<em> nltk.util </em>的<em>n元语法</em>函数。例如:</p>



<p>现在我们知道了如何创建n元语法，让我们把它们可视化。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> nltk.util <span class="hljs-keyword">import</span> ngrams
list(ngrams([<span class="hljs-string">'I'</span> ,<span class="hljs-string">'went'</span>,<span class="hljs-string">'to'</span>,<span class="hljs-string">'the'</span>,<span class="hljs-string">'river'</span>,<span class="hljs-string">'bank'</span>],<span class="hljs-number">2</span>))</pre>



<figure class="wp-block-image"><img src="../Images/4acdde3341baf692da7b27f30d0529d8.png" alt="bigrams" data-lazy-src="https://web.archive.org/web/20220928195456/https://i1.wp.com/neptune.ai/wp-content/uploads/output2.png?fit=691%2C131&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220928195456im_/https://i1.wp.com/neptune.ai/wp-content/uploads/output2.png?fit=691%2C131&amp;ssl=1"/><noscript><img data-lazy-fallback="1" src="../Images/4acdde3341baf692da7b27f30d0529d8.png" alt="bigrams" data-original-src="https://web.archive.org/web/20220928195456im_/https://i1.wp.com/neptune.ai/wp-content/uploads/output2.png?fit=691%2C131&amp;ssl=1"/></noscript></figure>



<p>为了构建我们的词汇表，我们将使用<em> Countvectorizer。</em>  <em> Countvectorizer </em>是一种简单的方法，用于对语料库进行记号化、矢量化，并以适当的形式表示出来。在<a href="https://web.archive.org/web/20220928195456/https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" target="_blank" rel="noreferrer noopener"><em>sk learn . feature _ engineering . text</em></a>中有</p>



<p>有了这些，我们将分析新闻标题中的重要人物。</p>



<p>top _ n _ bigrams = get _ top _ ngram(news[' headline _ text ']，2)[:10] x，y=map(list，zip(* top _ n _ bigrams))SNS . bar plot(x = y，y=x)</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_top_ngram</span><span class="hljs-params">(corpus, n=None)</span>:</span>
    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=<span class="hljs-number">0</span>) 
    words_freq = [(word, sum_words[<span class="hljs-number">0</span>, idx]) 
                  <span class="hljs-keyword">for</span> word, idx <span class="hljs-keyword">in</span> vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-keyword">True</span>)
    <span class="hljs-keyword">return</span> words_freq[:<span class="hljs-number">10</span>]</pre><p>



top_n_bigrams=get_top_ngram(news[‘headline_text’],2)[:10]
x,y=map(list,zip(*top_n_bigrams))
sns.barplot(x=y,y=x)



</p>


<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/2-0-top-ngrams-barchart-671a187d-c3b4-475a-bc9e-8aa6c937923b/c427446f-7b0e-4621-b791-47b0fd31a39e" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d386e3ff9f" class="separator separator-10">我们可以观察到，与战争有关的“反战”、“阵亡”等连词占据了新闻标题。</p>



<p>三元模型怎么样？</p>



<p>我们可以看到，这些三元组中有许多是由<em>、</em>、<em>、【反战抗议】组合而成的。</em> <strong>这意味着我们应该努力清理数据</strong>，看看我们能否将这些同义词合并成一个干净的令牌。</p>



<pre class="hljs">top_tri_grams=get_top_ngram(news[<span class="hljs-string">'headline_text'</span>],n=<span class="hljs-number">3</span>)
x,y=map(list,zip(*top_tri_grams))
sns.barplot(x=y,y=x)</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/2-0-top-ngrams-barchart-671a187d-c3b4-475a-bc9e-8aa6c937923b/c427446f-7b0e-4621-b791-47b0fd31a39e" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38713ffa0" class="separator separator-10">pyLDAvis主题建模探索</p>



<p>主题建模是<strong>使用无监督学习技术提取文档集合中出现的主要主题的过程。</strong></p>



<h2 id="topic-modeling">潜在狄利克雷分配(LDA)是一种易于使用且高效的主题建模模型。每个文档由主题的分布来表示，每个主题由词的分布来表示。</h2>



<p>一旦我们按主题对文档进行分类，我们就可以对每个主题或主题组进行进一步的<strong>数据探索。</strong></p>



<p>但是在进入主题建模之前，我们必须对数据进行一些预处理。我们将:</p>



<p><strong><em/></strong>:将句子转换成一列标记或单词的过程。</p>



<p><strong> <em>删除停用词</em> </strong></p>


<div class="custom-point-list">
<ul><li><em><strong/></em>:将每个单词的屈折形式缩减为一个共同的基或根。</li><li><em> <strong>转换为单词包</strong> </em>:单词包是一个字典，其中的键是单词(或ngrams/tokens)，值是每个单词在语料库中出现的次数。</li><li>使用NLTK，您可以轻松地进行标记化和词条化:</li><li>现在，让我们使用gensim创建单词袋模型</li></ul>
</div>


<p>我们最终可以创建LDA模型:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> nltk
nltk.download(<span class="hljs-string">'punkt'</span>)
nltk.download(<span class="hljs-string">'wordnet'</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess_news</span><span class="hljs-params">(df)</span>:</span>
    corpus=[]
    stem=PorterStemmer()
    lem=WordNetLemmatizer()
    <span class="hljs-keyword">for</span> news <span class="hljs-keyword">in</span> df[<span class="hljs-string">'headline_text'</span>]:
        words=[w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> word_tokenize(news) <span class="hljs-keyword">if</span> (w <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop)]
        
        words=[lem.lemmatize(w) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> words <span class="hljs-keyword">if</span> len(w)&gt;<span class="hljs-number">2</span>]
        
        corpus.append(words)
    <span class="hljs-keyword">return</span> corpus

corpus=preprocess_news(news)</pre>



<p>题目0表示与伊拉克战争和警察有关的东西。主题3显示澳大利亚卷入伊拉克战争。</p>



<pre class="hljs">dic=gensim.corpora.Dictionary(corpus)
bow_corpus = [dic.doc2bow(doc) <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> corpus]</pre>



<p>您可以打印所有的主题并尝试理解它们，但是有一些工具可以帮助您更有效地进行数据探索。一个这样的工具是<a href="https://web.archive.org/web/20220928195456/https://github.com/bmabey/pyLDAvis" target="_blank" rel="noreferrer noopener nofollow"> pyLDAvis </a>，它<strong>交互地可视化LDA的结果。</strong></p>



<pre class="hljs">lda_model = gensim.models.LdaMulticore(bow_corpus, 
                                   num_topics = <span class="hljs-number">4</span>, 
                                   id2word = dic,                                    
                                   passes = <span class="hljs-number">10</span>,
                                   workers = <span class="hljs-number">2</span>)
lda_model.show_topics()</pre>



<figure class="wp-block-image"><img src="../Images/005b01bd60af2408641de70308bb474c.png" alt="lda topics results" data-lazy-src="https://web.archive.org/web/20220928195456/https://i0.wp.com/neptune.ai/wp-content/uploads/output3.png?fit=764%2C172&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220928195456im_/https://i0.wp.com/neptune.ai/wp-content/uploads/output3.png?fit=764%2C172&amp;ssl=1"/><noscript><img data-lazy-fallback="1" src="../Images/005b01bd60af2408641de70308bb474c.png" alt="lda topics results" data-original-src="https://web.archive.org/web/20220928195456im_/https://i0.wp.com/neptune.ai/wp-content/uploads/output3.png?fit=764%2C172&amp;ssl=1"/></noscript></figure>



<p>The topic 0 indicates something related to the Iraq war and police. Topic 3 shows the involvement of Australia in the Iraq war.</p>



<p>在左侧，每个圆圈的<strong>区域代表主题</strong>相对于语料库的重要性。因为有四个主题，所以我们有四个圈。</p>



<pre class="hljs">pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)
vis</pre>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20220928195456im_/https://neptune.ai/wp-content/uploads/pyldavis-1.mp4"/></figure>


<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/3-0-topic-modeling-vis-ddd6a861-62d0-40cb-9207-ebd5b47d74d0/e7cb3e68-cc7b-443e-992b-414640a55a0b" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38783ffa1" class="separator separator-10">圆心之间的<strong>距离表示主题之间的相似度</strong>。在这里，您可以看到主题3和主题4重叠，这表明主题更加相似。</p>


<div class="custom-point-list">
<ul><li>在右侧，每个话题的<strong>直方图显示了前30个相关词</strong>。例如，在主题1中，最相关的单词是警察、新闻、五月、战争等</li><li>所以在我们的案例中，我们可以在新闻标题中看到大量与战争相关的词汇和话题。</li><li>Wordcloud</li></ul>
</div>


<p>Wordcloud是一种表现文本数据的好方法。出现在单词云中的每个单词的大小和颜色表明了它的频率或重要性。</p>






<h2 id="wordcloud">用python 创建<a href="https://web.archive.org/web/20220928195456/https://amueller.github.io/word_cloud/index.html" target="_blank" rel="noreferrer noopener"> wordcloud很容易，但是我们需要语料库形式的数据。幸运的是，我在上一节中准备了它。</a></h2>



<p>Wordcloud is a great way to represent text data. The size and color of each word that appears in the wordcloud indicate it’s frequency or importance.</p>



<p>同样，您可以看到与战争相关的术语被突出显示，这表明这些词在新闻标题中频繁出现。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud, STOPWORDS
stopwords = set(STOPWORDS)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_wordcloud</span><span class="hljs-params">(data)</span>:</span>
    wordcloud = WordCloud(
        background_color=<span class="hljs-string">'white'</span>,
        stopwords=stopwords,
        max_words=<span class="hljs-number">100</span>,
        max_font_size=<span class="hljs-number">30</span>,
        scale=<span class="hljs-number">3</span>,
        random_state=<span class="hljs-number">1</span>)
   
    wordcloud=wordcloud.generate(str(data))

    fig = plt.figure(<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">12</span>))
    plt.axis(<span class="hljs-string">'off'</span>)

    plt.imshow(wordcloud)
    plt.show()

show_wordcloud(corpus)</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/4-0-wordclouds-853dfded-4d17-4f37-83e4-15ec53f74e60/5833b046-3cf9-4c0f-8fbf-4a5933da924e" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38833ffa2" class="separator separator-10">有<strong>许多参数可以调整</strong>。一些最突出的例子是:</p>



<p><strong><em/></strong>:被阻止在图像中出现的一组单词。</p>



<p><strong> <em> max_words </em> </strong>:显示最大字数。</p>


<div class="custom-point-list">
<ul><li><strong> <em> max_font_size </em> </strong>:最大字体大小。</li><li>有更多的选择来创建美丽的单词云。更多详情可以参考这里。</li><li>情感分析</li></ul>
</div>


<p>情感分析是一项非常常见的自然语言处理任务，我们在其中确定文本是正面、负面还是中性的。这对于发现与评论相关的情感非常有用，评论可以让我们从文本数据中获得一些有价值的见解。</p>



<h2 id="sentiment-analysis">有很多项目会帮助你用python做情感分析。我个人喜欢<a href="https://web.archive.org/web/20220928195456/https://github.com/sloria/TextBlob" target="_blank" rel="noreferrer noopener"> TextBlob </a>和<a href="https://web.archive.org/web/20220928195456/https://github.com/cjhutto/vaderSentiment" target="_blank" rel="noreferrer noopener">维德的情调。</a></h2>



<p>文本blob</p>






<p>Textblob是构建在nltk之上的python库。它已经存在了一段时间，使用起来非常简单方便。</p>



<h3 id="textblob">TextBlob的情感函数返回两个属性:</h3>



<p><strong> <em>极性:</em> </strong>是位于<em> [-1，1] </em>范围内的浮点数，其中<strong> 1表示正</strong>语句，<strong> -1表示负</strong>语句。</p>



<p><strong> <em>主观性:</em> </strong>指<strong>某人的判断是如何被个人观点</strong>和感受所塑造的。主观性表示为位于[0，1]范围内的浮点值。</p>


<div class="custom-point-list">
<ul><li>我将在我们的新闻标题上运行这个功能。</li><li>TextBlob声称文本<em>“100人在伊拉克丧生”</em>是负面的，不是一种观点或感觉，而是一种事实陈述。我想我们可以同意TextBlob的观点。</li></ul>
</div>


<p>既然我们知道了如何计算这些情感分数<strong>，我们就可以使用直方图来可视化它们，并进一步探索数据。</strong></p>



<pre class="hljs"><span class="hljs-keyword">from</span> textblob <span class="hljs-keyword">import</span> TextBlob
TextBlob(<span class="hljs-string">'100 people killed in Iraq'</span>).sentiment</pre>



<figure class="wp-block-image"><img src="../Images/3d0ee54fa6b0c84325d781d3a70b3eff.png" alt="textblob sentiment" data-lazy-src="https://web.archive.org/web/20220928195456/https://i0.wp.com/neptune.ai/wp-content/uploads/output4.png?fit=787%2C52&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220928195456im_/https://i0.wp.com/neptune.ai/wp-content/uploads/output4.png?fit=787%2C52&amp;ssl=1"/><noscript><img data-lazy-fallback="1" src="../Images/3d0ee54fa6b0c84325d781d3a70b3eff.png" alt="textblob sentiment" data-original-src="https://web.archive.org/web/20220928195456im_/https://i0.wp.com/neptune.ai/wp-content/uploads/output4.png?fit=787%2C52&amp;ssl=1"/></noscript></figure>



<p>TextBlob claims that the text <em>“100 people killed in Iraq”</em> is negative and is not an opinion or feeling but rather a factual statement. I think we can agree with TextBlob here.</p>



<p>可以看到极性主要在0.00到0.20之间。这表明大多数新闻标题是中性的。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">polarity</span><span class="hljs-params">(text)</span>:</span>
    <span class="hljs-keyword">return</span> TextBlob(text).sentiment.polarity

news[<span class="hljs-string">'polarity_score'</span>]=news[<span class="hljs-string">'headline_text'</span>].\
   apply(<span class="hljs-keyword">lambda</span> x : polarity(x))
news[<span class="hljs-string">'polarity_score'</span>].hist()</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/5-0-polarity-score-histogram-7435097b-2554-423d-82f9-a4dfce94ea9b#state=cac0f382-5483-48bb-8b86-d684b92b2190&amp;code=8wztFsr_7DtvPSsM3xCyK9TE4leIVsx9UUK3JhAkpo0.85db2d27-454e-4eca-9ff9-ef66a986d643" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38883ffa3" class="separator separator-10">让我们再深入一点，根据分数将新闻分为负面、正面和中性。</p>



<p>You can see that the polarity mainly ranges between 0.00 and 0.20. This indicates that the <strong>majority of the news headlines are neutral.</strong></p>



<p>是的，70 %的新闻是中性的，只有18%是正面的，11%是负面的。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sentiment</span><span class="hljs-params">(x)</span>:</span>
    <span class="hljs-keyword">if</span> x&lt;<span class="hljs-number">0</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">'neg'</span>
    <span class="hljs-keyword">elif</span> x==<span class="hljs-number">0</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">'neu'</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">'pos'</span>
    
news[<span class="hljs-string">'polarity'</span>]=news[<span class="hljs-string">'polarity_score'</span>].\
   map(<span class="hljs-keyword">lambda</span> x: sentiment(x))

plt.bar(news.polarity.value_counts().index,
        news.polarity.value_counts())</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/5-1-sentiment-barchart-1da2f77b-db4e-4636-b186-0328dcbb791b/ea6a3450-6d61-4b3f-9274-f1f0c241fa5c" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d388c3ffa4" class="separator separator-10">让我们来看看<strong>一些正面和负面的标题。</strong></p>



<p>积极的新闻标题大多是关于体育运动中的一些胜利。</p>



<p>是的，非常负面的新闻标题。</p>



<pre class="hljs">news[news[<span class="hljs-string">'polarity'</span>]==<span class="hljs-string">'pos'</span>][<span class="hljs-string">'headline_text'</span>].head()
</pre>







<p>维德情感分析</p>



<pre class="hljs">news[news[<span class="hljs-string">'polarity'</span>]==<span class="hljs-string">'neg'</span>][<span class="hljs-string">'headline_text'</span>].head()
</pre>







<p>我们要讨论的下一个图书馆是VADER。维德在探测负面情绪方面表现更好。在社交媒体文本情感分析的情况下非常有用。</p>



<h3 id="vader"><strong> VADER或价感知词典和情感推理机</strong>是一个基于规则/词典的开源情感分析器预建库，受麻省理工学院许可证保护。</h3>



<p>VADER情感分析类<strong>返回一个字典，该字典包含文本正面、负面和中性的概率。</strong>然后我们就可以筛选选择概率最大的情感。</p>



<p>我们将使用VADER做同样的分析，并检查是否有很大的差异。</p>



<p>VADER sentiment analysis class <strong>returns a dictionary that contains the probabilities of the text for being positive, negative and neutral.</strong> Then we can filter and choose the sentiment with most probability.</p>



<p>是的，在分布上有一点点不同。甚至更多的标题被归类为中性85 %，负面新闻标题的数量有所增加(至13 %)。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> nltk.sentiment.vader <span class="hljs-keyword">import</span> SentimentIntensityAnalyzer

nltk.download(<span class="hljs-string">'vader_lexicon'</span>)
sid = SentimentIntensityAnalyzer()

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_vader_score</span><span class="hljs-params">(sent)</span>:</span>
    
    ss = sid.polarity_scores(sent)
    
    <span class="hljs-keyword">return</span> np.argmax(list(ss.values())[:<span class="hljs-number">-1</span>])

news[<span class="hljs-string">'polarity'</span>]=news[<span class="hljs-string">'headline_text'</span>].\
    map(<span class="hljs-keyword">lambda</span> x: get_vader_score(x))
polarity=news[<span class="hljs-string">'polarity'</span>].replace({<span class="hljs-number">0</span>:<span class="hljs-string">'neg'</span>,<span class="hljs-number">1</span>:<span class="hljs-string">'neu'</span>,<span class="hljs-number">2</span>:<span class="hljs-string">'pos'</span>})

plt.bar(polarity.value_counts().index,
        polarity.value_counts())</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/5-1-sentiment-barchart-1da2f77b-db4e-4636-b186-0328dcbb791b/ea6a3450-6d61-4b3f-9274-f1f0c241fa5c" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38923ffa5" class="separator separator-10">命名实体识别</p>



<p>命名实体识别是一种信息提取方法，其中文本中存在的实体被分类为预定义的实体类型，如“人”、“地点”、“组织”等。通过使用<strong> NER，我们可以获得关于给定文本数据集</strong>中存在的实体类型的深刻见解。</p>



<h2 id="entity">让我们考虑一篇新闻报道的例子。</h2>



<p>在上面的新闻中，命名实体识别模型应该能够识别诸如RBI作为一个组织，Mumbai和India作为地点等实体。</p>



<p>有三个标准库可以进行命名实体识别:</p>



<div class="wp-block-image"><figure class="aligncenter"><a href="https://web.archive.org/web/20220928195456/https://economictimes.indiatimes.com/industry/banking/finance/banking/rbi-warns-banks-over-focus-on-retail-loans/articleshow/72962399.cms?from=mdr" target="_blank" rel="noreferrer noopener"><img src="../Images/0e1818cc873f14845f4ff03d6adddb27.png" alt="news headline example" data-lazy-src="https://web.archive.org/web/20220928195456/https://i1.wp.com/neptune.ai/wp-content/uploads/news_image.png?fit=658%2C495&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220928195456im_/https://i1.wp.com/neptune.ai/wp-content/uploads/news_image.png?fit=658%2C495&amp;ssl=1"/><noscript><img data-lazy-fallback="1" src="../Images/0e1818cc873f14845f4ff03d6adddb27.png" alt="news headline example" data-original-src="https://web.archive.org/web/20220928195456im_/https://i1.wp.com/neptune.ai/wp-content/uploads/news_image.png?fit=658%2C495&amp;ssl=1"/></noscript></a><figcaption>The Economic Times – Indian Times 2019</figcaption></figure></div>



<p>在本教程中，<strong>我将使用spaCy </strong>，这是一个用于高级自然语言处理任务的开源库。它是用Cython编写的，以其工业应用而闻名。除了NER，<strong> spaCy还提供了许多其他功能，如词性标注、词到向量转换等。</strong></p>



<p><a href="https://web.archive.org/web/20220928195456/https://spacy.io/api/annotation#section-named-entities" target="_blank" rel="noreferrer noopener"> SpaCy的命名实体识别</a>已经在<a href="https://web.archive.org/web/20220928195456/https://catalog.ldc.upenn.edu/LDC2013T19" target="_blank" rel="noreferrer noopener"> OntoNotes 5 </a>语料库上进行训练，它支持以下实体类型:</p>





<p>spaCy的英语有三种<a href="https://web.archive.org/web/20220928195456/https://spacy.io/models/en/" target="_blank" rel="noreferrer noopener">预训练模式。我将使用<em> en_core_web_sm </em>来完成我们的任务，但您可以尝试其他模型。</a></p>



<p>要使用它，我们必须先下载它:</p>







<p>现在我们可以初始化语言模型了:</p>



<p>Spacy的一个好处是我们只需要应用<em> nlp函数</em>一次，整个后台管道就会返回我们需要的对象。</p>



<pre class="hljs">python -m spacy download en_core_web_sm
</pre>



<p>我们可以看到，印度和伊朗被认为是地理位置(GPE)，Chabahar是人，星期四是日期。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> spacy

nlp = spacy.load(<span class="hljs-string">"en_core_web_sm"</span>)</pre>



<p>我们也可以使用spaCy中的<em> displacy </em>模块来可视化输出。</p>



<pre class="hljs">doc=nlp(<span class="hljs-string">'India and Iran have agreed to boost the economic viability \
of the strategic Chabahar port through various measures, \
including larger subsidies to merchant shipping firms using the facility, \
people familiar with the development said on Thursday.'</span>)

[(x.text,x.label_) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> doc.ents]</pre>



<figure class="wp-block-image"><img src="../Images/6f8693935d84a7ed86cc8a52ceda08bb.png" alt="spacy ner" data-lazy-src="https://web.archive.org/web/20220928195456/https://i1.wp.com/neptune.ai/wp-content/uploads/output7.png?fit=785%2C103&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220928195456im_/https://i1.wp.com/neptune.ai/wp-content/uploads/output7.png?fit=785%2C103&amp;ssl=1"/><noscript><img data-lazy-fallback="1" src="../Images/6f8693935d84a7ed86cc8a52ceda08bb.png" alt="spacy ner" data-original-src="https://web.archive.org/web/20220928195456im_/https://i1.wp.com/neptune.ai/wp-content/uploads/output7.png?fit=785%2C103&amp;ssl=1"/></noscript></figure>



<p>这创建了一个非常简洁的句子的可视化<strong>,其中每个实体类型用不同的颜色标记。</strong></p>



<p>现在我们知道了如何执行NER，我们可以通过对从数据集中提取的命名实体进行各种可视化来进一步探索数据。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> spacy <span class="hljs-keyword">import</span> displacy

displacy.render(doc, style=<span class="hljs-string">'ent'</span>)</pre>







<p>首先，我们将在新闻标题上运行命名实体识别并存储实体类型。</p>



<p>现在，我们可以可视化实体频率:</p>






<p>First, we will <strong>run the named entity recognition on our news</strong> headlines and store the entity types.</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ner</span><span class="hljs-params">(text)</span>:</span>
    doc=nlp(text)
    <span class="hljs-keyword">return</span> [X.label_ <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> doc.ents]

ent=news[<span class="hljs-string">'headline_text'</span>].\
    apply(<span class="hljs-keyword">lambda</span> x : ner(x))
ent=[x <span class="hljs-keyword">for</span> sub <span class="hljs-keyword">in</span> ent <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> sub]

counter=Counter(ent)
count=counter.most_common()</pre>



<p>现在我们可以看到，GPE和ORG占据了新闻标题，其次是PERSON实体。</p>



<pre class="hljs">x,y=map(list,zip(*count))
sns.barplot(x=y,y=x)</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/6-0-named-entity-barchart-9012f4a0-3761-4ebf-9c25-d4f363858010/ac08ec73-ddd3-4a42-a35b-b2311eb9d075" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d389a3ffa6" class="separator separator-10">我们还可以<strong>可视化每个实体最常见的令牌。</strong>我们来看看哪些地方在新闻标题中出现的次数最多。</p>



<p>Now we can see that the GPE and ORG dominate the news headlines followed by the PERSON entity.</p>



<p>我想我们可以确认这样一个事实，即新闻标题中的“美国”指的是美国。让我们也找出新闻标题中最常见的名字。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ner</span><span class="hljs-params">(text,ent=<span class="hljs-string">"GPE"</span>)</span>:</span>
    doc=nlp(text)
    <span class="hljs-keyword">return</span> [X.text <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> doc.ents <span class="hljs-keyword">if</span> X.label_ == ent]

gpe=news[<span class="hljs-string">'headline_text'</span>].apply(<span class="hljs-keyword">lambda</span> x: ner(x))
gpe=[i <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> gpe <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x]
counter=Counter(gpe)

x,y=map(list,zip(*counter.most_common(<span class="hljs-number">10</span>)))
sns.barplot(y,x)</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/6-1-most-common-named-entity-barchart-0614fdac-0400-4460-ac3a-b3c5669906a0/4d3d398d-df9d-484c-97ec-07390ba4dd21" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d389d3ffa7" class="separator separator-10"/>



<p>萨达姆·侯赛因和乔治·布什是战时伊拉克和美国的总统。此外，我们可以看到，该模型在将<em>“维克政府”</em>或<em>“新南威尔士州政府”</em>归类为个人而非政府机构方面远非完美。</p>



<pre class="hljs">per=news[<span class="hljs-string">'headline_text'</span>].apply(<span class="hljs-keyword">lambda</span> x: ner(x,<span class="hljs-string">"PERSON"</span>))
per=[i <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> per <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x]
counter=Counter(per)

x,y=map(list,zip(*counter.most_common(<span class="hljs-number">10</span>)))
sns.barplot(y,x)</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/6-1-most-common-named-entity-barchart-0614fdac-0400-4460-ac3a-b3c5669906a0/4d3d398d-df9d-484c-97ec-07390ba4dd21" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38a13ffa8" class="separator separator-10">探索Python中的词性标注</p>



<p>词性(POS)标注是一种<strong>方法，将词性标签分配给句子中的单词。</strong>有八种主要的词类:</p>



<h2 id="speech-tagging">名词(NN)-约瑟夫，伦敦，桌子，猫，老师，钢笔，城市</h2>



<p>动词(VB)-读，说，跑，吃，玩，住，走，有，喜欢，是，是</p>


<div class="custom-point-list">
<ul><li>形容词(JJ)-美丽，快乐，悲伤，年轻，有趣，三</li><li>副词(RB)-慢慢地，悄悄地，非常，总是，从来没有，太，嗯，明天</li><li>介词(在)-在，在，在，从，与，近，之间，大约，在</li><li>连词(CC)- and，or，but，因为，所以，然而，除非，既然，如果</li><li>代词(PRP)-我，你，我们，他们，他，她，它，我，我们，他们，他，她，这个</li><li>感叹词(INT)-哎哟！哇！太好了！救命啊！哦！嘿！嗨！</li><li>这不是一项简单的任务，因为同一个词可能在不同的上下文中用在不同的句子中。然而，一旦你这样做了，你就可以创建许多有用的可视化工具，让你对数据集有更多的了解。</li><li>我将使用nltk来做词性标注，但是也有其他做得很好的库(spacy，textblob)。</li></ul>
</div>


<p>让我们看一个例子。</p>



<p>注意:</p>



<p>我们可以在这里观察到各种依赖标签。例如，<em> DET </em>标签表示限定词“the”和名词“stories”之间的关系。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> nltk
sentence=<span class="hljs-string">"The greatest comeback stories in 2019"</span>
tokens=word_tokenize(sentence)
nltk.pos_tag(tokens)</pre>



<figure class="wp-block-image"><img src="../Images/5f5bebdd3b7c614d899a72b5012ce351.png" alt="nltk pos tagging" data-lazy-src="https://web.archive.org/web/20220928195456/https://i0.wp.com/neptune.ai/wp-content/uploads/output9.png?fit=786%2C133&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220928195456im_/https://i0.wp.com/neptune.ai/wp-content/uploads/output9.png?fit=786%2C133&amp;ssl=1"/><noscript><img data-lazy-fallback="1" src="../Images/5f5bebdd3b7c614d899a72b5012ce351.png" alt="nltk pos tagging" data-original-src="https://web.archive.org/web/20220928195456im_/https://i0.wp.com/neptune.ai/wp-content/uploads/output9.png?fit=786%2C133&amp;ssl=1"/></noscript></figure>


<div class="note">
    <h3> </h3>
    <div class="content">
                    
                    <div class="html_code">
                                <pre class="hljs">doc = nlp(<span class="hljs-string">'The greatest comeback stories in 2019'</span>)
displacy.render(doc, style=<span class="hljs-string">'dep'</span>, jupyter=<span class="hljs-keyword">True</span>, options={<span class="hljs-string">'distance'</span>: <span class="hljs-number">90</span>})</pre>                            </div>
                    
                    <div class="wysiwyg_editor">
                                    <p>你可以在这里查看依赖标签列表及其含义<a href="https://web.archive.org/web/20220928195456/https://universaldependencies.org/u/dep/index.html" target="_blank" rel="noopener">。</a></p>
<p>好了，现在我们知道了什么是词性标注，让我们用它来研究我们的标题数据集。</p>
                            </div>
                    <div class="wysiwyg_editor">
                                    <p>You can check the list of dependency tags and their meanings <a href="https://web.archive.org/web/20220928195456/https://universaldependencies.org/u/dep/index.html" target="_blank" rel="noopener">here</a>.</p>
                            </div>
            </div>
</div>



<p>我们可以清楚地看到，名词(NN)在新闻标题中占主导地位，其次是形容词(JJ)。这对于新闻文章来说是典型的，而对于艺术形式来说，更高的形容词(ADJ)频率可能发生得相当多。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pos</span><span class="hljs-params">(text)</span>:</span>
    pos=nltk.pos_tag(word_tokenize(text))
    pos=list(map(list,zip(*pos)))[<span class="hljs-number">1</span>]
    <span class="hljs-keyword">return</span> pos

tags=news[<span class="hljs-string">'headline_text'</span>].apply(<span class="hljs-keyword">lambda</span> x : pos(x))
tags=[x <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> tags <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> l]
counter=Counter(tags)

x,y=list(map(list,zip(*counter.most_common(<span class="hljs-number">7</span>))))
sns.barplot(x=y,y=x)</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/7-0-parts-of-speach-barchart-9140250c-50d2-4343-b5e2-b3f5fb9c2089/15b07733-f02d-4a7c-b2fc-05ecffdf3e7b" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38b43ffa9" class="separator separator-10">你可以通过调查哪一个单数名词在新闻标题中最常见来深入了解这个问题。让我们来了解一下。</p>



<p>We can clearly see that the noun (NN) dominates in news headlines followed by the adjective (JJ). This is typical for news articles while <strong>for artistic forms higher adjective(ADJ) frequency</strong> could happen quite a lot.</p>



<p><em>“战争”、“伊拉克”、“人”</em>等名词在新闻标题中占据主导地位。您可以使用上面的函数可视化和检查其他词类。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_adjs</span><span class="hljs-params">(text)</span>:</span>
    adj=[]
    pos=nltk.pos_tag(word_tokenize(text))
    <span class="hljs-keyword">for</span> word,tag <span class="hljs-keyword">in</span> pos:
        <span class="hljs-keyword">if</span> tag==<span class="hljs-string">'NN'</span>:
            adj.append(word)
    <span class="hljs-keyword">return</span> adj


words=news[<span class="hljs-string">'headline_text'</span>].apply(<span class="hljs-keyword">lambda</span> x : get_adjs(x))
words=[x <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> words <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> l]
counter=Counter(words)

x,y=list(map(list,zip(*counter.most_common(<span class="hljs-number">7</span>))))
sns.barplot(x=y,y=x)</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/7-1-most-common-part-of-speach-barchart-3f896e91-e21c-4ea7-811f-02acb497479f/a41302f3-8803-47ce-98e5-bb1a73eda5cc" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38b73ffaa" class="separator separator-10">探索文本的复杂性</p>



<p>了解文本的可读性(难读性)以及什么类型的读者能够完全理解它是非常有益的。我们需要大学学历才能理解这个信息吗？还是一个一年级的学生就能清楚地明白重点是什么？</p>



<h2 id="text-complexity">实际上，你可以在文档或文本上标注一个叫做可读性指数的数字。可读性指数是一个数值，表示阅读和理解文本的难易程度。</h2>



<p>有许多可读性分数公式可用于英语。一些最突出的例子是:</p>



<p>You can actually put a number called readability index on a document or text. <strong>Readability index is a numeric value that indicates how difficult (or easy) it is to read and understand a text.</strong></p>



<p>可读性测试</p>


<p id="block_61af67a5ad2e4" class="separator separator-10">解释</p>


<div class="medium-table">
        <div class="mt-row heading">
            <p class="mt-col">公式</p>
            <p class="mt-col">自动化可读性索引</p>
            <p class="mt-col">输出是理解一篇文章所需的美国等级水平的近似表示。</p>
        </div>
    
            <div class="mt-row">
                            <div class="mt-col">
                                                                <p>ARI = 4.71 *(字符/单词)+ <br/> 0.5 *(单词/句子)-21.43</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Interpretation:
                    </span>
                                                                <p>轻松阅读(FRE)</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Formula:
                    </span>
                                                                <p>更高的分数表示材料更容易阅读，<br/>更低的数字表示更难阅读的段落:<br/>–0-30岁大学<br/>–50-60岁高中<br/>–60岁+四年级</p>
                                    </div>
                    </div>
            <div class="mt-row">
                            <div class="mt-col">
                                                                <p>FRE = 206.8351.015 *(总字数/总句子数)<br/>-84.6 *(总音节数/总字数)</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Interpretation:
                    </span>
                                                                <p>FleschKincaid等级级别(FKGL)</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Formula:
                    </span>
                                                                <p>结果是一个与美国年级水平相对应的数字。</p>
                                    </div>
                    </div>
            <div class="mt-row">
                            <div class="mt-col">
                                                                <p>FKGL = 0.39 *(总字数/总句子数)<br/> + 11.8(总音节数/总字数)-15.59</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Interpretation:
                    </span>
                                                                <p>结果是一个与美国年级水平相对应的数字。</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Formula:
                    </span>
                                                                <p>GFI = 0.4 *(单词/句子)+ <br/> 100 *(复合词/单词))</p>
                                    </div>
                    </div>
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Interpretation:
                    </span>
                                                                <p>The result is a number that corresponds with a U.S grade level.</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Formula:
                    </span>
                                                                <p>Textstat是一个很酷的Python库，它提供了所有这些文本统计计算方法的实现。让我们用Textstat来实现Flesch阅读容易指数。</p>
                                    </div>
                    </div>
    </div>


<p id="block_5ffddd80deb67" class="separator separator-20">现在，您可以绘制分数的直方图并可视化输出。</p>



<p><a href="https://web.archive.org/web/20220928195456/https://github.com/shivam5992/textstat" target="_blank" rel="noreferrer noopener">Textstat</a> is a cool Python library that provides an implementation of all these text statistics calculation methods. Let’s use Textstat to implement Flesch Reading Ease index.</p>



<p>几乎所有的可读性分数都在60分以上。这意味着一个普通的11岁学生可以阅读和理解新闻标题。让我们检查可读性分数低于5分的所有新闻标题。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> textstat <span class="hljs-keyword">import</span> flesch_reading_ease

news[<span class="hljs-string">'headline_text'</span>].\
   apply(<span class="hljs-keyword">lambda</span> x : flesch_reading_ease(x)).hist()</pre>






<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/8-0-text-complexity-histogram-b00e38f2-5710-4efe-85c2-77b6366dbe3b/b6b8dd8f-3ec6-4fd1-a548-7daf889444e5" target="_blank">
    Code snippet that generates this chart</a>


<p id="block_601d38ba3ffab" class="separator separator-10">你可以在新闻标题中看到一些复杂的词汇，如<em>“投降”、“过渡”、“诱捕”</em>等。这些话可能导致分数降到了5分以下。</p>



<p>最后的想法</p>



<pre class="hljs">x=[i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(reading)) <span class="hljs-keyword">if</span> reading[i]&lt;<span class="hljs-number">5</span>]
news.iloc[x][<span class="hljs-string">'headline_text'</span>].head()</pre>



<figure class="wp-block-image"><img src="../Images/96c37e278f3727b9be66d06aadceb096.png" alt="text complexity" data-lazy-src="https://web.archive.org/web/20220928195456/https://i1.wp.com/neptune.ai/wp-content/uploads/output11.png?fit=788%2C139&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220928195456im_/https://i1.wp.com/neptune.ai/wp-content/uploads/output11.png?fit=788%2C139&amp;ssl=1"/><noscript><img data-lazy-fallback="1" src="../Images/96c37e278f3727b9be66d06aadceb096.png" alt="text complexity" data-original-src="https://web.archive.org/web/20220928195456im_/https://i1.wp.com/neptune.ai/wp-content/uploads/output11.png?fit=788%2C139&amp;ssl=1"/></noscript></figure>



<p>在本文中，我们讨论并实现了针对文本数据的各种探索性数据分析方法。有些很常见，有些不太为人所知，但所有这些都是对您的数据探索工具包的巨大补充。</p>



<h2>希望您会发现其中一些对您当前和未来的项目有用。</h2>



<p>为了使数据探索更加容易，我创建了一个<strong>“自然语言处理探索性数据分析模板”</strong>，您可以在工作中使用它。</p>



<p>Hopefully, you will find some of them useful in your current and future projects.</p>



<p>To make data exploration even easier, I have created a  <strong>“Exploratory Data Analysis for Natural Language Processing Template”</strong> that you can use for your work.</p>


<a class="button aligncenter blue-filled" href="https://web.archive.org/web/20220928195456/https://ui.neptune.ai/o/neptune-ml/org/eda-nlp-tools/n/8-0-text-complexity-histogram-b00e38f2-5710-4efe-85c2-77b6366dbe3b/95b28cf3-a123-4104-bdd9-358e123b8a58" target="_blank">
    Get exploratory data analysis for Natural Language Processing template </a>


<p id="block_601d38473ff9a" class="separator separator-10">此外，正如您可能已经看到的，对于本文中的每个图表，都有一个代码片段来创建它。只需点击图表下方的按钮。</p>


<p id="block_5ffddd86deb68" class="separator separator-5">探索愉快！</p>



<p>沙胡尔ES</p>



<p>数据科学家，非常熟悉机器学习、NLP和音频处理领域。他是Kaggle大师，也喜欢做开源。</p>




<div id="author-box-new-format-block_60545fa1410ba" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name"><strong>阅读下一篇</strong></h3>
    
          <p class="article__authorContent-text">如何构建和管理自然语言处理(NLP)项目</p>
    
          
    
  </div>
</div>


<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>



<p class="has-text-color">Dhruvil Karani |发布于2020年10月12日</p>



<h2>How to Structure and Manage Natural Language Processing (NLP) Projects</h2>



<p class="has-small-font-size">如果说我在ML行业工作中学到了什么的话，那就是:<strong>机器学习项目很乱。</strong></p>


<p id="block_5ffc75def9f8e" class="separator separator-10">这并不是说人们不想把事情组织起来，只是在项目过程中有很多事情很难组织和管理。</p>



<p>你可以从头开始，但有些事情会阻碍你。</p>



<p>一些典型的原因是:</p>



<p>笔记本中的快速数据探索，</p>



<p>取自github上的研究报告的模型代码，</p>


<div class="custom-point-list">
<ul><li>当一切都已设置好时，添加新的数据集，</li><li>发现了数据质量问题并且需要重新标记数据，</li><li>团队中的某个人“只是快速地尝试了一些东西”,并且在没有告诉任何人的情况下改变了训练参数(通过argparse传递),</li><li>从高层推动将原型转化为产品“仅此一次”。</li><li>多年来，作为一名机器学习工程师，我学到了一堆<strong>东西，它们可以帮助你保持在事物的顶端，并检查你的NLP项目</strong>(就像你真的可以检查ML项目一样:)。</li><li>在这篇文章中，我将分享我在从事各种数据科学项目时学到的关键指针、指南、技巧和诀窍。许多东西在任何ML项目中都是有价值的，但有些是NLP特有的。</li></ul>
</div>


<p>Over the years working as a machine learning engineer I’ve learned a bunch of <strong>things that can help you stay on top of things and keep your NLP projects in check</strong> (as much as you can really have ML projects in check:)). </p>



<p>In this post I will share key pointers, guidelines, tips and tricks that I learned while working on various data science projects. Many things can be valuable in any ML project but some are specific to NLP. </p>


<a class="button continous-post blue-filled" href="/web/20220928195456/https://neptune.ai/blog/how-to-structure-and-manage-nlp-projects-templates" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>