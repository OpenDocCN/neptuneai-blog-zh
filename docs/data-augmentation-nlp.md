# NLP 中的数据扩充:来自 Kaggle Master 的最佳实践

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/data-augmentation-nlp>

NLP 中有许多任务，从文本分类到问题回答，但是无论您做什么，您必须训练模型的数据量都会严重影响模型的性能。

你能做些什么来扩大你的数据集？

简单的选项->获取更多数据:)。

但是获取和标记额外的观察值可能是一个昂贵且耗时的过程。

你能做些什么呢？

对文本数据应用数据扩充。

数据扩充技术用于**使用您已有的数据生成额外的合成数据。**增强方法在计算机视觉应用中非常流行，但它们对自然语言处理同样有效。

在本文中，我们将介绍 NLP 的所有主要数据扩充方法，您可以使用这些方法来增加文本数据集的大小并提高模型性能。

## 视觉与自然语言处理的数据增强

在计算机视觉应用中，数据扩充几乎无处不在，以获得更大的训练数据，并使模型更好地泛化。

使用的主要方法包括:

*   裁剪，
*   翻转，
*   缩放，
*   旋转，
*   噪声注入，
*   和许多其他人。

在**计算机视觉**中，这些转换是使用数据生成器在移动中完成的**。当一批数据被输入你的神经网络时，它会被随机转换(增强)。训练前不需要准备任何东西。**

这与 **NLP** 的情况不同，由于文本的语法结构，数据扩充应该小心进行。这里讨论的方法都是训练前使用的**。**预先生成一个新的扩充数据集，然后输入数据加载器来训练模型。

## 数据扩充方法

在本文中，我将主要关注以下项目中提供的 NLP 数据扩充方法:

所以，让我们深入了解每一个问题。

### **反向翻译**

在这个方法中，我们**将文本数据翻译成某种语言，然后再翻译回原来的**语言。这可以帮助生成具有不同单词的文本数据，同时保留文本数据的上下文。

像 google translate、Bing、Yandex 这样的语言翻译 API 用于执行翻译。例如，给出这样一句话:

你可以看到，这些句子是不一样的，但回译后它们的内容保持不变。如果您想对数据集尝试这种方法，您可以使用这个[笔记本](https://web.archive.org/web/20221119001251/https://www.kaggle.com/miklgr500/how-to-use-translators-for-comments-translation)作为参考。

### **简单的数据扩充**

简易数据扩充使用传统且非常简单的数据扩充方法。EDA 包括**四个简单的操作，在防止过度拟合和帮助训练更健壮的模型方面做得出奇的好**。

从句子中随机选择 *n* 个不是停用词的单词。用随机选择的同义词替换这些单词。

例如，给出这样一句话:

*这篇**文章**将重点总结自然语言处理中的数据增强**技术**。*

方法随机选取 n 个单词(说两个)，单词*文章*和*技法*，分别用*写法*和*方法*代替。

*这篇**文章**将重点总结 NLP 中的数据扩充**方法**。*

找出句子中不是停用词的随机词的随机同义词。将同义词插入句子中任意位置。这样做 *n* 次。

例如，给出这样一句话:

*这篇**文章**将重点总结自然语言处理中的数据增强**技术**。*

方法随机选取 n 个单词(比如两个)，将单词*文章*和*技巧*分别找到同义词为*写作*和*方法*。然后将这些同义词插入句子中的任意位置。

*本文将关注**综述**NLP**方法**中的数据扩充技术。*

随机选择句子中的两个单词，交换位置。这样做 *n* 次。

例如，给定一个句子

*这篇**文章**将重点总结自然语言处理中的数据增强**技术**。*

该方法随机选择 n 个单词(比如两个)，单词*文章*和*技巧*并交换它们来创建一个新句子。

*本**技巧**将重点总结 NLP 中的数据增强**文章**。*

以概率 *p* 随机去掉句子中的每个单词。

例如，给定一个句子

*这篇**文章**将重点总结自然语言处理中的数据增强**技术**。*

该方法选择 n 个单词(比如说两个)，单词*将*和*技术*，并将它们从句子中移除。

*这篇* ***文章**重点总结 NLP 中的数据增强。*

如果您想将这些技术应用到您的项目中，您可以访问这个[库](https://web.archive.org/web/20221119001251/https://github.com/jasonwei20/eda_nlp)。

### **NLP 蛋白沉淀**

之前，我们讨论了计算机视觉数据增强和 NLP 数据增强之间的差异。但是在本节中，我们将看到如何在 NLP 中应用 CV 数据扩充中使用的一些思想。

为此，我们将使用[白蛋白](https://web.archive.org/web/20221119001251/https://github.com/albumentations-team/albumentations)包。

让我们来看看这里的一些技术。

*   **混排句子变换**

在这种转换中，如果给定的文本样本包含多个句子，这些句子将被打乱以创建一个新的样本。

例如:

文本= ' <句子 1 >。<句子 2 >。第四句>。第四句>。第五句>。‘句子 5>’

被转换为:

文本= ' <句子 2 >。<句子 3 >。第一句>。句子 5 >。第五句>。<句子 4 >

*   **排除重复转换**

在这种转换中，如果给定的文本样本包含具有重复句子的多个句子，则这些重复句子被移除以创建新的样本。

例如给定样本，

文本= ' <句子 1 >。<句子 2 >。第四句>。第四句>。第五句>。‘句子 5>’

我们将其转化为:

**’<句子 1 >。<句子 2 >。第四句>。‘句子 5>’**

您可以使用这个库尝试许多其他转换。你可以查看这个精彩的[笔记本](https://web.archive.org/web/20221119001251/https://www.kaggle.com/shonenkov/nlp-albumentations)来看看完整的实现。

### **NLPAug 库**

到目前为止，我们已经讨论了在 NLP 中使用数据扩充的许多方法。

但是从头开始有效地实现这些方法需要做大量的工作。

在这一节中，我将向您介绍一个 **python 包，它可以让您轻松地完成所有这些数据扩充**，并且您可以使用各种参数来调整您需要的扩充级别。

NLPAug 帮助你为你的机器学习项目增加 NLP。让我们看看如何使用这个库来执行数据扩充。

NLPAug 提供三种类型的增强:

*   字符级增强
*   单词级增强
*   句子层次扩充

在每个级别中，NLPAug 都提供了前面章节中讨论的所有方法，例如:

*   随机删除，
*   随机插入，
*   洗牌，
*   同义词替换，
*   等等。

根据我的经验，最常用和有效的技术是通过单词嵌入替换同义词。

我们将 n 个单词替换为它的同义词(接近这些单词的单词嵌入)，以获得具有相同含义但不同单词的句子。

在执行同义词替换时，我们可以选择应该使用哪种预训练嵌入来查找给定单词的同义词。

使用 NLPaug，我们可以选择非上下文嵌入，例如:

或者上下文嵌入，比如:

例如:

```
aug = naw.ContextualWordEmbsAug(
    model_path='bert-base-uncased', action="insert")
augmented_text = aug.augment(text)
```

原件:

敏捷的棕色狐狸跳过懒惰的狗

扩充文本:

*即使是敏捷的棕色狐狸**通常**也会跳过懒狗*

## 进行 NLP 数据扩充时要记住的事情

正如我在介绍中所说的，在 NLP 中进行增强时，我们需要注意一些事情。

在对扩充数据进行训练时面临的主要问题是算法，如果做得不正确，就会严重过度拟合扩充的训练数据。

一些需要记住的事情:

*   不要使用扩充数据进行验证。
*   如果你正在进行 K-fold 交叉验证，请始终将原始样本和扩充样本放在同一个 fold 中，以避免过度拟合。
*   总是尝试不同的增强方法，并检查哪个效果更好。
*   混合使用不同的增强方法也是值得的，但是不要过度。
*   通过实验来确定要增加的最佳样本数，以获得最佳结果。
*   请记住，NLP 中的数据扩充并不总是有助于提高模型性能。

## 数据扩充工作流程

在本节中，我们将在[上尝试数据增强，真实与否？在 Kaggle 上举办的 NLP 与灾难推文](https://web.archive.org/web/20221119001251/https://www.kaggle.com/c/nlp-getting-started)竞赛。

在[我之前的一篇文章](/web/20221119001251/https://neptune.ai/blog/document-classification-small-datasets)中，我使用了这次比赛的数据来尝试不同的非上下文嵌入方法。在这里，我将使用我在那里使用的完全相同的分类管道，但是我将添加数据扩充，以查看它是否提高了模型性能。

首先，让我们加载训练数据集并检查目标类分布。

```
…
x=tweet.target.value_counts()
sns.barplot(x.index,x)
plt.gca().set_ylabel('samples')
```

我们可以看到这里有一个小的阶级不平衡。

让我们用同义词替换法生成一些正样本。

在数据扩充之前，我们将数据分为训练集和验证集，以便验证集中没有样本用于数据扩充。

```
train,valid=train_test_split(tweet,test_size=0.15)
```

现在，我们可以对训练数据集进行数据扩充。我已经选择从正类中生成 300 个样本。

```
def augment_text(df,samples=300,pr=0.2):
    aug_w2v.aug_p=pr
    new_text=[]

    df_n=df[df.target==1].reset_index(drop=True)

    for i in tqdm(np.random.randint(0,len(df_n),samples)):

            text = df_n.iloc[i]['text']
            augmented_text = aug_w2v.augment(text)
            new_text.append(augmented_text)

    new=pd.DataFrame({'text':new_text,'target':1})
    df=shuffle(df.append(new).reset_index(drop=True))
    return df

train = augment_text(train)
```

我们现在可以使用这些扩充的文本数据来训练模型。

如果你对学习如何从 NLP 的数据准备、训练分类器和运行推理中构建整个管道感兴趣，你可以查看我的另一篇文章。

那么，用同义词替换进行数据扩充有效吗？

| 没有数据扩充 | 用数据
增强 | ROC AUC 得分 |
| --- | --- | --- |
| 0.775 | 0.785 | **通过数据扩充，我们在模型性能(AUC)方面得到了很好的提升**。 |

使用不同的技术和[调整数据扩充方法的超参数](/web/20221119001251/https://neptune.ai/blog/hyperparameter-tuning-in-python-a-complete-guide-2020)可以进一步改善结果，但我现在将把它放在一边。

如果你想这么做，我准备了一个笔记本，你可以在上面玩东西。

最后的想法

在本文中，我们讨论并实现了文本数据的不同数据扩充方法。

## 据我所知，这些是完成这项任务的最好的公开技术和软件包。

希望你会发现它们对你的项目有用。

To my knowledge, these are the best publically available techniques and packages to do the task.

Hopefully, you will find them useful in your projects.