<html>
<head>
<title>Pix2pix: Key Model Architecture Decisions </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Pix2pix:关键模型架构决策</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/pix2pix-key-model-architecture-decisions#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/pix2pix-key-model-architecture-decisions#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p><a href="/web/20221117203630/https://neptune.ai/blog/generative-adversarial-networks-gan-applications" target="_blank" rel="noreferrer noopener">生成对抗网络或GANs </a>是一种属于无监督学习类的神经网络。它用于深度生成建模的任务。</p>



<p>在深度生成建模中，深度神经网络学习一组给定数据点上的概率分布，并生成相似的数据点。由于这是一个无人监督的学习任务，它在学习过程中不使用任何标签。</p>



<p>自2014年发布以来，深度学习社区一直在积极开发新的gan，以改善生成建模领域。本文旨在提供关于GAN的信息，特别是Pix2Pix GAN，它是最常用的生成模型之一。</p>



<h2 id="What-is-the-GAN?">甘是什么？</h2>



<p>GANs由Ian Goodfellow在2014年设计。GANs的主要意图是生成不模糊且具有丰富特征表示的样本。判别模型在这方面做得很好，因为它们能够在不同的类别之间进行分类。另一方面，深度生成模型的效率要低得多，因为在自动编码器中，很难近似许多棘手的概率计算。</p>



<p>自动编码器及其变体是显式似然模型，这意味着它们显式计算给定分布的概率密度函数。gan及其变体是隐式似然模型，这意味着它们不计算概率密度函数，而是学习潜在的分布。</p>



<p>gan通过将整个问题作为一个二进制分类问题来处理，来学习底层分布。在这种方法中，问题模型由两个模型表示:生成器和鉴别器。生成器的工作是生成新的样本，鉴别器的工作是分类或鉴别生成器生成的样本是真是假。</p>



<p>这两个模型在零和游戏中一起训练，直到生成器可以产生与真实样本相似的样本。或者换句话说，他们被训练到生成器可以骗过鉴别器。</p>



<h3>香草甘的架构</h3>



<p>先简单了解一下GANs的架构。从这一节开始，大多数主题将使用代码进行解释。首先，让我们定义所有需要的依赖项。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim
<span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> datasets
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms</pre>



<h4>发电机</h4>



<p>生成器是GAN中的一个组件，它接收定义为高斯分布的噪声，并产生与原始数据集相似的样本。随着GANs多年来的发展，他们已经采用了在计算机视觉任务中非常突出的CNN。但是为了简单起见，我们将使用Pytorch用线性函数来定义它。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Generator</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, z_dim, img_dim)</span>:</span>
       super().__init__()
       self.gen = nn.Sequential(
           nn.Linear(z_dim, <span class="hljs-number">256</span>),
           nn.LeakyReLU(<span class="hljs-number">0.01</span>),
           nn.Linear(<span class="hljs-number">256</span>, img_dim),
           nn.Tanh(),  
       )

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
       <span class="hljs-keyword">return</span> self.gen(x)</pre>



<h4>鉴别器</h4>



<p>鉴别器只是一个分类器，它对生成器生成的数据是真是假进行分类。它通过从真实数据中学习原始分布，然后在两者之间进行评估来实现这一点。我们将保持简单，使用线性函数定义鉴别器。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Discriminator</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_features)</span>:</span>
       super().__init__()
       self.disc = nn.Sequential(
           nn.Linear(in_features, <span class="hljs-number">128</span>),
           nn.LeakyReLU(<span class="hljs-number">0.01</span>),
           nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">1</span>),
           nn.Sigmoid(),
       )

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
       <span class="hljs-keyword">return</span> self.disc(x)</pre>



<p>生成器和鉴别器的关键区别是最后一层。前者产生与图像相同的形状，而后者只产生一个输出，0或1。</p>



<h4>损失函数和训练</h4>



<p>损失函数是任何深度学习算法中最重要的组件之一。例如，如果我们设计一个CNN来最小化真实情况和预测结果之间的欧几里德距离，它将倾向于产生模糊的结果。<strong>这是因为欧几里德距离通过平均所有可能的输出而最小化，这导致了模糊</strong>。</p>







<p>以上这一点很重要，我们必须记住。也就是说，我们将用于普通GAN的损失函数将是二进制交叉熵损失或BCELoss，因为我们正在执行二进制分类。</p>



<pre class="hljs">criterion = nn.BCELoss()</pre>



<p>现在我们来定义一下<strong>优化</strong>方法以及其他相关参数。</p>



<pre class="hljs">opt_disc = optim.Adam(disc.parameters(), lr=lr)
opt_gen = optim.Adam(gen.parameters(), lr=lr)


device = <span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>
lr = <span class="hljs-number">3e-4</span>
z_dim = <span class="hljs-number">64</span>
image_dim = <span class="hljs-number">28</span> * <span class="hljs-number">28</span> * <span class="hljs-number">1</span>  
batch_size = <span class="hljs-number">32</span>
num_epochs = <span class="hljs-number">100</span>

dataset = datasets.MNIST(root=<span class="hljs-string">"dataset/"</span>, transform=transforms, download=<span class="hljs-keyword">True</span>)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="hljs-keyword">True</span>)

writer_fake = SummaryWriter(f<span class="hljs-string">"logs/fake"</span>)
writer_real = SummaryWriter(f<span class="hljs-string">"logs/real"</span>)
step = <span class="hljs-number">0</span></pre>



<p>我们来理解一下训练循环。甘的<strong>训练循环开始于:</strong></p>



<ol><li>使用高斯分布从生成器生成样本</li><li>使用生成器产生的真实数据和虚假数据训练鉴别器</li><li>更新鉴别器</li><li>更新生成器</li></ol>



<p>下面是训练循环的样子:</p>



<pre class="hljs"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
   <span class="hljs-keyword">for</span> batch_idx, (real, _) <span class="hljs-keyword">in</span> enumerate(loader):
       real = real.view(<span class="hljs-number">-1</span>, <span class="hljs-number">784</span>).to(device)
       batch_size = real.shape[<span class="hljs-number">0</span>]
       
       noise = torch.randn(batch_size, z_dim).to(device)
       fake = gen(noise)
       disc_real = disc(real).view(<span class="hljs-number">-1</span>)
       lossD_real = criterion(disc_real, torch.ones_like(disc_real))
       disc_fake = disc(fake).view(<span class="hljs-number">-1</span>)
       lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))
       lossD = (lossD_real + lossD_fake) / <span class="hljs-number">2</span>
       disc.zero_grad()
       lossD.backward(retain_graph=<span class="hljs-keyword">True</span>)
       opt_disc.step()
       
       
       
       output = disc(fake).view(<span class="hljs-number">-1</span>)
       lossG = criterion(output, torch.ones_like(output))
       gen.zero_grad()
       lossG.backward()
       opt_gen.step()
       <span class="hljs-keyword">if</span> batch_idx == <span class="hljs-number">0</span>:
           print(
               f<span class="hljs-string">"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)}
                     Loss D: {lossD:.4f}, loss G: {lossG:.4f}"</span>
           )
           <span class="hljs-keyword">with</span> torch.no_grad():
               fake = gen(fixed_noise).reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)
               data = real.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)
               img_grid_fake = torchvision.utils.make_grid(fake, normalize=<span class="hljs-keyword">True</span>)
               img_grid_real = torchvision.utils.make_grid(data, normalize=<span class="hljs-keyword">True</span>)
               writer_fake.add_image(
                   <span class="hljs-string">"Mnist Fake Images"</span>, img_grid_fake, global_step=step
               )
               writer_real.add_image(
                   <span class="hljs-string">"Mnist Real Images"</span>, img_grid_real, global_step=step
               )
               step += <span class="hljs-number">1</span>
</pre>



<p>以上循环的要点:</p>



<ol><li>鉴别器的<strong>损失函数计算两次:一次用于真实图像，另一次用于虚假图像。</strong><ul><li>对于<strong>实像</strong>，地面真实被转换成使用torch.ones_like函数的真实，该函数返回定义形状的一个矩阵。</li><li>对于<strong>假图像</strong>，使用torch.zeros_like函数将地面真实转换为一，该函数返回定义形状的零矩阵。</li></ul></li><li>发电机的<strong>损失函数只计算一次。如果你仔细观察，鉴别器使用相同的损失函数来计算假图像的损失。唯一的区别是不使用torch.zeros_like函数，而是使用torch.ones_like函数。标签从0到1的互换使得生成器能够学习将产生真实图像的表示，因此欺骗了鉴别器。</strong></li></ol>



<p>数学上，我们可以将整个过程定义为:</p>





<p>其中Z是噪声，x是真实数据，G是发生器，D是鉴频器。</p>



<h3>GANs的应用</h3>



<p>gan广泛用于:</p>



<ul><li><strong>生成训练样本:</strong> GANs通常用于生成特定任务的样本，如恶性和良性癌细胞的分类，特别是在数据较少的情况下训练分类器。</li><li>人工智能艺术或生成艺术:人工智能或生成艺术是GANs被广泛使用的另一个新领域。自从引入不可替代的代币以来，全世界的艺术家都在以非正统的方式创作艺术，即数字化和生成性。像DeepDaze，BigSleep，BigGAN，CLIP，VQGAN等GAN是创作者最常用的。</li></ul>



<p id="separator-block_61a727fdb43af" class="block-separator block-separator--0"> </p>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-full"><img decoding="async" src="../Images/d7a5d91c9577ab401b88bc9be59e753e.png" alt="AI Art or Generative Art" class="wp-image-57301" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_4.jpg?ssl=1"/><figcaption><em>AI Art or Generative Art | Source: Author</em></figcaption></figure></div>


<ul><li><strong>图像到图像的翻译</strong>:图像到图像的翻译再次被数字创作者使用。这里的想法是将某种类型的图像转换成目标域中的图像。例如，将日光图像转换为夜间图像，或将冬季图像转换为夏季图像(见下图)。像pix2pix、cycleGAN、styleGAN这样的GAN是少数几个最受欢迎的GAN。</li></ul>



<p id="separator-block_61a72801b43b0" class="block-separator block-separator--0"> </p>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/110111292a85f48705c9fb848cec3330.png" alt="Image-to-image translation" class="wp-image-57295" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_10.png?resize=840%2C357&amp;ssl=1"/><figcaption><em>Image-to-image translation | <a href="https://web.archive.org/web/20221117203630/https://research.nvidia.com/publication/2017-12_Unsupervised-Image-to-Image-Translation" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<ul><li>文本到图像的翻译:文本到图像的翻译就是将文本或给定的字符串转换成图像。这是一个非常热门的领域，而且是一个不断发展的社区。如前所述，来自OpenAI的DeepDaze、BigSleep和DALL E等GANs在这方面非常受欢迎。</li></ul>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/2f993b3819f733cf6fad298cb8fddf9e.png" alt="Text-to-image translation" class="wp-image-57299" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_6.png?ssl=1"/><figcaption><em>Text-to-image translation | <a href="https://web.archive.org/web/20221117203630/https://openai.com/blog/dall-e/" target="_blank" rel="noreferrer noopener nofollow">Source</a> </em></figcaption></figure></div>


<h3>甘的问题</h3>



<p>虽然GANs可以从随机高斯分布中产生与真实图像相似的图像，但这个过程在大多数时候并不完美。原因如下:</p>



<ul><li><strong>模式崩溃:</strong>模式崩溃是指生成器能够通过从整体数据中学习较少的数据样本来欺骗鉴别器的问题。由于模式崩溃，GAN不能学习多种分布，并且仍然局限于少数几种。</li><li><strong>递减梯度</strong>:递减或<strong>消失梯度下降发生在网络的导数非常小时，以至于对原始权重的更新几乎可以忽略不计。为了克服这个问题，建议使用WGANs。</strong></li><li><strong>不收敛:</strong>它发生在网络无法收敛到全局最小值的时候。这是不稳定训练的结果。这个问题可以通过光谱归一化来解决。你可以在这里阅读光谱归一化<a href="https://web.archive.org/web/20221117203630/https://medium.com/perceptronai/review-spectral-normalization-for-gans-fa97cd2363c4" target="_blank" rel="noreferrer noopener nofollow"/>。</li></ul>







<h3>GAN的变体</h3>



<p>自从第一个GAN发布以来，已经出现了许多GAN的变体。以下是一些最受欢迎的GANs:</p>



<ul><li>CycleGAN</li><li>StyleGAN</li><li>像素网络</li><li>文本2图像</li><li>迪斯科根</li><li>伊斯甘</li></ul>







<p><strong>本文只关注Pix2Pix GAN </strong>。在下一节中，我们将了解一些关键组件，如架构、损失函数等。</p>



<h2 id="What-Is-the-Pix2Pix-GAN?">Pix2Pix GAN是什么？</h2>



<p>Pix2Pix GAN是由<a href="https://web.archive.org/web/20221117203630/http://web.mit.edu/phillipi/" target="_blank" rel="noreferrer noopener nofollow"> Phillip Isola </a>等人开发的有条件GAN ( <a href="https://web.archive.org/web/20221117203630/https://golden.com/wiki/Conditional_generative_adversarial_network_(cGAN)" target="_blank" rel="noreferrer noopener nofollow"> cGAN </a>)，与只使用真实数据和噪声来学习和生成图像的vanilla GAN不同，cGAN使用真实数据、噪声以及标签来生成图像。</p>



<p>本质上，生成器从真实数据以及噪声中学习映射。</p>





<p>类似地，鉴别器也从标签和真实数据中学习表示。</p>





<p>此设置使cGAN适用于图像到图像的转换任务，其中生成器根据输入图像生成相应的输出图像。换句话说，生成器使用条件分布(或数据)如指导或蓝图来生成目标图像(见下图)。</p>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/0eb83a50967a9252ef645a45aa6dd469.png" alt="Pix2Pix is a conditional GAN" class="wp-image-57284" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_21.png?ssl=1"/><figcaption><em>Pix2Pix is a conditional GAN | Source: Author</em></figcaption></figure></div>

<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/437cd39edad97b86674b85c309244fad.png" alt="Application of Pix2Pix" class="wp-image-57285" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_20.png?ssl=1"/><figcaption><em>Application of Pix2Pix | <a href="https://web.archive.org/web/20221117203630/https://phillipi.github.io/pix2pix/" target="_blank" rel="noreferrer noopener nofollow">Source</a> </em></figcaption></figure></div>


<p>Pix2Pix的想法依赖于为训练提供的数据集。将图像翻译与训练样本{x，y}配对是一对，它们之间具有对应关系。</p>



<h2 id="Network-architectures">Pix2Pix网络架构</h2>



<p>pix2pix有两个重要的架构，一个用于生成器，另一个用于鉴别器，即U-net和patchGAN。让我们更详细地探讨一下这两个问题。</p>



<h3>u网生成器</h3>



<p>如前所述，pix2pix使用的架构称为U-net。U-net最初是由Ronneberger等人为生物医学图像分割而开发的。艾尔。2015年。</p>





<p>UNet由两个主要部分组成:</p>



<ol><li>由卷积层(左侧)组成的<strong>收缩路径</strong>，在提取信息的同时对数据进行下采样。</li><li>由上转置卷积层(右侧)组成的<strong>扩展路径</strong>对信息进行上采样。</li></ol>



<p>假设我们的下采样有三个卷积层C_l(1，2，3)，那么我们必须确保我们的上采样有三个转置卷积层C_u(1，2，3)。这是因为我们想要使用<strong>跳过连接</strong>来连接相同大小的相应块。</p>



<p id="separator-block_61a728a9b43b1" class="block-separator block-separator--5">向下采样</p>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/af0051f7203cf15d747d250cf49d1c3b.png" alt="Skip connection" class="wp-image-57302" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_3.png?resize=378%2C308&amp;ssl=1"/><figcaption><em>Skip connection | Source: Author</em></figcaption></figure></div>


<h4>在下采样期间，每个卷积块提取空间信息，并将该信息传递给下一个卷积块以提取更多信息，直到它到达被称为<strong>瓶颈</strong>的中间部分。上采样从瓶颈开始。</h4>



<p>上采样</p>



<h4>在上采样期间，每个转置卷积块扩展来自前一块的信息，同时连接来自相应下采样块的信息。通过连接信息，网络可以学习根据这些信息组合更精确的输出。</h4>



<p>这种架构能够定位，即，它能够逐个像素地找到感兴趣的对象。此外，UNet还允许网络将上下文信息从较低分辨率层传播到较高分辨率层。这允许网络产生高分辨率样本。</p>



<p>马尔可夫鉴别器</p>



<h3>鉴频器采用贴片GAN架构。该架构包含多个转置卷积模块。它取图像的一个NxN部分，并试图发现它是真的还是假的。n可以是任意大小。它可以比原始图像小，但仍然能够产生高质量的结果。鉴别器在整个图像上卷积应用。此外，因为鉴别器更小，即与发生器相比它具有更少的参数，所以它实际上更快。</h3>



<p>PatchGAN可以有效地将图像建模为马尔可夫随机场，其中NxN被视为独立的面片。所以PatchGAN可以理解为一种质感/风格的丧失。</p>



<p>损失函数</p>



<h3>损失函数是:</h3>



<p>上面的等式有两个部分:一个用于鉴别器，另一个用于发生器。让我们一个一个的了解他们两个。</p>





<p>在任何GAN中，在每次迭代中首先训练鉴别器，以便它可以识别真实和虚假数据，从而可以在它们之间进行鉴别或分类。本质上，</p>



<p><strong> D(x，y) = 1即实数和，</strong></p>



<p><strong> D(x，G(z)) = 0即伪。</strong></p>



<p>值得注意的是，G(z)也将产生假样本，因此其值将更接近于零。理论上，鉴别器应该总是只将G(z)分类为零。因此鉴别器应该在每次迭代中保持真实和虚假之间的最大距离，即1和0。换句话说，鉴别器应该最大化损失函数。</p>



<p>在鉴别器之后，发生器被训练。生成器，即G(z)应该学习产生更接近真实样本的样本。为了学习原始分布，它从鉴别器获得帮助，即，我们将D(x，G(z)) = 1而不是D(x，G(z)) = 0。</p>



<p>随着标记的改变，发生器现在根据属于具有基本事实标记的鉴别器的参数来优化其参数。该步骤确保发生器现在可以产生接近真实数据的样本，即1。</p>



<p>损失函数也与L1损失混合，使得发生器不仅欺骗鉴别器，而且产生接近地面真实的图像。本质上，损耗函数对发电机有一个额外的L1损耗。</p>



<p>因此，最终损失函数为:</p>





<p>值得注意的是，L1损失能够保留图像中的低频细节，但它将无法捕捉高频细节。因此，它仍然会产生模糊的图像。为了解决这个问题，使用了PatchGAN。</p>





<p>最佳化</p>



<h3>优化和训练过程类似于香草甘。但是训练本身是一个困难的过程，因为GAN的目标函数更多地是凹-凹的而不是凸-凹的。正因为如此，很难找到一个鞍点，这就是为什么训练和优化GANs很困难。</h3>



<p>正如我们之前看到的，生成器不是直接训练的，而是通过鉴别器训练的。这实质上限制了发电机的优化。如果鉴别器未能捕获高维空间，那么可以肯定的是，生成器将不能产生好的样本。另一方面，如果我们能够以一种更加优化的方式训练鉴别器，那么我们就可以保证生成器也将得到优化的训练。</p>



<p>在训练的早期阶段，G未经训练，产生好样本的能力较弱。这使得鉴别器非常强大。因此，不是将log(1D(G(z))最小化，而是将发生器训练为将log D(G(z))最大化。这在训练的早期阶段创造了某种稳定性。</p>



<p>解决不稳定性的其他方法有:</p>



<p>在模型的每一层使用<strong>光谱归一化</strong></p>



<ol><li>使用<strong> Wasserstein损失</strong>计算真实或虚假图像的平均分数。</li><li>Pix2Pix动手示例</li></ol>







<h2 id="Hands-on-with-Pix2Pix">让我们用PyTorch对Pix2Pix进行编码，直观地了解它的工作原理及其背后的各种组件。本节将让您清楚地了解Pix2Pix是如何工作的。</h2>



<p>让我们从下载数据开始。以下代码可用于下载数据。</p>



<p>数据可视化</p>



<pre class="hljs">!wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz
!tar -xvf facades.tar.gz</pre>



<h3>一旦下载了数据，我们就可以将它们可视化，以了解根据需求格式化数据所需的必要步骤。</h3>



<p>我们将导入以下库进行数据可视化。</p>



<p>从上面的图像中，我们可以看到数据有两个图像连接在一起。如果我们看到上面的图像的形状，我们发现宽度是512，这意味着图像可以很容易地分成两部分。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

path = <span class="hljs-string">'/content/facades/train/'</span>
plt.imshow(cv2.imread(f<span class="hljs-string">'{path}91.jpg'</span>))</pre>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-full is-resized"><img decoding="async" loading="lazy" src="../Images/44a072ab90851b332a00b6797c389192.png" alt="Data visualization" class="wp-image-57288" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_17.png?resize=633%2C338&amp;ssl=1"/><figcaption><em>Source: Author</em></figcaption></figure></div>


<p><em> &gt; &gt;图像的形状:(256，512，3) </em></p>



<pre class="hljs">print(<span class="hljs-string">'Shape of the image: '</span>,cv2.imread(f<span class="hljs-string">'{path}91.jpg'</span>).shape)</pre>



<p>为了分离图像，我们将使用以下命令:</p>



<p>左边的图像将是我们的基础真理，而右边的图像将是我们的条件图像。我们将它们分别称为y和x。</p>



<pre class="hljs">
image = cv2.imread(f<span class="hljs-string">'{path}91.jpg'</span>)
w = image.shape[<span class="hljs-number">1</span>]//<span class="hljs-number">2</span>
image_real = image[:, :w, :]
image_cond = image[:, w:, :]
fig, axes = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">18</span>,<span class="hljs-number">6</span>))
axes[<span class="hljs-number">0</span>].imshow(image_real, label=<span class="hljs-string">'Real'</span>)
axes[<span class="hljs-number">1</span>].imshow(image_cond, label=<span class="hljs-string">'Condition'</span>)
plt.show()</pre>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/cc020425c470b18ef2ec4574719ca462.png" alt="Data visualization" class="wp-image-57294" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_11.png?ssl=1"/><figcaption><em>Source: Author</em></figcaption></figure></div>


<p>创建数据加载器</p>



<h3>Dataloader是一个允许我们按照PyTorch要求格式化数据的功能。这将包括两个步骤:</h3>



<p>1.格式化数据，即从源中读取数据，裁剪数据，然后将其转换为Pytorch张量。</p>



<p>2.在将数据输入神经网络之前，使用Pytorch的DataLoader函数加载数据以创建批处理。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">data</span><span class="hljs-params">(Dataset)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, path=<span class="hljs-string">'/content/facades/train/'</span>)</span>:</span>
       self.filenames = glob(path+<span class="hljs-string">'*.jpg'</span>)


   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>
       <span class="hljs-keyword">return</span> len(self.filenames)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, idx)</span>:</span>
       filename = self.filenames[idx]

       image = cv2.imread(filename)
       image_width = image.shape[<span class="hljs-number">1</span>]
       image_width = image_width // <span class="hljs-number">2</span>
       real = image[:, :image_width, :]
       condition = image[:, image_width:, :]

       real = transforms.functional.to_tensor(real)
       condition = transforms.functional.to_tensor(condition)

       <span class="hljs-keyword">return</span> real, condition</pre>



<p>请记住，我们将为培训和验证创建一个数据加载器。</p>



<pre class="hljs">train_dataset = data()
train_loader = DataLoader(train_dataset, batch_size=<span class="hljs-number">4</span>, shuffle=<span class="hljs-keyword">True</span>)

val_dataset = data(path=<span class="hljs-string">'/content/facades/val/'</span>)
val_loader = DataLoader(train_dataset, batch_size=<span class="hljs-number">4</span>, shuffle=<span class="hljs-keyword">True</span>)</pre>



<p>Utils</p>



<h3>在本节中，我们将创建用于构建生成器和鉴别器的组件。我们将创建的组件是用于下采样的卷积函数和用于上采样的转置卷积函数，分别称为cnn_block和tcnn_block。</h3>



<p>发电机</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cnn_block</span><span class="hljs-params">(in_channels,out_channels,kernel_size,stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">0</span>, first_layer = False)</span>:</span>

   <span class="hljs-keyword">if</span> first_layer:
       <span class="hljs-keyword">return</span> nn.Conv2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding)
   <span class="hljs-keyword">else</span>:
       <span class="hljs-keyword">return</span> nn.Sequential(
           nn.Conv2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding),
           nn.BatchNorm2d(out_channels,momentum=<span class="hljs-number">0.1</span>,eps=<span class="hljs-number">1e-5</span>),
           )

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tcnn_block</span><span class="hljs-params">(in_channels,out_channels,kernel_size,stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">0</span>,output_padding=<span class="hljs-number">0</span>, first_layer = False)</span>:</span>
   <span class="hljs-keyword">if</span> first_layer:
       <span class="hljs-keyword">return</span> nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding,output_padding=output_padding)

   <span class="hljs-keyword">else</span>:
       <span class="hljs-keyword">return</span> nn.Sequential(
           nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding,output_padding=output_padding),
           nn.BatchNorm2d(out_channels,momentum=<span class="hljs-number">0.1</span>,eps=<span class="hljs-number">1e-5</span>),
           )</pre>



<h3>现在，让我们定义生成器。我们将使用这两个组件来定义相同的。</h3>



<p>鉴别器</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Generator</span><span class="hljs-params">(nn.Module)</span>:</span>
 <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,instance_norm=False)</span>:</span>
   super(Generator,self).__init__()
   self.e1 = cnn_block(c_dim,gf_dim,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>, first_layer = <span class="hljs-keyword">True</span>)
   self.e2 = cnn_block(gf_dim,gf_dim*<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,)
   self.e3 = cnn_block(gf_dim*<span class="hljs-number">2</span>,gf_dim*<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,)
   self.e4 = cnn_block(gf_dim*<span class="hljs-number">4</span>,gf_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,)
   self.e5 = cnn_block(gf_dim*<span class="hljs-number">8</span>,gf_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,)
   self.e6 = cnn_block(gf_dim*<span class="hljs-number">8</span>,gf_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,)
   self.e7 = cnn_block(gf_dim*<span class="hljs-number">8</span>,gf_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,)
   self.e8 = cnn_block(gf_dim*<span class="hljs-number">8</span>,gf_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>, first_layer=<span class="hljs-keyword">True</span>)

   self.d1 = tcnn_block(gf_dim*<span class="hljs-number">8</span>,gf_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
   self.d2 = tcnn_block(gf_dim*<span class="hljs-number">8</span>*<span class="hljs-number">2</span>,gf_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
   self.d3 = tcnn_block(gf_dim*<span class="hljs-number">8</span>*<span class="hljs-number">2</span>,gf_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
   self.d4 = tcnn_block(gf_dim*<span class="hljs-number">8</span>*<span class="hljs-number">2</span>,gf_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
   self.d5 = tcnn_block(gf_dim*<span class="hljs-number">8</span>*<span class="hljs-number">2</span>,gf_dim*<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
   self.d6 = tcnn_block(gf_dim*<span class="hljs-number">4</span>*<span class="hljs-number">2</span>,gf_dim*<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
   self.d7 = tcnn_block(gf_dim*<span class="hljs-number">2</span>*<span class="hljs-number">2</span>,gf_dim*<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
   self.d8 = tcnn_block(gf_dim*<span class="hljs-number">1</span>*<span class="hljs-number">2</span>,c_dim,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>, first_layer = <span class="hljs-keyword">True</span>)
   self.tanh = nn.Tanh()

 <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self,x)</span>:</span>
   e1 = self.e1(x)
   e2 = self.e2(F.leaky_relu(e1,<span class="hljs-number">0.2</span>))
   e3 = self.e3(F.leaky_relu(e2,<span class="hljs-number">0.2</span>))
   e4 = self.e4(F.leaky_relu(e3,<span class="hljs-number">0.2</span>))
   e5 = self.e5(F.leaky_relu(e4,<span class="hljs-number">0.2</span>))
   e6 = self.e6(F.leaky_relu(e5,<span class="hljs-number">0.2</span>))
   e7 = self.e7(F.leaky_relu(e6,<span class="hljs-number">0.2</span>))
   e8 = self.e8(F.leaky_relu(e7,<span class="hljs-number">0.2</span>))
   d1 = torch.cat([F.dropout(self.d1(F.relu(e8)),<span class="hljs-number">0.5</span>,training=<span class="hljs-keyword">True</span>),e7],<span class="hljs-number">1</span>)
   d2 = torch.cat([F.dropout(self.d2(F.relu(d1)),<span class="hljs-number">0.5</span>,training=<span class="hljs-keyword">True</span>),e6],<span class="hljs-number">1</span>)
   d3 = torch.cat([F.dropout(self.d3(F.relu(d2)),<span class="hljs-number">0.5</span>,training=<span class="hljs-keyword">True</span>),e5],<span class="hljs-number">1</span>)
   d4 = torch.cat([self.d4(F.relu(d3)),e4],<span class="hljs-number">1</span>)
   d5 = torch.cat([self.d5(F.relu(d4)),e3],<span class="hljs-number">1</span>)
   d6 = torch.cat([self.d6(F.relu(d5)),e2],<span class="hljs-number">1</span>)
   d7 = torch.cat([self.d7(F.relu(d6)),e1],<span class="hljs-number">1</span>)
   d8 = self.d8(F.relu(d7))

   <span class="hljs-keyword">return</span> self.tanh(d8)</pre>



<h3>让我们使用下采样函数来定义鉴别器。</h3>



<p>定义参数</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Discriminator</span><span class="hljs-params">(nn.Module)</span>:</span>
 <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,instance_norm=False)</span>:</span>
   super(Discriminator,self).__init__()
   self.conv1 = cnn_block(c_dim*<span class="hljs-number">2</span>,df_dim,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>, first_layer=<span class="hljs-keyword">True</span>) 
   self.conv2 = cnn_block(df_dim,df_dim*<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
   self.conv3 = cnn_block(df_dim*<span class="hljs-number">2</span>,df_dim*<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
   self.conv4 = cnn_block(df_dim*<span class="hljs-number">4</span>,df_dim*<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
   self.conv5 = cnn_block(df_dim*<span class="hljs-number">8</span>,<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, first_layer=<span class="hljs-keyword">True</span>)

   self.sigmoid = nn.Sigmoid()
 <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x, y)</span>:</span>
   O = torch.cat([x,y],dim=<span class="hljs-number">1</span>)
   O = F.leaky_relu(self.conv1(O),<span class="hljs-number">0.2</span>)
   O = F.leaky_relu(self.conv2(O),<span class="hljs-number">0.2</span>)
   O = F.leaky_relu(self.conv3(O),<span class="hljs-number">0.2</span>)
   O = F.leaky_relu(self.conv4(O),<span class="hljs-number">0.2</span>)
   O = self.conv5(O)

   <span class="hljs-keyword">return</span> self.sigmoid(O)</pre>



<h3>在本节中，我们将定义参数。这些参数将帮助我们训练神经网络。</h3>



<p>初始化模型</p>



<pre class="hljs">
batch_size = <span class="hljs-number">4</span>
workers = <span class="hljs-number">2</span>

epochs = <span class="hljs-number">30</span>

gf_dim = <span class="hljs-number">64</span>
df_dim = <span class="hljs-number">64</span>

L1_lambda = <span class="hljs-number">100.0</span>

in_w = in_h = <span class="hljs-number">256</span>
c_dim = <span class="hljs-number">3</span>

device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)</pre>



<h3>让我们初始化这两个模型，并启用CUDA进行更快的训练。</h3>



<p>我们还将定义优化器和损失函数。</p>



<pre class="hljs">G = Generator().to(device)
D = Discriminator().to(device)</pre>



<p>培养</p>



<pre class="hljs">G_optimizer = optim.Adam(G.parameters(), lr=<span class="hljs-number">2e-4</span>,betas=(<span class="hljs-number">0.5</span>,<span class="hljs-number">0.999</span>))
D_optimizer = optim.Adam(D.parameters(), lr=<span class="hljs-number">2e-4</span>,betas=(<span class="hljs-number">0.5</span>,<span class="hljs-number">0.999</span>))

bce_criterion = nn.BCELoss()
L1_criterion = nn.L1Loss()</pre>



<h3>一旦定义了所有重要的函数，我们将初始化训练循环。</h3>



<p>监控我们的模型</p>



<pre class="hljs"><span class="hljs-keyword">for</span> ep <span class="hljs-keyword">in</span> range(epochs):
 <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> enumerate(train_loader):

   y, x = data
   x = x.to(device)
   y = y.to(device)

   b_size = x.shape[<span class="hljs-number">0</span>]

   real_class = torch.ones(b_size,<span class="hljs-number">1</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>).to(device)
   fake_class = torch.zeros(b_size,<span class="hljs-number">1</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>).to(device)

   
   D.zero_grad()

   real_patch = D(y,x)
   real_gan_loss=bce_criterion(real_patch,real_class)

   fake=G(x)

   fake_patch = D(fake.detach(),x)
   fake_gan_loss=bce_criterion(fake_patch,fake_class)

   D_loss = real_gan_loss + fake_gan_loss
   D_loss.backward()
   D_optimizer.step()


   
   G.zero_grad()
   fake_patch = D(fake,x)
   fake_gan_loss=bce_criterion(fake_patch,real_class)

   L1_loss = L1_criterion(fake,y)
   G_loss = fake_gan_loss + L1_lambda*L1_loss
   G_loss.backward()

   G_optimizer.step()

   
   run[<span class="hljs-string">"Gen Loss"</span>].log(G_loss.item())
   run[<span class="hljs-string">"Dis Loss"</span>].log(D_loss.item())
   run[<span class="hljs-string">'L1 Loss'</span>].log(L1_loss.item())
   run[<span class="hljs-string">'Gen GAN Loss'</span>].log(fake_gan_loss.item())
   
   torch.save(G.state_dict(), <span class="hljs-string">'PIX2PIX.ckpt'</span>)
   run[<span class="hljs-string">'model_checkpoints'</span>].upload(<span class="hljs-string">'PIX2PIX.ckpt'</span>)

   <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>)%<span class="hljs-number">5</span> == <span class="hljs-number">0</span> :
     print(<span class="hljs-string">'Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f},D(real): {:.2f}, D(fake):{:.2f},g_loss_gan:{:.4f},g_loss_L1:{:.4f}'</span>
           .format(ep, epochs, i+<span class="hljs-number">1</span>, len(train_loader), D_loss.item(), G_loss.item(),real_patch.mean(), fake_patch.mean(),fake_gan_loss.item(),L1_loss.item()))
     G_losses.append(G_loss.item())
     D_losses.append(D_loss.item())
     G_GAN_losses.append(fake_gan_loss.item())
     G_L1_losses.append(L1_loss.item())

     <span class="hljs-keyword">with</span> torch.no_grad():
       G.eval()
       fake = G(fixed_x).detach().cpu()
       G.train()
     figs=plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))
     plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)
     plt.axis(<span class="hljs-string">"off"</span>)
     plt.title(<span class="hljs-string">"conditional image (x)"</span>)
     plt.imshow(np.transpose(vutils.make_grid(fixed_x, nrow=<span class="hljs-number">1</span>,padding=<span class="hljs-number">5</span>, normalize=<span class="hljs-keyword">True</span>).cpu(),(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>)))

     plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)
     plt.axis(<span class="hljs-string">"off"</span>)
     plt.title(<span class="hljs-string">"fake image"</span>)
     plt.imshow(np.transpose(vutils.make_grid(fake, nrow=<span class="hljs-number">1</span>,padding=<span class="hljs-number">5</span>, normalize=<span class="hljs-keyword">True</span>).cpu(),(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>)))

     plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)
     plt.axis(<span class="hljs-string">"off"</span>)
     plt.title(<span class="hljs-string">"ground truth (y)"</span>)
     plt.imshow(np.transpose(vutils.make_grid(fixed_y, nrow=<span class="hljs-number">1</span>,padding=<span class="hljs-number">5</span>, normalize=<span class="hljs-keyword">True</span>).cpu(),(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>)))

     plt.savefig(os.path.join(<span class="hljs-string">'./'</span>,<span class="hljs-string">'pix2pix'</span>+<span class="hljs-string">"-"</span>+str(ep) +<span class="hljs-string">".png"</span>))

     run[<span class="hljs-string">'Results'</span>].log(File(f<span class="hljs-string">'pix2pix-{str(ep)}.png'</span>))
     plt.close()
     img_list.append(figs)</pre>



<h3>训练模型不是最后一步。您需要监控和跟踪培训，以分析绩效并在必要时实施更改。考虑到监控有太多损失、图和指标要处理的GAN的性能是多么费力，我们将在这一步使用Neptune。</h3>



<p>Neptune允许用户:</p>



<p><span class="c-list__counter"> 1 </span>监控模特的现场表演</p>



<div id="case-study-numbered-list-block_61a72948b43b2" class="block-case-study-numbered-list ">

    
    <h2 id="h-"><span class="c-list__counter"> 2 </span>监控硬件的性能</h2>

    <ul class="c-list">
                    <li class="c-list__item"><span class="c-list__counter"> 3 </span>存储和比较不同运行的不同元数据(如指标、参数、性能、数据等。)</li>
                    <li class="c-list__item">与他人分享工作</li>
                    <li class="c-list__item">要开始，只需遵循以下步骤:</li>
                    <li class="c-list__item">1.在本地系统上使用<code>pip install neptune-client</code>或<code>conda install -c conda-forge neptune-client</code>安装neptune-client。</li>
            </ul>
</div>



<p>2.创建账号，登录<a href="https://web.archive.org/web/20221117203630/https://docs.neptune.ai/getting-started/installation" target="_blank" rel="noreferrer noopener"> Neptune.ai </a>。</p>



<p>3.登录后，<a href="https://web.archive.org/web/20221117203630/https://docs.neptune.ai/administration/projects#create-project" target="_blank" rel="noreferrer noopener">创建一个<strong>新项目</strong> </a>。</p>



<pre class="hljs">!pip install neptune-client</pre>



<p>4.现在，您可以将不同的元数据记录到Neptune。点击了解更多信息<a href="https://web.archive.org/web/20221117203630/https://docs.neptune.ai/you-should-know/what-can-you-log-and-display" target="_blank" rel="noreferrer noopener">。</a></p>



<p>对于这个项目，我们将把我们的参数记录到Neptune仪表板中。要将参数或任何信息记录到仪表板中，请创建字典。</p>



<p>一旦创建了字典，我们将使用以下命令记录它们:</p>



<p>请记住，损耗、生成的图像和模型的权重都是使用“run”命令记录到Neptune仪表盘中的。</p>



<pre class="hljs">
PARAMS = {<span class="hljs-string">'Epoch'</span>: epochs,
         <span class="hljs-string">'Batch Size'</span>: batch_size,
         <span class="hljs-string">'Input Channels'</span>: c_dim,

         <span class="hljs-string">'Workers'</span>: workers,
         <span class="hljs-string">'Optimizer'</span>: <span class="hljs-string">'Adam'</span>,
         <span class="hljs-string">'Learning Rate'</span>: <span class="hljs-number">2e-4</span>,
         <span class="hljs-string">'Metrics'</span>: [<span class="hljs-string">'Binary Cross Entropy'</span>, <span class="hljs-string">'L1 Loss'</span>],
         <span class="hljs-string">'Activation'</span>: [<span class="hljs-string">'Leaky Relu'</span>, <span class="hljs-string">'Tanh'</span>, <span class="hljs-string">'Sigmoid'</span> ],
         <span class="hljs-string">'Device'</span>: device}
</pre>



<p>例如，在上面的培训中，您会发现以下命令:</p>



<pre class="hljs">run[<span class="hljs-string">'parameters'</span>] = PARAMS</pre>



<p>这些基本上是用来记录数据到海王星仪表板。</p>



<p>培训初始化后，所有记录的信息将自动记录到仪表板中。Neptune从训练中获取实时信息，允许<a href="https://web.archive.org/web/20221117203630/https://docs.neptune.ai/how-to-guides/model-monitoring" target="_blank" rel="noreferrer noopener">实时监控整个过程</a>。</p>



<pre class="hljs">
   run[<span class="hljs-string">"Gen Loss"</span>].log(G_loss.item())
   run[<span class="hljs-string">"Dis Loss"</span>].log(D_loss.item())
   run[<span class="hljs-string">'L1 Loss'</span>].log(L1_loss.item())
   run[<span class="hljs-string">'Gen GAN Loss'</span>].log(fake_gan_loss.item())</pre>



<p>以下是监控过程截图。</p>







<p>您还可以访问所有元数据并查看生成的样本。</p>



<p>最后，您可以比较不同运行的元数据。这是很有用的，例如，当您想要查看在调整一些参数后，您的模型是否比前一个模型执行得更好。</p>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221117203630/https://neptune.ai/pix2pix-key-model-architecture-decisions_22" target="_blank" rel="noopener"><img decoding="async" src="../Images/16ab1b5ccb0bc657094019ebb5f7cb9d.png" alt="Monitoring the performance of the model" class="wp-image-57283" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_22-3173379115-1638343226282.png?ssl=1"/></a><figcaption><em>Monitoring the performance of the model | <em><a href="https://web.archive.org/web/20221117203630/https://app.neptune.ai/nielspace/Pix2Pix/e/PIX-13/charts" target="_blank" rel="noreferrer noopener">Source</a></em></em></figcaption></figure></div>

<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221117203630/https://neptune.ai/pix2pix-key-model-architecture-decisions_12" target="_blank" rel="noopener"><img decoding="async" src="../Images/d7f18002266b56538ae654123e1415bb.png" alt="Monitoring the performance of the hardware" class="wp-image-57293" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_12-2517155816-1638343240689.png?ssl=1"/></a><figcaption><em>Monitoring the performance of the hardware | <em><a href="https://web.archive.org/web/20221117203630/https://app.neptune.ai/nielspace/Pix2Pix/e/PIX-13/monitoring">Source</a></em></em></figcaption></figure></div>


<p>关键要点</p>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221117203630/https://neptune.ai/pix2pix-key-model-architecture-decisions_7" target="_blank" rel="noopener"><img decoding="async" src="../Images/fbd321745f226642c132f2edf91061d9.png" alt="Access to all metadata" class="wp-image-57298" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_7-3877548907-1638343349506.png?ssl=1"/></a><figcaption><em>Access to all metadata | <em><a href="https://web.archive.org/web/20221117203630/https://app.neptune.ai/nielspace/Pix2Pix/e/PIX-13/all">Source</a></em></em></figcaption></figure></div>

<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221117203630/https://neptune.ai/pix2pix-key-model-architecture-decisions_19" target="_blank" rel="noopener"><img decoding="async" src="../Images/3a697f762aca3e4d5b62387f167fd41d.png" alt="Access to the generated samples" class="wp-image-57286" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2pix-Key-Model-Architecture-Decisions_19-31090272-1638343428502.png?ssl=1"/></a><figcaption><em>Access to the generated samples | <em><a href="https://web.archive.org/web/20221117203630/https://app.neptune.ai/nielspace/Pix2Pix/e/PIX-13/all?path=&amp;attribute=Results">Source</a></em></em></figcaption></figure></div>


<p>Pix2Pix是一个有条件的GAN，它使用图像和标签来生成图像。</p>


<div class="wp-block-image is-style-default">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221117203630/https://neptune.ai/blog/pix2pix-key-model-architecture-decisions/attachment/pix2pix-compare-runs" target="_blank" rel="noopener"><img decoding="async" src="../Images/e6f6486ec431d5f38efa0586f5e48a9e.png" alt="Pix2Pix compare runs" class="wp-image-57429" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221117203630im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Pix2Pix-compare-runs.png?ssl=1"/></a><figcaption><em>Comparing metadata from different runs | <a href="https://web.archive.org/web/20221117203630/https://app.neptune.ai/nielspace/Pix2Pix/experiments?compare=IwJgNMQ&amp;split=cmp&amp;dash=charts&amp;viewId=standard-view" target="_blank" rel="noreferrer noopener">Source</a></em></figcaption></figure></div>


<h2 id="h-key-takeaways">它使用两种架构:</h2>



<ul><li>发电机的u形网</li><li>鉴别器的PatchGAN<ul><li>PatchGAN在生成的图像中使用NxN大小的较小补丁来区分真假，而不是一次性区分整个图像。</li><li>Pix2Pix有一个专门针对生成器的额外损耗，以便它可以生成更接近地面真实情况的图像。</li></ul></li><li>Pix2Pix是一种成对图像翻译算法。</li><li>您可以探索的其他gan包括:</li><li>CycleGAN:它类似于Pix2Pix，因为除了数据部分，大部分方法都是相同的。它不是成对图像翻译，而是不成对图像翻译。学习和探索CycleGAN会容易得多，因为它是由相同的作者开发的。</li></ul>



<h4>如果您对文本到图像的翻译感兴趣，那么您应该探索:</h4>



<ol><li>您可能想尝试的其他有趣的GANs项目:</li><li>StyleGAN</li><li>阿尼梅根<ul><li>比根</li><li>年龄-cGAN</li><li>starman参考资料</li><li><a href="https://web.archive.org/web/20221117203630/https://arxiv.org/abs/1611.07004" target="_blank" rel="noreferrer noopener nofollow">使用条件对抗网络的图像到图像翻译</a></li><li><a href="https://web.archive.org/web/20221117203630/https://www.deeplearningbook.org/" target="_blank" rel="noreferrer noopener nofollow">深度学习书籍:Ian Goodfellow </a></li></ul></li></ol>



<h3><a href="https://web.archive.org/web/20221117203630/https://arxiv.org/abs/1406.2661" target="_blank" rel="noreferrer noopener nofollow">生成性对抗网络:古德菲勒等人</a></h3>



<ol><li><a href="/web/20221117203630/https://neptune.ai/blog/generative-adversarial-networks-gan-applications" target="_blank" rel="noreferrer noopener">生成性对抗网络和一些GAN应用——你需要知道的一切</a></li><li><a href="https://web.archive.org/web/20221117203630/https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/" target="_blank" rel="noreferrer noopener nofollow">生成性对抗性网络损失函数的温和介绍</a>。</li><li><a href="https://web.archive.org/web/20221117203630/https://arxiv.org/abs/1406.2661" target="_blank" rel="noreferrer noopener nofollow">Generative Adversarial Networks: Goodfellow et al.</a></li><li><a href="/web/20221117203630/https://neptune.ai/blog/generative-adversarial-networks-gan-applications" target="_blank" rel="noreferrer noopener">Generative Adversarial Networks and Some of GAN Applications – Everything You Need to Know</a></li><li><a href="https://web.archive.org/web/20221117203630/https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/" target="_blank" rel="noreferrer noopener nofollow">A Gentle Introduction to Generative Adversarial Network Loss Functions</a>.</li></ol>
        </div>
        
    </div>    
</body>
</html>