<html>
<head>
<title>ML Model Interpretation Tools: What, Why, and How to Interpret </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>ML模型解释工具:什么，为什么，以及如何解释</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/ml-model-interpretation-tools#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/ml-model-interpretation-tools#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p><a href="https://web.archive.org/web/20221206010752/https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739#:~:text=Model%20interpretation%20at%20heart%2C%20is,impact%20to%20business%20and%20society." target="_blank" rel="noreferrer noopener nofollow">解释</a>字面意思是解释或展示自己对某事的理解。</p>



<p>当您创建一个ML模型时，它只不过是一个可以学习模式的算法，对于其他项目涉众来说，它可能感觉像一个黑盒。有时甚至对你。</p>



<p>这就是为什么我们有模型解释工具。</p>



<h2 id="h-what-is-model-interpretation">什么是模型解释？</h2>



<p>一般来说，ML模型必须获得预测，并使用这些预测和最终的见解来解决一系列问题。我们已经可以问几个后续问题了:</p>



<ul><li>这些预测有多可信？</li><li>他们是否足够可靠，可以做出重大决策？</li></ul>



<p>模型解释将你的注意力从‘结论是什么？’“为什么会得出这个结论？”。您可以了解模型的决策过程，即究竟是什么驱动模型对数据点进行正确或错误的分类。</p>



<h2 id="h-why-is-model-interpretation-important">为什么模型解释很重要？</h2>



<p>考虑一个哈士奇与狼(狗品种)分类器的例子，其中一些哈士奇被错误地分类为狼。使用可解释的机器学习，您可能会发现这些错误分类主要是因为图像中的雪而发生的，分类器将雪用作预测狼的特征。</p>



<p>这是一个简单的例子，但是您已经可以看到为什么模型解释是重要的了。它至少在几个方面有助于您的模型:</p>



<ul><li>公平——公司用来决定加薪和晋升的一个可解释的模型可以告诉你为什么某个人得到或没有得到晋升。</li><li><strong>可靠性</strong>–输入的微小变化不会导致多米诺骨牌效应，也不会大幅改变输出。</li><li><strong>因果关系</strong>–只有因果关系对决策有用。</li><li><strong>信任</strong>——对于所有的项目涉众来说，尤其是在非技术方面，更容易信任一个可以用外行的术语解释的模型。</li></ul>



<h2 id="h-how-to-interpret-an-ml-model">如何解读一个ML模型？</h2>



<p>机器学习模型的复杂程度和性能各不相同。一种尺寸并不适合所有人。于是，就有了不同的解读方式。这些方法主要可分为以下几类:</p>



<ol><li><strong>特定型号/不特定型号</strong><ul><li>特定于模型的方法是特定于某些模型的，它们依赖于模型的内部机制来做出某些结论。这些方法可能包括解释广义线性模型(GLMs)中的系数权重，或神经网络中的权重和偏差。</li><li>模型无关的方法可以用在任何模型上。它们通常在培训后使用。它们通常通过分析要素输入-输出对之间的关系来工作，并且无法访问模型的内部机制，如权重或假设。</li></ul></li><li><strong>局部/全局范围</strong><ul><li>局部范围仅涵盖单个预测，仅捕获指定预测背后的原因。</li><li>全局范围超越了单个数据点，涵盖了模型的一般行为。</li></ul></li></ol>



<p>让我们创建一个模型来解释。我们将对模型创建步骤做一个简短的演练，然后我们将关注不同的与模型无关的工具和框架来解释创建的模型，而不是解决实际的问题。</p>



<h2 id="h-model-creation">模型创建</h2>



<h3>1.加载数据集</h3>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/d1dde99d1a41ee3345b63c1ac369c5a8.png" alt="Model interpretation_dataset" class="wp-image-45069" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206010752im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Model-interpretation_dataset.png?ssl=1"/></figure>



<p>数据集架构:</p>



<p id="separator-block_61af3d97e362c" class="block-separator block-separator--10">属性</p>



<div id="medium-table-block_61af3d9ee362d" class="block-medium-table c-table__outer-wrapper ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">描述</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">                                                      </p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>社交媒体帖子类型</p> </div></td>

                    
                        <td class="c-ceil">社交媒体帖子的类型</td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>社交媒体帖子的领域</p> </div></td>

                    
                        <td class="c-ceil">社交媒体帖子的领域</td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>帖子的实际文字对话</p> </div></td>

                    
                        <td class="c-ceil">帖子的实际文本对话</td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>



<p id="separator-block_609159de680f8" class="block-separator block-separator--20">患者标签</p>



<p>(1 =患者，0 =非患者)</p>



<p id="separator-block_61af3e44e3649" class="block-separator block-separator--10"> </p>



<div id="medium-table-block_61af3e4be364a" class="block-medium-table c-table__outer-wrapper ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">2.执行探索性数据分析和数据预处理</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">填充空值。</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

        
        </tbody>
    </table>

</div>



<p id="separator-block_609162a9680fa" class="block-separator block-separator--20">删除冗余特征，如时间(GMT)。</p>



<h3>通过删除所有字母数字字符来清理文本数据。</h3>



<ol><li>编码分类值属性的标签</li><li>处理某些属性中出现的错误值</li><li>基于文本的特征的词汇化和Tf-Idf矢量化。</li><li>3.特征工程</li><li>将所有文本特征组合成单个文本特征以弥补缺失值。</li><li>将数据映射到工作日。</li></ol>







<h3>将时间映射到小时。</h3>



<ul><li>基于对话文本的长度创建另一个特征。</li><li>4.最终培训和测试数据集</li><li>可以看出，tf-idf矢量化以及特征工程导致了训练和测试数据集中属性数量的增加。</li><li>5.训练分类器</li></ul>







<h3>虽然一系列模型可以用于这项任务，但我们将使用随机森林分类器，它不容易解释，因为它很复杂。我们希望使用一些工具和框架来使它变得可解释。</h3>



<p>6.获取测试数据集的预测</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/87d494d1c03ccb134435f420520e0be1.png" alt="Model interpretation_dataset" class="wp-image-45070" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206010752im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Model-interpretation_dataset-2.png?ssl=1"/></figure>



<h3>7.模型性能评估</h3>



<p>由于我们没有来自测试数据集的正确标签，让我们通过训练分类报告和K倍交叉验证分数来看看我们的模型是如何执行的。</p>



<pre class="hljs">From sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
rfc = RandomForestClassifier()
rfc.fit(train_X,y)</pre>



<h3>培训分类报告:</h3>



<pre class="hljs">testpred = rfc.predict(test_X)
</pre>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/181e3a4a815423f95a6350dd0edbc91d.png" alt="Model interpretation_dataset" class="wp-image-45071" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206010752im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Model-interpretation_dataset-3.png?ssl=1"/></figure>



<h3>k倍交叉验证分数:</h3>



<p>既然我们已经建立了一个模型，是时候开始使用解释工具来解释我们模型的预测了。我们将从最流行的工具之一ELI5开始。</p>



<p>1.ELI5</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/3bb637c16d2597dbfe1196d195dd5fce.png" alt="Model interpretation_training classification" class="wp-image-45072" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206010752im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Model-interpretation_training-classification.png?ssl=1"/></figure>



<p>ELI5是“像我5岁一样解释”的首字母缩写。这是一个很受欢迎的Python库，因为它很容易使用。它主要用于:</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/c572f8ea0ffddbb2461d5c109d5f8ddb.png" alt="Model interpretation_scores" class="wp-image-45073" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206010752im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Model-interpretation_scores.png?ssl=1"/></figure>











<p>了解在预测中起关键作用的重要特征。</p>



<h3>来分析某个特定的个人预测，看看到底是什么导致了这个模型的预测。</h3>



<p>ELI5更多地以局部/全局范围的方式解释模型，而不是我们上面讨论的特定/不可知的方式。</p>



<ul><li>此外，目前ELI5只能用于几个主要的模型Sklearn广义线性模型(GLMs)和基于树的模型，Keras模型，LightGBM，XGBoost，CatBoost。</li><li>如何安装Eli5</li></ul>



<p>可以使用pip命令安装Eli5。</p>



<p>或者，可以用conda命令安装。</p>







<h4>在我们的模型上使用Eli5</h4>



<p>让我们首先导入所需的依赖项</p>



<pre class="hljs">pip install eli5
</pre>



<p>答:<strong>特征权重和重要性</strong></p>



<pre class="hljs">conda install -c conda-forge eli5</pre>



<h4>让我们从一个标准的函数调用开始。</h4>



<p>这是从另一端出来的。</p>



<pre class="hljs">Import eli5
</pre>



<p>“权重”列包含“特征”列中显示的相关特征的权重。现在你可能会想，如果特征是数字，你怎么理解任何东西？这些见解很难解释。为了解决这个问题，我们需要对我们的原始函数调用进行一些修改。</p>



<p>这里，我们使用vect(Tf-Idf矢量器)和包含工程特性的可变列来获取特性名称。然后，特性名称作为* *参数传递给同一个函数。这是我们现在得到的。</p>



<pre class="hljs">eli5.show_weights(rfc)</pre>



<p>因为我们有5000个Tf-Idf特性，所以看到很多词被赋予高度重要性是有意义的。除了单词之外，还有一个特征“text_num_words”，它是在特征工程之后获得的。它被列为第12个最重要的功能。这可能是一个迭代的过程，你可以看到工程属性的活力，如果需要，重新设计它们。</p>







<p>我们还可以:</p>



<pre class="hljs">columns = [<span class="hljs-string">'Source'</span>,<span class="hljs-string">'weekday'</span>,<span class="hljs-string">'hour'</span>,<span class="hljs-string">'text_num_words'</span>]
feature_names = list(vect.get_feature_names()) + columns
eli5.show_weights(rfc, feature_names = feature_names)
</pre>



<p>使用“Top”参数指定我们是需要所有功能还是只需要前“n”个功能。</p>







<p>使用features_re和features_filter参数只获取那些符合我们的条件和约束的功能。</p>



<p>现在我们来看看Eli5的另一个用例。</p>



<ul><li>B: <strong>个体预测分析</strong></li><li>孤立地分析某些预测可能非常有价值。你可以用通俗易懂的语言向所有利益相关者解释所获得的预测背后的数学机制。</li></ul>



<p>先查一个真阳性的案例。</p>



<p>这是导致模型预测该数据点属于患者的原因。</p>



<p>因此，模型推断这个数据点属于一个病人，因为有类似'<strong>增大的心脏</strong>'、<strong>医院心脏</strong>'、<strong>眩晕</strong>'、<strong>癌症</strong>等特征..有道理。&lt;BIAS&gt;’，即基于训练集分布的模型输出的预期平均得分，起着最关键的作用。</p>



<p>现在让我们来看一个真否定的例子。</p>



<pre class="hljs">eli5.show_prediction(rfc,train_X.toarray()[<span class="hljs-number">1</span>],
feature_names=feature_names, top=<span class="hljs-number">20</span>, show_feature_values=<span class="hljs-keyword">True</span>)
</pre>



<p>与第一张图片相比，我们可以清楚地看到</p>







<p>健康/医疗条件太差了。即使像“已诊断”、“充血性心脏”这样的几个特征进入了前几名，它们的相关贡献和价值也是零。</p>



<p>让我们转向另一个令人兴奋的工具。</p>







<p>2.石灰</p>



<p><a href="https://web.archive.org/web/20221206010752/https://github.com/marcotcr/lime" target="_blank" rel="noreferrer noopener nofollow"> LIME </a>代表<strong>L</strong>ocal<strong>I</strong>interpretable<strong>M</strong>model-agnostic<strong>E</strong>explanations。让我们更深入地了解一下这个名字:</p>



<p>术语“局部”意味着对单个预测的分析。LIME让我们对某个预测背后发生的事情有了更深入的了解。</p>



<h3>它是模型不可知的，这意味着它将每个模型都视为一个黑盒，因此它可以在不访问模型内部的情况下进行解释，从而允许它与广泛的模型一起工作。</h3>



<p>正如名字一样直观，其解释逻辑背后的思想也一样直观:</p>



<ul><li>LIME主要测试一旦模型在输入数据中出现某些变化时，预测会发生什么情况。</li><li>为了测试这一点，LIME在一个新的数据集上训练了一个可解释的模型，该数据集由扰动的样本和黑盒模型的相应预测组成。</li></ul>



<p>这个新的学习模型需要是一个好的局部近似(对于某个个体预测)，但不必是一个好的全局近似。它在数学上可以表示为:</p>



<ul><li>解释模型，例如x，是由LIME创建的可解释模型g，其最小化损失函数L，损失函数L测量解释与原始模型f的预测有多接近，同时模型复杂度ω(g)保持较低(较少特征)。g是一族可能的解释，即所有的glm。</li><li>石灰怎么装？</li><li>像ELI5一样，它也可以用一个简单的pip命令安装。</li></ul>







<p>或者使用conda命令。</p>



<h4>在我们的模型上使用石灰</h4>



<p>LIME主要提供三种解释方法，这三种方法处理不同类型的数据:</p>



<pre class="hljs">pip install lime
</pre>



<p>表格解释，</p>



<pre class="hljs">conda install -c conda-forge lime</pre>



<h4>文字解读，</h4>



<p>图像解读。</p>



<ul><li>在我们的5004个可训练属性中，有5000个是基于Tf-Idf的特性，而这些特性除了单词什么也不是。s，我们就用莱姆的文本解读法。为此，我们必须对我们的训练做一些改变。</li><li>如你所见，我们现在只使用矢量化的文本特征进行建模。</li><li>正如我们所知，LIME准备了一个新的数据集，并在此基础上训练自己的可解释模型。但是对于文本数据，LIME是如何做到的呢？这是怎么回事:</li></ul>



<p>通过切换原始文本中随机选择的单词的存在/不存在来创建新文本。</p>



<pre class="hljs">rfc.fit(vectorized_train_text,y)</pre>



<p>如果包含相应的单词，则特征为1，如果移除，则特征为0，从而使其成为二进制表示。</p>



<p>理论部分讲够了，让我们来看看石灰的作用。</p>



<ul><li>导入所需的依赖项:</li><li>进行所需的函数调用:</li></ul>



<p>首先，我们创建一个文本解释器的实例。然后，因为我们正在处理文本特征，所以我们使用sklearn的管道方法将我们的矢量器(vect)与我们的模型(rfc)相结合，以便文本特征(将获得输入)可以被矢量化，并且预测可以发生。</p>



<p>就像ELI5一样，让我们先检查一个真正的正实例。</p>



<pre class="hljs">Import lime
From sklearn.pipeline <span class="hljs-keyword">import</span> make_pipeline
</pre>



<p>让我们画出实例结果，看看我们会得到什么。</p>



<pre class="hljs">explainer = lime.lime_text.LimeTextExplainer(
class_names=[‘Not Patient’, ‘Patient’])
pl = make_pipeline(vect,rfc)
</pre>



<p>在阅读文本时，它变得非常清楚，它是在谈论一些在急需的心脏移植后挽救了生命的病人，同时感谢医务人员。我们可以看到突出显示的单词出现在“患者”列中，因此负责将该数据点正确分类为“患者”。</p>



<p>现在让我们看看一个真正的负实例的解释图是什么样子的。</p>



<pre class="hljs">exp = explainer.explain_instance(
train[‘combined_text’][<span class="hljs-number">689</span>], pl.predict_proba)
</pre>



<p>第一眼看到突出显示的关键词，如“冷”、“脂肪”、“烧伤”等。，该数据点看起来与“患者”类别相关联。在实际阅读时，我们理解文本是在谈论洗冷水澡的好处，显然，我们的模型也理解这一点。</p>



<pre class="hljs">exp.show_in_notebook()
</pre>







<p>到目前为止一切顺利。但是如果我们的特征是表格格式的连续值，或者是图像的像素值呢？</p>



<p>为此，我们只需要记住我们上面讨论的内容:</p>







<p><span>数据类型</span></p>



<p><span>功能</span></p>



<p><span>正文</span></p>




<table class="tg">
<colgroup>
<col/>
<col/>
</colgroup>
<thead>
  <tr>
    <th class="tg-tgfy"><span> lime.lime_text。LimeTextExplainer() </span></th>
    <th class="tg-tgfy"><span>表格</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-41n8"><span>lime . lime _ tabular . limetable plainer()</span></td>
    <td class="tg-41n8"><span>图像</span></td>
  </tr>
  <tr>
    <td class="tg-41n8"><span>lime . lime _ image . lime image explainer()</span></td>
    <td class="tg-41n8"> </td>
  </tr>
  <tr>
    <td class="tg-41n8">好了，让我们进入下一个模型解释工具——SHAP。</td>
    <td class="tg-41n8">3.SHAP</td>
  </tr>
</tbody>
</table>



<p id="separator-block_60915bd3680f9" class="block-separator block-separator--20">SHapley<strong>A</strong>additive ex<strong>P</strong>解释是一种解释任何机器学习模型输出的博弈论方法。<a href="https://web.archive.org/web/20221206010752/https://christophm.github.io/interpretable-ml-book/shap.html" target="_blank" rel="noreferrer noopener nofollow"> SHAP </a>通过计算每个特征对预测的贡献来解释一个实例的预测。它使用Shapley值。</p>



<p><strong>什么是沙普利值？</strong></p>



<h3>沙普利值——一种来自联盟博弈论的方法——告诉我们如何在特性之间分配“支出”。</h3>



<p>因此，可以通过假设实例的每个特征值是游戏中的“玩家”来解释预测，其中预测是支出。</p>



<p>【Shapely值是怎么计算出来的？</p>



<ul><li>Shapely值是所有可能组合中某个特征值的平均边际贡献。</li><li>假设我们有一个形状为N x M的数据集，其中N是样本数，M是特征数，例如5–A、B、C、D &amp; E。</li></ul>



<p>e是具有连续值的从属属性，而A，B，C，D是我们的分类值预测值。</p>



<ul><li>现在，假设我们要计算特征A的贡献，即计算它的Shapely值。</li><li>我们通过从数据集中随机选取一个实例并使用其特性d的值来模拟只有A、B和C在一个联盟中，然后我们预测这个组合的E，假设它是x。</li><li>现在我们用从A的域中随机抽取的值替换这个组合中的特征A的值(假设它是不同的),并再次预测E，假设这次它是Y。</li><li>X-Y之间的差异，无论是正的还是负的，都是特征A在预测中的贡献。</li><li>对A值的这个采样步骤被反复重复，并且贡献被平均以获得A的Shapely值。</li><li>SHAP的解释可以用数学方法表达为:</li><li>其中g是解释模型，z′ϵ{0,1}<sup>m</sup>是联合向量，m是最大联合大小，而<sub> j </sub> ϵ R是特征j的匀称值</li><li>理论到此为止，让我们看看SHAP在我们的模型上表现如何。</li></ul>



<p>如何安装SHAP</p>







<p>就像其他库一样，可以用pip命令安装，只要确保pip版本在19.0以上即可。</p>



<p>如果你在使用pip时遇到任何错误，你总是可以使用conda进行安装。</p>



<h4>在我们的模型上使用SHAP</h4>



<p>导入所需的依赖项:</p>



<pre class="hljs">pip install shape
</pre>



<p>根据您的型号，您可以使用SHAP提供的不同解释器。因为我们正在处理一个随机的森林分类器，我们将使用SHAP的树解释器。</p>



<pre class="hljs">conda install -c conda-forge shap
</pre>



<h4>让我们为我们的特征计算shap值。请记住，由于我们的大多数特性都是基于文本的，我们将利用它们来理解我们的模型，就像我们对LIME所做的那样。</h4>



<p>shap_values是一个包含2个数组作为元素的列表，对应于我们数据集中的2个类。所以，我们可以从这两个类的角度来解释这个预测。</p>



<pre class="hljs">Import shap
</pre>



<p>按照我们的方法，让我们首先从解释一个真实的正例开始。为了展示一致性，我们将检查与LIME相同的数据点。</p>



<pre class="hljs">explainer = shap.TreeExplainer(rfc)
</pre>



<p>情节是这样的:</p>



<pre class="hljs">shap_values =
explainer.shap_values(vectorized_train_text.toarray(),check_additivity=<span class="hljs-keyword">False</span>)
</pre>



<p>现在，代码中的索引，以及情节本身，似乎有点令人不知所措。让我们一步一步地分解它。</p>



<p>与“患者”类相关的值出现在expected_value和shap_values的第一个索引处，因此该图是从“患者”类的角度绘制的。</p>



<pre class="hljs">shap.initjs()
shap.force_plot(explainer.expected_value[<span class="hljs-number">1</span>], shap_values = shap_values[<span class="hljs-number">1</span>][<span class="hljs-number">689</span>], features = vectorized_train_text.toarray()[<span class="hljs-number">0</span>:][<span class="hljs-number">689</span>], feature_names = vect.get_feature_names())
</pre>



<p>关于剧情:</p>







<p>所有特征值的预测得分为0.74，以粗体显示。</p>



<p>基值= 0.206是训练模型的所有输出值的平均值。</p>



<p>显示为粉红色(红色)的特征值会影响对类别1(患者)的预测，而显示为蓝色的特征值会将结果拉向类别0(非患者)。</p>



<ul><li>彩色块的大小表示特征重要性的大小。</li><li>由于我们的预测得分(0.74)&gt;基础值(0.206)，该数据点已被积极分类，即类别=患者。</li><li>如果我们从另一个类的角度来看这个实例，会发生什么？</li><li>我们很容易理解这里发生了什么:</li><li>我们将expected_value和shap_values的索引从1切换到0，因为我们希望视角从“患者”反转到“非患者”。</li></ul>



<p>因此，所有出现在粉红色(红色)中的特征已经切换到蓝色，并且现在对“非患者”类别的预测产生负面影响。</p>



<pre class="hljs">shap.initjs()
shap.force_plot(explainer.expected_value[<span class="hljs-number">1</span>], shap_values = shap_values[<span class="hljs-number">1</span>][<span class="hljs-number">689</span>], features = vectorized_train_text.toarray()[<span class="hljs-number">0</span>:][<span class="hljs-number">689</span>], feature_names = vect.get_feature_names())
</pre>







<p>尽管预测得分(0.27)</p>



<ul><li>现在让我们检查真正的否定实例。</li><li>剧情出来是这样的。</li><li>由于预测分数小于基础值，因此被归类为“没有耐心”。</li></ul>



<p>除了局部解释，SHAP还可以通过全局解释来解释模型的一般行为。</p>



<pre class="hljs">shap.initjs()
shap.force_plot(explainer.expected_value[<span class="hljs-number">1</span>], shap_values = shap_values[<span class="hljs-number">1</span>][<span class="hljs-number">120</span>], features = vectorized_train_text.toarray()[<span class="hljs-number">0</span>:][<span class="hljs-number">120</span>], feature_names = vect.get_feature_names())
</pre>



<p>从代码中可以明显看出，我们是从“患者”类别的角度绘制的，我们可以看到“充血性心脏”、“充血性心力衰竭”和“麻烦”等特征向红色光谱延伸，因此对“患者”类别起着必不可少的作用。</p>







<p>一开始，SHAP提供的一系列功能可能会让你有些不知所措，但是一旦你掌握了它，没有什么比这更直观的了。</p>



<p>我们将查看另一个解释库，MLXTEND。</p>



<pre class="hljs">shap.summary_plot(shap_values = shap_values[<span class="hljs-number">1</span>], features = vectorized_train_text.toarray(), feature_names = vect.get_feature_names())
</pre>







<p>4.MLXTEND</p>



<p>MLxtend或机器学习扩展是一个用于日常数据科学和机器学习任务的有用工具库。它提供了广泛的功能。</p>



<p>到目前为止，我们只分析了文本特征，其他特征仍然是局外人。这一次，让我们使用<a href="https://web.archive.org/web/20221206010752/http://rasbt.github.io/mlxtend/" target="_blank" rel="noreferrer noopener nofollow"> MLXTEND </a>来看看其余的特性。</p>



<h3>如何安装MLXTEND</h3>



<p>它可以用一个简单的pip命令安装。</p>



<p>或者，您可以从<a href="https://web.archive.org/web/20221206010752/https://pypi.python.org/pypi/mlxtend" target="_blank" rel="noreferrer noopener nofollow">https://pypi.python.org/pypi/mlxtend</a>手动下载这个包，解压缩它，导航到这个包，并使用命令:</p>



<h4>在我们的模型上使用MLXTEND</h4>



<p>导入所需的依赖项:</p>



<pre class="hljs">pip install mlxtend
</pre>



<p>MLXTEND提供不同的功能，例如:</p>



<pre class="hljs">python setup.py install
</pre>



<h4><strong> 1。PCA相关圆</strong></h4>



<p>一种有趣的观察结果的方式是通过主成分分析。</p>



<pre class="hljs">Import mlxtend
</pre>



<p>MLXTEND允许您使用plot_pca_correlation_graph函数绘制PCA相关圆。</p>



<p>我们基本上计算我们的特征和主成分之间的相关性。</p>



<ul><li>然后这些相关性被绘制成单位圆上的向量，其轴是主分量。</li><li>特定的主成分可以作为元组传递给dimensions函数参数。</li><li>相关圆轴显示了相应主成分解释的方差百分比。</li><li>让我们为剩下的特征画出这个相关圆，看看我们会得到什么。</li><li>第一主成分解释了总方差的31.7%，而第二主成分解释了25.8%</li><li>Text_num_words &amp; Source更符合第一台电脑，而hour和weekday更符合第二台电脑。</li></ul>



<p><strong> 2。偏差-方差分解</strong></p>



<pre class="hljs"><span class="hljs-keyword">from</span> mlxtend.plotting <span class="hljs-keyword">import</span> plot_pca_correlation_graph
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
X = StandardScaler().fit_transform(train[[<span class="hljs-string">'text_num_words'</span>, <span class="hljs-string">'weekday'</span>,<span class="hljs-string">'hour'</span>,<span class="hljs-string">'Source'</span>]].values)
fig, corr_matrix = plot_pca_correlation_graph(
    X,
    [<span class="hljs-string">'text_num_words'</span>, <span class="hljs-string">'weekday'</span>,<span class="hljs-string">'hour'</span>,<span class="hljs-string">'Source'</span>],
    dimensions=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>),
    figure_axis_size=<span class="hljs-number">6</span>
)
</pre>







<ul><li>大家都知道偏差-方差权衡，这个问题困扰着所有的机器学习项目。</li><li>通常目标是在两者之间找到一个最佳点，通过保持低偏差来避免欠拟合，通过保持低方差来避免过拟合。</li></ul>



<p>很难获得任何预测模型的偏差-方差分数，但MLXTEND可以将模型的泛化误差分解为偏差、方差和误差分数。</p>



<ul><li>让我们尝试为我们的随机森林分类器计算这个分数。</li><li>为了进行这种计算，必须将我们的数据集分成训练和测试。</li><li>这就是结果。</li></ul>



<p>在这个练习中，我们根据文本特征来训练模型。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> mlxtend.evaluate <span class="hljs-keyword">import</span> bias_variance_decomp
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
X_train, X_test, y_train, y_test = train_test_split(vectorized_train_text, y,test_size=<span class="hljs-number">0.25</span>,
random_state=<span class="hljs-number">1</span>,shuffle=<span class="hljs-keyword">True</span>,stratify=y)
</pre>



<p>根据我们得到的分数，我们可以推断我们的模型可能擅长泛化，也就是说，它没有过度拟合。</p>



<pre class="hljs">avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(
        clf2, X_train, y_train, X_test, y_test,
        loss=<span class="hljs-string">'mse'</span>,
        num_rounds=<span class="hljs-number">50</span>,
        random_seed=<span class="hljs-number">1</span>
)
print(f<span class="hljs-string">"Average expected loss: {avg_expected_loss.round(3)}"</span>)
print(f<span class="hljs-string">"Average bias: {avg_bias.round(3)}"</span>)
print(f<span class="hljs-string">"Average variance: {avg_var.round(3)}"</span>)
</pre>



<p>由于它具有相对较高的偏差，这可能意味着它在某种程度上不适合我们的数据集。</p>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/e7cacfd259b87cfa2713c0a1dc616918.png" alt="" class="wp-image-45101" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206010752im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Model-interpretation_MLXTEND-2.png?ssl=1"/></figure>



<ul><li><strong> 3。绘制决策边界&amp;模型区域</strong></li><li>使用MLXTEND，我们还可以查看模型的二维决策边界，并了解模型如何区分不同类别的数据点。</li><li>然而，这种解释技术有一个缺点。对于这种可视化，一次只能使用两个特征，因此我们将在这里两个一组地使用我们的非文本特征。</li></ul>



<p>让我们看看我们在这里得到了什么样的决策边界。</p>



<ul><li>进行所需的进口:</li><li>实例化模型:</li></ul>



<p>现在，绘制:</p>



<p>这是我们得到的结果:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> mlxtend.plotting <span class="hljs-keyword">import</span> plot_decision_regions
<span class="hljs-keyword">from</span> mlxtend.classifier <span class="hljs-keyword">import</span> EnsembleVoteClassifier
<span class="hljs-keyword">import</span> matplotlib.gridspec <span class="hljs-keyword">as</span> gridspec
<span class="hljs-keyword">import</span> itertools
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
</pre>



<p>我们可以清楚地看到，这是一个非常糟糕的决策边界。输入要素不是很好的区分因素，因此巩固了我们用其他工具获得的结果。</p>



<pre class="hljs">Clf = RandomForestClassifier(random_state=<span class="hljs-number">1</span>)
value=<span class="hljs-number">1.5</span>
width=<span class="hljs-number">0.75</span>
gs = gridspec.GridSpec(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)
fig = plt.figure(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">8</span>))
labels = [<span class="hljs-string">'Random Forest'</span>]
</pre>



<p>包扎</p>



<pre class="hljs"><span class="hljs-keyword">for</span> clf, lab, grd <span class="hljs-keyword">in</span> zip([clf2], labels, itertools.product([<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>])):
clf.fit(train[[<span class="hljs-string">'Source'</span>, <span class="hljs-string">'weekday'</span>]].values, y)
ax = plt.subplot(gs[grd[<span class="hljs-number">0</span>], grd[<span class="hljs-number">1</span>]])
fig = plot_decision_regions(X=train[[<span class="hljs-string">'Source'</span>, <span class="hljs-string">'weekday'</span>]].values,
y=y, clf=clf)
plt.title(lab)
</pre>



<p>随着越来越复杂的架构，模型解释是你现在必须做的事情。我希望现在你已经知道如何去做了。</p>







<p>我们在本文中探索的工具并不是唯一可用的工具，还有许多方法可以理解模型预测。其中一些方法可能包含工具或框架，而另一些可能不包含，我鼓励您探索所有这些方法。</p>



<h2 id="h-wrapping-up">未来方向</h2>



<p>如果你喜欢你读到的内容，并想更深入地研究这个主题，你可以查看这个<a href="https://web.archive.org/web/20221206010752/https://wiki.pathmind.com/python-ai" target="_blank" rel="noreferrer noopener nofollow">链接</a>下的模型解释部分。它包含了许多你可能想要的解释模型的方法的信息。</p>



<p>暂时就这样了。敬请关注更多内容！</p>



<h3>Future directions</h3>



<p>If you liked what you read and want to dig deeper into this topic, you can check out the Model Explanation section under this <a href="https://web.archive.org/web/20221206010752/https://wiki.pathmind.com/python-ai" target="_blank" rel="noreferrer noopener nofollow">link</a>. It contains information about many of the approaches you might want in your arsenal for interpreting your model.</p>



<p>That’s it for now. Stay tuned for more!</p>
        </div>
        
    </div>    
</body>
</html>