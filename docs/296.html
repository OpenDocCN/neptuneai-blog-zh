<html>
<head>
<title>Deep Dive Into TensorBoard: Tutorial With Examples </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>深入研究TensorBoard:示例教程</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/tensorboard-tutorial#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/tensorboard-tutorial#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>有一个常见的商业说法是<strong>你不能改进你没有测量的东西</strong>。机器学习也是如此。有各种工具可以衡量深度学习模型的性能:Neptune AI、MLflow、Weights and Biases、Guild AI，仅举几个例子。在这篇文章中，我们将重点介绍<strong> TensorFlow的</strong>开源<strong>可视化工具包<a href="https://web.archive.org/web/20221207172226/https://www.tensorflow.org/tensorboard" target="_blank" rel="noreferrer noopener nofollow"> TensorBoard </a> </strong>。</p>



<p>该工具使您能够跟踪各种指标，如训练集或验证集的准确性和日志丢失。正如我们将在这篇文章中看到的，TensorBoard提供了几个我们可以在机器学习实验中使用的工具。这个工具也很容易使用。</p>







<p>以下是我们将在本文中涉及的一些内容:</p>



<ul><li><strong>在TensorBoard中可视化图像</strong></li><li>在张量板上检查<strong>模型重量和偏差</strong></li><li>可视化<strong>模型的架构</strong></li><li>将<strong>混淆矩阵</strong>的图像发送到TensorBoard</li><li><strong>剖析</strong>您的应用程序，以便查看其<strong>性能</strong>，以及</li><li>使用<strong>张量板</strong>与<strong> Keras </strong>、<strong> PyTorch </strong>和<strong> XGBoost </strong></li></ul>







<p>我们开始吧。</p>



<h2 id="h-how-to-use-tensorboard">如何使用TensorBoard</h2>



<p>本节将重点帮助您了解如何在您的机器学习工作流程中使用TensorBoard。</p>



<h3><strong>如何安装<strong>张量板</strong>T3】</strong></h3>



<p>在开始使用TensorBoard之前，您必须通过pip或conda安装它</p>



<pre class="hljs">pip install tensorboard
conda install -c conda-forge tensorboard</pre>



<h3><strong>使用<strong> TensorBoard </strong>搭配Jupyter笔记本和Google Colab </strong></h3>



<p>安装TensorBoard后，您现在可以将它加载到您的笔记本中。请注意，你可以在<strong> Jupyter笔记本</strong>或<strong>谷歌的Colab </strong>中使用它。</p>



<pre class="hljs">%load_ext tensorboard</pre>



<p>一旦完成，你必须设置一个<strong>日志目录</strong>。这是TensorBoard存放所有日志的地方。它将从这些日志中读取数据，以显示各种可视化效果。</p>



<pre class="hljs">log_folder = <span class="hljs-string">'logs'</span></pre>



<p>如果你想重新加载<strong> TensorBoard扩展</strong>，下面的命令将会变魔术——没有双关语。</p>



<pre class="hljs">%reload_ext tensorboard</pre>



<p>您可能希望清除当前日志，以便可以将新日志写入该文件夹。你可以通过在<strong> Google Colab </strong>上运行这个命令来实现</p>



<pre class="hljs">!rm -rf /logs/</pre>



<p>在Jupyter笔记本上</p>



<pre class="hljs">rm -rf logs</pre>



<p>如果您正在运行多个<strong>实验</strong>，您可能想要存储所有日志，以便您可以比较它们的结果。这可以通过创建带有时间戳的<strong>日志</strong>来实现。为此，请使用下面的命令:</p>



<pre class="hljs"><span class="hljs-keyword">import</span> datetime
log_folder = <span class="hljs-string">"logs/fit/"</span> + datetime.datetime.now().strftime(<span class="hljs-string">"%Y%m%d-%H%M%S"</span>)</pre>



<h3><strong>如何运行TensorBoard </strong></h3>



<p>运行Tensorboard只需要一行代码。在本节中，您将看到如何做到这一点。</p>



<p>现在让我们看一个例子，在这个例子中，您将使用TensorBoard来可视化模型指标。为此，您需要构建一个简单的图像分类模型。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train, X_test = X_train / <span class="hljs-number">255.0</span>, X_test / <span class="hljs-number">255.0</span>

model = tf.keras.models.Sequential([
   tf.keras.layers.Flatten(input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)),
   tf.keras.layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">'relu'</span>),
   tf.keras.layers.Dropout(<span class="hljs-number">0.2</span>),
   tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)])

model.compile(optimizer=<span class="hljs-string">'sgd'</span>,
   loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>,
   metrics=[<span class="hljs-string">'accuracy'</span>])</pre>



<p>接下来，加载<strong> TensorBoard笔记本扩展</strong>并创建一个指向<strong>日志文件夹</strong>的变量。</p>



<pre class="hljs"><span class="hljs-tag">%<span class="hljs-selector-tag">load_ext</span></span> tensorboard
log_folder = 'logs'</pre>



<h3><strong>如何使用TensorBoard回调</strong></h3>



<p>下一步是在模型的拟合方法中指定TensorBoard回调。为了做到这一点，你首先要导入<strong> TensorBoard回调</strong>。</p>



<p>该回调负责记录事件，例如<strong>激活直方图、</strong> <strong>度量概要图</strong>、<strong>剖析图</strong>和<strong>训练图可视化图</strong>。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> tensorflow.keras.callbacks <span class="hljs-keyword">import</span> TensorBoard</pre>



<p>准备就绪后，您现在可以创建<strong> TensorBoard回调</strong>并使用<strong> log_dir </strong>指定日志目录。TensorBoard回调还接受其他参数:</p>



<ul><li><strong> histogram_freq </strong>是计算模型层的激活和权重直方图的频率。将此项设置为0意味着将不计算直方图。为了实现这一点，您必须设置<strong>验证数据</strong>或<strong>验证分割</strong>。</li><li><strong> write_graph </strong>指示图形是否将在TensorBoard中可视化</li><li><strong> write_images </strong>设置为true时，模型权重在TensorBoard中显示为图像</li><li><strong> update_freq </strong>决定如何将<strong>损失</strong>和<strong>指标</strong>写入TensorBoard。如果设置为整数，比如100，则每100批记录一次损耗和指标。当设置为批处理时，损失和指标在每次批处理后设置。当设置为纪元时，它们在每个纪元后被写入</li><li><strong> profile_batch </strong>决定要评测哪些批次。默认情况下，会分析第二批。例如，您也可以设置为从5到10，以分析批次5到10，即profile_batch='5，10 '。将profile_batch设置为0将禁用分析。</li><li><strong> embeddings_freq </strong>嵌入层可视化的频率。将此项设置为零意味着嵌入不会被可视化</li></ul>



<pre class="hljs">callbacks = [TensorBoard(log_dir=log_folder,
                         histogram_freq=<span class="hljs-number">1</span>,
                         write_graph=<span class="hljs-keyword">True</span>,
                         write_images=<span class="hljs-keyword">True</span>,
                         update_freq=<span class="hljs-string">'epoch'</span>,
                         profile_batch=<span class="hljs-number">2</span>,
                         embeddings_freq=<span class="hljs-number">1</span>)]</pre>



<p>下一项是拟合模型并传入<strong>回调</strong>。</p>



<pre class="hljs">model.fit(X_train, y_train,
          epochs=<span class="hljs-number">10</span>,
          validation_split=<span class="hljs-number">0.2</span>,
          callbacks=callbacks)</pre>



<h3><strong>如何启动冲浪板</strong></h3>



<p>如果您通过pip安装了TensorBoard，您可以通过命令行启动它</p>



<pre class="hljs">tensorboard -- logdir=<span class="hljs-built_in">log</span></pre>



<p>在笔记本电脑上，您可以使用以下方式启动它:</p>



<pre class="hljs"><span class="hljs-tag">%<span class="hljs-selector-tag">tensorboard</span></span> -- logdir={log_folder}</pre>



<p>TensorBoard也可通过以下网址通过<strong>浏览器</strong>获得</p>



<pre class="hljs">http://localhost:6006</pre>



<h3><strong>远程运行TensorBoard】</strong></h3>



<p>在远程服务器上工作时，可以使用SSH隧道将远程服务器的端口转发到本地机器的端口(在本例中是端口6006)。这看起来是这样的:</p>



<pre class="hljs">ssh -L 6006:127.0.0.1:6006 your_user_name@my_server_ip</pre>



<p>有了它，你就可以用正常的方式运行TensorBoard了。</p>



<p>请记住，您在tensorboard命令中指定的端口(默认为6006)应该与ssh隧道中的端口相同。</p>



<pre class="hljs">tensorboard --logdir=/tmp  --port=6006</pre>



<p><strong>注意:</strong>如果您使用默认端口6006，您可以丢弃–port = 6006。您将能够在本地计算机上看到TensorBoard，但TensorBoard实际上是在远程服务器上运行的。</p>



<h2 id="h-tensorboard-dashboard">张量板仪表板</h2>



<p>现在让我们看看TensorBoard上的各个选项卡。</p>



<h3><strong>张量板标量</strong></h3>



<p><strong> <a href="https://web.archive.org/web/20221207172226/https://www.tensorflow.org/tensorboard/scalars_and_keras" target="_blank" rel="noreferrer noopener nofollow">标量</a> </strong>选项卡显示了各时期的损耗和指标变化。它可用于跟踪其他标量值，如学习率和训练速度。</p>







<h3><strong>张量板图像</strong></h3>



<p>这个仪表盘有显示重量的<strong> <a href="https://web.archive.org/web/20221207172226/https://www.tensorflow.org/tensorboard/image_summaries" target="_blank" rel="noreferrer noopener nofollow">图像</a> </strong>。调整滑块显示不同时期的权重。</p>







<h3><strong>张量图</strong></h3>



<p>此选项卡显示模型的层。您可以使用它来检查模型的架构是否符合预期。</p>







<h3><strong>张量板分布</strong></h3>



<p>“分布”选项卡显示张量的分布。例如，在下面的密集层中，您可以看到每个时期的权重和偏差分布。</p>







<h3><strong>张量板直方图</strong></h3>



<p><strong>直方图</strong>显示了张量随时间的分布。例如，查看下面的dense_1，您可以看到<strong>偏差</strong>在每个时期的分布。</p>







<h2 id="h-using-the-tensorboard-projector">使用TensorBoard投影仪</h2>



<p>您可以使用<strong> TensorBoard的投影仪</strong>来可视化任何<strong>矢量表示</strong>，例如文字<strong>嵌入</strong>和<strong> </strong> <a href="https://web.archive.org/web/20221207172226/https://medium.com/@kumon/visualizing-image-feature-vectors-through-tensorboard-b850ce1be7f1"> <strong>图像</strong> </a>。</p>



<p>单词嵌入是捕获它们的语义关系的单词的数字表示。投影仪帮助你看到这些图像。你可以在<strong>非活动</strong>下拉列表中找到它。</p>



<h2 id="h-plot-training-examples-with-tensorboard">使用TensorBoard绘制训练示例</h2>



<p>您可以使用<strong> TensorFlow图像摘要API </strong>来可视化训练图像。这在处理像这样的图像数据时特别有用。</p>



<p>现在，为图像创建一个新的日志目录，如下所示。</p>



<pre class="hljs">logdir = <span class="hljs-string">"logs/train_data/"</span></pre>



<p>下一步是创建一个<strong>文件写入器</strong>，并将其指向这个目录。</p>



<pre class="hljs">file_writer = tf.summary.create_file_writer(logdir)</pre>



<p>在本文开始时(在“如何运行TensorBoard”一节中)，您指定图像形状为28 x 28。在将图像写入TensorBoard之前对其进行整形时，这是非常重要的信息。您还需要将通道指定为1，因为图像是灰度的。然后，使用file_write将图像写入TensorBoard。</p>



<p>在本例中，索引为10到30的图像将被写入TensorBoard。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">with</span> file_writer.as_default():
    images = np.reshape(X_train[<span class="hljs-number">10</span>:<span class="hljs-number">30</span>], (<span class="hljs-number">-1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))
    tf.summary.image(<span class="hljs-string">"20 Digits"</span>, images, max_outputs=<span class="hljs-number">25</span>, step=<span class="hljs-number">0</span>)</pre>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20221207172226im_/https://neptune.ai/wp-content/uploads/Tensorboard-tutorial-images.mp4"/></figure>



<h2 id="h-visualize-images-in-tensorboard">在TensorBoard中可视化图像</h2>



<p>除了可视化图像张量，您还可以在TensorBoard中可视化实际图像。为了说明这一点，您需要使用Matplotlib将MNIST张量转换为图像。之后，您需要使用' tf.summary.image '在Tensorboard中绘制图像。</p>



<p>从清除日志开始，或者您可以使用带有时间戳的日志文件夹。之后，指定日志目录并创建一个“tf.summary.create_file_writer ”,用于将图像写入TensorBoard</p>



<pre class="hljs">!rm -rf logs </pre>



<pre class="hljs"><span class="hljs-keyword">import</span> io
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

class_names = [<span class="hljs-string">'Zero'</span>,<span class="hljs-string">'One'</span>,<span class="hljs-string">'Two'</span>,<span class="hljs-string">'Three'</span>,<span class="hljs-string">'Four'</span>,<span class="hljs-string">'Five'</span>,<span class="hljs-string">'Six'</span>,<span class="hljs-string">'Seven'</span>,<span class="hljs-string">'Eight'</span>,<span class="hljs-string">'Nine'</span>]
logdir = <span class="hljs-string">"logs/plots/"</span>
file_writer = tf.summary.create_file_writer(logdir)</pre>



<p>接下来，创建一个包含图像的网格。在这种情况下，网格将容纳36位数字。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">image_grid</span><span class="hljs-params">()</span>:</span>
    figure = plt.figure(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">36</span>):
        plt.subplot(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, i + <span class="hljs-number">1</span>)
        plt.xlabel(class_names[y_train[i]])
        plt.xticks([])
        plt.yticks([])
        plt.grid(<span class="hljs-keyword">False</span>)
        plt.imshow(X_train[i], cmap=plt.cm.coolwarm)

    <span class="hljs-keyword">return</span> figure

figure = image_grid()</pre>



<p>现在将这些数字转换成一个单独的图像，在张量板上可视化。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_to_image</span><span class="hljs-params">(figure)</span>:</span>
    buf = io.BytesIO()
    plt.savefig(buf, format=<span class="hljs-string">'png'</span>)
    plt.close(figure)
    buf.seek(<span class="hljs-number">0</span>)

    digit = tf.image.decode_png(buf.getvalue(), channels=<span class="hljs-number">4</span>)
    digit = tf.expand_dims(digit, <span class="hljs-number">0</span>)

    <span class="hljs-keyword">return</span> digit</pre>



<p>下一步是使用writer和‘plot _ to _ image’在TensorBoard上显示图像。</p>



<pre class="hljs"><span class="hljs-keyword">with</span> file_writer.as_default():
    tf.summary.image(<span class="hljs-string">"MNIST Digits"</span>, plot_to_image(figure), step=<span class="hljs-number">0</span>)
</pre>



<pre class="hljs"><span class="hljs-tag">%<span class="hljs-selector-tag">tensorboard</span></span> -- logdir logs/plots</pre>







<h2 id="h-log-confusion-matrix-to-tensorboard">将混淆矩阵记录到张量板上</h2>



<p>使用相同的示例，您可以记录所有时期的混淆矩阵。首先，定义一个函数，该函数将返回一个Matplotlib图，其中保存着<strong>混淆矩阵</strong>。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> itertools

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_confusion_matrix</span><span class="hljs-params">(cm, class_names)</span>:</span>
    figure = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))
    plt.imshow(cm, interpolation=<span class="hljs-string">'nearest'</span>, cmap=plt.cm.Accent)
    plt.title(<span class="hljs-string">"Confusion matrix"</span>)
    plt.colorbar()
    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=<span class="hljs-number">45</span>)
    plt.yticks(tick_marks, class_names)

    cm = np.around(cm.astype(<span class="hljs-string">'float'</span>) / cm.sum(axis=<span class="hljs-number">1</span>)[:, np.newaxis], decimals=<span class="hljs-number">2</span>)
    threshold = cm.max() / <span class="hljs-number">2.</span>

    <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> itertools.product(range(cm.shape[<span class="hljs-number">0</span>]), range(cm.shape[<span class="hljs-number">1</span>])):
        color = <span class="hljs-string">"white"</span> <span class="hljs-keyword">if</span> cm[i, j] &gt; threshold <span class="hljs-keyword">else</span> <span class="hljs-string">"black"</span>
        plt.text(j, i, cm[i, j], horizontalalignment=<span class="hljs-string">"center"</span>, color=color)

    plt.tight_layout()
    plt.ylabel(<span class="hljs-string">'True label'</span>)
    plt.xlabel(<span class="hljs-string">'Predicted label'</span>)

    <span class="hljs-keyword">return</span> figure</pre>



<p>接下来，清除以前的日志，为混淆矩阵定义日志目录，并创建一个写入日志文件夹的writer变量。</p>



<pre class="hljs">!<span class="hljs-keyword">rm</span> -rf logs</pre>



<pre class="hljs">logdir = <span class="hljs-string">"logs"</span>
file_writer_cm = tf.summary.create_file_writer(logdir)</pre>



<p>接下来的步骤是创建一个函数，该函数将根据模型进行预测，并将混淆矩阵记录为图像。</p>



<p>之后，使用“文件写入器cm”将混淆矩阵写入日志目录。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log_confusion_matrix</span><span class="hljs-params">(epoch, logs)</span>:</span>
    predictions = model.predict(X_test)
    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)

    cm = metrics.confusion_matrix(y_test, predictions)
    figure = plot_confusion_matrix(cm, class_names=class_names)
    cm_image = plot_to_image(figure)

    <span class="hljs-keyword">with</span> file_writer_cm.as_default():
        tf.summary.image(<span class="hljs-string">"Confusion Matrix"</span>, cm_image, step=epoch)</pre>



<p>接下来是TensorBoard回调和<code>LambdaCallback</code>的定义。</p>



<p><code>LambdaCallback</code>将记录每个时期的混淆矩阵。最后使用这两个回调函数来拟合模型。</p>



<p>由于您之前已经拟合了模型，建议您重新启动运行时，并确保只拟合一次模型。</p>



<pre class="hljs">callbacks = [
   TensorBoard(log_dir=log_folder,
               histogram_freq=<span class="hljs-number">1</span>,
               write_graph=<span class="hljs-keyword">True</span>,
               write_images=<span class="hljs-keyword">True</span>,
               update_freq=<span class="hljs-string">'epoch'</span>,
               profile_batch=<span class="hljs-number">2</span>,
               embeddings_freq=<span class="hljs-number">1</span>),
   keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)
]

model.fit(X_train, y_train,
          epochs=<span class="hljs-number">10</span>,
          validation_split=<span class="hljs-number">0.2</span>,
          callbacks=callbacks)</pre>



<p>现在运行TensorBoard并检查<strong>图像</strong>选项卡上的混淆矩阵。</p>



<pre class="hljs"><span class="hljs-tag">%<span class="hljs-selector-tag">tensorboard</span></span> -- logdir logs</pre>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20221207172226im_/https://neptune.ai/wp-content/uploads/Tensorboard-tutorial-confusion-matrix.mp4"/></figure>



<h2 id="h-hyperparameter-tuning-with-tensorboard">用张量板调整超参数</h2>



<p>你可以用TensorBoard做的另一件很酷的事情是用它来可视化<strong>参数优化</strong>。以同一个MNIST为例，您可以尝试调整模型的超参数(手动或使用自动超参数优化)并在TensorBoard中可视化它们。</p>



<p>这是你期望得到的最终结果。仪表板位于<strong>参数</strong>选项卡下。</p>











<p>为此，您必须清除以前的日志并导入hparams插件。</p>



<pre class="hljs">!<span class="hljs-keyword">rm</span> -rvf logs</pre>



<pre class="hljs">logdir = <span class="hljs-string">"logs"</span>

<span class="hljs-keyword">from</span> tensorboard.plugins.hparams <span class="hljs-keyword">import</span> api <span class="hljs-keyword">as</span> hp</pre>







<p>下一步是定义要调整的参数。在这种情况下，密集层中的单位、辍学率和优化器函数将被调整。</p>



<pre class="hljs">HP_NUM_UNITS = hp.HParam(<span class="hljs-string">'num_units'</span>, hp.Discrete([<span class="hljs-number">300</span>, <span class="hljs-number">200</span>,<span class="hljs-number">512</span>]))
HP_DROPOUT = hp.HParam(<span class="hljs-string">'dropout'</span>, hp.RealInterval(<span class="hljs-number">0.1</span>,<span class="hljs-number">0.5</span>))
HP_OPTIMIZER = hp.HParam(<span class="hljs-string">'optimizer'</span>, hp.Discrete([<span class="hljs-string">'adam'</span>, <span class="hljs-string">'sgd'</span>, <span class="hljs-string">'rmsprop'</span>]))
</pre>



<p>接下来，使用tf.summary.create_file_writer定义存储日志的文件夹。</p>



<pre class="hljs">METRIC_ACCURACY = <span class="hljs-string">'accuracy'</span>

<span class="hljs-keyword">with</span> tf.summary.create_file_writer(<span class="hljs-string">'logs/hparam_tuning'</span>).as_default():
    hp.hparams_config(
        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],
        metrics=[hp.Metric(METRIC_ACCURACY, display_name=<span class="hljs-string">'Accuracy'</span>)],)</pre>



<p>这样一来，您需要像以前一样定义模型。唯一的区别是，第一个密集层的神经元数量、辍学率和优化器函数不会被硬编码。</p>



<p>这将在稍后运行实验时使用的函数中完成。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_model</span><span class="hljs-params">(hparams)</span>:</span>
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)),
        tf.keras.layers.Dense(hparams[HP_NUM_UNITS],  activation=<span class="hljs-string">'relu'</span>),
        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),
        tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)])

    model.compile(optimizer=hparams[HP_OPTIMIZER],
                  loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>,
                  metrics=[<span class="hljs-string">'accuracy'</span>])

    model.fit(X_train, y_train, epochs=<span class="hljs-number">5</span>)
    loss, accuracy = model.evaluate(X_test, y_test)

    <span class="hljs-keyword">return</span> accuracy</pre>



<p>您需要创建的下一个函数将使用前面定义的参数运行上面的函数。然后它会记录精确度。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">experiment</span><span class="hljs-params">(experiment_dir, hparams)</span>:</span>

    <span class="hljs-keyword">with</span> tf.summary.create_file_writer(experiment_dir).as_default():
        hp.hparams(hparams)
        accuracy = create_model(hparams)
        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=<span class="hljs-number">1</span>)</pre>



<p>之后，您需要对上面定义的所有参数组合运行该函数。每个实验都将存储在自己的文件夹中。</p>



<pre class="hljs">experiment_no = <span class="hljs-number">0</span>

<span class="hljs-keyword">for</span> num_units <span class="hljs-keyword">in</span> HP_NUM_UNITS.domain.values:
    <span class="hljs-keyword">for</span> dropout_rate <span class="hljs-keyword">in</span> (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
        <span class="hljs-keyword">for</span> optimizer <span class="hljs-keyword">in</span> HP_OPTIMIZER.domain.values:
            hparams = {
                HP_NUM_UNITS: num_units,
                HP_DROPOUT: dropout_rate,
                HP_OPTIMIZER: optimizer,}

            experiment_name = f<span class="hljs-string">'Experiment {experiment_no}'</span>
            print(f<span class="hljs-string">'Starting Experiment: {experiment_name}'</span>)
            print({h.name: hparams[h] <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> hparams})
            experiment(<span class="hljs-string">'logs/hparam_tuning/'</span> + experiment_name, hparams)
            experiment_no += <span class="hljs-number">1</span></pre>







<p>最后，运行TensorBoard来查看您在本节开始时看到的可视化效果。</p>



<pre class="hljs"><span class="hljs-tag">%<span class="hljs-selector-tag">tensorboard</span></span> -- logdir logs/hparam_tuning</pre>



<p>在<strong> HPARAMS选项卡上，</strong><strong>表格视图</strong>显示所有的模型运行及其相应的准确性、丢失率和密集层神经元。<strong>平行坐标视图</strong>将每次运行显示为一条穿过每个超参数和精度指标轴的直线。</p>



<p>单击其中一个将显示试验和超参数，如下所示。</p>







<p><strong>散点图视图</strong>将超参数和指标之间的比较可视化。</p>







<h2 id="h-tensorflow-profiler">TensorFlow Profiler</h2>



<p>您还可以使用<strong> <a href="https://web.archive.org/web/20221207172226/https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras" target="_blank" rel="noreferrer noopener nofollow">分析器</a> </strong>跟踪TensorFlow模型的性能。分析对于理解TensorFlow操作的硬件资源消耗至关重要。在此之前，您必须安装profiler插件。</p>



<pre class="hljs">pip install -U tensorboard-plugin-profile</pre>



<p>安装完成后，它将出现在非活动下拉列表中。这是侧写器上众多视觉效果之一的快照。</p>







<p>现在您唯一要做的事情就是定义一个回调，并包含将要分析的批处理。</p>



<p>之后，当你符合模型时，你通过回调。别忘了给TensorBoard打电话，这样你就可以看到可视化效果。</p>



<pre class="hljs">callbacks = [tf.keras.callbacks.TensorBoard(log_dir=log_folder,
                                            profile_batch=<span class="hljs-string">'10,20'</span>)]

model.fit(X_train, y_train,
          epochs=<span class="hljs-number">10</span>,
          validation_split=<span class="hljs-number">0.2</span>,
          callbacks=callbacks)</pre>



<pre class="hljs"><span class="hljs-tag">%<span class="hljs-selector-tag">tensorboard</span></span> --logdir=logs</pre>







<h3><strong>概览页面</strong></h3>



<p><strong>档案选项卡</strong>上的<strong>概览页面</strong>显示了该型号性能的高级概览。从下图可以看出，<strong>性能总结</strong>显示了:</p>



<ul><li>编译内核所花费的时间，</li><li>读取数据所花费的时间，</li><li>启动内核所花费的时间，</li><li>生产产出所花费的时间，</li><li>设备上的计算时间，以及</li><li>主机计算时间</li></ul>



<p><strong>步进时间图</strong>显示了所有已采样步进的器件步进时间。图表上的不同颜色描述了花费时间的不同类别:</p>



<ul><li><strong>红色</strong>部分对应于器件在等待输入数据时空闲的步进时间。</li><li>绿色的部分显示设备实际工作的时间。</li></ul>







<p>不过，在概览页面上，您可以看到运行时间最长的<strong> TensorFlow操作</strong>。</p>







<p><strong>运行环境</strong>显示使用的主机数量、<strong>设备类型</strong>、<strong>设备内核数量</strong>等环境信息。在这种情况下，您可以看到在Colab的运行时，有一台主机的GPU包含一个内核。</p>







<p>从这一页你可以看到的另一件事是<strong>优化</strong>模型性能的建议。</p>







<h3><strong>跟踪查看器</strong></h3>



<p><strong>跟踪查看器</strong>可用于了解输入管道中的性能瓶颈。它显示了在<strong>评测</strong>期间GPU或CPU上发生的不同事件的时间线。</p>



<p>纵轴显示各种事件组，横轴显示事件轨迹。在下图中，我使用了快捷键<strong> w </strong>来放大事件。要缩小，使用键盘快捷键<strong> S </strong>。<strong> A </strong>和<strong> D </strong>可分别用于向左和向右移动。</p>







<p>您可以单击单个事件来进一步分析它。使用<strong>浮动工具栏</strong>上的光标或使用键盘快捷键<strong> 1 </strong>。</p>



<p>下图显示了对显示开始和墙壁持续时间的<strong>SparseSoftmaxCrossEntropyWithLogits</strong>事件(一批数据的损失计算)的分析结果。</p>







<p>您还可以通过按住<strong> Ctrl键</strong>并选择它们来检查各种事件的摘要。</p>







<h3><strong>输入管道分析器</strong></h3>



<p><strong>输入管道分析器</strong>可用于分析模型输入管道中的低效问题。</p>











<p>该功能显示输入流水线分析的<strong>摘要、设备端分析细节</strong>和<strong>主机端分析细节</strong>。</p>



<p>输入管道分析总结显示了<strong>总输入管道</strong>。它是通知应用程序是否被输入绑定以及绑定多少的部分。</p>







<p><strong>器件侧分析细节</strong>显示器件步进时间和器件等待输入数据的时间。</p>



<p><strong>主机端分析</strong>显示主机端的分析，如主机上输入处理时间的分解。</p>



<p>在<strong>输入流水线分析器上，</strong>你还可以看到关于单个<strong>输入操作</strong>、花费的<strong>时间</strong>及其<strong>类别</strong>的统计。以下是各列所代表的内容:</p>



<ul><li><strong>输入操作</strong> —输入操作的张量流操作名</li><li><strong> Count </strong> —分析期间操作执行的实例数</li><li><strong>总时间</strong> —在上述每个实例上花费的累计时间总和</li><li><strong>总时间% </strong> —是花费在操作上的总时间占花费在输入处理上的总时间的百分比</li><li><strong>总自我时间</strong> —在每个实例上花费的自我时间的累计总和。</li><li><strong>总自我时间% </strong> —总自我时间占输入处理总时间的百分比</li><li><strong>类别</strong> —输入操作的处理类别</li></ul>







<h3><strong>张量流统计</strong></h3>



<p>该仪表板显示了在主机上执行的每个TensorFlow操作的性能。</p>



<ul><li><strong>第一张</strong> <strong>饼状图</strong>展示了主机上每个操作自执行时间的分布。</li><li><strong>第二个</strong>显示主机上每个操作类型的自执行时间分布。</li><li>第<strong>第三</strong>显示设备上每个操作的自执行时间分布。</li><li>第四个显示设备上每个操作类型的自执行时间分布。</li></ul>







<p>饼图下方的表格显示了<strong>张量流操作</strong>。每个<strong>行</strong>都是一个操作。<strong>栏</strong>显示了每个操作的各个方面。您可以使用任何列对表进行过滤。</p>







<p>在上表下方，您可以看到按类型分组的各种张量流操作。</p>







<h3><strong> GPU内核统计数据</strong></h3>



<p>该页面显示了<strong>性能统计数据</strong>以及每个GPU加速内核的原始操作。</p>







<p>内核统计数据下面是一个表格，其中显示了内核和各种操作花费的时间。</p>







<h3>内存配置文件页面</h3>



<p>该页面显示了在分析期间内存的<strong>利用率。它包含以下几个部分:内存配置文件摘要、内存时间线图和内存细分表。</strong></p>



<ul><li><strong>内存配置文件摘要</strong>显示TensorFlow应用程序的内存配置文件摘要。</li><li><strong>内存时间线图</strong>显示了内存使用量(以gib为单位)和碎片百分比(以毫秒为单位)与时间的关系图。这</li><li><strong>内存细分表</strong>显示在性能分析间隔内存使用率最高的点的活动内存分配。</li></ul>











<h2 id="h-how-to-enable-debugging-on-tensorboard">如何在TensorBoard上启用调试</h2>



<p>您也可以将<a href="https://web.archive.org/web/20221207172226/https://www.tensorflow.org/tensorboard/debugger_v2" target="_blank" rel="noreferrer noopener nofollow">调试</a>信息转储到您的TensorBoard。要做到这一点，你必须启用调试——它仍然处于实验模式</p>



<pre class="hljs">tf.debugging.experimental.enable_dump_debug_info(
   logdir,
   tensor_debug_mode=<span class="hljs-string">"FULL_HEALTH"</span>,
   circular_buffer_size=<span class="hljs-number">-1</span>)</pre>



<p>仪表板可以在<strong>调试器V2 </strong>的非活动下拉菜单下查看。</p>







<p>调试器V2 GUI有<strong>告警</strong>、<strong> Python执行时间线</strong>、<strong>图形执行、</strong>和<strong>图形结构</strong>。警报部分显示程序的异常情况。Python执行时间线部分显示了操作和图形的热切执行的历史。</p>



<p>图形执行显示所有在图形中计算过的浮点型张量的历史。图形结构部分包含源代码和堆栈跟踪，它们是在您与GUI交互时填充的。</p>



<h2 id="h-using-tensorboard-with-deep-learning-frameworks">将TensorBoard与深度学习框架结合使用</h2>



<p>你不局限于单独使用TensorFlow的TensorBoard。您还可以将它与其他框架一起使用，如Keras、PyTorch和XGBoost等。</p>



<h3><strong>py torch中的张量板</strong></h3>



<p>您首先通过<a href="https://web.archive.org/web/20221207172226/https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html" target="_blank" rel="noreferrer noopener nofollow">定义一个writer </a>来指向您想要写入日志的文件夹。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

writer = SummaryWriter(log_dir=<span class="hljs-string">'logs'</span>)</pre>



<p>下一步是使用summary writer添加您希望在TensorBoard上看到的项目。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">for</span> n_iter <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):
    writer.add_scalar(<span class="hljs-string">'Loss/train'</span>, np.random.random(), n_iter)
    writer.add_scalar(<span class="hljs-string">'Loss/test'</span>, np.random.random(), n_iter)
    writer.add_scalar(<span class="hljs-string">'Accuracy/train'</span>, np.random.random(), n_iter)
    writer.add_scalar(<span class="hljs-string">'Accuracy/test'</span>, np.random.random(), n_iter)</pre>



<h3><strong>喀拉斯的tensor board</strong></h3>



<p>由于<a href="https://web.archive.org/web/20221207172226/https://keras.io/api/callbacks/tensorboard/" target="_blank" rel="noreferrer noopener nofollow"> TensorFlow使用Keras </a>作为官方高级API，TensorBoard的实现类似于它在TensorFlow中的实现。我们已经看到了如何做到这一点:</p>



<p>创建回拨:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> tensorflow.keras.callbacks <span class="hljs-keyword">import</span> TensorBoard

tb_callback = TensorBoard(log_dir=log_folder,...)</pre>



<p>将它传递给“model.fit ”:</p>



<pre class="hljs">model.fit(X_train, y_train,
          epochs=<span class="hljs-number">10</span>,
          validation_split=<span class="hljs-number">0.2</span>,
          callbacks=[tb_callback])</pre>



<h3><strong>XG boost中的tensor board</strong></h3>



<p>使用XGBoost时，还可以将事件记录到TensorBoard。为此需要使用<a href="https://web.archive.org/web/20221207172226/https://github.com/lanpa/tensorboardX" target="_blank" rel="noreferrer noopener"> tensorboardX </a>包。例如，要记录度量和损失，您可以使用“SummaryWriter”和日志标量。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> tensorboardX <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">TensorBoardCallback</span><span class="hljs-params">()</span>:</span>
    writer = SummaryWriter()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">callback</span><span class="hljs-params">(env)</span>:</span>
        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> env.evaluation_result_list:
            writer.add_scalar(k, v, env.iteration)

    <span class="hljs-keyword">return</span> callback

xgb.train(callbacks=[TensorBoardCallback()])</pre>



<h2 id="h-tensorboard-dev">Tensorboard.dev</h2>



<p>Tensorboard.dev 是一个托管的Tensorboard平台，可以轻松托管、跟踪和共享ML实验。它允许人们发布他们的TensorBoard实验，排除故障以及与团队成员合作。一旦你有了一个TensorBoard实验，把它上传到TensorBoard.dev是非常简单的。</p>



<pre class="hljs">tensorboard dev upload --logdir logs
    --name <span class="hljs-string">"(optional) My latest experiment"</span>
    --description <span class="hljs-string">"(optional) Simple comparison of several      hyperparameters"</span></pre>



<p>一旦你运行这个命令，你会得到一个提示，要求你用谷歌账户授权TensorBoard.dev。一旦你这样做，你会得到一个验证码，你将进入认证。</p>







<p>这将产生一个独特的张量板。开发链接给你。这里有一个这样的<a href="https://web.archive.org/web/20221207172226/https://tensorboard.dev/experiment/Yf4oVs9bS7mUBZPTV8KcPQ/">链接</a>的例子。如你所见，这非常类似于在本地主机上查看TensorBoard，只是现在你是在线查看。</p>



<p>一旦你在这里着陆，你就可以和冲浪板互动，就像你在这个作品的前几部分一样。</p>







<p>需要注意的是，这个TensorBoard对互联网上的每个人都是可见的，所以请确保您没有上传任何敏感数据。</p>



<h2 id="h-limitations-of-using-tensorboard">使用TensorBoard的限制</h2>



<p>正如你所看到的，TensorBoard给了你很多很棒的功能。也就是说，使用TensorBoard并非一帆风顺。</p>



<p>它有一些限制:</p>



<ul><li>难以在需要协作的团队环境中使用</li><li>没有用户和工作区管理:大型组织通常需要这些功能</li><li>您不能执行数据和模型版本化来跟踪各种实验</li><li>无法将其扩展到百万次运行；运行太多次，你会开始遇到UI问题</li><li>用于记录图像的界面有点笨拙</li><li>您不能记录和可视化其他数据格式，如音频/视频或自定义html</li></ul>



<h2 id="h-final-thoughts">最后的想法</h2>



<p>这篇文章中有几件事我们没有涉及到。值得一提的两个有趣特性是:</p>



<ul><li><a href="https://web.archive.org/web/20221207172226/https://www.tensorflow.org/tensorboard/fairness-indicators" target="_blank" rel="noreferrer noopener nofollow">公平指标</a>仪表板(目前处于测试阶段)。它允许计算二进制和多类分类器的公平性度量。</li><li><a href="https://web.archive.org/web/20221207172226/https://www.tensorflow.org/tensorboard/what_if_tool" target="_blank" rel="noreferrer noopener nofollow"> What-If工具</a> (WIT)使你能够探索和研究经过训练的机器学习模型。这是使用不需要任何代码的可视化界面来完成的。</li></ul>



<p>希望你在这里学到的一切能帮助你监控和调试你的训练，并最终建立更好的模型！</p>
        </div>
        
    </div>    
</body>
</html>