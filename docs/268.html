<html>
<head>
<title>HyperBand and BOHB: Understanding State of the Art Hyperparameter Optimization Algorithms </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>超宽带和BOHB:了解超参数优化算法的最新发展</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>如果你想了解最先进的超参数优化算法(HPO)，在这篇文章中我会告诉你它们是什么以及它们是如何工作的。</p>



<p>作为一名ML研究人员，我已经阅读并使用了相当多的最新HPO算法，在接下来的几节中，我将与您分享我迄今为止的发现。</p>



<p>希望你喜欢！</p>







<h2 id="Approaches">一点关于HPO的方法</h2>



<p>HPO是一种帮助解决机器学习算法的超参数调整挑战的方法。</p>



<p>杰出的ML算法具有多个不同的复杂超参数，这些超参数产生巨大的搜索空间。大量的初创企业选择在其管道的核心使用深度学习，深度学习方法中的搜索空间甚至比传统的ML算法更大。调谐到一个巨大的搜索空间是一项艰巨的挑战。</p>



<p>要解决HPO问题，我们需要使用数据驱动的方法。手动方法无效。</p>



<p>已经提出了许多解决HPO问题的方法:</p>







<p>我们将讨论四种主要的、最有效的方法。</p>





<h2 id="Bayesian-Optimization">贝叶斯优化</h2>



<p>要理解BO，我们应该对网格搜索和随机搜索方法有所了解(在本文的<a href="https://web.archive.org/web/20221024193328/https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noreferrer noopener nofollow">中有很好的解释)。我只是要总结一下这些方法。</a></p>



<p>假设我们的搜索空间只包含两个超参数，一个是重要的，另一个是不重要的。我们希望对它们进行调整，以提高模型的准确性。如果它们中的每一个都有3个不同的值，那么整个搜索空间将有9个可能的选择。我们可以尝试其中的每一个来找到两个超参数的最佳值。</p>







<p>但是从上图可以看出，网格搜索无法找到重要的超参数的最佳值。对此的一个解决方案可能是随机遍历搜索空间，就像下图一样。</p>







<h3><strong>贝叶斯优化为什么有效？</strong></h3>



<p>随机搜索最终收敛到最优答案，但这种方法就是这样的盲目搜索！有没有更智能的搜索方式？是的——贝叶斯优化，由J . mo kus提出。你可以在这本<a href="https://web.archive.org/web/20221024193328/https://static.sigopt.com/b/20a144d208ef255d3b981ce419667ec25d8412e2/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf" target="_blank" rel="noreferrer noopener nofollow">贝叶斯优化入门</a>和<a href="https://web.archive.org/web/20221024193328/http://papers.nips.cc/paper/4522-practical-bayesian-optimization" target="_blank" rel="noreferrer noopener nofollow">机器学习算法实用贝叶斯优化</a>中找到更多关于BO的信息。</p>



<p>本质上，贝叶斯优化是一种概率模型，它想通过基于先前观察的学习来学习一个代价昂贵的目标函数。它有两个强大的功能:代理模型和获取功能。</p>







<p>在上图中，你可以看到基于<a href="https://web.archive.org/web/20221024193328/https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1" target="_blank" rel="noreferrer noopener nofollow">自动机器学习</a>中超参数优化章节的贝叶斯优化的一个很好的解释。在该图中，我们希望找到虚线所示的真实目标函数。假设我们有一个连续的超参数，在第二次迭代中我们观察到两个黑点，然后我们拟合了一个替代模型(回归模型),即黑线。黑线周围的蓝管是我们的不确定性。</p>



<p>我们还有一个获取函数，这是我们探索搜索空间以找到新的观测最优值的方法。换句话说，获取函数帮助我们改进代理模型并选择下一个值。在上图中，采集函数显示为橙色曲线。获取最大值意味着不确定性最大，而预测值较低。</p>



<h3><strong>贝叶斯优化的利弊</strong></h3>



<p>贝叶斯优化最重要的优点是它可以很好地运行黑箱函数。BO也是数据高效的，并且对噪声具有鲁棒性。但是它不能很好地与并行资源duo一起工作，因为优化过程是顺序的。</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><p class="wp-block-embed__wrapper"><span class="embed-youtube"> <iframe class="youtube-player" src="https://web.archive.org/web/20221024193328if_/https://www.youtube.com/embed/3_E4A4G7nME?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation">视频</iframe> </span></p></figure>



<p class="has-text-align-center has-small-font-size"><em>图片来自Marius Lindauer </em>  <em>在开放数据科学大会上的演讲</em> <a href="https://web.archive.org/web/20221024193328/https://youtu.be/3_E4A4G7nME" target="_blank" rel="noreferrer noopener nofollow"> <em/></a></p>



<h3><strong>贝叶斯优化的实现</strong></h3>



<p>是时候看看一些贝叶斯优化实现了。我列出了最受欢迎的几个:</p>


<div class="medium-table">
        <div class="mt-row heading">
            <p class="mt-col">实施名称</p>
            <p class="mt-col">代理模型</p>
            <p class="mt-col">链接</p>
        </div>
    
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Surrogate model:
                    </span>
                                                                <p>随机森林</p>
                                    </div>
                            
                    </div>
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Surrogate model:
                    </span>
                                                                <p>树Parzen估计量</p>
                                    </div>
                            
                    </div>
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Surrogate model:
                    </span>
                                                                <p>高斯过程</p>
                                    </div>
                            
                    </div>
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Surrogate model:
                    </span>
                                                                <p>高斯过程、RF等。</p>
                                    </div>
                            
                    </div>
    </div>


<p id="block_600a9278a1ad1" class="separator separator-15">多重保真优化</p>



<h2 id="Algorithms">在贝叶斯方法中，目标函数的估计是非常昂贵的。有没有更便宜的估计目标函数的方法？多重保真优化方法是答案。我会告诉你:</h2>



<p>连续有超带</p>


<div class="custom-point-list">
<ol><li>BOHB</li><li>作为额外的资源，在下面的视频中，Andreas Mueller非常好地解释了多保真度优化方法。</li><li><span class="embed-youtube"> <iframe class="youtube-player" src="https://web.archive.org/web/20221024193328if_/https://www.youtube.com/embed/tqtTHRwa8dE?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation">视频</iframe> </span></li></ol>
</div>


<p><em> <a href="https://web.archive.org/web/20221024193328/https://www.youtube.com/channel/UCMEXgDffQy6nS2a74Gby8ZA" target="_blank" rel="noreferrer noopener nofollow">安德里亚斯·穆勒</a> : <a href="https://web.archive.org/web/20221024193328/https://youtu.be/tqtTHRwa8dE" target="_blank" rel="noreferrer noopener nofollow">应用机器学习2019 </a> </em></p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><p class="wp-block-embed__wrapper">连续拥有</p></figure>



<p class="has-text-align-center has-small-font-size">连续减半试图给最有希望的方法提供最多的预算。它假设所有配置都可以提前停止，并且可以获得验证分数。</p>



<h2>假设您有N种不同的配置和预算(例如时间)。在每次迭代中，正如你在下图中看到的，连续的一半保留了配置中最好的一半，而丢弃了不好的一半算法。它将继续下去，直到我们只有一个单一的配置。此方法将在达到其预算的最大值时完成。</h2>



<p>连续减半最初是在Kevin Jamieson和Ameet Talwalkar撰写的<a href="https://web.archive.org/web/20221024193328/http://proceedings.mlr.press/v51/jamieson16.pdf" target="_blank" rel="noreferrer noopener nofollow">非随机最佳臂识别和超参数优化</a>中提出的。</p>



<p><strong>连续减半有什么问题？</strong></p>







<p>在连续减半中，我们需要在开始时选择多少配置和需要多少切割之间进行权衡。在下一节中，您将看到Hyperband如何解决这个问题。</p>



<h3>超波段</h3>



<p>这种方法是连续减半算法的扩展，由李丽莎和其他人提出的一种新的基于Bandit的超参数优化方法。</p>



<h2>我提到过，连续减半方法在选择数字配置和分配预算之间存在权衡。为了解决这个问题，HyperBand提出使用不同的预算频繁执行连续减半方法，以找到最佳配置。在下图中，你可以看到HyperBand比随机搜索有更好的性能。</h2>



<p>你可以在automl.org的<a href="https://web.archive.org/web/20221024193328/https://github.com/automl" target="_blank" rel="noreferrer noopener nofollow">的</a><a href="https://web.archive.org/web/20221024193328/https://github.com/automl/HpBandSter" target="_blank" rel="noreferrer noopener nofollow"> HpBandSter </a> GitHub页面中找到HyperBand的一个简单实现。如果你想知道如何使用这个Python工具，可以查看一下<a href="https://web.archive.org/web/20221024193328/https://automl.github.io/HpBandSter/build/html/optimizers.html" target="_blank" rel="noreferrer noopener nofollow">文档</a>。</p>



<p>BOHB</p>







<p>BOHB是一种最先进的超参数优化算法，由Stefan Falkner、Aaron Klein和Frank Hutter撰写的<a href="https://web.archive.org/web/20221024193328/https://arxiv.org/abs/1807.01774" target="_blank" rel="noreferrer noopener nofollow"> BOHB:大规模鲁棒和高效的超参数优化</a>中提出。BOHB算法背后的想法是基于一个简单的问题——为什么我们要重复运行连续减半？</p>



<h2>BOHB使用贝叶斯优化算法，而不是在连续减半的基础上盲目重复的方法。事实上，BOHB将HyperBand和BO结合起来，以一种高效的方式使用这两种算法。丹·瑞恩在他的演讲中完美地解释了BOHB方法。<a href="https://web.archive.org/web/20221024193328/https://youtu.be/IqQT8se9ofQ" target="_blank" rel="noreferrer noopener nofollow">将其添加到您的观察列表</a>。</h2>



<p><span class="embed-youtube"> <iframe class="youtube-player" src="https://web.archive.org/web/20221024193328if_/https://www.youtube.com/embed/IqQT8se9ofQ?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation">视频</iframe> </span></p>



<p><em>Dan Ryan在PyData Miami 2019 </em>上关于高效灵活的超参数优化的精彩<a href="https://web.archive.org/web/20221024193328/https://youtu.be/IqQT8se9ofQ" target="_blank" rel="noreferrer noopener nofollow">演讲</a></p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><p class="wp-block-embed__wrapper">BOHB是一种多保真度优化方法，这些方法依赖于预算，因此找到相应的预算很重要。另一方面，BOHB是健壮的、灵活的和可伸缩的。如果你需要更详细的信息，你可能会想要查看安德烈·比登卡普和弗兰克·胡特关于BOHB的官方博客文章。</p></figure>



<p class="has-text-align-center has-small-font-size">另外，<a href="https://web.archive.org/web/20221024193328/https://github.com/automl/HpBandSter" target="_blank" rel="noreferrer noopener nofollow"> HpBandSter </a>是BOHB和HyperBand的一个很好的实现。你可以在这里找到它的文档<a href="https://web.archive.org/web/20221024193328/https://automl.github.io/HpBandSter/build/html/optimizers/bohb.html">。</a></p>



<p>在Neptune中随机搜索vs . HyperBand vs . BOHB+结果比较</p>



<p>现在我们已经知道了描述，也熟悉了方法，下面我们就用<a href="https://web.archive.org/web/20221024193328/https://ui.neptune.ai/mjbahmani/HyperBand-BOHB" target="_blank" rel="noreferrer noopener nofollow">海王星</a>基于这些方法做一些实验和比较。</p>



<h2 id="Comparison">如果你想跟着我:</h2>



<p>因为我决定在平等的基础上做这个实验，所以我使用了<a href="https://web.archive.org/web/20221024193328/https://github.com/automl/HpBandSter" target="_blank" rel="noreferrer noopener nofollow"> HpBandSter </a>，它有一个BOHB的实现，HyperBand和RandomSearch作为优化器。一个官方的例子可以在<a href="https://web.archive.org/web/20221024193328/https://automl.github.io/HpBandSter/build/html/auto_examples/example_5_mnist.html" target="_blank" rel="noreferrer noopener nofollow">这里</a>找到。基于这个例子，我们在Pytorch中有一个小CNN，它将针对<a href="https://web.archive.org/web/20221024193328/https://en.wikipedia.org/wiki/MNIST_database" target="_blank" rel="noreferrer noopener nofollow"> MNIST </a>数据集进行调谐。我基于三种不同的优化器运行了这个示例:</p>



<p>BOHB</p>





<p>超波段</p>


<div class="custom-point-list">
<ol><li>随机搜索。</li><li>对于每个优化器，我使用了以下预算:</li><li>RandomSearch. </li></ol>
</div>


<p>配置</p>


<p id="block_61adf2d9098c8" class="separator separator-10">范围值</p>



<div class="wp-container-5 wp-block-columns">
<div class="wp-container-3 wp-block-column"><div class="medium-table">
        <div class="mt-row heading">
            <p class="mt-col">[1,2,3,4,8,10]</p>
            <p class="mt-col">
            Range value        </p>
        </div>
    
            
            
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Range value:
                    </span>
                                                                <p>[1,2,3,4,8,10]</p>
                                    </div>
                    </div>
    </div>
</div>



<p class="wp-container-4 wp-block-column">这意味着我们已经运行了不同的组合，至少26个实验来检查优化器的能力(BOHB，超波段，随机搜索)。此外，通过这个示例，我们希望根据以下超参数找到CNN的最佳配置。</p>
</div>


<p id="block_600a92faa1ad2" class="separator separator-15"/>



<p>第一个配置层中的过滤器数量</p>


<p id="block_61adf2cc098c7" class="separator separator-10">全连接层中隐藏单元的数量</p>



<div class="wp-container-8 wp-block-columns">
<div class="wp-container-6 wp-block-column"><div class="medium-table">
        
    
            
            
            <div class="mt-row">
                            <div class="mt-col">
                                                                <p>Number of filters in the first conf layer</p>
                                    </div>
                            
                    </div>
            
            <div class="mt-row">
                            <div class="mt-col">
                                                                <p>Number of hidden units in fully connected layer</p>
                                    </div>
                            
                    </div>
    </div>
</div>



<p class="wp-container-7 wp-block-column"><em>配置取自<a href="https://web.archive.org/web/20221024193328/https://automl.github.io/HpBandSter/build/html/auto_examples/example_5_pytorch_worker.html" target="_blank" rel="noreferrer noopener nofollow">惠普主机文档</a>T3】</em></p>
</div>


<p id="block_600a9305a1ad3" class="separator separator-15">在海王星中用各种配置运行这些实验后，我得到了一些有见地的结果。</p>



<p><a href="https://web.archive.org/web/20221024193328/https://ui.neptune.ai/mjbahmani/HyperBand-BOHB/experiments?viewId=9b273e07-5189-4333-8197-e913931c97de" target="_blank" rel="noreferrer noopener"> <em>跟随海王星</em> </a>的所有实验</p>



<p>在这里，您可以看到n_iteration=3和max_budget=3的优化器之间的良好对比。我发现，如果我增加迭代次数，所有优化器最终都会获得最佳性能，但当预算很大时，BOHB可以做得更好。</p>







<p class="has-text-align-center">最终，对于max_budget=5和n_iteration=4，每个优化器都会找到一个最佳配置，您可以在下表中查看。</p>



<p>Here you can see a nice contrast between those optimizers for n_iteration=3 and max_budget=3. I find that if I increase the number of iterations, all optimizers finally get best performance, but when the budget is significant, BOHB could do it better.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img data-attachment-id="30238" data-permalink="https://web.archive.org/web/20221024193328/https://neptune.ai/neptune-hyperband-bohb-comparison" data-orig-file="https://web.archive.org/web/20221024193328/https://i0.wp.com/neptune.ai/wp-content/uploads/Neptune-hyperband-bohb-comparison.png?fit=602%2C589&amp;ssl=1" data-orig-size="602,589" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Neptune-hyperband-bohb-comparison" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20221024193328/https://i0.wp.com/neptune.ai/wp-content/uploads/Neptune-hyperband-bohb-comparison.png?fit=300%2C294&amp;ssl=1" data-large-file="https://web.archive.org/web/20221024193328/https://i0.wp.com/neptune.ai/wp-content/uploads/Neptune-hyperband-bohb-comparison.png?fit=602%2C589&amp;ssl=1" src="../Images/50ec3b52ff2e568f7f875eeebdd119d4.png" alt="Neptune hyperband bohb comparison" class="wp-image-30238 jetpack-lazy-image" data-recalc-dims="1" data-lazy-src="https://web.archive.org/web/20221024193328/https://i0.wp.com/neptune.ai/wp-content/uploads/Neptune-hyperband-bohb-comparison.png?resize=602%2C589&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://web.archive.org/web/20221024193328im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Neptune-hyperband-bohb-comparison.png?resize=602%2C589&amp;ssl=1"/><noscript><img data-lazy-fallback="1" data-attachment-id="30238" data-permalink="https://web.archive.org/web/20221024193328/https://neptune.ai/neptune-hyperband-bohb-comparison" data-orig-file="https://web.archive.org/web/20221024193328/https://i0.wp.com/neptune.ai/wp-content/uploads/Neptune-hyperband-bohb-comparison.png?fit=602%2C589&amp;ssl=1" data-orig-size="602,589" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Neptune-hyperband-bohb-comparison" data-image-description="" data-image-caption="" data-medium-file="https://web.archive.org/web/20221024193328/https://i0.wp.com/neptune.ai/wp-content/uploads/Neptune-hyperband-bohb-comparison.png?fit=300%2C294&amp;ssl=1" data-large-file="https://web.archive.org/web/20221024193328/https://i0.wp.com/neptune.ai/wp-content/uploads/Neptune-hyperband-bohb-comparison.png?fit=602%2C589&amp;ssl=1" src="../Images/50ec3b52ff2e568f7f875eeebdd119d4.png" alt="Neptune hyperband bohb comparison" class="wp-image-30238" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221024193328im_/https://i0.wp.com/neptune.ai/wp-content/uploads/Neptune-hyperband-bohb-comparison.png?resize=602%2C589&amp;ssl=1"/></noscript><figcaption><em>Comparison between (BOHB, HyperBand, RandomSearch) for n_iteration=3 and max_budget=</em></figcaption></figure></div>



<p>超参数</p>


<p id="block_61adf41b098ed" class="separator separator-10">调谐前的范围</p>


<div class="medium-table">
        <div class="mt-row heading">
            <p class="mt-col">调优后:随机搜索</p>
            <p class="mt-col">调谐后:超波段</p>
            <p class="mt-col">调谐后:BOHB</p>
            <p class="mt-col">[1e-6，1e-2]</p>
            <p class="mt-col">0.010</p>
        </div>
    
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Range before tuning:
                    </span>
                                                                <p>0.00031</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Random search:
                    </span>
                                                                <p>0.0078</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Hyperband:
                    </span>
                                                                <p>[1,3]</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: BOHB:
                    </span>
                                                                <p>2</p>
                                    </div>
                    </div>
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Range before tuning:
                    </span>
                                                                <p>1</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Random search:
                    </span>
                                                                <p>第一个配置层中的过滤器数量</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Hyperband:
                    </span>
                                                                <p>[4, 64]</p>
                                    </div>
                            
                    </div>
            <div class="mt-row">
                            <div class="mt-col">
                                                                <p>13</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Range before tuning:
                    </span>
                                                                <p>9</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Random search:
                    </span>
                                                                <p>[0, 0.9]</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Hyperband:
                    </span>
                                                                <p>0.77</p>
                                    </div>
                            
                    </div>
            <div class="mt-row">
                            
                            <div class="mt-col">
                                        <span class="column-name">
                        Range before tuning:
                    </span>
                                                                <p>0.06</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Random search:
                    </span>
                                                                <p>全连接层中隐藏单元的数量</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Hyperband:
                    </span>
                                                                <p>[8,256]</p>
                                    </div>
                            
                    </div>
            <div class="mt-row">
                            <div class="mt-col">
                                                                <p>177</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        Range before tuning:
                    </span>
                                                                <p>248</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Random search:
                    </span>
                                                                <p>177</p>
                                    </div>
                            <div class="mt-col">
                                        <span class="column-name">
                        After tuning: Hyperband:
                    </span>
                                                                <p>最后的想法</p>
                                    </div>
                            
                    </div>
    </div>


<p id="block_600a9323a1ad4" class="separator separator-15">感谢您加入我的旅程！我们涵盖了:</p>



<h2>贝叶斯优化的工作原理。</h2>



<p>多重保真优化。</p>


<div class="custom-point-list">
<ul><li>BOHB和超级乐队，以及他们是如何工作的。</li><li>哪个优化器(HyperBand和BOHB)在实验中效果更好？</li><li>希望你喜欢这本书！</li><li>MJ巴赫马尼</li></ul>
</div>


<p>一位对机器学习领域充满热情的经验丰富的软件工程师。他对开发尽可能高效和自动地解决真正的机器学习问题的软件特别感兴趣。</p>




<div id="author-box-new-format-block_60545857e3b65" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name"><strong>阅读下一篇</strong></h3>
    
          <p class="article__authorContent-text">如何跟踪机器学习模型的超参数？</p>
    
          
    
  </div>
</div>


<div class="wp-container-9 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>



<p class="has-text-color">卡米尔·卡什马雷克|发布于2020年7月1日</p>



<h2>How to Track Hyperparameters of Machine Learning Models?</h2>



<p class="has-small-font-size"><strong>机器学习算法可通过称为超参数</strong>的多个量规进行调整。最近的深度学习模型可以通过数十个超参数进行调整，这些超参数与数据扩充参数和训练程序参数一起创建了非常复杂的空间。在强化学习领域，您还应该计算环境参数。</p>


<p id="block_5ffc75def9f8e" class="separator separator-10">数据科学家要<strong>控制好</strong> <strong>超参数</strong> <strong>空间</strong>，才能<strong>使</strong> <strong>进步</strong>。</p>



<p>在这里，我们将向您展示<strong>最近的</strong> <strong>实践</strong>，<strong>提示&amp;技巧，</strong>和<strong>工具</strong>以最小的开销高效地跟踪超参数。你会发现自己掌控了最复杂的深度学习实验！</p>



<p>为什么我应该跟踪我的超参数？也就是为什么这很重要？</p>



<p>几乎每一个深度学习实验指南，像<a href="https://web.archive.org/web/20221024193328/https://www.deeplearningbook.org/contents/guidelines.html" target="_blank" rel="noreferrer noopener">这本深度学习书籍</a>，都建议你如何调整超参数，使模型按预期工作。在<strong>实验-分析-学习循环</strong>中，数据科学家必须控制正在进行的更改，以便循环的“学习”部分正常工作。</p>



<h2>哦，忘了说<strong>随机种子也是一个超参数</strong>(特别是在RL领域:例如检查<a href="https://web.archive.org/web/20221024193328/https://www.reddit.com/r/MachineLearning/comments/76th74/d_why_random_seeds_sometimes_have_quite_large/" target="_blank" rel="noreferrer noopener">这个Reddit </a>)。</h2>



<p>超参数跟踪的当前实践是什么？</p>



<p>让我们逐一回顾一下管理超参数的常见做法。我们关注于如何构建、保存和传递超参数给你的ML脚本。</p>



<h2>What is current practice in the hyperparameters tracking?</h2>



<p>Let’s review one-by-one common practices for managing hyperparameters. We focus on how to build, keep and pass hyperparameters to your ML scripts.</p>


<a class="button continous-post blue-filled" href="/web/20221024193328/https://neptune.ai/blog/how-to-track-hyperparameters" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>