<html>
<head>
<title>How to Deploy NLP Models in Production </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>如何在生产中部署NLP模型</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/deploy-nlp-models-in-production#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/deploy-nlp-models-in-production#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>自然语言处理目前是最令人兴奋的领域之一，因为变形金刚和大型语言模型的出现，如GPT和伯特已经重新定义了该领域的可能性。然而，博客和大众媒体的大部分关注点是模型本身，而不是非常重要的实际细节，比如如何在生产中部署这些模型。本文试图弥合这一差距，并解释NLP模型部署的一些最佳实践。</p>



<p>我们将讨论模型部署过程的许多关键方面，例如:</p>



<ul>
<li>选择模型框架，</li>



<li>决定一个API后端，</li>



<li>使用Flask创建微服务，</li>



<li>使用Docker等工具将模型容器化，</li>



<li>监控部署，</li>



<li>以及使用Kubernetes等工具和AWS Lambda等服务来扩展云基础设施。</li>
</ul>



<p>这样，我们将从头到尾浏览一个部署文本分类模型的小例子，并提供一些关于模型部署最佳实践的想法。</p>







<h2 id="h-model-training-frameworks-vs-model-deployment">模型培训框架与模型部署</h2>



<p>您对NLP框架的选择将对模型的部署方式产生影响。Sklearn是支持向量机、朴素贝叶斯或逻辑回归等简单分类模型的流行选择，它与Python后端集成得很好。Spacy还因其一体化的语言处理功能而备受认可，如句子解析、词性标注和命名实体识别。它也是一个Python包。</p>



<p>基于深度学习的模型通常是在Python的PyTorch库中编写的，因为其通过运行定义的自动签名接口是构建模型的理想选择，这些模型可能会创建响应动态输入的计算图，如解析变长句子。许多流行的库也是建立在PyTorch之上的，比如HuggingFace Transformers，这是一个受人尊敬的使用预先训练好的transformer模型的工具。显然，Python生态系统对于ML和NLP来说是极其流行的；然而，还有其他选择。</p>



<p>预先训练的单词嵌入，如FastText、GloVe和Word2Vec，可以简单地从文本文件中读取，并可以用于任何后端语言和框架。Tensorflow.js是Tensorflow的扩展，允许直接用Javascript编写深度学习模型，并使用Node.js部署在后端，微软CNTK框架可以很容易地集成在。基于. NET和C#的后端。类似的机器学习包可以在许多其他语言和框架中找到，尽管它们的质量各不相同。尽管如此，Python仍然是创建和部署机器学习和NLP模型的事实上的标准。</p>







<h2 id="h-backend-frameworks-vs-model-deployment">后端框架与模型部署</h2>



<p>您对后端框架的选择对于成功的模型部署至关重要。虽然语言和框架的任何组合在技术上都可以工作，但是能够使用与您的模型相同的语言开发的后端通常是很好的。这使得将您的模型导入到您的后端系统变得很容易，而不必在不同的交互后端服务或不同系统之间的端口之间服务请求。它还减少了引入错误的机会，并保持后端代码干净，没有混乱和不必要的库。</p>



<p>Python生态系统中的两个主要后端解决方案是<a href="https://web.archive.org/web/20221203090307/https://www.djangoproject.com/" target="_blank" rel="noreferrer noopener nofollow"> Django </a>和<a href="https://web.archive.org/web/20221203090307/https://flask.palletsprojects.com/en/2.1.x/" target="_blank" rel="noreferrer noopener nofollow">Flask</a>T5】。推荐使用Flask来快速原型化模型微服务，因为它可以很容易地用几行代码建立并运行一个简单的服务器。但是，如果您要构建一个生产系统，Django的功能更加全面，并且集成了流行的Django REST框架，用于构建复杂的API驱动的后端。</p>



<p>流行的NLP库<strong> <a href="https://web.archive.org/web/20221203090307/https://huggingface.co/" target="_blank" rel="noreferrer noopener nofollow"> HuggingFace </a> </strong>，也提供了一种通过推理API部署模型的简单方法。当您使用HuggingFace库构建模型时，您可以训练它并将其上传到他们的模型中心。从那里，他们提供一个可扩展的计算后端，服务于中心托管的模型。只需几行代码，每天花费几美元，任何人都可以部署用HuggingFace库构建的安全、可伸缩的NLP模型。</p>



<p>另一个伟大的NLP专用部署解决方案是<strong> <a href="https://web.archive.org/web/20221203090307/https://github.com/Novetta/adaptnlp" target="_blank" rel="noreferrer noopener nofollow"> Novetta的AdaptNLP </a> </strong>:</p>



<ul>
<li>它们为快速原型化和部署NLP模型提供了各种易于使用的集成。例如，他们有一系列方法，使用<a href="https://web.archive.org/web/20221203090307/https://www.fast.ai/" target="_blank" rel="noreferrer noopener nofollow"> FastAI </a>回调和功能集成不同类型HuggingFace NLP模型的训练，从而加快部署中的训练和推理。</li>



<li>他们还提供现成的<a href="https://web.archive.org/web/20221203090307/https://novetta.github.io/adaptnlp/rest" target="_blank" rel="noreferrer noopener nofollow"> REST API微服务</a>，打包为Docker容器，围绕各种HuggingFace模型类型，如问题回答、令牌标记和序列分类。这些API拥有成熟的Swagger UIs，为测试模型提供了一个清晰的界面。</li>
</ul>



<h2 id="h-hands-on-deployment-of-the-nlp-model">NLP模型的实际部署</h2>



<p>现在，让我们看看如何使用Flask 部署逻辑回归文本分类器。我们将训练分类器来预测电子邮件是“垃圾邮件”还是“火腿”。</p>



<p>你可以访问这个<a href="https://web.archive.org/web/20221203090307/https://www.kaggle.com/datasets/team-ai/spam-text-message-classification?resource=download" target="_blank" rel="noreferrer noopener nofollow"> Kaggle页面</a>并下载数据集。然后，运行下面的命令创建一个conda环境来托管本教程的Python和库安装。</p>



<pre class="hljs">conda create -n model-deploy python=<span class="hljs-number">3.9</span><span class="hljs-number">.7</span>
</pre>



<p>安装完成后，通过运行以下命令激活环境:</p>



<pre class="hljs">conda activate model-deploy
</pre>



<p>然后，通过运行以下命令安装我们需要的库:</p>



<pre class="hljs">pip install Flask scikit-learn
</pre>



<p>在您等待的时候，请继续查看您下载的csv数据集。它有一个标题，指定了两个字段,“类别”(这将是我们的标签)和“消息”(这将是我们的模型输入)。</p>



<p>现在，打开代码编辑器，开始输入。首先，我们将建立分类模型。因为这篇文章是关于部署的教程，我们不会遍历所有的模型细节，但是我们在下面提供了它的代码。</p>



<p>进行所需的进口。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> csv
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
</pre>



<p>创建所需的功能。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_data</span><span class="hljs-params">(fpath)</span>:</span>
	
	cat_map = {
		<span class="hljs-string">"ham"</span>: <span class="hljs-number">0</span>,
		<span class="hljs-string">"spam"</span>: <span class="hljs-number">1</span>
	}
	tfidf = TfidfVectorizer()
	msgs, y = [], []
	filein = open(fpath, <span class="hljs-string">"r"</span>)
	reader = csv.reader(filein)
	<span class="hljs-keyword">for</span> i, line <span class="hljs-keyword">in</span> enumerate(reader):
		<span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:
			
			<span class="hljs-keyword">continue</span>
		cat, msg = line
		y.append(cat_map[cat])
		msg = msg.strip() 
		msgs.append(msg)
	X = tfidf.fit_transform(msgs)
	<span class="hljs-keyword">return</span> X, y, tfidf

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">featurize</span><span class="hljs-params">(text, tfidf)</span>:</span>
	features = tfidf.transform(text)
	<span class="hljs-keyword">return</span> features

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(X, y, model)</span>:</span>
	model.fit(X, y)
	<span class="hljs-keyword">return</span> model

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(X, model)</span>:</span>
	<span class="hljs-keyword">return</span> model.predict(X)

clf = LogisticRegression()
X, y, tfidf = load_data(<span class="hljs-string">'spamorham.csv'</span>)
train(X, y, clf)
</pre>



<p>现在，让我们设置Flask，并为服务微服务的模型创建端点。我们首先要导入Flask并创建一个简单的应用程序。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> model
<span class="hljs-keyword">import</span> json

<span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> (
	Flask,
	request
)

app = Flask(__name__)
app.config[<span class="hljs-string">"DEBUG"</span>] = <span class="hljs-keyword">True</span>

<span class="hljs-meta">@app.route('/predict', methods=['POST'])</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">()</span>:</span>
	args = request.json
	X = model.featurize([args[<span class="hljs-string">'text'</span>]], model.tfidf)
	labels = model.predict(X, model.clf).tolist()
	<span class="hljs-keyword">return</span> json.dumps({<span class="hljs-string">'predictions'</span>: labels})

app.run()
</pre>



<p>正如你所看到的，我们构建了一个Flask应用程序，并以“调试”模式运行，这样如果出现任何错误，它都会提醒我们。我们的应用程序有一条用端点“/predict”定义的“路线”。这是一个POST端点，它接收一个文本字符串，并将其分类为“垃圾邮件”或“垃圾邮件”。</p>



<p>我们以“request.json”的形式访问post参数。只有一个参数“text ”,它指定了我们想要分类的电子邮件消息的文本。为了提高效率，我们也可以重新编写这个程序，一次对多段文本进行分类。如果您愿意，可以尝试添加此功能:)。</p>



<p>预测函数很简单。它接收一封电子邮件，将其转换为TF-IDF特征向量，然后运行经过训练的逻辑回归分类器来预测它是垃圾邮件还是ham。</p>



<p>现在让我们测试一下这个应用程序，以确保它能正常工作！为此，请在命令行中运行以下命令:</p>



<pre class="hljs">python deploy.py
</pre>



<p>这将启动位于<a href="https://web.archive.org/web/20221203090307/http://localhost:5000/" rel="nofollow"> http://localhost:5000 </a>上的Flask服务器。现在，打开一个单独的Python提示符并运行以下代码。</p>



<pre class="hljs">res = requests.post(<span class="hljs-string">'http://127.0.0.1:5000/predict'</span>, json={<span class="hljs-string">"text"</span>: <span class="hljs-string">"You are a winner U have been specially selected 2 receive ¬£1000 or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810910p/min (18+)"</span>})</pre>



<p>您可以看到，我们正在向`/predict '端点发出一个POST请求，该请求带有一个json字段，该字段在参数“text”下指定了电子邮件消息。这显然是一条垃圾短信。让我们看看我们的模型返回什么。要从API获得响应，只需运行` res.json()`。您应该会看到以下结果:</p>



<p><strong> { '预测':[1]} </strong></p>



<p>您也可以通过在POSTMAN中发送请求来测试您的请求，如下所示:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><a href="https://web.archive.org/web/20221203090307/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/NLP-deployment-POSTMAN.png?ssl=1"><img decoding="async" src="../Images/791ddb9254223e936a49617b470a2610.png" alt="NLP deployment POSTMAN" class="wp-image-66570" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221203090307im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/NLP-deployment-POSTMAN.png?ssl=1"/></a><figcaption class="wp-element-caption"><em>Testing request by sending it to POSTMAN</em></figcaption></figure></div>


<p>您需要做的就是输入您的URL，将请求类型设置为POST，并将您的请求的JSON放在请求的“Body”字段中。然后，您应该会看到您的预测返回在较低的窗口。</p>



<p>如您所见，该模型返回的预测值为1，这意味着它将该邮件归类为垃圾邮件。万岁。这就是用Flask部署NLP模型的基础。</p>



<p>在接下来的小节中，我们将讨论更高级的概念，比如如何扩展部署来处理更大的请求负载。</p>



<h2 id="h-containerization-in-the-context-of-model-deployment">模型部署环境中的容器化</h2>



<p>任何模型部署的关键部分是<strong>集装箱化</strong>。像<a href="https://web.archive.org/web/20221203090307/https://www.docker.com/" target="_blank" rel="noreferrer noopener nofollow"> Docker </a>这样的工具允许你将你的代码打包到一个容器中，这个容器基本上是一个虚拟的运行时，包含系统工具、程序安装、库以及运行你的代码所需的任何东西。</p>



<p>将您的服务容器化使它们更加模块化，并允许它们在任何安装了Docker的系统上运行。有了容器，你的代码应该总是不需要任何预配置或者混乱的安装步骤就可以工作。容器还使得使用编排工具(如Docker-Compose和Kubernetes)在许多机器上处理服务的大规模部署变得容易，我们将在本教程的后面部分介绍这些工具。</p>



<p>在这里，我们为我们的文本分类微服务遍历一个docker文件。要让这个容器工作，您需要创建一个“requirements.txt”文件，指定运行我们的微服务所需的包。</p>



<p>您可以在终端的该目录下运行该命令来创建它。</p>



<pre class="hljs">pip freeze &gt; requirements.txt
</pre>



<p>我们还需要对我们的Flask脚本做一个修改，让它在Docker中工作。只需将“app.run()”改为“app.run(host='0.0.0.0 ')”。</p>



<p>现在来看文档。</p>



<pre class="hljs">FROM python:<span class="hljs-number">3.9</span><span class="hljs-number">.7</span>-slim

COPY requirements.txt /app/requirements.txt

RUN cd /app &amp;&amp;
	pip install -r requirements.txt

ADD . /app

WORKDIR /app

ENTRYPOINT [“python”, “deploy.py”]
</pre>



<p>让我们试着理解这几行是什么意思。</p>



<ul>
<li>第一行“来自python:3.9.7-slim ”,指定了容器的<em>基础映像</em>。您可以想象我们的映像继承了库、系统配置和其他元素。我们使用的基本映像提供了Python v3.9.7的最小安装。</li>
</ul>



<ul>
<li>下一行将我们的“requirements.txt”文件复制到“/app”目录下的Docker映像中。`/app `将存放我们的应用程序文件和相关资源。</li>
</ul>



<ul>
<li>在下面一行中，我们通过运行命令“pip install -r requirements.txt ”,进入/app并安装我们需要的python库。</li>
</ul>



<ul>
<li>现在，我们使用“add”将当前构建目录的内容添加到/app文件夹中。/app `。这将复制我们所有的烧瓶和模型脚本。</li>
</ul>



<ul>
<li>最后，我们通过运行“WORKDIR /app”将容器的工作目录设置为/app。然后我们指定入口点，这是容器启动时将运行的命令。我们将其设置为运行“python deploy.py ”,这将启动我们的Flask服务器。</li>
</ul>



<p>要构建docker映像，请从包含Docker文件的目录中运行“Docker build-t spam-or-ham-deploy”。假设一切都正常工作，您应该得到一个构建过程的读数，如下所示:</p>





<p>我们现在还可以在Docker桌面中看到我们的容器图像:</p>





<p>接下来，要运行包含Flask部署脚本的Docker容器，请键入:</p>



<pre class="hljs">docker run -p <span class="hljs-number">5000</span>:<span class="hljs-number">5000</span> -t spam-<span class="hljs-keyword">or</span>-ham-deploy</pre>



<p>`-p 5000:5000 '标志将容器中的端口5000发布到主机中的端口5000。这使得容器的服务可以从机器上的端口访问。现在容器正在运行，我们可以在Docker桌面中查看它的一些统计信息:</p>





<p>我们还可以像以前一样，在POSTMAN中再次尝试运行相同的请求:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/927542174a99becfcc7ad2417f3ad21d.png" alt="NLP deployment POSTMAN " class="wp-image-66574" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221203090307im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/NLP-deployment-POSTMAN-2.png?ssl=1"/><figcaption class="wp-element-caption"><em>Testing request by sending it to POSTMAN</em></figcaption></figure></div>


<h2 id="h-cloud-deployment">云部署</h2>



<p>到目前为止，我们的API只是被设计来处理适度的请求负载。如果您正在为数百万客户部署大规模服务，您将需要对如何部署模型进行许多调整。</p>



<h3>库伯内特斯</h3>



<p>Kubernetes是一个跨大型部署编排容器的工具。使用Kubernetes，您可以毫不费力地在许多机器上部署多个容器，并监控所有这些部署。学习使用Kubernetes是扩展到更大规模部署的一项基本技能。</p>







<p>要在本地运行Kubernetes，你必须<a href="https://web.archive.org/web/20221203090307/https://minikube.sigs.k8s.io/docs/start/">安装minikube </a>。</p>



<p>完成后，在你的终端上运行“minikube start”。下载Kubernetes和基本映像需要几分钟时间。您将得到如下所示的读数:</p>





<p>接下来，我们希望通过运行以下命令来创建部署:</p>



<pre class="hljs">kubectl create deployment hello-minikube --image=spam-<span class="hljs-keyword">or</span>-ham-deploy</pre>



<p>然后，我们希望使用以下方式公开我们的部署:</p>



<pre class="hljs">kubectl expose deployment hello-minikube --type=NodePort --port=<span class="hljs-number">8080</span></pre>



<p>如果我们运行“ku bectl get services hello-mini kube ”,它将显示一些关于我们服务的有用信息:</p>





<p>然后，我们可以通过运行“minikube service hello-minikube”在浏览器中启动该服务</p>





<p>您还可以通过运行“minikube dashboard”在仪表盘中查看您的服务。</p>





<p>欲了解更多信息，请查看<a href="https://web.archive.org/web/20221203090307/https://minikube.sigs.k8s.io/docs/start/" target="_blank" rel="noreferrer noopener nofollow"> Kubernetes入门文档</a>。</p>



<h3>自动气象站λ</h3>



<p>如果你喜欢自动化程度更高的解决方案，像AWS Lambda这样的弹性推理服务会非常有用。这些是事件驱动的服务，这意味着它们将自动启动和管理计算资源，以响应它们所经历的请求负载。你所需要做的就是定义运行你的模型推理代码的Lambda函数，AWS Lambda会为你处理部署和伸缩过程。</p>



<p>您可以在这里了解更多关于在AWS <a href="https://web.archive.org/web/20221203090307/https://pages.awscloud.com/Deploying-Machine-Learning-Models-in-Production_2019_0418-MCL_OD.html" target="_blank" rel="noreferrer noopener nofollow">上部署模型的信息。</a></p>



<h3>火炬服务</h3>



<p>如果您正在使用深度学习NLP模型，如Transformers，PyTorch的<a href="https://web.archive.org/web/20221203090307/https://pytorch.org/serve/" target="_blank" rel="noreferrer noopener nofollow"> TorchServe </a>库是扩展和管理PyTorch部署的绝佳资源。它有一个REST API和一个gRPC API来定义远程过程调用。它还包括用于处理日志记录、跟踪指标和监控部署的有用工具。</p>



<h2 id="h-challenges-in-nlp-model-deployment">NLP模型部署中的挑战</h2>



<p>1.NLP模型部署的一个关键方面是<strong>确保适当的MLOps工作流</strong>。MLOps工具允许您通过跟踪模型的训练和推理中涉及的步骤来确保模型的可重复性。这包括版本数据、代码、超参数和验证指标。</p>



<p><a href="/web/20221203090307/https://neptune.ai/blog/best-mlops-tools" target="_blank" rel="noreferrer noopener"> MLOps工具</a>如<a href="/web/20221203090307/https://neptune.ai/" target="_blank" rel="noreferrer noopener"> Neptune.ai </a>、<a href="https://web.archive.org/web/20221203090307/https://mlflow.org/" target="_blank" rel="noreferrer noopener nofollow"> MLFlow </a>等。为跟踪和记录参数(比如注意系数)和度量(比如NLP模型困惑)、代码版本(实际上，任何通过Git管理的东西)、模型工件和训练运行提供API。使用此类工具监控NLP模型的训练和部署对于防止模型漂移和确保模型继续准确反映系统中的全部数据至关重要。</p>







<p>2.另一个挑战是<strong> NLP </strong> <strong>模型可能需要定期重新训练</strong>。例如，考虑在生产中部署的翻译模型的用例。随着企业在不同国家增加更多的客户，它可能希望向模型中添加更多的语言翻译对。在这种情况下，确保添加新的训练数据和重新训练不会降低现有模型的质量非常重要。因此，如上所述，对各种NLP指标的持续模型监控非常重要。</p>



<p>3.<strong> NLP模型可能还需要在生产中进行增量和在线训练</strong>。例如，如果从原始文本中部署一个情感检测模型，您可能会突然获得一些与“讽刺”情感相对应的新数据。</p>



<p>通常，在这种情况下，您不会想要从头重新训练整个模型，尤其是当模型很大的时候。幸运的是，有许多算法和库可以用来在生产中部署流式NLP模型。例如，scikit-multiflow实现了分类算法，如<a href="https://web.archive.org/web/20221203090307/https://scikit-multiflow.readthedocs.io/en/stable/api/generated/skmultiflow.trees.HoeffdingTreeClassifier.html" target="_blank" rel="noreferrer noopener nofollow"> Hoeffding Trees </a>，这些算法被设计为在次线性时间内进行增量训练。</p>



<h2 id="h-conclusion">结论</h2>



<p>在部署NLP模型时，必须考虑许多因素，如部署的规模、部署的NLP模型的类型、推理延迟和服务器负载等。</p>



<p>部署NLP模型的最佳实践包括使用Django或Flask等Python后端，使用Docker进行容器化，使用MLFlow或Kubeflow进行MLOps管理，以及使用AWS Lambda或Kubernetes等服务进行扩展。</p>



<p>对于那些不想自己处理大规模部署的人来说，有一些易于使用的付费服务，如HuggingFace的推理API，可以为您处理部署。虽然需要一些时间来加快如何优化部署NLP模型，但这是一项非常值得的投资，因为它确保您可以将您的模型提供给世界其他地方！</p>



<h3>参考</h3>




        </div>
        
    </div>    
</body>
</html>