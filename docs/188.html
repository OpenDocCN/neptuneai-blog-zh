<html>
<head>
<title>Binarized Neural Network (BNN) and Its Implementation in Machine Learning </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>二值化神经网络(BNN)及其在机器学习中的实现</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/binarized-neural-network-bnn-and-its-implementation-in-ml#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/binarized-neural-network-bnn-and-its-implementation-in-ml#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>二值化神经网络(BNN)来自Courbariaux、Hubara、Soudry、El-Yaniv和beng io 2016年的一篇<a href="https://web.archive.org/web/20230214183854/https://arxiv.org/pdf/1602.02830.pdf" target="_blank" rel="noreferrer noopener nofollow">论文</a>。它引入了一种新的方法来训练神经网络，其中权重和激活在训练时被二值化，然后用于计算梯度。</p>



<p>通过这种方式，可以减少内存大小，并且位运算可以提高功效。GPU消耗巨大的功率，使得神经网络很难在低功率设备上训练。BNNs可以将功耗降低32倍以上。</p>



<p>该论文表明，可以使用二进制矩阵乘法来减少训练时间，这使得在MNIST上训练BNN的速度提高了7倍，达到了接近最先进的结果。</p>



<p>在本文中，我们将看到二进制神经网络是如何工作的。我们将深入研究该算法，并查看实现BNNs的库。</p>



<h2 id="h-how-binarized-neural-networks-work">二进制神经网络如何工作</h2>



<p>在我们深入挖掘之前，让我们看看BNNs是如何工作的。</p>



<p>在论文中，他们使用两个函数来二进制化x(权重/激活)的值——确定性的和随机的。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/7dc241dc375bb8ef34ee805d2178b41a.png" alt="" class="wp-image-43312" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230214183854im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/BNN-equation-1.png?ssl=1"/><figcaption class="wp-element-caption">Eq. 1</figcaption></figure></div>


<p>情商。1是确定性函数，其中signum函数用于实值变量。</p>



<p>随机函数使用硬sigmoid:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large"><img decoding="async" src="../Images/ae23025fc1a930a84c0cfa68391f5483.png" alt="BNN equation" class="wp-image-43313" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230214183854im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/BNN-equation-2.png?ssl=1"/><figcaption class="wp-element-caption">Eq. 2</figcaption></figure></div>


<p>方程式中的xb。1和Eq。2是实值变量(权重/激活)的二进制值。1很简单。</p>



<p>在Eq中。2</p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/118aa29d58a105d0309591b8e1be4f8d.png" alt="BNN equation" class="wp-image-43315" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230214183854im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/BNN-equation-3.png?resize=326%2C24&amp;ssl=1"/><figcaption class="wp-element-caption">Eq. 3</figcaption></figure></div>


<p>确定性函数在大多数情况下使用，除了少数实验中随机函数用于激活。</p>



<p>除了二进制化权重和激活之外，BNNs还有两个更重要的方面:</p>



<ul>
<li>为了让优化器工作，您需要实值权重，所以它们在实值变量中累积。即使我们使用二进制权重/激活，我们也使用实值权重进行优化。</li>



<li>当我们使用确定性或随机函数进行二值化时，会出现另一个问题。当我们反向传播时，这些函数的导数为零，这使得整个梯度为零。所以我们可以使用饱和STE(直通估计量)，这是之前由Hinton提出并由Bengio研究的。在饱和STE中，signum的导数用1 <sub> {x &lt; =1} </sub>代替，简单来说就是当x &lt; =1时，用恒等式(1)代替导数零。所以，当x太大时，它抵消了梯度，因为导数为零。</li>
</ul>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/e6881d5ae66e8e2fc175da64e518caf9.png" alt="Binary weight filter" class="wp-image-43317" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230214183854im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Binary-weight-filter.png?resize=339%2C281&amp;ssl=1"/><figcaption class="wp-element-caption"><em>Fig. </em>1 – A<em> binary weight filter from the 1st convolutional layer of BNN | <a href="https://web.archive.org/web/20230214183854/https://arxiv.org/pdf/1602.02830.pdf" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>


<h2 id="h-shift-based-batch-normalization-and-shift-based-adamax-optimization">基于移位的批处理规范化和基于移位的AdaMax优化</h2>



<p>除了常规批处理规范化和Adamax优化之外，还有一种替代方法。BatchNorm和Adam optimizer都包含大量乘法运算。为了加快这个过程，它们被基于移位的方法所取代。这些方法使用位运算来节省时间。BNN的论文声称，当用基于班次的批处理规范化和基于班次的Adam优化程序替换批处理规范化和Adam优化程序时，没有观察到准确度损失。</p>



<h2 id="h-speeding-up-the-training">加速训练</h2>



<p>BNN论文中介绍的方法可以加速BNNs的GPU实现。与使用cuBLAS相比，它可以提高时间效率。</p>



<p>cuBLAS是一个CUDA工具包库，提供GPU加速的基本线性代数子程序(BLAS)。</p>



<p>一种称为SWAR的方法，用于在寄存器内执行并行操作，用于加速计算。它将32位二进制变量连接到32位寄存器。</p>



<p>SWAR可以在Nvidia GPU上仅用6个时钟周期评估这32个连接，因此理论上速度提高了32/6 = 5.3倍。值+1和-1对于执行此操作非常重要，因此我们需要将变量二进制化为这两个值。</p>



<p>让我们来看看一些性能统计数据:</p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/59e6bc8ebe460e9c2622cfb3864dee8a.png" alt="BNN performance stats" class="wp-image-43318" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230214183854im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/BNN-performance-stats.png?resize=475%2C393&amp;ssl=1"/><figcaption class="wp-element-caption"><em>Fig. 2 – Comparison between Baseline kernel, cuBLAS and the XNOR kernel for time and accuracy. | <a href="https://web.archive.org/web/20230214183854/https://arxiv.org/pdf/1602.02830.pdf" rel="nofollow">Source</a></em></figcaption></figure></div>


<p>正如我们在图2中看到的，所有三种方法的精确度；使用cuBLAS库和paper的XNOR核的未优化基线核在图的第三部分是相同的。在第一部分中，将矩阵乘法时间与8192 x 8192 x 8192矩阵进行比较。在第二部分中，在多层感知器上推断MNIST的全部测试数据。我们可以清楚地看到，XNOR内核的性能更好。在矩阵乘法的情况下，XNOR比基线内核快23倍，比cuBLAS内核快3.4倍。</p>



<p>我们可以看到，在运行MNIST测试数据时，cuBLAS和XNOR内核之间的差异较小。这是因为在第一层，值不是二进制的，所以基线内核用于计算，从而导致一点延迟。但这不是什么大问题，因为输入图像通常只有3个通道，这意味着计算量更少。</p>



<h2 id="h-code">密码</h2>



<p>我们来看一些实现了BNNs的Github repos。</p>



<p>BNNs的前两个实现包含在原始论文中，虽然一个是用lua(torch)实现的，另一个是用Python实现的，但是是用theano实现的。</p>



<h3>提诺:</h3>



<p><a href="https://web.archive.org/web/20230214183854/https://github.com/MatthieuCourbariaux/BinaryNet">https://github . com/matthieurbarillas/binary net</a></p>



<h3>火炬:</h3>



<p><a href="https://web.archive.org/web/20230214183854/https://github.com/itayhubara/BinaryNet">https://github.com/itayhubara/BinaryNet</a></p>



<h3>PyTorch:</h3>



<p>BNN论文的作者之一提供了一个pytorch实现，包括alexnet二进制、resnet二进制和vgg二进制等架构，具有不同的层数(resent18、resnet34、resnet50等。)</p>



<p><a href="https://web.archive.org/web/20230214183854/https://github.com/itayhubara/BinaryNet.pytorch">https://github.com/itayhubara/BinaryNet.pytorch</a></p>



<p>没有文档，但是代码很直观。在子目录“模型”中，实现了三个二值化网络:vgg、resnet和alexnet。</p>



<p>使用文件“data.py”向BNN网络发送自定义数据集。‘preprocess . py’里也有很多变换选项。</p>



<h3>Keras/TensorFlow:</h3>



<div class="is-layout-flex wp-container-3 wp-block-columns are-vertically-aligned-center">
<div class="is-layout-flow wp-block-column is-vertically-aligned-center">
<p>到目前为止，我见过的最好的包之一是<a href="https://web.archive.org/web/20230214183854/https://larq.dev/"> Larq </a>，这是一个开源包，其中构建和训练二进制神经网络非常容易。</p>
</div>




</div>



<p>在前面讨论的包中，有可以使用的预先实现的网络。但是有了Larq，你可以用一种非常简单的方式创建新的网络。这就像Keras API，例如，如果你想添加一个二值化的conv层，而不是' tf.keras.layers.Conv2D '，你可以使用' larq.layers.Conv2D '。</p>



<p>关于这个包最好的一点是<a href="https://web.archive.org/web/20230214183854/https://docs.larq.dev/larq/" target="_blank" rel="noreferrer noopener nofollow">文档</a>非常好，社区正在积极开发它，所以支持也很好。</p>



<p>尽管它有很棒的文档，但让我们看看文档中的一个例子，这样您就可以大致了解该库的易用性。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> larq <span class="hljs-keyword">as</span> lq

kwargs = dict(input_quantizer=<span class="hljs-string">"ste_sign"</span>,
              kernel_quantizer=<span class="hljs-string">"ste_sign"</span>,
              kernel_constraint=<span class="hljs-string">"weight_clip"</span>,
              use_bias=<span class="hljs-keyword">False</span>)

model = tf.keras.models.Sequential([
    lq.layers.QuantConv2D(<span class="hljs-number">128</span>, <span class="hljs-number">3</span>,
                          kernel_quantizer=<span class="hljs-string">"ste_sign"</span>,
                          kernel_constraint=<span class="hljs-string">"weight_clip"</span>,
                          use_bias=<span class="hljs-keyword">False</span>,
                          input_shape=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)),
    tf.keras.layers.BatchNormalization(momentum=<span class="hljs-number">0.999</span>, scale=<span class="hljs-keyword">False</span>),

    lq.layers.QuantConv2D(<span class="hljs-number">128</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, **kwargs),
    tf.keras.layers.MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
    tf.keras.layers.BatchNormalization(momentum=<span class="hljs-number">0.999</span>, scale=<span class="hljs-keyword">False</span>),

    lq.layers.QuantConv2D(<span class="hljs-number">256</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, **kwargs),
    tf.keras.layers.BatchNormalization(momentum=<span class="hljs-number">0.999</span>, scale=<span class="hljs-keyword">False</span>),

    lq.layers.QuantConv2D(<span class="hljs-number">256</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, **kwargs),
    tf.keras.layers.MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
    tf.keras.layers.BatchNormalization(momentum=<span class="hljs-number">0.999</span>, scale=<span class="hljs-keyword">False</span>),

    lq.layers.QuantConv2D(<span class="hljs-number">512</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, **kwargs),
    tf.keras.layers.BatchNormalization(momentum=<span class="hljs-number">0.999</span>, scale=<span class="hljs-keyword">False</span>),

    lq.layers.QuantConv2D(<span class="hljs-number">512</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, **kwargs),
    tf.keras.layers.MaxPool2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
    tf.keras.layers.BatchNormalization(momentum=<span class="hljs-number">0.999</span>, scale=<span class="hljs-keyword">False</span>),
    tf.keras.layers.Flatten(),

    lq.layers.QuantDense(<span class="hljs-number">1024</span>, **kwargs),
    tf.keras.layers.BatchNormalization(momentum=<span class="hljs-number">0.999</span>, scale=<span class="hljs-keyword">False</span>),

    lq.layers.QuantDense(<span class="hljs-number">1024</span>, **kwargs),
    tf.keras.layers.BatchNormalization(momentum=<span class="hljs-number">0.999</span>, scale=<span class="hljs-keyword">False</span>),

    lq.layers.QuantDense(<span class="hljs-number">10</span>, **kwargs),
    tf.keras.layers.BatchNormalization(momentum=<span class="hljs-number">0.999</span>, scale=<span class="hljs-keyword">False</span>),
    tf.keras.layers.Activation(<span class="hljs-string">"softmax"</span>)
])
</pre>



<p><strong>注意，如前所述，我们没有将希格诺和STE用于输入层。</strong>我们来看看最终的架构。</p>



<pre class="hljs">model.summary()
</pre>



<pre class="hljs">Model: <span class="hljs-string">"sequential"</span>
_________________________________________________________________
Layer (type)                 Output Shape              Param 
=================================================================
quant_conv2d (QuantConv2D)   (<span class="hljs-keyword">None</span>, <span class="hljs-number">30</span>, <span class="hljs-number">30</span>, <span class="hljs-number">128</span>)       <span class="hljs-number">3456</span>
_________________________________________________________________
batch_normalization (BatchNo (<span class="hljs-keyword">None</span>, <span class="hljs-number">30</span>, <span class="hljs-number">30</span>, <span class="hljs-number">128</span>)       <span class="hljs-number">384</span>
_________________________________________________________________
quant_conv2d_1 (QuantConv2D) (<span class="hljs-keyword">None</span>, <span class="hljs-number">30</span>, <span class="hljs-number">30</span>, <span class="hljs-number">128</span>)       <span class="hljs-number">147456</span>
_________________________________________________________________
max_pooling2d (MaxPooling2D) (<span class="hljs-keyword">None</span>, <span class="hljs-number">15</span>, <span class="hljs-number">15</span>, <span class="hljs-number">128</span>)       <span class="hljs-number">0</span>
_________________________________________________________________
batch_normalization_1 (Batch (<span class="hljs-keyword">None</span>, <span class="hljs-number">15</span>, <span class="hljs-number">15</span>, <span class="hljs-number">128</span>)       <span class="hljs-number">384</span>
_________________________________________________________________
quant_conv2d_2 (QuantConv2D) (<span class="hljs-keyword">None</span>, <span class="hljs-number">15</span>, <span class="hljs-number">15</span>, <span class="hljs-number">256</span>)       <span class="hljs-number">294912</span>
_________________________________________________________________
batch_normalization_2 (Batch (<span class="hljs-keyword">None</span>, <span class="hljs-number">15</span>, <span class="hljs-number">15</span>, <span class="hljs-number">256</span>)       <span class="hljs-number">768</span>
_________________________________________________________________
quant_conv2d_3 (QuantConv2D) (<span class="hljs-keyword">None</span>, <span class="hljs-number">15</span>, <span class="hljs-number">15</span>, <span class="hljs-number">256</span>)       <span class="hljs-number">589824</span>
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (<span class="hljs-keyword">None</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">256</span>)         <span class="hljs-number">0</span>
_________________________________________________________________
batch_normalization_3 (Batch (<span class="hljs-keyword">None</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">256</span>)         <span class="hljs-number">768</span>
_________________________________________________________________
quant_conv2d_4 (QuantConv2D) (<span class="hljs-keyword">None</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">512</span>)         <span class="hljs-number">1179648</span>
_________________________________________________________________
batch_normalization_4 (Batch (<span class="hljs-keyword">None</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">512</span>)         <span class="hljs-number">1536</span>
_________________________________________________________________
quant_conv2d_5 (QuantConv2D) (<span class="hljs-keyword">None</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">512</span>)         <span class="hljs-number">2359296</span>
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (<span class="hljs-keyword">None</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">512</span>)         <span class="hljs-number">0</span>
_________________________________________________________________
batch_normalization_5 (Batch (<span class="hljs-keyword">None</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">512</span>)         <span class="hljs-number">1536</span>
_________________________________________________________________
flatten (Flatten)            (<span class="hljs-keyword">None</span>, <span class="hljs-number">4608</span>)              <span class="hljs-number">0</span>
_________________________________________________________________
quant_dense (QuantDense)     (<span class="hljs-keyword">None</span>, <span class="hljs-number">1024</span>)              <span class="hljs-number">4718592</span>
_________________________________________________________________
batch_normalization_6 (Batch (<span class="hljs-keyword">None</span>, <span class="hljs-number">1024</span>)              <span class="hljs-number">3072</span>
_________________________________________________________________
quant_dense_1 (QuantDense)   (<span class="hljs-keyword">None</span>, <span class="hljs-number">1024</span>)              <span class="hljs-number">1048576</span>
_________________________________________________________________
batch_normalization_7 (Batch (<span class="hljs-keyword">None</span>, <span class="hljs-number">1024</span>)              <span class="hljs-number">3072</span>
_________________________________________________________________
quant_dense_2 (QuantDense)   (<span class="hljs-keyword">None</span>, <span class="hljs-number">10</span>)                <span class="hljs-number">10240</span>
_________________________________________________________________
batch_normalization_8 (Batch (<span class="hljs-keyword">None</span>, <span class="hljs-number">10</span>)                <span class="hljs-number">30</span>
_________________________________________________________________
activation (Activation)      (<span class="hljs-keyword">None</span>, <span class="hljs-number">10</span>)                <span class="hljs-number">0</span>
=================================================================
Total params: <span class="hljs-number">10</span>,<span class="hljs-number">363</span>,<span class="hljs-number">550</span>
Trainable params: <span class="hljs-number">10</span>,<span class="hljs-number">355</span>,<span class="hljs-number">850</span>
Non-trainable params: <span class="hljs-number">7</span>,<span class="hljs-number">700</span>
</pre>



<p>现在你可以像训练一个在keras中实现的普通神经网络一样训练它。</p>



<section id="blog-intext-cta-block_72a1292aa25e7df401a2ac95b1dc73a5" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p><img decoding="async" loading="lazy" class="lazyload block-blog-intext-cta__arrow-image" src="../Images/fe34857d1d7e68b4c676b5b18f16e936.png" alt="" data-src="https://web.archive.org/web/20230214183854/https://neptune.ai/wp-content/themes/neptune/img/icon-arrow--right-gray.svg" data-original-src="https://web.archive.org/web/20230214183854/https://neptune.ai/wp-content/themes/neptune/img/icon-arrow--right-gray.svg"/>？海王星与<a href="https://web.archive.org/web/20230214183854/https://docs.neptune.ai/essentials/integrations/deep-learning-frameworks/pytorch" target="_blank" rel="noreferrer noopener"> PyTorch </a> <br/> <img decoding="async" loading="lazy" class="lazyload block-blog-intext-cta__arrow-image" src="../Images/fe34857d1d7e68b4c676b5b18f16e936.png" alt="" data-src="https://web.archive.org/web/20230214183854/https://neptune.ai/wp-content/themes/neptune/img/icon-arrow--right-gray.svg" data-original-src="https://web.archive.org/web/20230214183854/https://neptune.ai/wp-content/themes/neptune/img/icon-arrow--right-gray.svg"/>的融合？海王星与<a href="https://web.archive.org/web/20230214183854/https://docs.neptune.ai/essentials/integrations/deep-learning-frameworks/tensorflow-keras" target="_blank" rel="noreferrer noopener"> TensorFlow/Keras的整合</a></p>
    
    </section>



<h2 id="h-applications">应用程序</h2>



<p>bnn具有功率效率，因此可用于低功率器件。这是BNNs最大的优势之一。您可以使用LCE(Larq计算引擎)和Tensorflow Lite Java在Android上训练和推断神经网络，消耗更少的功率。</p>


<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/df717ee0549bdc1e58be4867276839e0.png" alt="BNN Tensorflow" class="wp-image-43319" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20230214183854im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/BNN-Tensorflow.png?resize=288%2C512&amp;ssl=1"/><figcaption class="wp-element-caption"><em>Fig. 3 – Example of Image Classification using LCE Lite | <a href="https://web.archive.org/web/20230214183854/https://docs.larq.dev/compute-engine/quickstart_android/">Source</a></em></figcaption></figure></div>


<p>你可以点击下面的<a href="https://web.archive.org/web/20230214183854/https://docs.larq.dev/compute-engine/quickstart_android/" target="_blank" rel="noreferrer noopener nofollow">链接</a>阅读更多关于在Android设备上使用BNNs的信息。</p>



<h2 id="h-conclusion">结论</h2>



<p>深度网络需要耗电的GPU，很难在低功耗设备上训练它们。因此，二元神经网络的概念似乎很有前途。</p>



<p>它们消耗更少的功率而没有任何精度损失，并且可以在移动设备中用于训练dnn。好像挺有用的！</p>



<p>感谢阅读。</p>



<h3>参考</h3>



<p>如果你想深入了解BNNs，这里有一些参考资料:</p>







<p/>
        </div>
        
    </div>    
</body>
</html>