<html>
<head>
<title>Markov Decision Process in Reinforcement Learning: Everything You Need to Know </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>强化学习中的马尔可夫决策过程:你需要知道的一切</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/markov-decision-process-in-reinforcement-learning#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/markov-decision-process-in-reinforcement-learning#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>花点时间找出你周围最近的大城市。如果你要去那里，你会怎么做？开车去，坐公交，坐火车？也许骑自行车，或者买张机票？</p>



<p>做出这个选择，你<strong>将概率融入你的决策过程</strong>。也许有70%的几率会下雨或发生车祸，这会导致交通堵塞。如果你的自行车轮胎旧了，它可能会坏掉——这当然是一个很大的概率因素。</p>



<p>另一方面，也有确定性成本(T0)和确定性回报(T1)，比如汽油或飞机票的成本，比如坐飞机的旅行时间。</p>



<p>这类问题——其中代理人必须平衡概率性和确定性的回报和成本——在决策中很常见。<a href="https://web.archive.org/web/20220928192845/https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da" target="_blank" rel="noreferrer noopener nofollow">马尔可夫决策过程</a>用于建模这些类型的优化问题，也可以应用于强化学习中更复杂的任务。</p>





<h2>在机器学习中定义马尔可夫决策过程</h2>



<p>为了说明马尔可夫决策过程，考虑一个骰子游戏:</p>


<div class="custom-point-list">
<ul><li>每一轮，你可以选择<strong>继续</strong>或者<strong>退出</strong>。</li><li>如果你<strong>退出</strong>，你将获得5美元，游戏结束。</li><li>如果你<strong>继续</strong>，你将获得3美元并掷出一个6面骰子。如果骰子是1或2，游戏结束。否则，游戏将进入下一轮。</li></ul>
</div>


<p>这里有一个明显的权衡。首先，我们可以用2美元的确定性收益来换取掷骰子并进入下一轮的机会。</p>



<p>要创建一个MDP来模拟这个游戏，首先我们需要定义一些东西:</p>


<div class="custom-point-list">
<ul><li><em>状态</em>是代理(决策者)可以拥有的状态。在骰子游戏中，代理既可以是游戏中的<em/>，也可以是游戏外的<em/>。</li><li>一个<em>动作</em>是代理可以选择的运动。它使代理在不同状态之间移动，并带有一定的惩罚或奖励。</li><li><em>转移概率</em>描述了在给定动作<em> a </em>的情况下，结束于状态s’(s质数)的概率。这些将通常被表示为函数<em> P </em> ( <em> s </em>，<em> a </em>，<em>s</em>’)，其输出在给定当前状态<em> s </em>和动作<em> a </em>的情况下结束于<em>s’</em>的概率。<br/>比如<em>p</em>(<em>s</em>=玩游戏，<em>a</em>=选择继续玩，<em>s’</em>=不玩游戏)就是⅓，既然掷骰子有六分之二(三分之一)的几率输。</li><li><em>奖励</em>根据行动给予。继续游戏的奖励是3美元，而退出游戏的奖励是5美元。“整体”奖励将得到优化。</li></ul>
</div>


<p>我们可以将一个马尔可夫决策过程形式化地描述为<em> m </em> = ( <em> S </em>，<em> A，</em> <em> P </em>，<em> R </em>，gamma)，其中:</p>


<div class="custom-point-list">
<ul><li><em> S </em>代表所有状态的集合。</li><li><em> A </em>代表一组可能的动作。</li><li><em> P </em>代表转移概率。</li><li><em> R </em>代表奖励。</li><li>Gamma被称为折扣因子(稍后将详细介绍)。</li></ul>
</div>


<p>MDP m的目标是找到一个能产生最佳长期回报的策略，通常记为pi。政策仅仅是每个状态<em>的</em>到行为分布<em>的</em>的映射。对于每个状态<em> s </em>，代理应该以一定的概率采取行动<em> a </em>。或者，策略也可以是确定性的(即代理<em>将在状态<em> s </em>中</em>采取动作<em> a </em>)。</p>



<p>我们的马尔可夫决策过程如下图所示。一个代理通过做决定和遵循概率来遍历图的两个状态。</p>







<p>提到<em>马尔可夫属性</em>很重要，它不仅适用于马尔可夫决策过程，也适用于任何与马尔可夫相关的东西(比如马尔可夫链)。</p>



<p>它指出下一个状态可以仅由当前状态决定——不需要“记忆”。这适用于代理如何遍历马尔可夫决策过程，但请注意，优化方法使用先前的学习来微调策略。这并不违反马尔可夫性质，它只适用于MDP的<em>遍历</em>。</p>





<h2>贝尔曼方程与动态规划</h2>



<p>贝尔曼方程是马尔可夫决策过程的核心。它概述了一个框架，通过回答这个问题来确定在状态<em> s </em>下的最优期望报酬:“如果一个代理人现在和将来做出最优决策，他们能得到的最大报酬是多少？”</p>



<p>尽管贝尔曼方程的各种版本可能会变得相当复杂，但从根本上讲，它们中的大多数都可以归结为以下形式:</p>







<p>用公式化的术语来说，这是一个相对常识性的想法。请注意gamma(介于0和1之间，含0和1)在确定最佳奖励时所起的作用。如果gamma设置为0，则V(s ')项完全被抵消，模型只关心眼前的回报。</p>



<p>另一方面，如果gamma设置为1，则该模型对潜在未来回报的权重与对即时回报的权重相同。gamma的最佳值通常在0到1之间，这样，奖励值越远，效果越差。</p>



<p>让我们用贝尔曼方程来确定在骰子游戏中我们能得到多少钱。我们可以在两个选择中进行选择，所以我们的扩展方程看起来像max(选择1的回报，选择2的回报)。</p>



<p>选择1——放弃——会产生5英镑的奖励。</p>



<p>另一方面，选择2产生的回报为3，加上三分之二的机会继续进入下一阶段，在这个阶段可以再次做出决定(我们是按预期回报计算的)。我们在表示计算s’(下一个状态)的项前添加一个折扣因子γ。</p>







<p>这个等式是递归的，但是不可避免地，它将收敛到一个值，假设下一次迭代的值以⅔递减，即使最大伽马为1。</p>



<p>在某种程度上，继续留在游戏中是无利可图的。让我们计算四次迭代，gamma为1，以保持简单，并计算总的长期最优回报。</p>



<p>在每一步，我们可以选择退出并获得额外的5美元期望值，或者留下来并获得额外的3美元期望值。每一轮新的，期望值乘以三分之二，因为有三分之二的概率继续，即使代理人选择留下。</p>







<p>在这里，计算十进制值，我们发现(根据我们当前的迭代次数),如果我们遵循最佳选择，我们可以期望得到7.8美元。</p>







<p>在这里，我们手动计算了最佳利润，这意味着我们的计算出现了错误:我们仅在四轮后就终止了计算。</p>



<p>如果我们继续计算几十行的期望值，我们会发现最佳值实际上更高。为了用程序有效地计算这个，你需要使用一个专门的数据结构。</p>



<p>另外，为了提高效率，我们不想单独计算每个期望值，而是与之前的值相关联。解决方案:动态编程。</p>



<p>贝尔曼方程的理查德·贝尔曼创造了术语动态规划，它被用来计算可以被分解成子问题的问题。例如，选择Stay &gt; Stay &gt; Stay &gt; Quit的期望值可以通过计算Stay &gt; Stay &gt; Stay first的值得到。</p>



<p>这些预计算将存储在一个二维数组中，其中行表示状态[In]或[Out]，列表示迭代。我们可以编写规则，将表格中的每个单元格与之前预先计算的单元格相关联(此图不包括gamma)。</p>







<p>然后，在计算足够的迭代后，解就是数组中的最大值。通过动态编程，计算期望值——马尔可夫决策过程和Q学习等方法的关键组成部分——变得高效。</p>





<h2>q学习:马尔可夫决策过程+强化学习</h2>



<p>让我们考虑一个不同的简单游戏，其中代理人(圆圈)必须在一个网格中导航，以便在给定的迭代次数下获得最大的回报。</p>



<p>有七种类型的块:</p>


<div class="custom-point-list">
<ul><li>-2惩罚，</li><li>-5惩罚，</li><li>-1惩罚，</li><li>+1奖励，</li><li>+10奖励，</li><li>以相等的概率将代理移动到空间A1或B3的块，</li><li>空街区。</li></ul>
</div>


<p>请注意，这是一个网格形式的<a href="https://web.archive.org/web/20220928192845/https://www.mathworks.com/help/reinforcement-learning/ug/train-reinforcement-learning-agent-in-mdp-environment.html" target="_blank" rel="noreferrer noopener nofollow">MDP</a>——有9个状态，每个状态都连接到它周围的状态。如果代理人的惩罚为-5或更少，或者代理人的奖励为5或更多，游戏终止。</p>







<p>在Q-learning中，我们不知道概率——它在模型中没有明确定义。相反，模型必须通过与环境的交互来自己学习这一点和景观。</p>



<p>这使得Q-learning适用于显式概率和值未知的情况。如果它们是已知的，那么你可能不需要使用Q-learning。</p>



<p>在我们的游戏中，我们知道概率、奖励和惩罚，因为我们严格定义它们。但是，比方说，如果我们训练一个机器人在复杂的环境中导航，我们就不能硬编码物理规则；使用Q-learning或另一种强化学习方法将是合适的。</p>



<p>每走一步，模型都会在Q表中更新它的学习。下表存储了可能的状态-动作对，反映了关于系统的<em>当前</em>已知信息，这些信息将用于驱动未来的决策。</p>







<p>每个单元格都包含Q值，它表示在采取当前操作的情况下系统的期望值。(这听起来耳熟吗？应该是——这又是贝尔曼方程！)<strong> </strong></p>



<p>表中的所有值都从0开始，并迭代更新。请注意，A3没有状态，因为代理无法从该点控制他们的移动。</p>



<p>为了更新Q表，代理首先选择一个动作。它不能上下移动，但是如果它向右移动，它将受到-5的惩罚，游戏终止。Q表可以相应地更新。</p>







<p>当代理第二次遍历环境时，它会考虑它的选项。给定当前Q表，它可以向右或向下移动。与向下移动相比，向右移动的损失为-5，当前设置为0。</p>



<p>为了便于模拟，让我们想象代理人沿着下面指示的路径行进，并在C1结束，以10英镑的奖励终止游戏。然后，我们可以填写代理人在这一过程中采取的每一项行动所获得的奖励。</p>







<p>显然，这个Q表是不完整的。即使代理从A1向下移动到A2，也不能保证它将获得10的奖励。经过足够的迭代之后，代理应该已经遍历了环境，Q表中的值告诉我们在每个位置应该做出的最佳和最差决策。</p>



<p>这个例子简化了Q值实际上是如何更新的，这涉及到上面讨论的贝尔曼方程。例如，根据gamma的值，我们可以基于更近且更准确的Q表来决定由代理收集的最近信息可能比旧信息更重要，因此我们可以在构建Q表时忽略旧信息的重要性。</p>



<p>这里需要注意的是<strong>勘探与开发的权衡</strong>。如果代理人沿着正确的路径朝着目标前进，但是由于某种原因，以不幸的惩罚结束，它将在Q表中记录负值，并将它采取的每一步都与这个惩罚相关联。</p>



<p>如果代理人纯粹是“剥削的”——它总是寻求最大化直接的即时收益——它可能永远也不敢在这条道路上迈出一步。</p>



<p>另一方面，如果一个代理人遵循小报酬的路径，纯粹的剥削代理人每次都会简单地遵循该路径，而忽略任何其他路径，因为它导致大于1的报酬。</p>



<p>通过允许代理“探索”更多，它可以更少地关注选择最佳路径，而更多地关注收集信息。这通常以随机性的形式发生，这允许代理在他们的决策过程中具有某种随机性。</p>



<p>然而，一个纯粹的“探索性”代理也是无用和低效的——它将采取明显导致大量惩罚的路径，并可能占用宝贵的计算时间。</p>



<p>结合一些中间的随机性是一个好的做法，这样代理人就可以根据以前的发现进行推理，但仍然有机会处理探索较少的路径。</p>



<p>结合勘探-开采权衡的一种复杂形式是<em>模拟退火</em>，它来自冶金学，金属的受控加热和冷却。</p>



<p>模拟退火不是允许模型在选择探索性或开发性时有某种固定的常数，而是通过让代理大量探索开始，然后随着时间的推移当它获得更多信息时变得更加开发性。</p>



<p>这种方法已经在离散问题上显示出巨大的成功，如旅行推销员问题，所以它也适用于马尔可夫决策过程。</p>



<p>因为模拟退火是从高探索开始的，所以它通常能够衡量哪些解决方案是有希望的，哪些是不太有希望的。随着模型变得更具开发性，它将注意力转向有希望的解决方案，最终以计算高效的方式接近最有希望的解决方案。</p>





<h2>摘要</h2>



<p>让我们总结一下本文中所探讨的内容:</p>



<p>马尔可夫决策过程(MDP)用于对决策进行建模，这些决策可能具有概率性和确定性的奖励和惩罚。</p>



<p>市场发展计划有五个核心要素:</p>


<div class="custom-point-list">
<ul><li>一组代理可能处于的状态，</li><li>a，代理在特定状态下可以采取的一组可能的动作，</li><li>r，在状态S做出动作A的奖励；</li><li>p，在原始状态S采取行动A后转移到新状态S '的概率；</li><li>gamma，它控制马尔可夫决策过程代理的前瞻性。</li></ul>
</div>


<p>包括MDP在内的所有马尔可夫过程都必须遵循<em>马尔可夫性质</em>，即下一个状态可以纯粹由当前状态决定。</p>



<p>贝尔曼方程决定了代理人在当前状态和所有后续状态下做出最优决策时可以获得的最大回报。它递归地将当前状态的值定义为当前状态奖励的最大可能值加上下一个状态的值。</p>



<p>动态编程利用网格结构来存储以前计算的值，并在此基础上计算新值。它可以用来有效地计算策略的值，不仅可以解决马尔可夫决策过程，还可以解决许多其他递归问题。</p>



<p>Q学习是在环境中学习Q值，这通常类似于马尔可夫决策过程。它适用于特定概率、奖励和惩罚不完全已知的情况，因为代理反复遍历环境以自己学习最佳策略。</p>



<p>希望你喜欢和我一起探讨这些话题。感谢您的阅读！</p>




<div id="author-box-new-format-block_605459bc5a880" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">Andre Ye</h3>
    
          <p class="article__authorContent-text">机器学习和数据科学爱好者。对深度学习充满热情，并喜欢跟上推动机器智能的边界的新论文。</p>
    
          
    
  </div>
</div>


<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>



<p class="has-text-color"><strong>阅读下一篇</strong></p>



<h2>如何构建、组织、跟踪和管理强化学习(RL)项目</h2>



<p class="has-small-font-size">7分钟阅读|弗拉基米尔·利亚申科|发布于2020年12月23日</p>


<p id="block_5ffc75def9f8e" class="separator separator-10"/>



<p>构建和管理机器学习项目可能是一件棘手的事情。</p>



<p>当您投入到一个项目中时，您可能会很快意识到您淹没在Python脚本、数据、算法、函数、更新等等的海洋中。在某些时候，你会忘记你的实验，甚至说不出哪个脚本或更新产生了最好的结果。</p>



<p>因此，<strong>组织你的项目并跟踪实验</strong>是成功的关键部分。</p>



<p>从这个角度来看，从事一个<strong> ML </strong>项目总体来说可能具有挑战性，但是有些领域比其他领域更复杂。<strong>强化学习</strong> ( <strong> RL </strong>)就是其中比较复杂的一种<strong>。</strong></p>



<p>本文致力于<strong>构建和管理RL项目</strong>。我会尽量精确，并提供一个全面的分步指南和一些有用的提示。</p>



<p>我们将涵盖:</p>


<div class="custom-point-list">
<ul><li><strong>一般提示</strong>—项目目录结构，<strong> Cookiecutter </strong>，使用<strong> Neptune </strong>跟踪实验，适当评估</li><li><strong>将问题定义为RL问题</strong>–强化学习、监督学习、优化问题、最大化和最小化</li><li><strong>挑选RL环境</strong>–open ai健身房</li><li><strong>选择RL库和算法</strong>–RL _蔻驰、张量力、稳定基线、RL _蔻驰准则</li><li><strong>测试代理的性能</strong></li><li><strong>准备发布</strong>–自述文件、需求、可读代码、可视化</li></ul>
</div>


<p>让我们跳进来。</p>


<a class="button continous-post blue-filled" href="/web/20220928192845/https://neptune.ai/blog/how-to-structure-organize-track-and-manage-reinforcement-learning-rl-projects" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>