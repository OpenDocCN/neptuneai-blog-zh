<html>
<head>
<title>Best Tools For NLP Projects That Every Data Scientist and ML Engineer Should Try </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>每个数据科学家和ML工程师都应该尝试的NLP项目的最佳工具</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/best-tools-for-nlp-projects#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/best-tools-for-nlp-projects#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p><a href="https://web.archive.org/web/20221206133311/https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32" target="_blank" rel="noreferrer noopener nofollow">自然语言处理</a> (NLP)是人工智能极其重要的子集。在智能设备使用(也称为人机通信)的增加、自然语言处理在医疗保健领域的进步以及基于云的解决方案的采用等因素的推动下，自然语言处理在企业中得到广泛采用。但是NLP到底是什么，为什么它很重要？</p>



<p>NLP处于语言学、计算机科学和人工智能的交汇点。NLP的应用程序处理和分析大量自然语言数据——人类所说的所有语言都是自然语言，无论该语言是英语、法语还是普通话——以便以类似人类的方式模仿人类之间的交互。一个好的NLP系统能够理解文档的内容，包括其中的细微差别。</p>







<h2 id="h-why-is-nlp-so-important">为什么NLP如此重要？</h2>



<p>我们比以往任何时候都更依赖于机器，这在很大程度上要归功于它们比我们更准确的能力，以及让我们更高效的能力。他们不会累。他们没有抱怨。他们从不感到无聊。然而，对于NLP任务，有一个主要的障碍…</p>



<p>对于人类来说，掌握一门语言相对简单，但对于机器来说，掌握自然语言是一个很大的挑战。自然语言的创造性和语言的模糊性使得自然语言处理成为一个高要求的领域。解决问题意味着，对于我们的问题，我们提出了一个解决语言创造性和模糊性问题的解决方案，因此，我们向被视为非结构化的数据添加了结构(即，文本没有模式，不像商店中的交易历史记录)。</p>



<h2 id="h-different-types-of-nlp-tasks">不同类型的NLP任务</h2>



<p>NLP可以执行许多不同的任务，但是有一组基本任务在各种NLP项目中经常重复出现。由于它们的重复性，这些任务比其他NLP任务得到了更深入的研究。这些基本的NLP任务是:</p>



<h3>语言建模</h3>



<p>语言建模(LM)是给任何单词序列分配一个概率。本质上，在语言建模任务中，我们试图预测序列中出现的下一个单词，给定之前出现的单词的历史。LM在NLP的各种应用中至关重要，是机器能够理解定性信息的原因。语言建模的一些应用包括:语音识别、光学字符识别、手写识别、机器翻译和拼写纠正。</p>



<h3>文本分类</h3>



<p>文本分类根据文本的内容将预定义的类别分配给文本。到目前为止，文本分类是NLP最受欢迎的应用，用于构建各种工具，如垃圾邮件检测器和情感分析机器人。</p>







<h3>信息提取</h3>



<p>信息抽取(IE)是从非结构化和/或半结构化的文本文档中自动抽取相关信息。这些类型的文档包括电子邮件中的日历事件，或社交媒体上帖子中提到的人名。</p>



<h3>信息检索</h3>



<p>每个在线的人都与某种信息检索(IR)系统互动，例如谷歌搜索。信息检索的任务是从大量的文档集合中找到与用户查询相关的文档。</p>



<h3>对话代理</h3>



<p>对话式智能体属于对话式人工智能。对话式人工智能包括建立对话系统，模仿人类在对话方面的互动。对话式人工智能的流行例子包括Alexa、Siri、Google Home和面向Windows爱好者的Cortana。像聊天机器人这样的技术也是由对话代理驱动的，并且在企业公司中越来越受欢迎。</p>



<h3>文本摘要</h3>



<p>自动摘要是通过计算缩短一组数据的过程，以创建代表原始内容中最重要或最相关信息的子集[来源:<a href="https://web.archive.org/web/20221206133311/https://en.wikipedia.org/wiki/Automatic_summarization" target="_blank" rel="noreferrer noopener nofollow">维基百科</a> ]。</p>



<h3>问题回答</h3>



<p>问题回答的任务是构建能够自动回答人类用自然语言提出的问题的系统。</p>



<h3>机器翻译</h3>



<p>机器翻译(MT)是计算语言学的一个分支，涉及将一段文本从一种语言转换成另一种语言。这种类型的一个流行应用是谷歌翻译。</p>



<h3>主题建模</h3>



<p>主题建模是一种无监督的机器学习技术，它揭示了大量文档集合的主题结构。NLP的这种应用是一种非常常见的工具，用于各种领域，如文学和生物信息学。</p>



<p>虽然这些任务各不相同，但是牢牢掌握这些任务足以让任何有抱负的NLP实践者具备构建各种NLP应用程序的良好基础。掌握这些应用程序的一部分包括学习在解决问题时可以用来提高生产力的技术。</p>











<p>有各种开源工具可以在非结构化文本(或其他形式的自然语言)中找到有价值的见解，并解决各种问题。下面提供的框架列表绝不是详尽的，但是它们是一个很好的开始，使得自然语言处理对于企业或者任何希望在他们的项目中使用NLP的人来说都是可行的。事不宜迟，这里列出了自然语言处理(NLP)项目中最常用的框架。</p>



<h3>我是NLTK</h3>



<p>自然语言工具包(NLTK)是构建Python程序来处理和分析人类语言数据的领先平台之一。<a href="https://web.archive.org/web/20221206133311/https://www.nltk.org/" target="_blank" rel="noreferrer noopener nofollow"> NLTK文档</a>声明“<em>”提供了50多个语料库和词汇资源(如WordNet)的易用接口，以及一套用于分类、标记化、词干化、标记、解析和语义推理的文本处理库、工业级NLP库的包装器和一个活跃的讨论论坛。</em>'</p>



<p>像编程世界中的大多数事情一样，掌握NLTK需要一些时间。幸运的是，有很多资源可以帮助你掌握这个框架，比如NLTK的创造者自己写的《用Python进行自然语言处理<a href="https://web.archive.org/web/20221206133311/http://www.nltk.org/book/" target="_blank" rel="noreferrer noopener nofollow"><em/></a>》一书——这是一种非常实用的自然语言处理任务编程方法。</p>



<p>可以用NLTK执行的一些任务的例子包括标记化、标记、词干化、词汇化、解析、分类等等。看看下面来自<a href="https://web.archive.org/web/20221206133311/https://www.nltk.org/" target="_blank" rel="noreferrer noopener nofollow"> NLTK文档</a>的代码片段。</p>



<pre class="hljs">
<span class="hljs-keyword">import</span> nltk

sentence = <span class="hljs-string">"""At eight o'clock on Thursday morning Arthur didn't feel very good."""</span>
tokens = nltk.word_tokenize(sentence)
print(tokens)

&gt;&gt;&gt;&gt; [<span class="hljs-string">'At'</span>, <span class="hljs-string">'eight'</span>, <span class="hljs-string">"o'clock"</span>, <span class="hljs-string">'on'</span>, <span class="hljs-string">'Thursday'</span>, <span class="hljs-string">'morning'</span>, <span class="hljs-string">'Arthur'</span>, <span class="hljs-string">'did'</span>, <span class="hljs-string">"n't"</span>, <span class="hljs-string">'feel'</span>, <span class="hljs-string">'very'</span>, <span class="hljs-string">'good'</span>, <span class="hljs-string">'.'</span>]


tagged = nltk.pos_tag(tokens)
print(tagged[<span class="hljs-number">0</span>:<span class="hljs-number">6</span>])

&gt;&gt;&gt;&gt; [(<span class="hljs-string">'At'</span>, <span class="hljs-string">'IN'</span>), (<span class="hljs-string">'eight'</span>, <span class="hljs-string">'CD'</span>), (<span class="hljs-string">"o'clock"</span>, <span class="hljs-string">'JJ'</span>), (<span class="hljs-string">'on'</span>, <span class="hljs-string">'IN'</span>),
(<span class="hljs-string">'Thursday'</span>, <span class="hljs-string">'NNP'</span>), (<span class="hljs-string">'morning'</span>, <span class="hljs-string">'NN'</span>)]


entities = nltk.chunk.ne_chunk(tagged)
print(entities)

&gt;&gt;&gt;&gt; Tree(<span class="hljs-string">'S'</span>, [(<span class="hljs-string">'At'</span>, <span class="hljs-string">'IN'</span>), (<span class="hljs-string">'eight'</span>, <span class="hljs-string">'CD'</span>), (<span class="hljs-string">"o'clock"</span>, <span class="hljs-string">'JJ'</span>),
           (<span class="hljs-string">'on'</span>, <span class="hljs-string">'IN'</span>), (<span class="hljs-string">'Thursday'</span>, <span class="hljs-string">'NNP'</span>), (<span class="hljs-string">'morning'</span>, <span class="hljs-string">'NN'</span>),
       Tree(<span class="hljs-string">'PERSON'</span>, [(<span class="hljs-string">'Arthur'</span>, <span class="hljs-string">'NNP'</span>)]),
           (<span class="hljs-string">'did'</span>, <span class="hljs-string">'VBD'</span>), (<span class="hljs-string">"n't"</span>, <span class="hljs-string">'RB'</span>), (<span class="hljs-string">'feel'</span>, <span class="hljs-string">'VB'</span>),
           (<span class="hljs-string">'very'</span>, <span class="hljs-string">'RB'</span>), (<span class="hljs-string">'good'</span>, <span class="hljs-string">'JJ'</span>), (<span class="hljs-string">'.'</span>, <span class="hljs-string">'.'</span>)])


<span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> treebank

t = treebank.parsed_sents(<span class="hljs-string">'wsj_0001.mrg'</span>)[<span class="hljs-number">0</span>]
t.draw()</pre>







<h3>空间</h3>



<p>SpaCy的首次发布是在2015年2月，使其成为Python自然语言处理应用程序的最新开源框架之一。与2001年创建的NLTK相比，SpaCy的创建者有足够的时间来学习NLTK并了解它的不足之处。与NTLK相比，最显著的改进之一是性能增强，因为SpaCy使用了一些最新最好的算法。</p>



<p>此外，SpaCy有很好的文档记录，并被设计为支持大量数据。它还包括一系列预训练的自然语言处理模型，这使得学习、教学和使用SpaCy进行自然语言处理变得更加容易。</p>



<p><strong>注意</strong> : <em>如果你希望将深度学习算法应用于你的非结构化数据，SpaCy可能是你要使用的库——提取任务也是如此。</em></p>



<p>下面是SpaCy文档中SpaCy功能的一个例子。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> spacy


nlp = spacy.load(<span class="hljs-string">"en_core_web_sm"</span>)


text = (<span class="hljs-string">"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.
I can tell you very senior CEOs of major American car companies would
shake my hand and turn away because 'I wasn't worth talking to',
said Thrun, in an interview with Recode earlier this week."</span>)
doc = nlp(text)


print(<span class="hljs-string">"Noun phrases:"</span>, [chunk.text <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> doc.noun_chunks])
print(<span class="hljs-string">"Verbs:"</span>, [token.lemma_ <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> doc <span class="hljs-keyword">if</span> token.pos_ == <span class="hljs-string">"VERB"</span>])


<span class="hljs-keyword">for</span> entity <span class="hljs-keyword">in</span> doc.ents:
    print(entity.text, entity.label_)

&gt;&gt;&gt;&gt; Noun phrases: [<span class="hljs-string">'Sebastian Thrun'</span>, <span class="hljs-string">'self-driving cars'</span>, <span class="hljs-string">'Google'</span>, <span class="hljs-string">'few people'</span>, <span class="hljs-string">'the company'</span>, <span class="hljs-string">'him'</span>, <span class="hljs-string">'I'</span>, <span class="hljs-string">'you'</span>, <span class="hljs-string">'very senior CEOs'</span>, <span class="hljs-string">'major American car companies'</span>, <span class="hljs-string">'my hand'</span>, <span class="hljs-string">'I'</span>, <span class="hljs-string">'Thrun'</span>, <span class="hljs-string">'an interview'</span>, <span class="hljs-string">'Recode'</span>]
Verbs: [<span class="hljs-string">'start'</span>, <span class="hljs-string">'work'</span>, <span class="hljs-string">'drive'</span>, <span class="hljs-string">'take'</span>, <span class="hljs-string">'can'</span>, <span class="hljs-string">'tell'</span>, <span class="hljs-string">'would'</span>, <span class="hljs-string">'shake'</span>, <span class="hljs-string">'turn'</span>, <span class="hljs-string">'talk'</span>, <span class="hljs-string">'say'</span>]
Sebastian NORP
Google ORG
<span class="hljs-number">2007</span> DATE
American NORP
Recode ORG
earlier this week DATE</pre>



<h3>斯坦福·科伦普</h3>







<p>CoreNLP是一个非常受欢迎的用于自然语言处理任务的库，由斯坦福NLP社区构建——他们也积极维护该框架。与分别用Python或Cython编写的NLTK和SpaCy相反，CoreNLP是用Java编写的——这意味着你的计算机需要有JDK(但它有用于大多数编程语言的API)。</p>



<p>在CoreNLP主页上，开发者将CoreNLP描述为“<em>Java自然语言处理的一站式商店！CoreNLP使用户能够导出文本的语言注释，包括标记和句子边界、词性、命名实体、数值和时间值、依存和选区解析器、共指、情感、引用属性和关系。CoreNLP目前支持6种语言:阿拉伯语，中文，英语，法语，德语和西班牙语。</em></p>



<p>CoreNLP的主要优势之一是它的可伸缩性，这使它成为复杂任务的首选。另一个因素是，它在建造时就考虑到了速度——它已经过优化，速度极快。</p>



<h3>Gensim</h3>



<p>Gensim是一个专门的开源Python框架，用于以最有效、最轻松的方式将文档表示为语义向量。作者设计了Gensim来使用各种机器学习算法处理原始的、非结构化的纯文本——因此使用Gensim来处理主题建模等任务是一个好主意。此外，Gensim在识别文本相似性、索引文本和导航不同文档方面做得很好。</p>



<p>在<a href="https://web.archive.org/web/20221206133311/https://radimrehurek.com/gensim/intro.html" target="_blank" rel="noreferrer noopener nofollow">文档</a>中，作者明确指出Gensim是从零开始构建的，原因有三:</p>



<ul><li><strong>实用性</strong>–作为行业专家，我们专注于解决实际行业问题的久经考验的算法。更多关注工程，更少关注学术。</li><li><strong>内存独立性</strong>–不需要在任何时候将整个训练语料库完全驻留在RAM中。它可以使用数据流处理大型网络规模的语料库。</li><li><strong>性能</strong>–使用C、BLAS和内存映射对流行的向量空间算法进行了高度优化。</li></ul>



<p>下面是Gensim <a href="https://web.archive.org/web/20221206133311/https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py" target="_blank" rel="noreferrer noopener nofollow"> Word2Vec教程</a>文档页面上的一些代码示例。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> gensim.downloader <span class="hljs-keyword">as</span> api
wv = api.load(<span class="hljs-string">'word2vec-google-news-300'</span>)

<span class="hljs-keyword">for</span> index, word <span class="hljs-keyword">in</span> enumerate(wv.index2entity):
    <span class="hljs-keyword">if</span> index == <span class="hljs-number">10</span>:
        <span class="hljs-keyword">break</span>
    print(f<span class="hljs-string">"word #{index}/{len(wv.index2entity)} is {word}"</span>)

&gt;&gt;&gt;&gt; word 
     word 
     word 
     word 
     word 
     word 
     word 
     word 
     word 
     word 
</pre>



<pre class="hljs">pairs = [
    (<span class="hljs-string">'car'</span>, <span class="hljs-string">'minivan'</span>),   
    (<span class="hljs-string">'car'</span>, <span class="hljs-string">'bicycle'</span>),   
    (<span class="hljs-string">'car'</span>, <span class="hljs-string">'airplane'</span>),  
    (<span class="hljs-string">'car'</span>, <span class="hljs-string">'cereal'</span>),    
    (<span class="hljs-string">'car'</span>, <span class="hljs-string">'communism'</span>),
]
<span class="hljs-keyword">for</span> w1, w2 <span class="hljs-keyword">in</span> pairs:
    print(<span class="hljs-string">'%rt%rt%.2f'</span> % (w1, w2, wv.similarity(w1, w2)))

&gt;&gt;&gt;&gt; ‘car’    ‘minivan’    <span class="hljs-number">0.69</span>
     ‘car’    ‘bicycle’    <span class="hljs-number">0.54</span>
     ‘car’    ‘airplane’   <span class="hljs-number">0.42</span>
     ‘car’    ‘cereal’     <span class="hljs-number">0.14</span>
     ‘car’    ‘communism’  <span class="hljs-number">0.06</span>
</pre>



<h3>TensorFlow &amp; PyTorch</h3>



<p>尽管是两个非常不同的框架，我认为最好列出这两个框架，因为它们都被认为是深度学习的流行框架。Tensorflow是较早的一个，它是由谷歌的大脑团队开发的——他们也积极地将该框架用于研究和生产级别的项目。另一方面，Pytorch是一个基于torch库的开源库，主要由脸书的AI研究(FAIR)实验室开发。</p>



<p>关于Tensorflow或PyTorch的争论由来已久，这绝对超出了本文的范围。我给那些不确定学习哪一个的人的建议是学习你的组织使用的，或者你想为之工作的组织。如果他们还没有完全采用深度学习，那么我会说PyTorch有一个更容易的学习曲线。</p>



<p>下面你可以看到如何使用这两个框架建立一个LSTM模型。首先是tensor flow——完整的源代码，请访问<a href="https://web.archive.org/web/20221206133311/https://www.machinecurve.com/index.php/2021/01/07/build-an-lstm-model-with-tensorflow-and-keras/" target="_blank" rel="noreferrer noopener nofollow"> Christian Versloot机器曲线博客。</a></p>



<pre class="hljs">
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras.datasets <span class="hljs-keyword">import</span> imdb
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Embedding, Dense, LSTM
<span class="hljs-keyword">from</span> tensorflow.keras.losses <span class="hljs-keyword">import</span> BinaryCrossentropy
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences


additional_metrics = [<span class="hljs-string">'accuracy'</span>]
batch_size = <span class="hljs-number">128</span>
embedding_output_dims = <span class="hljs-number">15</span>
loss_function = BinaryCrossentropy()
max_sequence_length = <span class="hljs-number">300</span>
num_distinct_words = <span class="hljs-number">5000</span>
number_of_epochs = <span class="hljs-number">5</span>
optimizer = Adam()
validation_split = <span class="hljs-number">0.20</span>
verbosity_mode = <span class="hljs-number">1</span>


tf.compat.v1.disable_eager_execution()


(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_distinct_words)
print(x_train.shape)
print(x_test.shape)


padded_inputs = pad_sequences(x_train, maxlen=max_sequence_length, value = <span class="hljs-number">0.0</span>) 
padded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length, value = <span class="hljs-number">0.0</span>) 


model = Sequential()
model.add(Embedding(num_distinct_words, embedding_output_dims, input_length=max_sequence_length))
model.add(LSTM(<span class="hljs-number">10</span>))
model.add(Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>))


model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)


model.summary()


history = model.fit(padded_inputs, y_train, batch_size=batch_size, epochs=number_of_epochs, verbose=verbosity_mode, validation_split=validation_split)


test_results = model.evaluate(padded_inputs_test, y_test, verbose=<span class="hljs-keyword">False</span>)
print(f<span class="hljs-string">'Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%'</span>)

&gt;&gt;&gt;&gt; Test results - Loss: <span class="hljs-number">0.3655</span> - Accuracy: <span class="hljs-number">85.6880</span></pre>



<p>这是PyTorch中的一个LSTM——要获得完整的运行和源代码，请访问PyTorch文档中的<a href="https://web.archive.org/web/20221206133311/https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html" target="_blank" rel="noreferrer noopener nofollow">序列模型教程</a>。</p>



<pre class="hljs">
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim

torch.manual_seed(<span class="hljs-number">1</span>)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_sequence</span><span class="hljs-params">(seq, to_ix)</span>:</span>
    idxs = [to_ix[w] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> seq]
    <span class="hljs-keyword">return</span> torch.tensor(idxs, dtype=torch.long)
training_data = [
    
    
    (<span class="hljs-string">"The dog ate the apple"</span>.split(), [<span class="hljs-string">"DET"</span>, <span class="hljs-string">"NN"</span>, <span class="hljs-string">"V"</span>, <span class="hljs-string">"DET"</span>, <span class="hljs-string">"NN"</span>]),
    (<span class="hljs-string">"Everybody read that book"</span>.split(), [<span class="hljs-string">"NN"</span>, <span class="hljs-string">"V"</span>, <span class="hljs-string">"DET"</span>, <span class="hljs-string">"NN"</span>])
]
word_to_ix = {}

<span class="hljs-keyword">for</span> sent, tags <span class="hljs-keyword">in</span> training_data:
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent:
        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> word_to_ix:  
            word_to_ix[word] = len(word_to_ix)  
tag_to_ix = {<span class="hljs-string">"DET"</span>: <span class="hljs-number">0</span>, <span class="hljs-string">"NN"</span>: <span class="hljs-number">1</span>, <span class="hljs-string">"V"</span>: <span class="hljs-number">2</span>}  



EMBEDDING_DIM = <span class="hljs-number">6</span>
HIDDEN_DIM = <span class="hljs-number">6</span>


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LSTMTagger</span><span class="hljs-params">(nn.Module)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, embedding_dim, hidden_dim, vocab_size, tagset_size)</span>:</span>
        super(LSTMTagger, self).__init__()
        self.hidden_dim = hidden_dim

        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        
        
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

        
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, sentence)</span>:</span>
        embeds = self.word_embeddings(sentence)
        lstm_out, _ = self.lstm(embeds.view(len(sentence), <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>))
        tag_space = self.hidden2tag(lstm_out.view(len(sentence), <span class="hljs-number">-1</span>))
        tag_scores = F.log_softmax(tag_space, dim=<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> tag_scores


model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))
loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.1</span>)




<span class="hljs-keyword">with</span> torch.no_grad():
    inputs = prepare_sequence(training_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], word_to_ix)
    tag_scores = model(inputs)
    print(tag_scores)

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">300</span>):  
    <span class="hljs-keyword">for</span> sentence, tags <span class="hljs-keyword">in</span> training_data:
        
        
        model.zero_grad()

        
        
        sentence_in = prepare_sequence(sentence, word_to_ix)
        targets = prepare_sequence(tags, tag_to_ix)

        
        tag_scores = model(sentence_in)

        
        
        loss = loss_function(tag_scores, targets)
        loss.backward()
        optimizer.step()


<span class="hljs-keyword">with</span> torch.no_grad():
    inputs = prepare_sequence(training_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], word_to_ix)
    tag_scores = model(inputs)

    
    
    
    
    
    
    print(tag_scores)
&gt;&gt;&gt;&gt; tensor([[<span class="hljs-number">-1.1389</span>, <span class="hljs-number">-1.2024</span>, <span class="hljs-number">-0.9693</span>],
        [<span class="hljs-number">-1.1065</span>, <span class="hljs-number">-1.2200</span>, <span class="hljs-number">-0.9834</span>],
        [<span class="hljs-number">-1.1286</span>, <span class="hljs-number">-1.2093</span>, <span class="hljs-number">-0.9726</span>],
        [<span class="hljs-number">-1.1190</span>, <span class="hljs-number">-1.1960</span>, <span class="hljs-number">-0.9916</span>],
        [<span class="hljs-number">-1.0137</span>, <span class="hljs-number">-1.2642</span>, <span class="hljs-number">-1.0366</span>]])
tensor([[<span class="hljs-number">-0.0462</span>, <span class="hljs-number">-4.0106</span>, <span class="hljs-number">-3.6096</span>],
        [<span class="hljs-number">-4.8205</span>, <span class="hljs-number">-0.0286</span>, <span class="hljs-number">-3.9045</span>],
        [<span class="hljs-number">-3.7876</span>, <span class="hljs-number">-4.1355</span>, <span class="hljs-number">-0.0394</span>],
        [<span class="hljs-number">-0.0185</span>, <span class="hljs-number">-4.7874</span>, <span class="hljs-number">-4.6013</span>],
        [<span class="hljs-number">-5.7881</span>, <span class="hljs-number">-0.0186</span>, <span class="hljs-number">-4.1778</span>]])</pre>



<section id="blog-intext-cta-block_60794e9f9dfaa" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>了解如何使用<a href="https://web.archive.org/web/20221206133311/https://docs.neptune.ai/essentials/integrations/deep-learning-frameworks/tensorflow-keras" target="_blank" rel="noopener"> TensorFlow + Neptune </a>集成或<a href="https://web.archive.org/web/20221206133311/https://docs.neptune.ai/essentials/integrations/deep-learning-frameworks/pytorch" target="_blank" rel="noopener"> PyTorch + Neptune </a>集成来跟踪模型训练元数据。</p>
    
    </section>



<h3>拥抱脸</h3>



<p>近年来，变压器模型极大地推进了NLP。它本质上是一种新颖的架构，在解决序列间任务时考虑了长期依赖性。在大多数情况下，NLP模型非常大，需要大量的计算才能得到一个像样的工作模型。拥抱脸Python框架为各种NLP任务提供了对大量预训练模型的访问。甚至像亚马逊，谷歌人工智能和脸书人工智能利用这个包。</p>



<p>下面是一个用于情感分析的拥抱脸管道的例子——你可以在<a href="https://web.archive.org/web/20221206133311/https://huggingface.co/transformers/quicktour.html">文档</a>中读到更多相关信息。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline


clf = pipeline(<span class="hljs-string">"sentiment-analysis"</span>)


data = [<span class="hljs-string">"I am happy to be reading this article"</span>,
        <span class="hljs-string">"I am not happy to read this article"</span>,
        <span class="hljs-string">"This is a really informative article"</span>,
        <span class="hljs-string">"Thank you for reading"</span>]


results = clf(data)

<span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
  print(f<span class="hljs-string">"label: {result['label']}, score: {round(result['score'], 4)}"</span>)

&gt;&gt;&gt;&gt; label: POSITIVE, score: <span class="hljs-number">0.9999</span>
     label: NEGATIVE, score: <span class="hljs-number">0.9989</span>
     label: POSITIVE, score: <span class="hljs-number">0.9998</span>
     label: POSITIVE, score: <span class="hljs-number">0.9998</span>
</pre>







<p>与传统的机器学习项目类似，NLP项目是高度迭代的。在项目生命周期的旅程中，迭代特定的部分直到NLP系统满足一些期望的内在性能水平是很常见的。内在评估更关注中间目标，例如NLP组件如何在定义的子任务上执行。</p>



<p>一旦项目被部署到生产环境中，它还远远没有完成。现实世界中的交互、反馈和变化会改变当前NLP组件的需求，这意味着循环回到项目生命周期中的早期步骤来更新(或改进)它。</p>



<p>为了让你的NLP项目上线，有各种各样的工具不仅可以帮助开发人员，也可以帮助那些不太懂技术的AI团队成员。这些工具包括消息应用、实验管理工具、<a href="/web/20221206133311/https://neptune.ai/experiment-tracking" target="_blank" rel="noreferrer noopener">跟踪工具</a>等等。这里有一个有价值的项目管理工具的列表，你可能想用在你的NLP项目中:</p>



<h3>海王星AI</h3>







<p>海王星。AI是一个轻量级的实验跟踪和模型注册。它极大地促进了合作，并且可以跟踪你所有的实验。它非常灵活，可以很好地与许多框架集成(包括上面提到的那些)。使用该工具，您可以记录、存储、显示、组织和查询您的所有机器学习操作(MLOps)元数据。</p>



<p><strong>海王AI提供了什么？</strong></p>



<ul><li>记录和显示机器学习模型的元数据</li><li>组织实验和模型训练运行的中心</li><li>轻松比较实验和机器学习模型</li><li>观看您的机器学习实验实时运行</li><li>可重复的实验和模型训练运行</li><li>与队友分享可视化效果的特殊链接</li><li>以编程方式查询实验和模型训练元数据</li><li>从任何地方运行您的代码(笔记本电脑、云基础设施或集群)</li></ul>



<h3>MLFlow</h3>







<p>MLFlow的一个很酷的地方是，您可以将它用于任何机器学习库和任何编程语言，因为所有可用的函数都是通过REST API和CLI访问的。除了从技术角度来看的可访问性之外，MLflow文档编写得非常好，易于理解。<a href="https://web.archive.org/web/20221206133311/https://mlflow.org/docs/latest/index.html" target="_blank" rel="noreferrer noopener nofollow">文档</a>声明:</p>



<p>“MLflow是一个管理端到端机器学习生命周期的开源平台。它处理四个主要功能:</p>



<ul><li>跟踪实验以记录和比较参数和结果(<a href="https://web.archive.org/web/20221206133311/https://www.mlflow.org/docs/latest/tracking.html#tracking" target="_blank" rel="noreferrer noopener nofollow"> MLflow Tracking </a>)。</li><li>以可重用、可复制的形式包装ML代码，以便与其他数据科学家共享或转移到生产中(<a href="https://web.archive.org/web/20221206133311/https://www.mlflow.org/docs/latest/projects.html#projects" target="_blank" rel="noreferrer noopener nofollow"> MLflow项目</a>)。</li><li>从各种ML库中管理和部署模型到各种模型服务和推理平台(<a href="https://web.archive.org/web/20221206133311/https://www.mlflow.org/docs/latest/models.html#models" target="_blank" rel="noreferrer noopener nofollow"> MLflow Models </a>)。</li><li>提供一个中央模型库来协作管理MLflow模型的整个生命周期，包括模型版本化、阶段转换和注释(<a href="https://web.archive.org/web/20221206133311/https://www.mlflow.org/docs/latest/model-registry.html#registry" target="_blank" rel="noreferrer noopener nofollow"> MLflow模型注册中心</a>)。"</li></ul>



<p>这基本上概括了MLflow提供的一切—我告诉过你他们的文档写得很好。</p>



<p>另请阅读:<a href="/web/20221206133311/https://neptune.ai/blog/the-best-mlflow-alternatives" target="_blank" rel="noreferrer noopener">最佳物流替代方案(2021年更新)</a></p>



<h3>开源代码库</h3>







<p>Github是面向开发者的社交网站。它为全球超过5600万开发者提供互联网托管以及使用Git的版本控制。Github使协作变得简单而不费力，其特性允许代码托管和审查、全面的项目管理和方便的软件构建。通常，项目经理和开发人员利用Github在单一环境中协调、跟踪和更新他们的工作。</p>



<p>平台有很多特性，每个特性都是多方面的。总的来说，Github提供的功能分为七类:</p>



<ul><li>协作编码</li><li>自动化和持续集成/持续开发(CI/CD)</li><li>安全性</li><li>客户端应用程序</li><li>项目管理</li><li>团队管理</li><li>社区</li></ul>



<h3>彗星ML</h3>







<p>彗星的主页。ML的网站上写着“<em>彗星。ML允许数据科学家和开发人员轻松地监控、比较和优化他们的机器学习模型</em>——没有比这更清楚的了。其中最受欢迎的功能是他们的现场实验图表；彗星。ML为您提供了一个引人注目的仪表板，它将您的ML实验代码及其结果绑定在一起，还提供了一些功能，帮助从业者通过调整超参数来优化他们的模型。</p>



<p>NLP项目在许多方面类似于传统的软件应用程序，但也有很大的不同。这是相似的，因为两个应用程序都是在受控的开发环境中制作的。这是不同的，因为NLP和机器学习项目通常还包括来自一个永无止境的来源，称为现实世界的数据，因此我们用来构建应用程序的数据也必须被跟踪。</p>



<p>这就是彗星。ML进来了。彗星。ML允许用户:</p>



<ul><li>追踪数据集，</li><li>跟踪代码的更改，</li><li>跟踪实验历史和机器学习见解。</li></ul>



<p>另外彗星。ML为从业者更快地构建更好的模型提供了有价值的见解和数据，同时也提高了生产率、协作性和可解释性。</p>



<p>另请阅读:<a href="/web/20221206133311/https://neptune.ai/blog/the-best-comet-ml-alternatives" target="_blank" rel="noreferrer noopener">最佳Comet.ml替代方案</a></p>



<h3>松弛的</h3>







<p>想想WhatsApp，Facebook Messenger，或者iMessage……现在给它吃类固醇吧。那是懈怠。Slack是一个面向团队和整个工作组织的消息应用程序。它可以跨各种不同的设备和平台使用。该应用程序具有许多强大的功能，可以让个人在各种聊天室以及一对一的房间中进行交流——自从全球疫情大受欢迎以来，Slack已经成为企业和团队的一个宝贵工具。</p>



<p>为了让团队的所有成员保持一致，有各种功能，例如:</p>



<ul><li><strong>频道</strong>——对话的中心空间。各种主题可能有多个渠道(如资源、支持、项目1)。</li><li><strong>Slack Connect</strong>–与来自不同公司的团队合作(对于参与B2B服务的公司来说非常好)</li><li><strong>语音和视频通话</strong></li></ul>



<p>Slack有许多应用程序和集成，可以全面提高生产率。我个人最喜欢的是Google Drive集成，它允许用户共享和管理对文件的访问，以及接收更新等，所有这些都在Slack中完成。还集成了其他常见应用程序，如OneDrive、Zoom和Outlook。</p>



<h3>吉拉</h3>







<p>吉拉是由Atlassian开发的，用于专有问题跟踪，使它成为团队以灵活和自动化的方式规划项目、跟踪项目和发布产品或软件的伟大工具。这个工具非常适合敏捷团队，因为它包含了项目管理。用户可以自由地管理他们的项目，给团队成员分配任务(包括给程序员分配bug)，创建里程碑，计划有指定期限的任务。</p>



<p>吉拉的一些功能包括(在<a href="https://web.archive.org/web/20221206133311/https://www.atlassian.com/software/jira/features" target="_blank" rel="noreferrer noopener nofollow">吉拉功能</a>页面上阅读关于这些功能的更多信息):</p>



<ul><li>Scrum板</li><li>看板板</li><li>路线图</li><li>敏捷报告</li></ul>



<p>吉拉很受欢迎，是NLP项目非常合适的解决方案，因为它促进了协作以及简化、组织和结构化工作流。</p>



<h2 id="h-final-note">最后一个音符</h2>



<p>数据科学家、人工智能团队和企业可以使用大量优秀的工具来简化NLP项目。</p>



<p>重要的是，您找到了最适合您需求的工具，并且集成了让您的项目顺利完成所需的功能。</p>



<p>感谢您的阅读，祝您的项目好运！</p>
        </div>
        
    </div>    
</body>
</html>