<html>
<head>
<title>Implementing the Macro F1 Score in Keras: Do’s and Don'ts </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>在Keras中实现宏观F1分数:注意事项</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>作为<a href="https://web.archive.org/web/20230224202511/https://www.tensorflow.org/guide/effective_tf2" target="_blank" rel="noreferrer noopener nofollow"> TensorFlow 2.0 </a>生态系统的一部分，<a href="https://web.archive.org/web/20230224202511/https://keras.io/" target="_blank" rel="noreferrer noopener nofollow"> Keras </a>是用于训练和评估神经网络模型的最强大、但易于使用的深度学习框架之一。</p>



<p>当我们<a href="https://web.archive.org/web/20230224202511/https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/" target="_blank" rel="noreferrer noopener nofollow">构建神经网络模型</a>时，我们遵循与任何其他机器学习模型相同的模型生命周期步骤:</p>



<ul>
<li>用超参数构造和编译网络，</li>



<li>适合网络，</li>



<li>评估网络，</li>



<li>使用最佳调整的模型进行预测。</li>
</ul>



<p>具体来说，在网络评估步骤中，选择和定义适当的性能指标至关重要——本质上是判断您的模型性能的函数，包括<a href="https://web.archive.org/web/20230224202511/https://peltarion.com/knowledge-center/documentation/evaluation-view/classification-loss-metrics/macro-f1-score" target="_blank" rel="noreferrer noopener nofollow">宏F1分数</a>。</p>



<h2 id="h-model-performance-evaluation-metrics-vs-loss-function">模型性能评估指标与损失函数</h2>



<p>预测模型的建立过程只不过是连续的反馈循环。我们构建一个初始模型，接收来自性能指标的反馈，调整模型以进行改进，并进行迭代，直到获得我们想要的预测结果。</p>



<p>数据科学家，尤其是机器学习/预测建模实践的新手，经常混淆<strong>性能指标</strong>的概念和<strong>损失函数</strong>的概念。为什么在训练过程中，我们试图最大化给定的评估指标，如准确性，而算法本身试图最小化完全不同的损失函数，如交叉熵？对我来说，这是一个完全有效的问题！</p>







<p>在我看来，答案有两部分:</p>



<ol>
<li>损失函数(如交叉熵)与评估度量(如准确性)相比，通常更容易优化，因为损失函数相对于模型参数是可微分的，而评估度量则不是；</li>



<li>评估指标主要依赖于我们试图解决的特定业务问题陈述，对于非技术利益相关者来说更容易理解。例如，当向C级主管展示我们的分类模型时，解释什么是熵是没有意义的，相反，我们要展示准确度或精确度。</li>
</ol>



<p>这两点结合起来解释了为什么损失函数和性能度量通常在相反的方向上被优化。损失函数最小化，性能指标最大化。</p>



<p>尽管如此，我仍然认为我们试图优化的损失函数应该与我们最关心的评估指标相对应。你能想出一个损失函数等于性能指标的场景吗？回归模型的某些指标，如MSE(均方误差),既可以作为损失函数，也可以作为性能指标！</p>



<h2 id="h-performance-metrics-for-imbalanced-classification-problems">不平衡分类问题的性能度量</h2>



<p>对于分类问题，最基本的衡量标准是准确性——正确预测与数据中样本总数的比率。开发预测模型是为了实现高准确性，就好像它是判断分类模型性能的最终权威。</p>



<p>毫无疑问，对于具有平衡类分布的数据集(在二进制分类中大约为50%)，准确性是一个有效的度量。然而，当我们的数据集变得不平衡时，这是大多数现实世界业务问题的情况，准确性无法提供全貌。更糟糕的是，它可能会产生误导。</p>



<p><strong>高精度并不表示少数类</strong>的预测能力高，少数类最有可能是感兴趣的类。如果这个概念听起来很陌生，你可以在关于<a href="https://web.archive.org/web/20230224202511/https://en.wikipedia.org/wiki/Accuracy_paradox" target="_blank" rel="noreferrer noopener nofollow">准确性悖论</a>和<a href="https://web.archive.org/web/20230224202511/http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf" target="_blank" rel="noreferrer noopener nofollow">精确回忆曲线</a>的论文中找到很好的解释。</p>



<p>现在，不平衡数据集的理想性能指标是什么？由于正确识别少数类通常是我们的目标，<a href="https://web.archive.org/web/20230224202511/https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics" target="_blank" rel="noreferrer noopener nofollow">召回/灵敏度、精确度、F测量分数</a>将是有用的，其中:</p>





<p id="separator-block_7c6f5dbf80337c8b8c110f736b7afc23" class="block-separator block-separator--5">Keras metrics</p>





<p id="separator-block_7c6f5dbf80337c8b8c110f736b7afc23" class="block-separator block-separator--5">清楚地了解了评估指标、它们与损失函数的区别以及哪些指标可用于不平衡数据集后，让我们简要回顾一下Keras中的指标规范。对于Keras中可用的指标，最简单的方法是在<strong> model.compile() </strong>方法中指定<strong> "metrics" </strong>参数:</p>





<h2 id="h-keras-metrics">自<a href="https://web.archive.org/web/20230224202511/https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes" target="_blank" rel="noreferrer noopener nofollow"> Keras 2.0 </a>以来，传统评估指标——F分数、精确度和召回率——已经从即用列表中删除。用户必须自己定义这些指标。因此，作为处理神经网络中不平衡数据集的基础，我们将重点关注在Keras中实现F1分数度量，并讨论您应该做什么，以及不应该做什么。</h2>



<p>用海王星跟踪神经网络模型实验</p>



<pre class="hljs"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> metrics
model.compile(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>,
metrics=[metrics.categorical_accuracy])
</pre>



<p>在模型训练过程中，许多数据科学家(包括我自己)从excel电子表格或包含日志信息的文本文件开始，来跟踪我们的实验。这样我们可以看到什么有效，什么无效。这种方法没有错，尤其是考虑到它对我们繁琐的模型构建是多么的方便。然而，问题是这些笔记的结构并不有序。因此，当我们几年后试图回到它们时，我们不知道它们是什么意思。</p>







<h2 id="h-neural-network-model-experiment-tracking-with-neptune">幸运的是，海王星来救援。它跟踪并记录了我们模型训练过程中的几乎所有内容，从超参数规范到最佳模型保存，再到结果图等等。使用Neptune跟踪的<a href="/web/20230224202511/https://neptune.ai/experiment-tracking" target="_blank" rel="noreferrer noopener">实验的酷之处在于，它会自动生成性能图表，用于比较不同的运行，并选择最佳的运行。这是与您的团队共享模型和结果的一个很好的方式。</a></h2>



<p>关于如何配置你的海王星环境和设置你的实验的更详细的解释，请查看<a href="https://web.archive.org/web/20230224202511/https://docs.neptune.ai/getting-started/installation" target="_blank" rel="noreferrer noopener nofollow">这个完整的指南</a>。它非常简单，所以我没有必要在这里介绍海王星初始化。</p>



<p>我将演示如何在Keras F1指标实现过程中利用Neptune，并向您展示模型训练过程变得多么简单和直观。</p>







<p>你兴奋吗？我们开始吧！</p>



<p>创造海王星实验</p>



<p>首先，我们需要导入所有的包和函数:</p>



<h3>现在，让我们在Neptune中专门为这个练习创建一个项目:</h3>



<p>接下来，我们将创建一个连接到我们的<strong> KerasMetricNeptune </strong>项目的Neptune实验，以便我们可以在Neptune上记录和监控模型训练信息:</p>



<pre class="hljs">
<span class="hljs-keyword">import</span> neptune as neptune.new

<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> sample, seed
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict

<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split, GridSearchCV, KFold, StratifiedKFold
<span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score, make_scorer, confusion_matrix, accuracy_score, precision_score, recall_score, precision_recall_curve


<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Sequential, Model
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, Dropout, Reshape, Activation
<span class="hljs-keyword">from</span> tensorflow.keras.callbacks <span class="hljs-keyword">import</span> Callback, EarlyStopping, ModelCheckpoint

pd.options.display.max_columns = <span class="hljs-number">100</span>
np.set_printoptions(suppress=<span class="hljs-keyword">True</span>)

os.chdir(<span class="hljs-string">'PATH_TO_YOUR_WORK_DIRECTORY'</span>)</pre>



<p>这里有两点需要注意:</p>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20230224202511im_/https://neptune.ai/wp-content/uploads/1.Neptune_CreateProject.mp4"/></figure>



<p><strong> neptune.init() </strong>中的<em> api_token </em> arg取您在配置步骤中生成的Neptune API</p>



<pre class="hljs">
myProject = <span class="hljs-string">"YourUserName/YourProjectName"</span>
project = neptune.init(api_token=os.getenv(<span class="hljs-string">'NEPTUNE_API_TOKEN'</span>),
                       project=myProject)
project.stop()



npt_exp = neptune.init(
        api_token=os.getenv(<span class="hljs-string">'NEPTUNE_API_TOKEN'</span>),
        project=myProject,
        name=<span class="hljs-string">'step-by-step-implement-fscores'</span>,
        tags=[<span class="hljs-string">'keras'</span>, <span class="hljs-string">'classification'</span>, <span class="hljs-string">'macro f-scores'</span>,<span class="hljs-string">'neptune'</span>])
</pre>



<p><strong> neptune.init() </strong>中的<em>标签</em> arg是可选的，但是最好为给定的项目指定标签，以便于共享和跟踪。</p>



<ul>
<li>随着Neptune项目—<strong>KerasMetricNeptune</strong>在我的演示中——以及成功创建的初始实验，我们可以继续进行建模部分。</li>



<li>第一次尝试:自定义F1得分指标</li>
</ul>



<p>根据<a href="https://web.archive.org/web/20230224202511/https://keras.io/api/metrics/#custom-metrics" target="_blank" rel="noreferrer noopener nofollow"> Keras文档</a>，用户可以在神经网络编译步骤传递自定义指标。听起来很容易，不是吗？我实现了一个度量函数<strong> <em> custom_f1 </em> </strong>。它接受真实结果和预测结果作为参数:</p>



<h3>数据集:信用卡欺诈检测</h3>



<p>为了展示这个自定义指标函数是如何工作的，我将使用信用卡欺诈检测数据集作为例子。这是最受欢迎的不平衡数据集之一(更多细节<a href="https://web.archive.org/web/20230224202511/https://www.kaggle.com/mlg-ulb/creditcardfraud" target="_blank" rel="noreferrer noopener nofollow">在这里</a>)。</p>



<pre class="hljs">

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">custom_f1</span><span class="hljs-params">(y_true, y_pred)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">recall_m</span><span class="hljs-params">(y_true, y_pred)</span>:</span>
        TP = K.sum(K.round(K.clip(y_true * y_pred, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)))
        Positives = K.sum(K.round(K.clip(y_true, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)))

        recall = TP / (Positives+K.epsilon())
        <span class="hljs-keyword">return</span> recall


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">precision_m</span><span class="hljs-params">(y_true, y_pred)</span>:</span>
        TP = K.sum(K.round(K.clip(y_true * y_pred, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)))
        Pred_Positives = K.sum(K.round(K.clip(y_pred, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)))

        precision = TP / (Pred_Positives+K.epsilon())
        <span class="hljs-keyword">return</span> precision

    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)

    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span>*((precision*recall)/(precision+recall+K.epsilon()))
</pre>



<h3>基本的探索性数据分析表明，0级(99.83%)和1级(0.17%)存在极端的阶级不平衡:</h3>



<p>出于演示目的，我将在我的神经网络模型中包括所有输入特征，并将20%的数据保存为保留测试集:</p>



<p>使用神经网络的模型结构</p>



<pre class="hljs">
credit_dat = pd.read_csv(<span class="hljs-string">'creditcard.csv'</span>)

counts = credit_dat.Class.value_counts()
class0, class1 = round(counts[<span class="hljs-number">0</span>]/sum(counts)*<span class="hljs-number">100</span>, <span class="hljs-number">2</span>), round(counts[<span class="hljs-number">1</span>]/sum(counts)*<span class="hljs-number">100</span>, <span class="hljs-number">2</span>)
print(f<span class="hljs-string">'Class 0 = {class0}% and Class 1 = {class1}%'</span>)


sns.set(style=<span class="hljs-string">"whitegrid"</span>)
ax = sns.countplot(x=<span class="hljs-string">"Class"</span>, data=credit_dat)
<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> ax.patches:
    ax.annotate(<span class="hljs-string">'{:.2f}%'</span>.format(p.get_height()/len(credit_dat)*<span class="hljs-number">100</span>), (p.get_x()+<span class="hljs-number">0.15</span>, p.get_height()+<span class="hljs-number">1000</span>))
ax.set(ylabel=<span class="hljs-string">'Count'</span>,
       title=<span class="hljs-string">'Credit Card Fraud Class Distribution'</span>)


npt_exp[<span class="hljs-string">'Distribution'</span>].upload(neptune.types.File.as_image(ax.get_figure()))

dat = credit_dat
</pre>





<p>在对数据进行预处理之后，我们现在可以进入建模部分。在这篇文章中，我将构建一个具有两个隐藏层的神经网络用于二进制分类(使用sigmoid作为输出层的激活函数):</p>



<pre class="hljs">
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">myformat</span><span class="hljs-params">(value, decimal=<span class="hljs-number">4</span>)</span>:</span>
    <span class="hljs-keyword">return</span> str(round(value, decimal))



<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Pre_proc</span><span class="hljs-params">(dat, current_test_size=<span class="hljs-number">0.2</span>, current_seed=<span class="hljs-number">42</span>)</span>:</span>
    x_train, x_test, y_train, y_test = train_test_split(dat.iloc[:, <span class="hljs-number">0</span>:dat.shape[<span class="hljs-number">1</span>]<span class="hljs-number">-1</span>],
                                                        dat[<span class="hljs-string">'Class'</span>],
                                                        test_size=current_test_size,
                                                        random_state=current_seed)
    sc = StandardScaler()
    x_train = sc.fit_transform(x_train)
    x_test = sc.transform(x_test)

    y_train, y_test = np.array(y_train), np.array(y_test)
    <span class="hljs-keyword">return</span> x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test = Pre_proc(dat)</pre>



<h3>使用自定义F1指标建模</h3>



<p>接下来，我们使用交叉验证(CV)来训练模型。由于构建准确的模型超出了本文的范围，我设置了一个5重CV，每个CV只有20个时期，以展示F1度量函数是如何工作的:</p>



<pre class="hljs">
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">runModel</span><span class="hljs-params">(x_tr, y_tr, x_val, y_val, epos=<span class="hljs-number">20</span>, my_batch_size=<span class="hljs-number">112</span>)</span>:</span>
    
    inp = Input(shape = (x_tr.shape[<span class="hljs-number">1</span>],))

    x = Dense(<span class="hljs-number">1024</span>, activation=<span class="hljs-string">'relu'</span>)(inp)
    x = Dropout(<span class="hljs-number">0.5</span>)(x)
    x = BatchNormalization()(x)
    x = Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">'relu'</span>)(x)
    x = Dropout(<span class="hljs-number">0.5</span>)(x)
    x = BatchNormalization()(x)

    out = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)(x)
    model = Model(inp, out)

    <span class="hljs-keyword">return</span> model
</pre>



<h3><strong>几个注意事项:</strong></h3>



<p>预定义函数<em> custom_f1 </em>在model.compile步骤中指定；</p>



<pre class="hljs">f1_cv, precision_cv, recall_cv = [], [], []

current_folds = <span class="hljs-number">5</span>
current_epochs = <span class="hljs-number">20</span>
current_batch_size = <span class="hljs-number">112</span>

kfold = StratifiedKFold(current_folds, random_state=<span class="hljs-number">42</span>, shuffle=<span class="hljs-keyword">True</span>)

<span class="hljs-keyword">for</span> k_fold, (tr_inds, val_inds) <span class="hljs-keyword">in</span> enumerate(kfold.split(X=x_train, y=y_train)):
    print(<span class="hljs-string">'---- Starting fold %d ----'</span>%(k_fold+<span class="hljs-number">1</span>))

    x_tr, y_tr = x_train[tr_inds], y_train[tr_inds]
    x_val, y_val = x_train[val_inds], y_train[val_inds]

    model = runModel(x_tr, y_tr, x_val, y_val, epos=current_epochs)

    
    model.compile(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer= <span class="hljs-string">"adam"</span>, metrics=[custom_f1, <span class="hljs-string">'accuracy'</span>])

    
    <span class="hljs-keyword">for</span> val <span class="hljs-keyword">in</span> history.history[<span class="hljs-string">'custom_f1'</span>]:
            npt_exp[<span class="hljs-string">'Custom F1 metric'</span>].log(val)

    model.fit(x_tr,
              y_tr,
              epochs=current_epochs,
              batch_size=current_batch_size,
              verbose=<span class="hljs-number">1</span>)

    y_val_pred = model.predict(x_val)
    y_val_pred_cat = (np.asarray(y_val_pred)).round()

    
    f1, precision, recall = f1_score(y_val, y_val_pred_cat), precision_score(y_val, y_val_pred_cat), recall_score(y_val, y_val_pred_cat)

    metric_text = f<span class="hljs-string">'Fold {k_fold+1} f1 score = '</span>
    
    npt_exp[metric_text] = myformat(f1)

    f1_cv.append(round(f1, <span class="hljs-number">6</span>))
    precision_cv.append(round(precision, <span class="hljs-number">6</span>))
    recall_cv.append(round(recall, <span class="hljs-number">6</span>))


metric_text_final = <span class="hljs-string">'Mean f1 score through CV = '</span>
npt_exp[metric_text_final] = myformat(np.mean(f1_cv))</pre>



<p>我们从我们的训练实验中提取f1值，并使用<em> send_metric() </em>函数在Neptune上跟踪这些f1值；</p>



<ul>
<li>在每次折叠之后，计算性能指标，即f1、精度和召回率，并因此使用<em> send_text() </em>函数发送到Neptune</li>



<li>当整个交叉验证完成时，通过取每个CV的f1分数的平均值来计算最终的f1分数。同样，这个值被发送到Neptune进行跟踪。</li>



<li>在您启动模型后，您将立即看到Neptune开始跟踪训练过程，如下所示。因为还没有要记录的指标，所以在此阶段只显示CPU和内存信息:</li>



<li>随着模型训练的进行，会记录更多的性能指标值。单击项目ID旁边的小眼睛图标，我们将启用交互式跟踪图表，显示每个训练迭代期间的f1值:</li>
</ul>



<p>训练过程结束后，我们可以点击项目ID来查看Neptune自动存储的所有元数据。正如您在下面的视频中所看到的，该元数据包括每个折叠的f1分数，以及5个折叠CV的f1分数的平均值。在元数据之上，图表选项显示由我们的自定义度量函数为每个时期计算的f1值，即5倍* 20个时期= 100个f1值:</p>





<p>目前为止一切正常！然而，当我们检查Neptune上的详细日志记录时，我们注意到一些意想不到的事情。<strong>在训练期间计算的F1分数(例如，0.137)与为每个验证集计算的分数(例如，0.824)显著不同</strong>。这种趋势在图表中更加明显(在右下方)，其中最大F1值约为0.14。</p>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20230224202511im_/https://neptune.ai/wp-content/uploads/2.Neptune_Training_showChart.mp4"/></figure>



<p>为什么会这样？</p>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20230224202511im_/https://neptune.ai/wp-content/uploads/3.Neptune_Training_finished.mp4"/></figure>



<p>使用回调来指定指标</p>





<p>深入研究这个问题，我们意识到Keras是通过批量创建自定义度量函数来进行计算的。每个度量在每个批次后应用，然后平均以获得特定时期的全局近似值。这个信息是误导性的，因为我们所监控的应该是每个时期的宏观训练表现。这就是为什么Keras 2.0版本中删除了这些指标。综上所述，实施宏观F1指标的正确方法是什么？答案是回调功能:</p>



<h3>这里，我们定义了一个回调类<strong> NeptuneMetrics </strong>来计算和跟踪每个时期结束时的模型性能指标，也就是宏分数。</h3>



<p>然后我们这样编译和拟合我们的模型:</p>



<pre class="hljs">
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NeptuneMetrics</span><span class="hljs-params">(Callback)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, neptune_experiment, validation, current_fold)</span>:</span>
        super(NeptuneMetrics, self).__init__()
        self.exp = neptune_experiment
        self.validation = validation
        self.curFold = current_fold

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">on_train_begin</span><span class="hljs-params">(self, logs={})</span>:</span>
        self.val_f1s = []
        self.val_recalls = []
        self.val_precisions = []

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">on_epoch_end</span><span class="hljs-params">(self, epoch, logs={})</span>:</span>
        val_targ = self.validation[<span class="hljs-number">1</span>]
        val_predict = (np.asarray(self.model.predict(self.validation[<span class="hljs-number">0</span>]))).round()

        val_f1 = round(f1_score(val_targ, val_predict), <span class="hljs-number">4</span>)
        val_recall = round(recall_score(val_targ, val_predict), <span class="hljs-number">4</span>)
        val_precision = round(precision_score(val_targ, val_predict), <span class="hljs-number">4</span>)

        self.val_f1s.append(val_f1)
        self.val_recalls.append(val_recall)
        self.val_precisions.append(val_precision)

        print(f<span class="hljs-string">' — val_f1: {val_f1} — val_precision: {val_precision}, — val_recall: {val_recall}'</span>)

        
	    self.exp[<span class="hljs-string">'Epoch End Loss'</span>].log(logs[<span class="hljs-string">'loss'</span>])
        self.exp[<span class="hljs-string">'Epoch End F1-score'</span>].log(val_f1)
        self.exp[<span class="hljs-string">'Epoch End Precision'</span>].log(val_precision)
        self.exp[<span class="hljs-string">'Epoch End Recall'</span>].log(val_recall)

        <span class="hljs-keyword">if</span> self.curFold == <span class="hljs-number">4</span>:
            
            msg = f<span class="hljs-string">' End of epoch {epoch} val_f1: {val_f1} — val_precision: {val_precision}, — val_recall: {val_recall}'</span>
            
            self.exp[f<span class="hljs-string">'Epoch End Metrics (each step) for fold {self.curFold}'</span>] = msg
</pre>



<p>现在，如果我们重新运行CV训练，Neptune将自动创建一个新的模型跟踪–在我们的示例中为KER1-9–以便于比较(不同实验之间):</p>



<p>与之前一样，在训练发生时检查由新回调方法生成的详细日志记录，我们观察到我们的NeptuneMetrics对象为训练过程和验证生成一致的F1分数(大约0.7-0.9)，如Neptune视频剪辑所示:</p>



<pre class="hljs">model.compile(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer= <span class="hljs-string">"adam"</span>, metrics=[])
model.fit(x_tr,
          y_tr,
          callbacks=[NeptuneMetrics(npt_exp, validation=(x_val, y_val), current_fold=k_fold)],  
          epochs=current_epochs,
          batch_size=current_batch_size,
          verbose=<span class="hljs-number">1</span>)
</pre>



<p>完成模型训练后，让我们检查并确认在最后一个CV文件夹的每个(时期)步骤中记录的性能指标符合预期:</p>





<p>太好了！一切看起来都在合理的范围内。</p>



<figure class="wp-block-video"><video controls="" src="https://web.archive.org/web/20230224202511im_/https://neptune.ai/wp-content/uploads/4.Neptune_Training_Callback.mp4"/></figure>



<p>让我们比较一下我们刚刚试验的这两种方法之间的区别，即<strong>自定义F1指标与NeptuneMetrics回调</strong>:</p>





<p>我们可以清楚地看到，<strong>自定义F1指标(左边)</strong>实现是不正确的，而<strong> NeptuneMetrics回调</strong>实现是理想的方法！</p>



<p>现在，最后一次检查。使用回调方法预测测试集给我们提供了F1分数= 0.8125，这与训练相当接近:</p>





<p>最后的想法</p>



<p>你有它！在您的神经网络模型中计算和监控F1分数的正确和不正确方法。如果你感兴趣的话，相似的过程可以应用于召回和精确。我希望这篇博客对你有所帮助。完整的代码可以在<a href="https://web.archive.org/web/20230224202511/https://github.com/YiLi225/NeptuneBlogs/blob/main/Implement_F1score_neptune_git_NewVersion.py" target="_blank" rel="noreferrer noopener nofollow">这个Github repo </a>中找到，整个Neptune模型可以在<a href="https://web.archive.org/web/20230224202511/https://app.neptune.ai/katyl/KerasMetricNeptuneNewVersion/experiments?split=bth&amp;dash=charts&amp;viewId=standard-view" target="_blank" rel="noreferrer noopener">这里</a>找到。</p>



<pre class="hljs">
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(x_test)</span>:</span>
    model_num = len(models)
    <span class="hljs-keyword">for</span> k, m <span class="hljs-keyword">in</span> enumerate(models):
        <span class="hljs-keyword">if</span> k==<span class="hljs-number">0</span>:
            y_pred = m.predict(x_test, batch_size=current_batch_size)
        <span class="hljs-keyword">else</span>:
            y_pred += m.predict(x_test, batch_size=current_batch_size)

    y_pred = y_pred / model_num

    <span class="hljs-keyword">return</span> y_pred

y_test_pred_cat = predict(x_test).round()

cm = confusion_matrix(y_test, y_test_pred_cat)
f1_final = round(f1_score(y_test, y_test_pred_cat), <span class="hljs-number">4</span>)


npt_exp[<span class="hljs-string">'TestSet F1 score'</span>] = myformat(f1_final)


<span class="hljs-keyword">from</span> scikitplot.metrics <span class="hljs-keyword">import</span> plot_confusion_matrix
fig_confmat, ax = plt.subplots(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>))
plot_confusion_matrix(y_test, y_test_pred_cat.astype(int).flatten(), ax=ax)


npt_exp[<span class="hljs-string">'Confusion Matrix'</span>].upload(neptune.types.File.as_image(fig_confmat))
npt_exp.stop()
</pre>







<h2 id="h-final-thoughts">在我让你走之前，这个NeptuneMetrics回调计算F1分数，但这并不意味着模型是在F1分数上训练的。为了基于优化F1分数进行“训练”,这有时是处理不平衡分类的首选，我们需要额外的模型/回调配置。请继续关注我的下一篇文章，在那里我将讨论F1分数调整和阈值移动。感谢阅读！</h2>



<p>There you have it! The correct and incorrect ways to calculate and monitor the F1 score in your neural network models. Similar procedures can be applied for recall and precision if it’s your measure of interest. I hope that you find this blog helpful. The full code is available in <a href="https://web.archive.org/web/20230224202511/https://github.com/YiLi225/NeptuneBlogs/blob/main/Implement_F1score_neptune_git_NewVersion.py" target="_blank" rel="noreferrer noopener nofollow">this Github repo</a>, and the entire Neptune model can be found <a href="https://web.archive.org/web/20230224202511/https://app.neptune.ai/katyl/KerasMetricNeptuneNewVersion/experiments?split=bth&amp;dash=charts&amp;viewId=standard-view" target="_blank" rel="noreferrer noopener">here</a>.</p>



<p>Before I let you go, this NeptuneMetrics callback calculates the F1 score, but it doesn’t mean that the model is trained on the F1 score. In order to ‘train’ based on optimizing the F1 score, which sometimes is preferred for handling imbalanced classification, we need additional model/callback configurations. Stay tuned for my next article, where I will be discussing F1 score tuning and threshold-moving. Thanks for reading!</p>
        </div>
        
    </div>    
</body>
</html>