<html>
<head>
<title>Hyperparameter Tuning in Python: a Complete Guide </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Python中超参数调优:完全指南</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>为机器学习或深度学习模型选择正确的超参数是从模型中提取最后汁液的最佳方式之一。在本文中，我将向您展示目前可用的一些最佳超参数调优方法。</p>



<h2 id="h-what-is-the-difference-between-parameter-and-hyperparameter">参数和超参数有什么区别？</h2>



<p>首先我们来了解一下机器学习中超参数和参数的<a href="https://web.archive.org/web/20230304181723/https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/" target="_blank" rel="noreferrer noopener nofollow">区别。</a></p>



<ul>
<li><strong>模型参数</strong>:这些是模型从给定的数据中估计出来的参数。例如深度神经网络的权重。</li>



<li><strong>模型超参数</strong>:这些是模型无法从给定数据中估计出来的参数。这些参数用于估计模型参数。比如深度神经网络中的学习速率。</li>
</ul>



<p id="separator-block_b2aed17bae4e5c6e5716f0a1bca7fbe4" class="block-separator block-separator--10"> </p>



<div id="medium-table-block_f62d98e6a067de1eeb4ec275bccef2ba" class="block-medium-table c-table__outer-wrapper ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">因素</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">超参数</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>它们是进行预测所需要的</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>它们是估算模型参数所需要的</p> </div></td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil"><div class="c-ceil__inner"><p/></div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>它们是通过超参数调谐</p> </div>来估计的</td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>它们不是手动设置的</p> </div></td>

                    
                        <td class="c-ceil">                                                      </td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>训练后发现的最终参数将决定模型如何对未发现的数据执行</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>超参数的选择决定了训练的效率。在梯度下降中，学习率决定了优化过程在估计参数时的效率和准确性</p> </div></td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>



<p id="separator-block_8448c7222bfd75ea202f62fbf5aedf45" class="block-separator block-separator--5"> </p>



<p class="has-text-align-center"><em>模型参数vs模型超参数|来源:<a href="https://web.archive.org/web/20230304181723/https://www.geeksforgeeks.org/difference-between-model-parameters-vs-hyperparameters/" target="_blank" rel="noreferrer noopener nofollow">GeeksforGeeks</a>T3】</em></p>



<h2 id="h-what-is-hyperparameter-tuning-and-why-it-is-important">什么是超参数调整，为什么它很重要？</h2>



<p><a href="https://web.archive.org/web/20230304181723/https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624" target="_blank" rel="noreferrer noopener nofollow">超参数调整</a>(或超参数优化)是确定最大化模型性能的正确超参数组合的过程。它的工作原理是在一个训练过程中进行多次试验。每次试验都是使用您选择的超参数值(在您指定的范围内设置)完整执行您的训练应用程序。该过程一旦完成，将为您提供最适合模型的一组超参数值，以获得最佳结果。</p>



<p>不用说，这是任何机器学习项目中的重要一步，因为它会导致模型的最优结果。如果您希望看到它的实际应用，<a href="https://web.archive.org/web/20230304181723/https://arxiv.org/pdf/2007.07588.pdf" target="_blank" rel="noreferrer noopener nofollow">这里有一篇研究论文</a>，它通过在数据集上进行实验，讲述了超参数优化的重要性。</p>



<h2 id="h-how-to-do-hyperparameter-tuning-how-to-find-the-best-hyperparameters">如何进行超参数调谐？如何找到最佳超参数？</h2>



<p>选择正确的超参数组合需要理解超参数和业务用例。但是，从技术上来说，有两种方法可以设置它们。</p>



<h3>手动超参数调谐</h3>



<p>手动超参数调整包括手动试验不同的超参数集，即您将执行一组超参数的每次试验。这项技术需要一个强大的实验跟踪器，它可以跟踪从图像、日志到系统指标的各种变量。</p>



<p>有几个实验跟踪器可以勾选所有的框。<a href="/web/20230304181723/https://neptune.ai/" target="_blank" rel="noreferrer noopener"> neptune.ai </a>就是其中之一。它提供了一个直观的界面和一个开源包neptune-client来方便你登录代码。您可以轻松记录超参数，并查看所有类型的数据结果，如图像、指标等。查看文档，看看如何<a href="https://web.archive.org/web/20230304181723/https://docs.neptune.ai/logging/what_you_can_log/" target="_blank" rel="noreferrer noopener">将不同的元数据记录到Neptune </a>。</p>



<p>替代解决方案包括W&amp;B、Comet或MLflow。点击查看更多<a href="/web/20230304181723/https://neptune.ai/blog/best-ml-experiment-tracking-tools" target="_blank" rel="noreferrer noopener">实验跟踪工具&amp;管理。</a></p>



<p><strong>手动超参数优化的优势</strong>:</p>



<ul>
<li>手动调整超参数意味着对过程的更多控制。</li>



<li>如果您正在研究调优以及它如何影响网络权重，那么手动进行调优是有意义的。</li>
</ul>



<p><strong>手动超参数的缺点<strong>优化</strong> </strong>:</p>



<ul>
<li>手动调谐是一个乏味的过程，因为可能有许多尝试，并且跟踪可能证明是昂贵和耗时的。</li>



<li>当有许多超参数需要考虑时，这不是一个非常实用的方法。</li>
</ul>



<p>点击了解<a href="https://web.archive.org/web/20230304181723/https://machinelearningmastery.com/manually-optimize-hyperparameters/" target="_blank" rel="noreferrer noopener nofollow">如何手动优化机器学习模型超参数。</a></p>



<h3>自动超参数调谐</h3>



<p>自动超参数调整利用现有的算法来自动化该过程。您需要遵循的步骤是:</p>



<ul>
<li>首先，指定一组超参数和对这些超参数值的限制(注意:每个算法都要求这组参数是特定的数据结构，例如，在处理算法时，字典是常见的)。</li>



<li>然后算法会帮你完成繁重的工作。它会运行这些试验，并为您获取最佳的超参数集，以获得最佳结果。</li>
</ul>



<p>在博客中，我们将讨论一些可以用来实现自动调优的算法和工具。我们开始吧。</p>



<h2 id="h-hyperparameter-tuning-methods">超参数调谐方法</h2>



<p>在这一节中，我将介绍当今流行的所有超参数优化方法。</p>



<h3>随机搜索</h3>



<p>在<a href="https://web.archive.org/web/20230304181723/https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noreferrer noopener nofollow">随机搜索方法</a>中，我们为超参数创建一个可能值的网格。每次迭代尝试这个网格中超参数的随机组合，记录性能，最后返回提供最佳性能的超参数组合。</p>



<h3>网格搜索</h3>



<p>在<a href="https://web.archive.org/web/20230304181723/https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e" target="_blank" rel="noreferrer noopener nofollow">网格搜索方法</a>中，我们为超参数创建一个可能值的网格。每次迭代以特定的顺序尝试超参数的组合。它在每个可能的超参数组合上拟合模型，并记录模型性能。最后，它返回具有最佳超参数的最佳模型。</p>





<h3>贝叶斯优化</h3>



<p>为您的模型调整和找到正确的超参数是一个优化问题。我们希望通过改变模型参数来最小化模型的损失函数。贝叶斯优化帮助我们在最少的步骤中找到最小的点。<a href="https://web.archive.org/web/20230304181723/https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f" target="_blank" rel="noreferrer noopener nofollow">贝叶斯优化</a>还使用一个采集函数，该函数将采样指向可能比当前最佳观测值有所改进的区域。</p>



<h3>树结构Parzen估计量(TPE)</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://optunity.readthedocs.io/en/latest/user/solvers/TPE.html" target="_blank" rel="noreferrer noopener nofollow">基于树的Parzen优化</a>的思想类似于贝叶斯优化。TPE模型P(x|y)和p(y)不是寻找p(y|x)的值，其中y是要最小化的函数(例如，验证损失), x是超参数的值。树结构Parzen估计器的一个大缺点是它们没有模拟超参数之间的相互作用。也就是说，TPE在实践中工作得非常好，并且在大多数领域都经过了实战考验。</p>



<h2 id="h-hyperparameter-tuning-algorithms">超参数调整算法</h2>



<p>这些是专门为进行超参数调整而开发的算法。</p>



<h3>超波段</h3>



<p>Hyperband是随机搜索的一种变体，但是使用了一些<a href="https://web.archive.org/web/20230304181723/https://en.wikipedia.org/wiki/Multi-armed_bandit#Empirical_motivation" target="_blank" rel="noreferrer noopener nofollow">探索-利用</a>理论来为每种配置找到最佳的时间分配。你可以查看这篇<a href="https://web.archive.org/web/20230304181723/https://arxiv.org/abs/1603.06560" target="_blank" rel="noreferrer noopener nofollow">研究论文</a>以获取更多参考。</p>



<h3>基于人口的培训</h3>



<p>这种技术是两种最常用的搜索技术的混合:随机搜索和应用于神经网络模型的手动调整。</p>



<p>PBT从用随机超参数并行训练许多神经网络开始。但是这些网络并不是完全相互独立的。</p>



<p>它使用来自其余群体的信息来改进超参数，并确定要尝试的超参数的值。你可以查看这篇<a href="https://web.archive.org/web/20230304181723/https://deepmind.com/blog/article/population-based-training-neural-networks" target="_blank" rel="noreferrer noopener nofollow">文章</a>了解更多关于PBT的信息。</p>





<h3>BOHB</h3>



<p>BOHB(贝叶斯优化和超带)混合了超带算法和贝叶斯优化。可以查看这篇<a href="https://web.archive.org/web/20230304181723/https://www.automl.org/blog_bohb/" target="_blank" rel="noreferrer noopener nofollow">文章</a>进一步参考。</p>











<p>现在你知道了什么是方法和算法，让我们来谈谈工具，有很多这样的工具。</p>



<p><strong>一些最好的超参数优化库是:</strong></p>



<ol>
<li><a href="#scikit-learn"> Scikit-learn </a></li>



<li><a href="#scikit-optimize">sci kit-优化</a></li>



<li><a href="#optuna"> Optuna </a></li>



<li><a href="#hyperopt">远视</a></li>



<li><a href="#ray">雷.调</a></li>



<li><a href="#talos">塔罗斯</a></li>



<li><a href="#bayesianoptimization">贝叶斯优化</a></li>



<li><a href="#moe">度量优化引擎(MOE) </a></li>



<li><a href="#spearmint">留兰香</a></li>



<li><a href="#gpyopt"> GPyOpt </a></li>



<li><a href="#sigopt"> SigOpt </a></li>



<li><a href="#fabolas">法博拉斯</a></li>
</ol>



<h3 id="scikit-learn">1. Scikit-learn</h3>



<p>Scikit-learn 实现了网格搜索和随机搜索，如果您正在使用sklearn构建模型，这是一个很好的起点。</p>



<p>对于这两种方法，scikit-learn在各种参数选择上以k倍交叉验证设置训练和评估模型，并返回最佳模型。</p>



<p>具体来说:</p>



<ul class="is-style-default">
<li><strong>随机搜索:</strong>与<a href="https://web.archive.org/web/20230304181723/https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" target="_blank" rel="noreferrer noopener nofollow"> <code>randomsearchcv</code> </a>在一些随机参数组合上运行搜索</li>



<li><strong>网格搜索:</strong> <a href="https://web.archive.org/web/20230304181723/https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" target="_blank" rel="noreferrer noopener nofollow"> <code>gridsearchcv</code> </a>对网格中的所有参数集进行搜索</li>
</ul>



<p>用scikit-learn调优模型是一个好的开始，但是还有更好的选择，而且它们通常有随机搜索策略。</p>







<h3 id="scikit-optimize">2.sci kit-优化</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://scikit-optimize.github.io/stable/index.html" target="_blank" rel="noreferrer noopener nofollow"> Scikit-optimize </a>使用基于<a href="https://web.archive.org/web/20230304181723/https://ml.informatik.uni-freiburg.de/papers/11-LION5-SMAC.pdf">序列模型的优化</a>算法，在更短的时间内找到超参数搜索问题的最优解。</p>



<p>Scikit-optimize提供了除超参数优化之外的许多功能，例如:</p>



<ul class="is-style-default">
<li>存储和加载优化结果，</li>



<li>收敛图，</li>



<li>比较代理模型</li>
</ul>



<h3 id="optuna">3.奥普图纳</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://optuna.org/" target="_blank" rel="noreferrer noopener nofollow"> Optuna </a>使用轨迹细节的历史记录来确定有希望的区域，以搜索优化超参数，从而在最短的时间内找到最佳超参数。</p>



<p>它具有修剪功能，可以在训练的早期阶段自动停止不被看好的轨迹。optuna提供的一些主要功能有:</p>



<ul class="is-style-default">
<li>轻量级、通用且平台无关的架构</li>



<li>Pythonic搜索空间</li>



<li>高效优化算法</li>



<li>易于并行化</li>



<li>快速可视化</li>
</ul>



<p>关于如何开始使用optuna的教程，可以参考官方<a href="https://web.archive.org/web/20230304181723/https://optuna.readthedocs.io/en/stable/tutorial/first.html" target="_blank" rel="noreferrer noopener nofollow">文档</a>。</p>









<h3 id="hyperopt">4.远视</h3>



<p><a href="https://web.archive.org/web/20230304181723/http://hyperopt.github.io/hyperopt/" target="_blank" rel="noreferrer noopener nofollow"> Hyperopt </a>是目前最流行的超参数调谐包之一。Hyperopt允许用户描述一个搜索空间，在该搜索空间中，用户期望得到最佳结果，从而允许hyperopt中的算法更有效地搜索。</p>



<p>目前，hyperopt中实现了三种算法。</p>







<p>要使用远视，您应该首先描述:</p>



<ul class="is-style-default">
<li>最小化的目标函数</li>



<li>要搜索的空间</li>



<li>存储搜索的所有点评估的数据库</li>



<li>要使用的搜索算法</li>
</ul>



<p>这个<a href="https://web.archive.org/web/20230304181723/https://github.com/hyperopt/hyperopt/wiki/FMin" target="_blank" rel="noreferrer noopener nofollow">教程</a>将带你了解如何构建代码并使用hyperopt包来获得最佳的超参数。</p>



<p>您也可以阅读<a href="https://web.archive.org/web/20230304181723/https://mlwhiz.com/blog/2019/10/10/hyperopt2/" target="_blank" rel="noreferrer noopener">这篇</a>文章，了解更多关于如何使用Hyperopt的信息。</p>







<h3 id="ray">5.射线调谐</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://docs.ray.io/en/latest/tune/index.html" target="_blank" rel="noreferrer noopener nofollow">射线调谐</a>是实验和超参数调谐在任何规模的流行选择。Ray使用分布式计算的能力来加速超参数优化，并在大规模上实现了几种最先进的优化算法。</p>



<p>“光线调节”提供的一些核心功能包括:</p>



<ul class="is-style-default">
<li>通过<a href="https://web.archive.org/web/20230304181723/https://github.com/ray-project/ray" target="_blank" rel="noreferrer noopener nofollow">利用Ray </a>实现开箱即用的分布式异步优化。</li>



<li>易于扩展。</li>



<li>提供了<a href="https://web.archive.org/web/20230304181723/https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband" target="_blank" rel="noreferrer noopener nofollow"> ASHA </a>、<a href="https://web.archive.org/web/20230304181723/https://ray.readthedocs.io/en/latest/tune-searchalg.html#bohb" target="_blank" rel="noreferrer noopener nofollow"> BOHB </a>、<a href="https://web.archive.org/web/20230304181723/https://ray.readthedocs.io/en/latest/tune-schedulers.html#population-based-training-pbt" target="_blank" rel="noreferrer noopener nofollow">基于人口的训练</a>等SOTA算法。</li>



<li>支持Tensorboard和MLflow。</li>



<li>支持Sklearn、XGBoost、TensorFlow、PyTorch等多种框架。</li>
</ul>



<p>你可以参考这个<a href="https://web.archive.org/web/20230304181723/https://docs.ray.io/en/latest/tune/tutorials/overview.html" target="_blank" rel="noreferrer noopener nofollow">教程</a>来学习如何针对你的问题实现光线调谐。</p>



<h3 id="keras-tuner">6. Keras Tuner</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://keras.io/keras_tuner/" target="_blank" rel="noreferrer noopener nofollow"> Keras Tuner </a>是一个帮助您为TensorFlow程序选择最佳超参数集的库。当您为超参数调整构建模型时，除了模型架构之外，您还定义了超参数搜索空间。您为超参数调整设置的模型被称为<em>超模型</em>。</p>



<p>您可以通过两种方法定义超级模型:</p>



<ul class="is-style-default">
<li>通过使用模型构建器功能</li>



<li>通过子类化Keras Tuner API的超级模型类</li>
</ul>



<p>对于计算机视觉应用，您还可以使用两个预定义的超级模型类——<a href="https://web.archive.org/web/20230304181723/https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperxception-class" target="_blank" rel="noreferrer noopener nofollow">hyperexception</a>和<a href="https://web.archive.org/web/20230304181723/https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperresnet-class" target="_blank" rel="noreferrer noopener nofollow"> HyperResNet </a>。</p>



<p>进一步的实现细节可以参考这个<a href="https://web.archive.org/web/20230304181723/https://www.tensorflow.org/tutorials/keras/keras_tuner" target="_blank" rel="noreferrer noopener nofollow">官方教程</a>。</p>



<h3 id="bayesianoptimization">7.贝叶斯最优化</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://github.com/fmfn/BayesianOptimization" target="_blank" rel="noreferrer noopener nofollow"> BayesianOptimization </a>是一个软件包，旨在最大限度地减少寻找接近最佳组合的参数组合所需的步骤。</p>



<p>这种方法使用了一个代理优化问题(寻找获取函数的最大值)，虽然这仍然是一个困难的问题，但在计算意义上它更便宜，并且可以使用常见的工具。因此，贝叶斯优化最适合对要优化的函数进行采样是一项非常昂贵的工作的情况。</p>



<p>点击这里访问GitHub repo <a href="https://web.archive.org/web/20230304181723/https://github.com/fmfn/BayesianOptimization" target="_blank" rel="noreferrer noopener nofollow">查看它的运行情况。</a></p>



<h3 id="moe">8.度量优化引擎</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://github.com/Yelp/MOE" target="_blank" rel="noreferrer noopener nofollow"> MOE(公制优化引擎)</a>当评估参数耗时或昂贵时，MOE是优化系统参数的有效方法。</p>



<p>它是解决以下问题的理想选择</p>



<ul>
<li>优化问题的目标函数是一个黑盒，不一定是凸的或凹的，</li>



<li>衍生品不可用，</li>



<li>我们寻求的是全局最优，而不仅仅是局部最优。</li>
</ul>



<p>这种处理黑盒目标函数的能力允许我们使用MOE来优化几乎任何系统，而不需要任何内部知识或访问。</p>



<p>访问GitHub <a href="https://web.archive.org/web/20230304181723/https://github.com/Yelp/MOE" target="_blank" rel="noreferrer noopener nofollow"> repo </a>了解更多信息。</p>



<h3 id="spearmint">9.留兰香</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://github.com/HIPS/Spearmint" target="_blank" rel="noreferrer noopener nofollow"> Spearmint </a>是一个软件包，也执行贝叶斯优化。该软件被设计成自动运行实验(因此代号为spearmint ),以迭代的方式调整多个参数，以便在尽可能少的运行中最小化一些目标。</p>



<p>阅读并实验GitHub <a href="https://web.archive.org/web/20230304181723/https://github.com/HIPS/Spearmint" target="_blank" rel="noreferrer noopener nofollow"> repo </a>中关于留兰香的内容。</p>



<h3 id="gpyopt">10.GPyOpt</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://github.com/SheffieldML/GPyOpt" target="_blank" rel="noreferrer noopener nofollow"> GPyOpt </a>是使用<a href="https://web.archive.org/web/20230304181723/http://sheffieldml.github.io/GPy/"> GPy </a>的高斯过程优化。它使用不同的采集函数执行全局优化。</p>



<p>在其他功能中，可以使用GPyOpt来优化物理实验(顺序或批量)和调整机器学习算法的参数。它能够通过稀疏高斯过程模型处理大型数据集。</p>



<p>不幸的是，GPyOpt的维护已经被repo的作者关闭了，但是你仍然可以在你的实验中使用这个包。</p>



<p>前往GitHub repo <a href="https://web.archive.org/web/20230304181723/https://github.com/SheffieldML/GPyOpt" target="_blank" rel="noreferrer noopener nofollow">这里</a>。</p>



<h3 id="sigopt">11.SigOpt</h3>



<p><a href="https://web.archive.org/web/20230304181723/https://sigopt.com/" target="_blank" rel="noreferrer noopener nofollow"> SigOpt </a>将自动超参数调整与训练跑步跟踪完全集成，让您了解更广阔的前景和达到最佳模式的途径。</p>



<p>凭借高度可定制的搜索空间和多尺度优化等功能，SigOpt可以在将模型投入生产之前，使用简单的API对其进行复杂的超参数调整。</p>



<p>访问文档<a href="https://web.archive.org/web/20230304181723/https://app.sigopt.com/docs/intro/intelligent_optimization" target="_blank" rel="noreferrer noopener nofollow">此处</a>了解更多关于SigOpt超参数调整的信息。</p>



<h3 id="fabolas">12.法博拉斯</h3>



<p>传统的贝叶斯超参数优化器将机器学习算法在给定数据集上的损失建模为要最小化的黑盒函数，而大型数据集上的快速贝叶斯优化(FABOLAS)对数据集大小上的损失和计算成本进行建模，并使用这些模型来执行具有额外自由度的贝叶斯优化。</p>



<p>你可以在这里查看实现fabolas <a href="https://web.archive.org/web/20230304181723/https://github.com/automl/RoBO/blob/master/robo/fmin/fabolas.py#L31" target="_blank" rel="noreferrer noopener nofollow">的函数，在这里</a>查看研究论文<a href="https://web.archive.org/web/20230304181723/https://arxiv.org/pdf/1605.07079.pdf" target="_blank" rel="noreferrer noopener nofollow">。</a></p>







<h2 id="h-hyperparameter-tuning-resources-and-examples">超参数调整资源和示例</h2>



<p>在这一节中，我将分享一些为不同的ML和DL框架实现的超参数调优示例。</p>



<h3>随机森林超参数调整</h3>







<h3>XGBoost超参数调整</h3>







<h3>LightGBM超参数调谐</h3>







<h3>CatBoost超参数调节</h3>







<h3>Keras超参数调谐</h3>







<h3>PyTorch超参数调谐</h3>







<h2 id="h-final-thoughts">最后的想法</h2>



<p>恭喜你，你成功了！超参数调优是任何机器学习项目不可或缺的一部分，因此这个主题总是值得深入研究。在这篇博客中，我们讨论了广泛使用和研究的不同超参数调优算法和工具。但即使如此，我们也涵盖了大量的技术和工具，正如一位智者曾经说过的，知识永无止境。</p>



<p>以下是该领域的一些最新研究，你可能会感兴趣:</p>







<p>就这些了，请继续关注更多，再见！</p>



<p/>
        </div>
        
    </div>    
</body>
</html>