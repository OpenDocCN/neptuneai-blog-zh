# 使用 Autoencoder 了解表征学习

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/representation-learning-with-autoencoder>

机器学习是人工智能的一个子领域，我们试图建立具有大脑功能和行为的智能系统。通过人工智能，我们试图建造能够计算、提取模式、自动化日常任务、诊断生物异常以及证明科学理论和假设的机器。

因为机器学习是 AI 的子集，它不依赖于硬编码的算法来找到通往核心解决方案的道路，而是用它可以从给定信息中提取知识的思想来加强 AI，并且在没有硬编码的情况下找到通往核心思想的道路。

> 随着机器学习的出现，我们现在可以设计能够“处理涉及真实世界知识的问题并做出看起来主观的决定”的算法。
> 
> (伊恩·古德费勒《深度学习》)

在所有机器学习问题中，数据都起着关键作用。

为什么这么重要？

数据是信息的离散排列，提供一系列连续的事件。在这种安排中，所有的表现形式都被隐藏起来了。如果机器可以提取代表特定事件的模式，我们可以说机器已经学习了这些信息，如果新的数据或信息被输入其中，那么它可以提供适当的解决方案和预测。

表征学习

## 想象一下，一名工程师设计了一种基于大脑扫描预测恶性细胞的 ML 算法。为了设计算法，工程师必须严重依赖患者数据，因为所有答案都在那里。

数据中的每个观察或特征描述了患者的属性。预测结果的机器学习算法必须学习每个特征如何与不同的结果相关联:良性或恶性。

因此，如果数据中有任何噪声或差异，结果可能完全不同，这是大多数机器学习算法的问题。大多数机器学习算法对数据的理解都很肤浅。

那么解决办法是什么呢？

向机器提供更抽象的数据表示。

对于许多任务，不可能知道应该提取什么特征。艾伦·图灵和他破译恩尼格玛密码的同事观察到了信息中有规律出现的模式。这就是代表学习的想法真正进入视野的地方。

在表示学习中，机器被提供数据，并且它自己学习表示。这是一种寻找数据表示(特征、距离函数、相似性函数)的方法，它决定了预测模型的性能。

表征学习的工作原理是将高维数据简化为低维数据，从而更容易发现模式和异常，并让我们更好地理解数据的行为。

它还降低了数据的复杂性，因此减少了异常和噪声。这种噪声的减少对于监督学习算法非常有用。

**不变性和解纠缠**

### 表征学习的问题在于，很难获得能够解决给定问题的表征。幸运的是，深度学习和深度神经网络开始证明非常面向目标和有效。

深度神经网络可以从简单的概念中构建复杂的概念，这一想法是深度学习的核心[Ian Goodfellow，“深度学习”]。

那么，表征学习的理念在哪里呢？

人们试图理解深度学习的成功。两个主要结论是 [**表征学习**](https://web.archive.org/web/20221201174108/https://opensource.com/article/17/9/representation-learning) 和**优化**。

深度学习通常被视为一个黑盒，其中有限数量的函数被用来寻找产生良好泛化能力的参数。这是通过优化来实现的，在优化中，算法试图通过用实际情况评估模型输出来自我校正。

这个过程一直进行到优化函数达到称为全局最小值的最小值点。大多数深度学习网络都严重过度参数化，并表明它们可能过度拟合——即算法在训练数据上表现良好，但在新数据上表现不佳。最近的工作表明，这与损失景观的性质有关，也与随机梯度下降(SGD)执行的隐式正则化有关，但总体输出仍有噪声[Zhang 等人，2017]。

另一方面，表征学习专注于网络各层(激活)学习的表征的属性，同时在很大程度上保持对所使用的特定优化过程的不可知[深层表征中不变性和解缠结的出现，2018]。

通常出现在任何数据分布中的两个主要因素是**方差**和**纠缠**。需要消除这两个因素，以便从数据中获得良好的表示。数据中的方差也可以被认为是敏感性，而这些敏感性可以将结果颠倒过来。我们建立的任何模型都必须对方差具有鲁棒性，也就是说，它必须是不变的，因为这会极大地损害深度学习模型的结果。

纠缠是数据中的向量与数据中的其他向量连接或关联的方式。这些联系使得数据非常复杂，难以破译。我们应该做的是寻找关系简单的变量。这是一种将高维数据转换为低维数据的简单方法，或者以一种可以轻松分离的方式来转换高维数据。

从上一节中，我们了解到表征学习的能力是它学习对数据有意义的抽象模式，而深度学习通常被认为是深度网络学习表征的能力，这些表征对诸如平移、旋转、遮挡以及“解开缠结”或分离高维数据空间中的因素等干扰不变(不敏感)[Bengio，2009]。但是，学习通过创建不变的和不混乱的模型来简化复杂的数据排列仍然很重要。

“如果架构和损失函数都没有明确地实施不变性和解纠缠，这些属性如何能够在通过简单的一般优化训练的深度网络中一致地出现？”

> 事实证明，我们可以通过展示两样东西来回答这个问题:

使用统计决策和信息论的经典概念，我们表明深度神经网络中的不变性等价于它计算的最小表示，并且可以通过在现实和经验验证的假设下**堆叠层**并在计算中注入噪声来实现。

1.  使用经验损失的信息分解，我们表明过拟合可以通过**限制存储在权重**中的信息内容来减少。【深度表征中不变性与解纠缠的出现，2018】。

**信息瓶颈**

### 信息瓶颈是由 Tishby 等人(1999)提出的。它是在一个假设下推出的，即它可以通过压缩可以在整个网络中传输的信息量来提取相关信息，从而强制对输入数据进行有记忆的压缩。

这种压缩的表示方式不仅降低了维度，还降低了数据的复杂性【深度学习和信息瓶颈原理，Tishby 2015】。这个想法是，一个网络就像通过一个瓶颈挤压信息一样，去除了噪音输入数据中无关的细节，只留下与一般概念最相关的特征。

Tishby 的发现让人工智能社区议论纷纷。“我相信信息瓶颈的想法在未来的深度神经网络研究中可能非常重要”，谷歌研究院的 Alex Alemi 说，他已经开发了新的近似方法，用于将信息瓶颈分析应用于大型深度神经网络。Alemi 说，瓶颈不仅可以作为理解为什么我们的神经网络像目前一样工作的理论工具，也可以作为构建网络新目标和架构的工具。

**潜在变量**

### 潜在变量是无法直接观察到的随机变量，但它为数据如何分布奠定了基础。潜变量也给了我们高维数据的低层表示。它们为我们提供了数据分布的抽象表示。

那么我们为什么需要潜在变量呢？

所有的机器学习都有一个明确的学习复杂概率分布的问题 *p(x)* 。而这些分布是**约束**，只有有限的一组高维数据点 *x* 从这个分布中抽取。

例如，为了了解猫的图像的概率分布，我们需要定义一个**分布，这个分布**可以模拟构成每幅图像的所有像素之间的复杂相关性。直接对这种分布建模是一项乏味且具有挑战性的任务，甚至是不可行的无限时间。我们可以引入一个(未观察到的)潜在变量 z，并为数据定义一个条件分布 *p(x | z)* ，而不是直接对 *p(x)* 建模，这被称为可能性。用概率术语来说， *z* 可以解释为一个连续的随机变量。对于猫图像的例子，z 可以包含猫的类型、颜色或形状的隐藏表示。

有了 z，我们可以在潜变量上进一步引入一个**先验分布** *p(z* )来计算观察变量和潜变量的联合分布 *p(x，z) = p(x|z)p(z)* 。

为了获得数据分布 *p(x)* 我们需要忽略潜在变量。

在此之前，我们可以使用贝叶斯定理计算**后验分布**。

后验分布允许我们推断给定观测值的潜在变量。

请注意，边缘化方程中的积分对于我们处理的大多数数据没有解析解，我们必须应用某种方法来推断后验概率。本质上，具有潜在变量的模型可用于执行生成数据的**生成过程**。这就是所谓的**生成模型**。

这意味着如果我们想要生成一个新的数据点，我们首先需要得到一个样本 *z~ p(z)* ，然后用它从条件分布 *p(x|z)* 中采样一个新的观测值 *x* 。在这样做的同时，我们还可以评估该模型是否为数据分布 *p(x)* 提供了良好的近似。

根据定义，包含潜在变量的数学模型是潜在变量模型。*这些潜在变量的维数比观察到的输入向量*低得多。这产生了数据的**压缩表示。**

潜在变量基本都是在信息瓶颈处发现的。最终，在这个信息瓶颈中，我们可以找到给定高维输入的抽象表示。**流形假设**陈述高维数据位于低维流形上。

既然我们知道了潜在变量是什么，在哪里可以找到它们，我们现在可以转向使用表征学习的各种框架。

各种学习框架中的表征学习

## 当前的机器和深度学习模型仍然容易与给定数据发生差异和纠缠(如前所述)。为了提高模型的准确性和性能，我们需要使用表示学习，以便模型能够产生不变性和不纠缠的结果。

在本节中，我们将看到表示学习如何在三种学习框架中提高模型的性能——监督学习、非监督学习和强化学习。

**监督学习**

### 监督学习是指 ML 或 DL 模型将输入 X 映射到输出 y。监督学习背后的思想是，学习算法通过优化来学习从输入到输出的映射，其中算法试图通过使用基本事实评估模型输出来校正自身。这个过程一直进行到优化函数达到称为**全局最小值的最小点。**

但有时，即使优化函数达到了全局最小值，它在新数据上仍然表现不佳，导致过度拟合。事实证明，监督学习模型不需要大量的数据来学习从输入到输出的映射，但它实际上需要学习到的特征。当学习到的特征被传递到监督学习算法中时，它可以将预测精度提高 17%。【利用无监督学习的有效特征学习来改进大规模开放在线课程中的预测模型】。

**无监督学习**

### 无监督学习是一种 ML，我们不关心标签，只关心观察本身。无监督学习不用于分类和回归，它通常用于发现潜在模式、聚类、去噪、离群点检测、数据分解等。

当处理数据 x 时，我们必须非常小心地选择什么特征 *z* ,以便提取的模式反映真实的数据。据观察，更多的数据不一定能为您提供良好的表示。我们需要小心地设计一个既灵活又有表现力的模型，这样提取的特征才能提供重要的信息。

当我们有了正确的特征之后，就可以更有把握地执行诸如聚类、分解或异常检测之类的任务。

**强化学习**

### 强化学习是另一种类型的 ML，涉及开发算法，这些算法应该在环境中采取行动，以最大化成功的机会。

监督学习和强化学习都使用输入和输出之间的映射。在监督学习中，提供给代理的反馈是执行任务的正确动作集，强化学习使用奖励和惩罚作为积极和消极行为的信号。重点是在探索(未知领域)和开发(现有知识)之间找到平衡。

体系结构

![reinforcement learning](img/28484a9f35839b386a8f0e8b948fe64d.png)

在这一部分，我们将探索深度学习模型——特别是，如何在给定类型的神经网络中提取表示。

## **多层感知器或 MLP**

感知器[Rosenblatt，1958，1962]是最简单的神经单元，它由一系列与权重相结合的输入组成，然后与基本事实进行比较。

### MLP，或多层感知器，是一种通过堆叠多层感知器单元构建的前馈神经网络。MLP 由三层节点组成——输入层、隐藏层和输出层。MLP 作为一个非常基本的人工神经网络，有时也被称为香草神经网络。

隐藏节点使用一系列非线性激活函数，使网络能够区分和分离不可线性分离的数据。这就是人工神经网络或 MLP 的力量发挥作用的地方，因为通用逼近定理指出“*神经网络在给予适当的权重时可以代表各种各样有趣的功能。另一方面，它们没有提供配重的构造，而仅仅是说明这种构造是可能的"*。

这个概念是表征学习和潜在变量的基础。该定理的核心是我们的目标，即找到能够代表整个数据基本分布的变量或所需权重，这样，当我们将这些变量或所需权重插入看不见的数据时，我们就能获得与原始数据中相似的接近结果。

简而言之，人工神经网络帮助我们从给定的数据集中提取有用的模式。

**CNN**

卷积神经网络或 CNN 是应用最广泛的神经网络之一，尤其是在图像处理和计算机视觉方面。卷积神经网络是一种使用卷积运算的网络，卷积运算是一种特殊的线性运算，它至少在一层中取代了一般的矩阵乘法[Ian Goodfellow，Deep learning]。CNN 是多层感知器的正则化版本——一种用于控制过度拟合的模型。图像或自然图像具有有限数量的对于平移不变的统计特性。

CNN 通过以下方式实现平移、旋转和失真不变性:

### 局部利用空间特征，

共享重量，

等变表示。

*   他们还采取了不同的方法来实现正则化:他们利用数据中的**层次模式**，并使用更小更简单的模式来组装更复杂的模式。 *CNN 通过跨网络同时共享信息来考虑这些属性，因为它们能够考虑特征的* ***位置*** 。
*   构建 CNN 是为了利用输入数据的**空间特征**——它们的联合概率分布、空间分布等等——**——T3，通过在相邻层的神经元之间实施局部连接模式。因此，该架构确保所学习的特征返回对空间局部输入模式的适当响应。**
*   堆叠许多这样的层会产生越来越全球化或在整个网络中共享的非线性滤波器。这些过滤器可以处理更大的向量空间，这样网络首先创建输入的小部分的**表示**，通过非线性过滤器的帮助将它们分离，然后从它们组装更大区域的表示。但是请记住，并不是所有的表示都很重要；我们必须利用信息瓶颈来**修剪**它们。

我们还必须记住，通过参数共享，CNN 已经能够非常显著地减少独特的模型参数，同时能够增加模型大小。这种参数共享使图层具有一种称为等变表示的属性，这意味着如果输入发生变化，输出也会以同样的方式发生变化。这样我们可以防止模型过度参数化和过度拟合。

CNN 为我们提供了准确的工具来提取有用的信息，更具体地说是空间信息或特征，以后可以根据您的需要进行删减和定制。因此，使用 CNN 成为构建**自动编码器**和**生成模型**的常见做法。

**Autoencoders**

观察到由深度网络学习的表示对数据的复杂噪声或差异不敏感。这在一定程度上可以归因于架构。例如，卷积层和最大池的使用可以显示出对变换的不敏感性。

因此，自动编码器是一种神经网络，可以被训练来完成表征学习的任务。自动编码器试图通过编码器和解码器的组合将其输入复制到输出。通常，使用再循环(Hilton 和麦克莱兰，1988)来训练自动编码器，这是一种基于比较输入网络的激活和重构输入的激活的学习算法(Ian Goodfellow，Deep learning)。

传统上，这些被用于降维或特征学习的任务[LeCun，1987；布尔兰和坎普，1988 年；Hilton 和 Zemel，1994]，但最近人们认识到，自动编码器和潜在变量模型(利用先验和后验分布概念的模型，如变分自动编码器)可用于构建生成模型，即可以生成新数据的模型。它通过压缩信息瓶颈中的信息来实现这一点，以便从整个数据集中仅提取重要的特征，并且这些提取的特征或表示可用于生成新数据。

### 简而言之，编码器是一种将输入减少为不同表示形式的功能，而解码器也是一种功能，可以将从编码器获得的表示形式转换回原始格式。

我们将讨论四种类型的自动编码器:

在完全自动编码器下

正则化自动编码器

稀疏自动编码器

降噪自动编码器

1.  **不完整的自动编码器**
2.  如果数据很简单，将输入复制到输出可能会很好，但是如果数据非常复杂呢？在这种情况下，我们的自动编码器将会不合适。我们感兴趣的是保存有用的信息。模型应该学习有用的特性。
3.  为了获得这样的功能，我们的模型应该是浅层网络的**组合——用于编码和解码——瓶颈是比原始输入**更小维度的**约束。**
4.  尺寸小于输入尺寸的自动编码器被称为**欠完整自动编码器**。通过根据重构误差来惩罚网络，模型可以学习和捕获最显著的特征。

我们知道神经网络能够学习非线性函数，像这样的自动编码器可以被认为是非线性 PCA。

**规则化自动编码器**

当瓶颈被限制为具有比原始输入更小的维度时，欠完整自动编码器可以学习显著特征。但是我们必须知道编码器和解码器模型的容量。据观察，如果编码器和解码器都被赋予太多的容量，这些自动编码器将无法学习到任何有用的东西。

过完备自动编码器面临着与欠完备自动编码器相同的问题。如果瓶颈与输入具有相同甚至更大的维度，他们什么也学不到。在过完备自动编码器的情况下，也容易将输入复制到输出，而不是学习重要的特性。

理想情况下，我们可以成功地训练任何结构的自动编码器，根据分布的复杂性选择瓶颈维数和编码器和解码器的容量。

规范化的自动编码器给了我们这样做的能力。他们使用损失函数，使该模型能够学习有用的特征，包括:

#### 表示的稀疏性，

表示的导数很小，

对噪声或缺失输入的鲁棒性。

正则化自动编码器的一个特殊特性是，它们可以是非线性的和过完备的，但仍然可以了解一些关于数据分布的有用信息。

1.  **稀疏自动编码器**
2.  已经观察到，当以鼓励稀疏性的方式学习表示时，在分类任务上获得改进的性能。这些方法包括激活函数、采样步骤和不同种类的惩罚的组合。
3.  稀疏自动编码器是一种已经被正则化以响应独特统计特征的模型。欠完整自动编码器将对每个观测使用整个网络。稀疏自动编码器将被迫根据输入数据选择性地激活网络区域。这消除了网络从输入数据中记忆特征的能力，并且由于一些区域被激活而其他区域没有被激活，因此网络学习有用的信息和特征。

本质上，有两种方法可以施加这种稀疏约束。这些术语是:

L1 正规化

#### KL-散度

两者都涉及测量每个训练批次的隐藏层激活，并在损失函数中增加一项以惩罚过度激活。

**去噪自动编码器**

1.  开发通用自动编码器的另一种方法是创建一个新的数据集，比如说从 X；其中 x’是 x 的损坏版本。通过这种方法，我们构建了一个模型，该模型能够从轻微损坏的输入数据中进行归纳，但仍将未损坏的数据作为我们的目标输出。
2.  本质上，我们的模型不能简单地开发一个记忆训练数据的映射，因为我们的输入和目标输出不再相同。相反，该模型学习用于将输入数据映射到低维流形的矢量场。如果这个流形准确地描述了自然数据，我们就有效地“抵消”了增加的噪声。

**生成模型**

#### 根据定义，生成建模是机器学习中的一项无监督学习任务，它涉及自动发现和学习输入数据中的表示或模式，以这种方式，该模型可用于**生成**新示例。所有这些模型都以某种方式表示多个变量的概率分布。创成式模型生成的分布是高维的。例如，在分类和回归等经典深度学习方法中，我们对一维输出进行建模，而在生成模型中，我们对高维输出进行建模。

本质上，我们对数据的**联合分布**进行建模，并且我们没有任何标签。

创成式模型的一些用途包括:

密度估计和异常值检测

### 数据压缩

从一个域映射到另一个域

语言翻译，文本到语音

基于模型的强化学习中的规划

*   表征学习
*   理解数据
*   生成模型的类型:
    *   自回归模型
*   RNN 和变形金刚语言模型，NADE，PixelCNN，WaveNet
*   潜在变量模型
*   易处理:例如可逆/基于流动的模型(RealNVP、Glow 等。)

棘手的问题:例如变化的自动编码器

*   隐式模型
    *   生成对抗网络及其变体
*   *   **玻尔兹曼机器**
    *   玻尔兹曼机器是由对称连接的节点组成的神经网络，它们自己决定是激活还是保持空闲。玻尔兹曼机器最初是作为学习二元向量上的任意概率分布的一般方法而引入的[Fahlman 等人，1983；阿克利等人，1985 年；辛顿等人，1984 年；辛顿和塞伊诺夫斯基，1986]。
*   玻尔兹曼机器使用一种简单的随机学习算法来寻找代表输入数据中复杂模式的特征。它们只有输入(可见)和隐藏节点。该模型中没有输出节点，这使得该模型具有不确定性——它不依赖于任何类型的输出。
    *   该图显示了十个节点，它们都是相互连接的。它们通常被称为**状态**。红色的代表隐藏的节点(h)，蓝色的代表可见的节点(v)。

玻尔兹曼机器的输入是相连的，这是它们根本不同的地方。所有这些节点相互交换信息，并自生成后续数据，因此称为生成式深度模型。

#### 在这里，可见节点是我们测量的，隐藏节点是我们不测量的。这使他们能够在自己之间共享信息，并自行生成后续数据。当我们输入数据时，这些节点学习所有的*参数、它们的模式以及数据*之间的相关性。然后，这个模型准备好根据它所学到的东西来监控和研究异常行为。

虽然该程序在具有大量特征检测层的网络中非常慢，但在具有单层特征检测器的网络中却很快，这种检测器被称为**受限玻尔兹曼机器**。

RBMs 是一个具有生成能力的两层人工神经网络。他们有能力学习一组输入的概率分布。RBM 是 Geoffrey Hinton 发明的，可用于降维、分类、回归、协同过滤、特征学习和主题建模。

RBM 是一类特殊的玻尔兹曼机器，它们受限于可见单元和隐藏单元之间的联系。与玻尔兹曼机器相比，这使得实现它们更容易。

如前所述，它们是一个两层的神经网络(一层是可见层，另一层是隐藏层)，这两层由一个完全**的二分图**连接。这意味着可见层中的每个节点都连接到隐藏层中的每个节点，但同一组中没有两个节点相互连接。这种限制允许比一般玻尔兹曼机器可用的训练算法更有效的训练算法，特别是基于梯度的对比发散算法。

RBM 可以通过梯度下降和反向传播过程进行微调。这样的网络被称为**深度信念网络**。虽然 RBM 偶尔会被使用，但深度学习社区中的大多数人已经开始用一般的敌对网络或变型自动编码器来取代它们的使用。

**可逆模型**

可逆模型(也称为规范化流)是一类**易处理的**生成模型。主要思想是通过使用可逆函数转换先验分布来近似数据分布。可逆模型通过对来自先验的样本应用**可逆**和**可微**变换 *f [θ] (z)* 来生成观测值。

为了解释一个表示或先验，我们需要将意义固定到特征编码的各个部分。也就是说，我们必须将高维特征向量分解成多个多维因素。这种不纠缠的映射应该是双射的，因此不纠缠的特征必须转换回原始表示。

这实质上意味着，对于哪个表示产生了给定的观察结果，没有模糊性，因为函数是易处理的。

#### 这也意味着我们可以通过反转函数来计算潜在变量，并将其应用于观察到的数据，因此我们可以准确地恢复产生潜在变量的原始变量。这使得表象**相关**和**完全确定**。

可逆模型的局限性:

潜在空间和观察必须具有相同的维度

潜在变量必须是连续的

观察必须是连续的或量化的

像这样的表达模型需要很多层，所以计算和内存开销都很大

结构僵化:模型设计缺乏灵活性

2.  **变分推理**
3.  我们的生成模型必须不仅仅是一个生成样本或做出预测的黑匣子。我们希望找到我们正在处理的数据的意义，一些更内在的和可解释的东西。我们希望以一种捕捉可解释性的方式构建模型，而不仅仅是追溯潜在变量到观察到的数据，以找到它们之间的相关性。我们希望我们的模型能够灵活地适应新的数据。
4.  易处理模型的局限性之一是，它是完全确定的，因为潜在变量是易处理的。这也意味着模型是严格刚性的。如果输入的新数据有异常值，它可能表现不好。这是真实世界数据的主要问题。
5.  我们需要建立足够灵活的模型来解决这类问题。我们不是要找到精确的易处理的潜在变量，而是要找到处于近似推理分布下的潜在变量。

这类模型属于**难处理模型**的范畴。

#### 在一个难以处理的模型中，后验概率分布近似为 p(z|x ),或者通过我们在神经网络中喜欢的变分推断，或者通过我们在概率图形模型中经常使用的马尔可夫链蒙特卡罗方法。蒙特卡罗方法被证明是计算昂贵的，因为它试图从整个分布中找到精确的样本，而变分推断试图使用优化技术用易处理的分布来近似后验分布[Ian Goodfellow，Deep learning]。

我们用变分后验 q (z|x)来逼近精确后验 p(z|x)。是**变分参数**，我们将对其进行优化，以使变分后验符合精确后验。变分后验概率是从均值和标准差分别为 0 和 1 的标准正态分布中采样得到的。样本是随机选取的，因此使得模型难以处理。

变分推理模型由两个重要部分组成:

易处理的组件，它们是可微分的和连续的，使模型具有确定性，

难以处理的组件，这使得模型具有灵活性和概率性。

因此，我们可以定义一种称为**重新参数化技巧的技术，**建议我们从单位高斯随机采样ε，然后将随机采样的ε移动潜在分布的均值μ，并通过潜在分布的方差σ对其进行缩放。由于和易于处理，我们使用反向传播技术来优化参数，从而找到精确的后验概率。

这种方法使模型更加灵活，我们可以使用任何架构来生成样本。

*   **KL 发散**
*   Kullback-Leibler 散度或相对熵是两个分布之间差异的度量。在概率统计中，我们经常会用更简单的近似分布来代替观察数据或复杂的分布。KL 散度帮助我们测量当我们选择一个近似值时，我们损失了多少信息。

VI 的一般思想是从一个易处理的分布族中取一个近似值 *q(z)* ，然后使这个近似值尽可能地接近真实的后验 *p(z|x)。*这通常通过最小化两种分布之间的 Kullback-Leibler (KL)散度来实现，定义如下:

这减少了对优化问题的推断[5]。 *q(z)* 和 *p(z|x)* 越相似，KL 散度越小。请注意，这个量不是数学意义上的距离，因为如果我们交换分布，它就不是对称的。此外，在我们的例子中，交换分布意味着我们需要得到关于 *p(z|x)* 的期望，这被认为是难以处理的。

现在，上面的方程在对数的分子中仍然有难以处理的后验概率。

#### 我们知道后验分布的公式:

因此，我们可以重写:

边际可能性 *logp(x)* 可以从期望值中取出，因为它不依赖于 *z* 。量 *F(q)* 就是所谓的证据下界(ELBO)。KL 总是≥ 0，因此它代表证据的下限。ELBO 越接近边际似然，变分近似就越接近真实的后验分布。复杂的推理问题被简化为更简单的优化问题。

**变型自动编码器**

变分自动编码器，或 VAE[金玛，2013；Rezende 等人，2014]，是一个生成模型，它使用学习的近似推理生成连续的潜在变量[Ian Goodfellow，Deep learning]。它是自动编码器的改进版本。

自动编码器的局限性在于，它们学会了生成紧凑的表示并很好地重建它们的输入，但是除了像去噪自动编码器这样的一些应用之外，它们相当有限。

自动编码器的基本问题是，它们将输入转换到的潜在空间以及编码向量所在的位置可能不连续，或者不允许简单的插值。

另一方面，变分自动编码器有一个从根本上区别于普通自动编码器的独特属性，正是这个属性使它们对生成式建模如此有用:*它们的潜在空间是设计成连续的，允许简单的随机采样和插值。*

#### 它通过使其编码器不输出随机潜在变量，而是输出两个向量来实现这一点:一个均值向量μ和另一个标准差向量σ(这将是模型的优化参数)。因为我们假设先验服从正态分布，我们将输出两个向量来描述潜在状态分布的均值和方差。

直观地说，均值向量控制着输入的编码应该集中在哪里，而标准差控制着“区域”，即编码可以偏离均值多少。

通过构建我们的编码器模型来输出一系列可能的值，我们将从这些值中随机采样以馈入我们的解码器模型，我们本质上是在实施一个连续、平滑的潜在空间表示。对于潜在分布的任何采样，我们期望我们的解码器模型能够准确地重建输入。因此，在潜在空间中彼此接近的值应该对应于非常相似的重构。

我们理想中想要的是编码，所有的编码彼此尽可能接近，同时仍然是不同的，允许平滑插值，并支持新样本的构造。

这个取样过程需要特别注意。

训练模型时，我们需要能够使用反向传播计算网络中每个参数相对于最终输出损耗的关系。我们不能对随机抽样过程这样做。VAE 实现这一点的方法是通过**重新参数化技巧**。使用这种技术，我们可以优化精确分布的参数，同时仍然保持从该分布中随机采样的能力。

变分自动编码器在生成样本方面取得了一些很好的结果，是生成建模的最新方法之一。它的主要缺点是从模型生成的样本有时会很模糊。原因的最明显结论是最大似然的内在效应，它使 DKL(pdata || pmodel)最小化。这实质上意味着模型将高概率分配给训练集中出现的点，以及可能导致模糊图像的其他点[Ian Goodfellow，Deep learning]。

**甘斯**

[生成对抗网络或 GANs](/web/20221201174108/https://neptune.ai/blog/6-gan-architectures)【good fellow 等。，2014]是另一种类型的生成模型，利用了不同的方法。

生成性对抗网络是基于博弈论的，其中两个网络——生成者和对抗者——相互竞争。生成器 Gis 是一个定向潜在变量模型，其确定性地从 z 生成样本 x，鉴别器 D 是一个函数，其工作是**从真实数据集和生成器**中区分样本。

生成器网络直接产生样本，而它的对手鉴别器网络试图通过返回一个概率值来区分从训练数据中提取的样本和从生成器中提取的样本，该概率值表示该样本是真实的训练样本还是从模型中提取的假样本。

GAN 可以与强化学习进行比较，其中生成器从鉴别器接收奖励信号，让它知道生成的数据是否准确。

在训练期间，生成器试图更好地生成看起来真实的图像，而鉴别器则训练更好地将这些图像分类为假图像。当鉴别器不再能够区分真实图像和赝品时，该过程达到平衡。

#### 生成器模型将固定长度的随机向量作为输入，并生成从高斯分布中随机抽取的样本，该向量用于生成过程的种子。训练后，这个多维向量空间中的点将对应真实数据中的点，形成数据分布的**压缩表示**作为潜在变量。这个过程是有差别的和连续的。

在核心处，该模型像任何其他生成模型一样发现潜在的特征，然后可以以不同的方式利用这些特征来产生新的图像样本。

在 GANs 的情况下，生成器模型将意义应用于所选潜在空间中的点，使得从潜在空间中提取的新点可以被提供给生成器模型作为输入，并用于生成新的不同的输出示例。

另一方面，鉴别器模型将来自领域的示例作为输入(真实的或生成的)，并预测真实或虚假的二进制类别标签(生成的)。真实的例子来自训练数据集。生成的示例由生成器模型输出。

鉴别器是一个正常的分类模型。

在训练期间，鉴别器应该识别真实图像，因此 *D(x)* 应该接近 1，并且它还应该能够识别来自 z 的伪生成图像，因此 *D(G(z))* 应该接近 0。

根据这个等式，我们希望最大化鉴别器函数，同时最小化发生器函数。

如前所述，GAN 是一个微分模型，这意味着发电机模型中的参数可以使用梯度下降进行自我更新。同时，鉴别器模型使用梯度上升最大化损失函数。

在训练过程之后，鉴别器模型被丢弃，因为我们对生成器感兴趣。

**{XXX}2vec 型号**

到目前为止，我们一直关注非结构化数据以及如何从中提取表示。但是，对结构化数据执行机器学习是复杂的，因为这种数据没有矢量形式。出现了多种方法来构造结构化数据的矢量表示，从核和距离方法到递归、递归和卷积神经网络。其中一个过程被称为**嵌入**。

嵌入是从输入数据中学习另一组向量值，并通过另一组向量表示原始实际向量的过程。在神经网络的上下文中，嵌入是低维的，*学习离散变量的连续向量表示*。

神经网络嵌入是有用的，因为它们可以减少分类变量的维度，并在转换的空间中有意义地表示类别。

如前所述，传统的机器学习主要关注如何解决确定性问题的分类或回归等问题，使用[手动设计的数据表示](https://web.archive.org/web/20221201174108/https://www.sciencedirect.com/science/article/pii/S2405918816300459) [Bengio 等人，2013]。相比之下，表征学习首先关注的是获得矢量表征的挑战，这样先前的任务就变得容易解决[Bengio 等人，2013]。这种类型的方法有助于处理结构化数据，即序列、树和图，其中矢量表示不立即可用[Hamilton 等人，2017b]。

我想提一下经常使用嵌入的两个领域，一个是语言——自然语言处理——另一个是推荐系统。

### 嵌入技术包括:

矩阵分解

Word2Vec

xxx2vec

**矩阵分解**用于去除相似的特征向量。包括项目向量、用户向量、用户偏好向量。它主要用于推荐系统。

**word2vec** 对于做自然语言处理的人来说自然是再熟悉不过了。顾名思义，word2vec 将单词编码成向量。例如，单词“腹泻”被编码成[0.4442，0.11345]。

Word2Vec 是一种用于从大量文本语料库中以无监督的方式学习语义知识的模型，广泛应用于自然语言处理(NLP)。输入被一次性编码，然后输入到神经网络中，然后它使用这些向量来表征单词的语义信息，通过嵌入空间(潜在空间)学习文本，使语义相似的单词在该空间中非常接近。

1.  在 Word2Vec 中，主要有两种模型:Skip-Gram 和连续词袋(CBOW)。从直观的理解来说，Skip-Gram 是一个给定的输入词，用来预测上下文。CBOW 是在给定上下文的情况下预测输入单词。
2.  2vec 有许多变体，也称为 xxx2vec:
3.  Node2vec

Struc2vec

Metapath2vec

这些都是标题为**图结构上的表示学习**的变体。这些算法被设计成保留向量空间中的信息。邻近关系在邻近向量网络中是重要的。所有信息都被赋予一定的权重，这些权重相互聚集在一起。因此，如果我们能够提取重要的特征或表示，那么我们随后的任务就变得更容易和更准确。

未来进展面临的挑战

1.  创成式建模是一种直观的方法，用于在缺少数据的情况下生成样本。大多数情况下，这些场景发生在医学扫描或其他非常隐私的数据中，不应该向公众开放。因此，使用生成模型，我们可以创建数据样本，这些样本可以由各个领域的专家进行评估。由于所有深度学习模型都严重依赖输入数据的**表示**，我们必须确保:
2.  数据应该尽可能的干净。例如，我们必须确保数据由该领域的专家精心管理，以便提取的表示非常有效并生成良好的样本。
3.  **模型的架构不应过于宏大或平淡无奇。**

对于复杂的数据，使用表达模型来避免欠拟合。同样，对于简单数据，使用简单模型以避免过度拟合。

由于生成模型在生成超高清样本方面并不完美，**不同的模型应该结合起来，以利用它们的互补优势。**

## 我希望你能从这篇文章中学到一些新东西。感谢您的阅读！

Generative modelling is an intuitive method for generating samples when there is a lack of data to work with. Most of the time these scenarios happen in medical scans, or other data that is very private, and should not be given access to the public. So using generative models we can create samples of data, and those can be evaluated by the experts in the respective fields. Since all deep learning models rely heavily on **representation** of the input data, we have to make sure that: 

1.  **Data should be as clean as it can be.** For instance, we must make sure the data is carefully curated by experts in the field, so that representation extracted is highly effective and generates good samples.
2.  **Architecture of the model should not be overwhelming or underwhelming. **
3.  **For complex data use an expressive model to avoid underfitting.** Similarly for simple data use a simple model to avoid overfitting.
4.  Since generative models are not so perfect in terms of producing ultra high definition samples, **different models should be combined to take advantage of their complementary strengths.**

**I hope you learned something new from this article. Thank you for reading!**