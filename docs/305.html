<html>
<head>
<title>Keras Loss Functions: Everything You Need to Know </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Keras损失函数:你需要知道的一切</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/keras-loss-functions#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/keras-loss-functions#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>你已经在Keras中创建了一个深度学习模型，你准备了数据，现在你想知道你应该为你的问题选择哪个损失。</p>



<p>我们一会儿会讲到，但首先什么是损失函数？</p>



<p>在深度学习中，计算损失以获得关于模型权重的梯度，并通过反向传播相应地更新那些权重。在每次迭代之后，计算损耗并更新网络，直到模型更新没有在期望的评估度量中带来任何改进。</p>



<p>因此，当您在机器学习项目的(长时间)期间在验证集上继续使用相同的评估指标(如f1得分或AUC)时，可以改变、调整和修改损失，以获得最佳的评估指标性能。</p>



<p>您可以像考虑模型架构或优化器一样考虑损失函数，在选择它时进行一些思考是很重要的。在这篇文章中，我们将探讨:</p>


<div class="custom-point-list">
<ul><li><strong>Keras中可用的损失函数</strong>以及如何使用它们，</li><li>如何<strong>在Keras中定义自己的自定义损失函数</strong>，</li><li>如何添加<strong>样品称量</strong>以产生观察敏感损失，</li><li>如何避免<strong>南斯在</strong>的损失，</li><li><strong>如何通过绘图和回调监控损失函数</strong>。</li></ul>
</div>


<p>让我们开始吧！</p>



<h2>Keras损失函数101</h2>



<p>在Keras中，损失函数在编译阶段传递，如下所示。</p>



<p>在这个例子中，我们通过创建loss类的一个实例来定义loss函数。使用类是有利的，因为您可以传递一些附加参数。</p>



<pre class="hljs"><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers

model = keras.Sequential()
model.add(layers.Dense(<span class="hljs-number">64</span>, kernel_initializer=<span class="hljs-string">'uniform'</span>, input_shape=(<span class="hljs-number">10</span>,)))
model.add(layers.Activation(<span class="hljs-string">'softmax'</span>))

loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-keyword">True</span>)
model.compile(loss=loss_function, optimizer=<span class="hljs-string">'adam'</span>)
</pre>



<p>如果您想使用Keras内置的损失函数而不指定任何参数，您可以只使用字符串别名，如下所示:</p>



<pre class="hljs">model.compile(loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>)
</pre>



<p>你可能想知道，如何决定使用哪个损失函数？</p>



<p>Keras中有各种损失函数。其他时候，您可能需要实现自己的定制损失函数。</p>



<p>让我们深入所有这些场景。</p>



<h2>Keras中有哪些损失函数？</h2>



<h3><strong>二元分类</strong></h3>



<p>当解决只涉及两个类的问题时，二元分类损失函数开始发挥作用。例如，当预测信用卡交易中的欺诈时，交易要么是欺诈的，要么不是。</p>



<h4>二元交叉熵</h4>



<p>二元交叉熵将计算预测类和真实类之间的交叉熵损失。默认情况下，使用<em> sum_over_batch_size </em>缩减。这意味着损失将返回批次中每个样本损失的平均值。</p>



<pre class="hljs">y_true = [[<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.8</span>],[<span class="hljs-number">0.3</span>, <span class="hljs-number">0.7</span>],[<span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>]]
y_pred = [[<span class="hljs-number">0.6</span>, <span class="hljs-number">0.4</span>], [<span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>],[<span class="hljs-number">0.6</span>, <span class="hljs-number">0.4</span>],[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>]]
bce = tf.keras.losses.BinaryCrossentropy(reduction=<span class="hljs-string">'sum_over_batch_size'</span>)
bce(y_true, y_pred).numpy()
</pre>



<p>总和减少意味着损失函数将返回批次中每个样本损失的总和。</p>



<pre class="hljs">bce = tf.keras.losses.BinaryCrossentropy(reduction=<span class="hljs-string">'sum'</span>)
bce(y_true, y_pred).numpy()
</pre>



<p>将减少值设为none将返回每样本损失的完整数组。</p>



<pre class="hljs">bce = tf.keras.losses.BinaryCrossentropy(reduction=<span class="hljs-string">'none'</span>)
bce(y_true, y_pred).numpy()
array([<span class="hljs-number">0.9162905</span> , <span class="hljs-number">0.5919184</span> , <span class="hljs-number">0.79465103</span>, <span class="hljs-number">1.0549198</span> ], dtype=float32)</pre>



<p>在二元分类中，使用的激活函数是sigmoid激活函数。它将输出限制在0到1之间。</p>



<h3><strong>多类分类</strong></h3>



<p>涉及多个类别预测的问题使用不同的损失函数。在这一节中，我们将看几个例子:</p>



<h4>范畴交叉熵</h4>



<p>CategoricalCrossentropy还计算真实类和预测类之间的交叉熵损失。标签以<em> one_hot </em>格式给出。</p>



<pre class="hljs">cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()</pre>



<h4>稀疏分类交叉熵</h4>



<p>如果有两个或更多的类，并且标签是整数，那么应该使用SparseCategoricalCrossentropy。</p>



<pre class="hljs">y_true = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>,<span class="hljs-number">2</span>]
y_pred = [[<span class="hljs-number">0.05</span>, <span class="hljs-number">0.95</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.1</span>],[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.1</span>]]
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
</pre>



<h4>毒物流失</h4>



<p>您还可以使用泊松类来计算中毒损失。如果数据集来自泊松分布，例如呼叫中心每小时接到的电话数量，这是一个很好的选择。</p>



<pre class="hljs">y_true = [[<span class="hljs-number">0.1</span>, <span class="hljs-number">1.</span>,<span class="hljs-number">0.8</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.9</span>,<span class="hljs-number">0.1</span>],[<span class="hljs-number">0.2</span>, <span class="hljs-number">0.7</span>,<span class="hljs-number">0.1</span>],[<span class="hljs-number">0.3</span>, <span class="hljs-number">0.1</span>,<span class="hljs-number">0.6</span>]]
y_pred = [[<span class="hljs-number">0.6</span>, <span class="hljs-number">0.2</span>,<span class="hljs-number">0.2</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.6</span>,<span class="hljs-number">0.2</span>],[<span class="hljs-number">0.7</span>, <span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>],[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.1</span>,<span class="hljs-number">0.1</span>]]
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
</pre>



<h4>库尔巴克-莱布勒发散损失</h4>



<p>可以使用KLDivergence类来计算相对熵。根据PyTorch的官方文件:</p>



<p><strong> <em> KL散度</em> </strong> <em>对于连续分布来说是一种有用的距离度量，在对(离散采样的)连续输出分布空间执行直接回归时通常很有用。</em></p>



<pre class="hljs">y_true = [[<span class="hljs-number">0.1</span>, <span class="hljs-number">1.</span>,<span class="hljs-number">0.8</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.9</span>,<span class="hljs-number">0.1</span>],[<span class="hljs-number">0.2</span>, <span class="hljs-number">0.7</span>,<span class="hljs-number">0.1</span>],[<span class="hljs-number">0.3</span>, <span class="hljs-number">0.1</span>,<span class="hljs-number">0.6</span>]]
y_pred = [[<span class="hljs-number">0.6</span>, <span class="hljs-number">0.2</span>,<span class="hljs-number">0.2</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.6</span>,<span class="hljs-number">0.2</span>],[<span class="hljs-number">0.7</span>, <span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>],[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.1</span>,<span class="hljs-number">0.1</span>]]
kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
</pre>



<p>在多类问题中，使用的激活函数是softmax函数。</p>



<h3><strong>物体检测</strong></h3>



<h4>焦点损失</h4>



<p>在涉及<a href="/web/20221027132202/https://neptune.ai/blog/how-to-deal-with-imbalanced-classification-and-regression-data" target="_blank" rel="noreferrer noopener">不平衡数据</a>的分类问题和物体检测问题中，可以使用<a href="https://web.archive.org/web/20221027132202/https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noreferrer noopener nofollow">焦点损失。</a>损失引入了对交叉熵标准的调整。</p>



<figure class="wp-block-image"><img src="../Images/3d2d84f6ae0d4d26b063df788a93d49b.png" alt="" data-lazy-src="https://web.archive.org/web/20221027132202/https://lh6.googleusercontent.com/OuLt0i7DFHU7VP-Gg3Iyto1_9erIxijkSlgtdXh9qZiFIQusgm-1QPLTnrXpgrrpVYXCH2tEhsFpUQmGJKysj91FPtF7Vn2M8ZEGEYMS3-1Gg9abvR7OZy5T2XuPm9Yk4vlk0c30?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20221027132202im_/https://lh6.googleusercontent.com/OuLt0i7DFHU7VP-Gg3Iyto1_9erIxijkSlgtdXh9qZiFIQusgm-1QPLTnrXpgrrpVYXCH2tEhsFpUQmGJKysj91FPtF7Vn2M8ZEGEYMS3-1Gg9abvR7OZy5T2XuPm9Yk4vlk0c30"/><noscript><img data-lazy-fallback="1" src="../Images/3d2d84f6ae0d4d26b063df788a93d49b.png" alt="" data-original-src="https://web.archive.org/web/20221027132202im_/https://lh6.googleusercontent.com/OuLt0i7DFHU7VP-Gg3Iyto1_9erIxijkSlgtdXh9qZiFIQusgm-1QPLTnrXpgrrpVYXCH2tEhsFpUQmGJKysj91FPtF7Vn2M8ZEGEYMS3-1Gg9abvR7OZy5T2XuPm9Yk4vlk0c30"/></noscript></figure>



<p>这是通过改变其形状来实现的，其方式是分配给分类良好的样本的损失是向下加权的。这确保了模型能够平等地从少数和多数类学习。</p>



<p>随着正确类别的置信度增加，通过缩放在零处衰减的因子来缩放交叉熵损失。缩小因子在训练时对无挑战样本的贡献进行加权，并关注有挑战的样本。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> tensorflow_addons <span class="hljs-keyword">as</span> tfa

y_true = [[<span class="hljs-number">0.97</span>], [<span class="hljs-number">0.91</span>], [<span class="hljs-number">0.03</span>]]
y_pred = [[<span class="hljs-number">1.0</span>], [<span class="hljs-number">1.0</span>], [<span class="hljs-number">0.0</span>]]
sfc = tfa.losses.SigmoidFocalCrossEntropy()
sfc(y_true, y_pred).numpy()
array([<span class="hljs-number">0.00010971</span>, <span class="hljs-number">0.00329749</span>, <span class="hljs-number">0.00030611</span>], dtype=float32)</pre>



<h4>并集上的广义交集</h4>



<p>也可以使用来自TensorFlow add on的【Union上的广义交集损失。并集上的交集(IoU)是对象检测问题中非常常见的度量。然而，IoU在涉及非重叠边界框的问题上不是很有效。</p>



<p>引入了Union上的广义交集来解决IoU面临的这一挑战。它确保通过保持IoU的比例不变属性、将比较对象的形状属性编码到区域属性中、以及确保在对象重叠的情况下与IoU有很强的相关性来实现概括。</p>



<pre class="hljs">gl = tfa.losses.GIoULoss()
boxes1 = tf.constant([[<span class="hljs-number">4.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">7.0</span>, <span class="hljs-number">5.0</span>], [<span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>, <span class="hljs-number">10.0</span>, <span class="hljs-number">7.0</span>]])
boxes2 = tf.constant([[<span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">6.0</span>, <span class="hljs-number">8.0</span>], [<span class="hljs-number">14.0</span>, <span class="hljs-number">14.0</span>, <span class="hljs-number">15.0</span>, <span class="hljs-number">15.0</span>]])
loss = gl(boxes1, boxes2)
</pre>



<h3><strong>回归</strong></h3>



<p>在回归问题中，你必须计算预测值和真实值之间的差异，但通常有许多方法可以做到这一点。</p>



<h4>均方误差</h4>



<p>MeanSquaredError类可用于计算预测值和真实值之间的均方误差。</p>



<pre class="hljs">y_true = [<span class="hljs-number">12</span>, <span class="hljs-number">20</span>, <span class="hljs-number">29.</span>, <span class="hljs-number">60.</span>]
y_pred = [<span class="hljs-number">14.</span>, <span class="hljs-number">18.</span>, <span class="hljs-number">27.</span>, <span class="hljs-number">55.</span>]
mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
</pre>



<p>当您希望较大的误差比较小的误差受到更多的惩罚时，请使用均方误差。</p>



<h4>平均绝对百分比误差</h4>



<p>使用以下函数计算平均绝对百分比误差。</p>



<figure class="wp-block-image"><img src="../Images/b82e6e7342ff447ad70b3f0853e7f95e.png" alt="" title="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program." data-lazy-src="https://web.archive.org/web/20221027132202/https://latex.codecogs.com/gif.latex?large_loss_=_100_*_abs(y_true_-_y_pred)_/_y_true&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20221027132202im_/https://latex.codecogs.com/gif.latex?%5Clarge%20loss%20%3D%20100%20*%20abs%28y_%7Btrue%7D%20-%20y_%7Bpred%7D%29%20/%20y_%7Btrue%7D"/><noscript><img data-lazy-fallback="1" src="../Images/b82e6e7342ff447ad70b3f0853e7f95e.png" alt="" title="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program." data-original-src="https://web.archive.org/web/20221027132202im_/https://latex.codecogs.com/gif.latex?%5Clarge%20loss%20%3D%20100%20*%20abs%28y_%7Btrue%7D%20-%20y_%7Bpred%7D%29%20/%20y_%7Btrue%7D"/></noscript></figure>



<p>它的计算如下所示。</p>



<pre class="hljs">y_true = [<span class="hljs-number">12</span>, <span class="hljs-number">20</span>, <span class="hljs-number">29.</span>, <span class="hljs-number">60.</span>]
y_pred = [<span class="hljs-number">14.</span>, <span class="hljs-number">18.</span>, <span class="hljs-number">27.</span>, <span class="hljs-number">55.</span>]
mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
</pre>



<p>当你想要一个可以直观解释的损失时，考虑使用这个损失。人们很容易理解百分比。这种损失对异常值也是稳健的。</p>



<h4>均方对数误差</h4>



<p>均方对数误差可以用下面的公式计算:</p>



<figure class="wp-block-image"><img src="../Images/cbf402f027fe8c342cc2b5b5716e39f1.png" alt="" title="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program." data-lazy-src="https://web.archive.org/web/20221027132202/https://latex.codecogs.com/gif.latex?large_loss_=_square(log(y_true___1_)_%u2014_log(y_pred___1_))&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20221027132202im_/https://latex.codecogs.com/gif.latex?%5Clarge%20loss%20%3D%20square%28log%28y_%7Btrue%7D%20+%201.%29%20%u2014%20log%28y_%7Bpred%7D%20+%201.%29%29"/><noscript><img data-lazy-fallback="1" src="../Images/cbf402f027fe8c342cc2b5b5716e39f1.png" alt="" title="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program." data-original-src="https://web.archive.org/web/20221027132202im_/https://latex.codecogs.com/gif.latex?%5Clarge%20loss%20%3D%20square%28log%28y_%7Btrue%7D%20+%201.%29%20%u2014%20log%28y_%7Bpred%7D%20+%201.%29%29"/></noscript></figure>



<p>下面是同样的一个实现:</p>



<pre class="hljs">y_true = [<span class="hljs-number">12</span>, <span class="hljs-number">20</span>, <span class="hljs-number">29.</span>, <span class="hljs-number">60.</span>]
y_pred = [<span class="hljs-number">14.</span>, <span class="hljs-number">18.</span>, <span class="hljs-number">27.</span>, <span class="hljs-number">55.</span>]
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
</pre>



<p>均方对数误差对低估的惩罚大于高估。当您不想惩罚大的错误时，这是一个很好的选择，因此，它对异常值是健壮的。</p>



<h4>余弦相似损失</h4>



<p>如果您对计算真实值和预测值之间的余弦相似性感兴趣，您可以使用cosine similarity类。其计算方法如下:</p>



<figure class="wp-block-image"><img src="../Images/f9519a074f055e1f3a6ddbce106e6a15.png" alt="" title="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program." data-lazy-src="https://web.archive.org/web/20221027132202/https://latex.codecogs.com/gif.latex?large_loss_=_-sum(l2_norm(y_true)_*_l2_norm(y_pred))&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20221027132202im_/https://latex.codecogs.com/gif.latex?%5Clarge%20loss%20%3D%20-sum%28l2_%7Bnorm%7D%28y_%7Btrue%7D%29%20*%20l2_%7Bnorm%7D%28y_%7Bpred%7D%29%29"/><noscript><img data-lazy-fallback="1" src="../Images/f9519a074f055e1f3a6ddbce106e6a15.png" alt="" title="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program." data-original-src="https://web.archive.org/web/20221027132202im_/https://latex.codecogs.com/gif.latex?%5Clarge%20loss%20%3D%20-sum%28l2_%7Bnorm%7D%28y_%7Btrue%7D%29%20*%20l2_%7Bnorm%7D%28y_%7Bpred%7D%29%29"/></noscript></figure>



<p><strong>结果是一个介于-1和1 </strong>之间的数字。0表示正交性，而接近-1的值表示有很大的相似性。</p>



<pre class="hljs">y_true = [[<span class="hljs-number">12</span>, <span class="hljs-number">20</span>], [<span class="hljs-number">29.</span>, <span class="hljs-number">60.</span>]]
y_pred = [[<span class="hljs-number">14.</span>, <span class="hljs-number">18.</span>], [<span class="hljs-number">27.</span>, <span class="hljs-number">55.</span>]]
cosine_loss = tf.keras.losses.CosineSimilarity(axis=<span class="hljs-number">1</span>)
cosine_loss(y_true, y_pred).numpy()
</pre>



<h4>对数损失</h4>



<p>LogCosh类计算预测误差的双曲余弦的对数。</p>



<figure class="wp-block-image"><img src="../Images/e13ae949ef738407a7c207726e4a83d9.png" alt="" title="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program." data-lazy-src="https://web.archive.org/web/20221027132202/https://latex.codecogs.com/gif.latex?large_logcosh_=_log((exp(x)___exp(-x))/2)&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20221027132202im_/https://latex.codecogs.com/gif.latex?%5Clarge%20logcosh%20%3D%20log%28%28exp%28x%29%20+%20exp%28-x%29%29/2%29"/><noscript><img data-lazy-fallback="1" src="../Images/e13ae949ef738407a7c207726e4a83d9.png" alt="" title="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program." data-original-src="https://web.archive.org/web/20221027132202im_/https://latex.codecogs.com/gif.latex?%5Clarge%20logcosh%20%3D%20log%28%28exp%28x%29%20+%20exp%28-x%29%29/2%29"/></noscript></figure>



<p>下面是它作为独立函数的实现。</p>



<pre class="hljs">y_true = [[<span class="hljs-number">12</span>, <span class="hljs-number">20</span>], [<span class="hljs-number">29.</span>, <span class="hljs-number">60.</span>]]
y_pred = [[<span class="hljs-number">14.</span>, <span class="hljs-number">18.</span>], [<span class="hljs-number">27.</span>, <span class="hljs-number">55.</span>]]
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()
</pre>



<p>对数损失的工作原理类似于均方误差，但不会受到偶然的严重错误预测的强烈影响。—张量流文档</p>



<h4>胡伯损失</h4>



<p>对于对异常值不太敏感的回归问题，使用<a href="https://web.archive.org/web/20221027132202/https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber" target="_blank" rel="noreferrer noopener nofollow"> Huber损失</a>。</p>



<pre class="hljs">y_true = [<span class="hljs-number">12</span>, <span class="hljs-number">20</span>, <span class="hljs-number">29.</span>, <span class="hljs-number">60.</span>]
y_pred = [<span class="hljs-number">14.</span>, <span class="hljs-number">18.</span>, <span class="hljs-number">27.</span>, <span class="hljs-number">55.</span>]
h = tf.keras.losses.Huber()
h(y_true, y_pred).numpy()
</pre>



<h3><strong>学习嵌入</strong></h3>



<h4>三重损失</h4>



<p>你也可以通过TensorFlow插件用半硬负挖掘计算<a href="https://web.archive.org/web/20221027132202/https://arxiv.org/abs/1503.03832" target="_blank" rel="noreferrer noopener nofollow">三重态损失</a>。该损失促使具有相同标签的嵌入对之间的正距离小于最小负距离。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> tensorflow_addons <span class="hljs-keyword">as</span> tfa

model.compile(optimizer=<span class="hljs-string">'adam'</span>,
              loss=tfa.losses.TripletSemiHardLoss(),
              metrics=[<span class="hljs-string">'accuracy'</span>])
</pre>



<h2>在Keras中创建自定义损失函数</h2>



<p>有时候没有好的损失，或者你需要做一些修改。让我们来学习如何做到这一点。</p>



<p>可通过定义一个将真实值和预测值作为必需参数的函数来创建自定义损失函数。该函数应该返回一个损失数组。然后可以在编译阶段传递该函数。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">custom_loss_function</span><span class="hljs-params">(y_true, y_pred)</span>:</span>
   squared_difference = tf.square(y_true - y_pred)
   <span class="hljs-keyword">return</span> tf.reduce_mean(squared_difference, axis=<span class="hljs-number">-1</span>)

model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=custom_loss_function)
</pre>



<p>让我们看看如何将这个自定义损失函数应用于一组预测值和真实值。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

y_true = [<span class="hljs-number">12</span>, <span class="hljs-number">20</span>, <span class="hljs-number">29.</span>, <span class="hljs-number">60.</span>]
y_pred = [<span class="hljs-number">14.</span>, <span class="hljs-number">18.</span>, <span class="hljs-number">27.</span>, <span class="hljs-number">55.</span>]
cl = custom_loss_function(np.array(y_true),np.array(y_pred))
cl.numpy()
</pre>



<h2>使用Keras损失权重</h2>



<p>在训练过程中，可以通过观察或样本来衡量损失函数。权重可以是任意的，但是典型的选择是类权重(标签的分布)。每个观测值按其所属类的分数进行加权(反向)，因此在计算损失时，少数类观测值的损失更为重要。</p>



<p>做到这一点的方法之一是在训练过程中传递类权重。</p>



<p>使用包含每个类的权重的字典来传递权重。您可以使用<a href="https://web.archive.org/web/20221027132202/https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_sample_weight.html" target="_blank" rel="noreferrer noopener nofollow"> Scikit-learn </a>计算重量，或者根据您自己的标准计算重量。</p>



<pre class="hljs">weights = { <span class="hljs-number">0</span>:<span class="hljs-number">1.01300017</span>,<span class="hljs-number">1</span>:<span class="hljs-number">0.88994364</span>,<span class="hljs-number">2</span>:<span class="hljs-number">1.00704935</span>, <span class="hljs-number">3</span>:<span class="hljs-number">0.97863318</span>,      <span class="hljs-number">4</span>:<span class="hljs-number">1.02704553</span>, <span class="hljs-number">5</span>:<span class="hljs-number">1.10680686</span>,<span class="hljs-number">6</span>:<span class="hljs-number">1.01385603</span>,<span class="hljs-number">7</span>:<span class="hljs-number">0.95770152</span>, <span class="hljs-number">8</span>:<span class="hljs-number">1.02546573</span>,
               <span class="hljs-number">9</span>:<span class="hljs-number">1.00857287</span>}
model.fit(x_train, y_train,verbose=<span class="hljs-number">1</span>, epochs=<span class="hljs-number">10</span>,class_weight=weights)
</pre>



<p>第二种方法是在编译阶段传递这些权重。</p>



<pre class="hljs">weights = [<span class="hljs-number">1.013</span>, <span class="hljs-number">0.889</span>, <span class="hljs-number">1.007</span>, <span class="hljs-number">0.978</span>, <span class="hljs-number">1.027</span>,<span class="hljs-number">1.106</span>,<span class="hljs-number">1.013</span>,<span class="hljs-number">0.957</span>,<span class="hljs-number">1.025</span>, <span class="hljs-number">1.008</span>]

model.compile(optimizer=tf.keras.optimizers.SGD(),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              loss_weights=weights,
              metrics=[<span class="hljs-string">'accuracy'</span>])</pre>



<h2>如何监控Keras损失函数[示例]</h2>



<p>当模型正在训练时，在训练和验证集上监视损失函数通常是一个好主意。查看这些学习曲线是模型训练过度拟合或其他问题的良好指示。</p>





<p>如何做到这一点有两个主要选择。</p>



<h3><strong>使用控制台日志监控Keras丢失</strong></h3>



<p>记录和查看损失的最快和最简单的方法是简单地将它们打印到控制台上。</p>



<pre class="hljs"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / <span class="hljs-number">255.0</span>, x_test / <span class="hljs-number">255.0</span>

model = tf.keras.models.Sequential([
   tf.keras.layers.Flatten(input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)),
   tf.keras.layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">'relu'</span>),
   tf.keras.layers.Dropout(<span class="hljs-number">0.2</span>),
   tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)
])

model.compile(optimizer=<span class="hljs-string">'sgd'</span>,
             loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>,
             metrics=[<span class="hljs-string">'accuracy'</span>])

model.fit(x_train, y_train,verbose=<span class="hljs-number">1</span>, epochs=<span class="hljs-number">10</span>)</pre>





<p>这种方法的问题是，这些日志很容易丢失，很难看到进展，并且当在远程机器上工作时，您可能无法访问它。</p>



<h3><strong>使用回调监控Keras损失</strong></h3>



<p>另一个更干净的选择是使用回调，它将记录每个批处理和epoch结束时的丢失。</p>



<p>你需要决定你想在哪里记录什么，但是这真的很简单。</p>



<p>例如，记录keras对Neptune的损失可能如下所示:</p>



<pre class="hljs"><span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> Callback 

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NeptuneCallback</span><span class="hljs-params">(Callback)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">on_batch_end</span><span class="hljs-params">(self, batch, logs=None)</span>:</span>  
        <span class="hljs-keyword">for</span> metric_name, metric_value <span class="hljs-keyword">in</span> logs.items():
            neptune_run[f<span class="hljs-string">"{metric_name}"</span>].log(metric_value)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">on_epoch_end</span><span class="hljs-params">(self, epoch, logs=None)</span>:</span> 
        <span class="hljs-keyword">for</span> metric_name, metric_value <span class="hljs-keyword">in</span> logs.items():
            neptune_run[f<span class="hljs-string">"{metric_name}"</span>].log(metric_value)</pre>



<p>您可以自己创建监控回调，或者使用keras库中以及其他与它集成的库中许多可用的keras回调之一，如TensorBoard、<a href="https://web.archive.org/web/20221027132202/https://docs.neptune.ai/essentials/integrations/deep-learning-frameworks/tensorflow-keras" target="_blank" rel="noreferrer noopener"> Neptune </a>等。</p>



<p>一旦您准备好回调，您只需将它传递给<code>model.fit(...)</code>:</p>



<pre class="hljs">pip install neptune-client neptune-tensorboard</pre>



<pre class="hljs">
<span class="hljs-keyword">import</span> neptune.new <span class="hljs-keyword">as</span> neptune
<span class="hljs-keyword">from</span> neptunecontrib.monitoring.keras <span class="hljs-keyword">import</span> NeptuneMonitor
 
run = neptune.init(
       project=<span class="hljs-string">'common/tf-keras-integration'</span>,
       api_token=<span class="hljs-string">'ANONYMOUS'</span>
)

neptune_cbk = NeptuneCallback(run=run, base_namespace=<span class="hljs-string">'metrics'</span>)

model.fit(x_train, y_train, 
          validation_split=<span class="hljs-number">0.2</span>, 
          epochs=<span class="hljs-number">10</span>, 
          callbacks=[neptune_cbk])</pre>



<p>并且<a href="https://web.archive.org/web/20221027132202/https://app.neptune.ai/o/common/org/tf-keras-integration/e/TFK-35541/dashboard/metrics-b11ccc73-9ac7-4126-be1a-cf9a3a4f9b74" target="_blank" rel="noreferrer noopener">在UI中监控您的实验学习曲线</a>:</p>





<h2>为什么会发生卡雷损失南</h2>



<p>大多数情况下，您记录的损失只是一些常规值，但有时在使用Keras损失函数时，您可能会得到nan。</p>





<p>当这种情况发生时，您的模型将不会更新其权重，并将停止学习，因此需要避免这种情况。</p>



<p>nan丢失的原因可能有很多，但通常是这样的:</p>


<div class="custom-point-list">
<ul><li>训练集中的nans将导致失败，</li><li>训练集中的NumPy无穷大也会导致nans的丢失，</li><li>使用未缩放的训练集，</li><li>使用非常大的l2正则化子和高于1的学习率，</li><li>使用了错误的优化函数，</li><li>大(爆炸)梯度导致训练期间网络权重的大更新。</li></ul>
</div>


<p>所以为了避免nans中的损失，确保:</p>


<div class="custom-point-list">
<ul><li>检查您的训练数据是否适当缩放，并且不包含nans</li><li>检查你是否使用了正确的优化器，并且你的学习率不是太大；</li><li>检查l2正则化是否不太大；</li><li>如果你正面临爆炸梯度问题，你可以:重新设计网络或使用梯度剪辑，使你的梯度有一定的“最大允许模型更新”。</li></ul>
</div>


<h2>最后的想法</h2>



<p>希望这篇文章能给你一些Keras中损失函数的背景知识。</p>



<p>我们涵盖了:</p>


<div class="custom-point-list">
<ul><li>Keras中内置的损失函数，</li><li>实现您自己的自定义损失函数，</li><li>如何增加样品称量以产生观察敏感损失，</li><li>如何避免损失nans，</li><li>你如何想象你的模型在训练时的损失。</li></ul>
</div>


<p>欲了解更多信息，请查看Keras知识库和T2张量流损失函数文档。</p>




<div id="author-box-new-format-block_6007356684165" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">德里克·姆维蒂</h3>
    
          <p class="article__authorContent-text">Derrick Mwiti是一名数据科学家，他对分享知识充满热情。他是数据科学社区的热心贡献者，例如Heartbeat、Towards Data Science、Datacamp、Neptune AI、KDnuggets等博客。他的内容在网上被浏览了超过一百万次。德里克也是一名作家和在线教师。他还培训各种机构并与之合作，以实施数据科学解决方案并提升其员工的技能。你可能想看看他在Python课程中完整的数据科学和机器学习训练营。</p>
    
          
    
  </div>
</div>


<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>



<p class="has-text-color"><strong>阅读下一篇</strong></p>



<h2>Keras Metrics:您需要知道的一切</h2>



<p class="has-small-font-size">10分钟阅读|作者Derrick Mwiti |年6月8日更新</p>


<p id="block_5ffc75def9f8e" class="separator separator-10"/>



<p id="block-1ef7ce9f-52e6-46f9-b3db-bae9a615f3e4">Keras指标是用于评估深度学习模型性能的函数。为您的问题选择一个好的度量标准通常是一项困难的任务。</p>


<div class="custom-point-list">
<ul id="block-97361d8a-fd26-4f1b-b8bc-c53e01c96efa"><li>你需要了解【tf.keras和tf.keras中哪些指标已经可用以及如何使用它们，</li><li>在许多情况下，您需要<strong>定义您自己的定制指标</strong>，因为您正在寻找的指标没有随Keras一起提供。</li><li>有时，您希望<strong>通过在每个时期后查看ROC曲线或混淆矩阵</strong>等图表来监控模型性能。</li></ul>
</div>


<h3>本文将解释一些术语:</h3>


<div class="custom-point-list">
<ul id="block-4c25750e-81b5-410c-ba1f-bb452e425e74"><li>keras度量准确性</li><li>keras编译指标</li><li>keras自定义指标</li><li>回归的keras度量</li><li>keras混淆矩阵</li><li>tf.kerac.metrics.meaniou</li><li>tf.keras.metrics f1分数</li><li>tf.keras.metrics.auc</li></ul>
</div>

<a class="button continous-post blue-filled" href="/web/20221027132202/https://neptune.ai/blog/keras-metrics" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>