<html>
<head>
<title>The Best Deep Learning Papers from the ICLR 2020 Conference </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>ICLR 2020å¤§ä¼šæœ€ä½³æ·±åº¦å­¦ä¹ è®ºæ–‡</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://web.archive.org/web/https://neptune.ai/blog/iclr-2020-deep-learning#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/iclr-2020-deep-learning#0001-01-01</a></blockquote><div><div class="article__content col-lg-10">
<p>ä¸Šå‘¨ï¼Œæˆ‘å¾ˆé«˜å…´å‚åŠ äº†å­¦ä¹ è¡¨å¾å›½é™…ä¼šè®®(<strong> ICLR </strong>)ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡´åŠ›äºæ·±åº¦å­¦ä¹ å„ä¸ªæ–¹é¢çš„<strong>ç ”ç©¶çš„æ´»åŠ¨</strong>ã€‚æœ€åˆï¼Œä¼šè®®æœ¬åº”åœ¨åŸƒå¡ä¿„æ¯”äºšçš„äºšçš„æ–¯äºšè´å·´ä¸¾è¡Œï¼Œç„¶è€Œï¼Œç”±äºæ–°å‹å† çŠ¶ç—…æ¯’ç–«æƒ…ï¼Œä¼šè®®è™šæ‹ŸåŒ–äº†ã€‚æˆ‘æ•¢è‚¯å®šï¼Œå¯¹äºç»„ç»‡è€…æ¥è¯´ï¼Œå°†æ´»åŠ¨æ¬åˆ°ç½‘ä¸Šæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä½†æˆ‘è®¤ä¸ºæ•ˆæœéå¸¸ä»¤äººæ»¡æ„ï¼Œæ­£å¦‚ä½ å¯ä»¥<a href="https://web.archive.org/web/20220926095407/https://medium.com/@iclr_conf/gone-virtual-lessons-from-iclr2020-1743ce6164a3" target="_blank" rel="noreferrer noopener nofollow">åœ¨è¿™é‡Œ</a>çœ‹åˆ°çš„ï¼</p>



<p>è¶…è¿‡1300åå‘è¨€äººå’Œ5600åä¸ä¼šè€…è¯æ˜ï¼Œè™šæ‹Ÿå½¢å¼æ›´å®¹æ˜“ä¸ºå…¬ä¼—æ‰€æ¥å—ï¼Œä½†åŒæ—¶ï¼Œä¼šè®®ä»ä¿æŒäº’åŠ¨å’Œå‚ä¸ã€‚ä»ä¼—å¤šæœ‰è¶£çš„ä»‹ç»ä¸­ï¼Œæˆ‘å†³å®š<strong>é€‰æ‹©16ä¸ª</strong>ï¼Œæœ‰å½±å“åŠ›ï¼Œå‘äººæ·±çœã€‚ä»¥ä¸‹æ˜¯æ¥è‡ªICLR çš„<strong>æœ€ä½³æ·±åº¦å­¦ä¹ è®ºæ–‡ã€‚</strong></p>





<h2 id="Art1">æœ€ä½³æ·±åº¦å­¦ä¹ è®ºæ–‡</h2>



<h3><strong> 1ã€‚å…³äºç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹çš„é²æ£’æ€§</strong></h3>



<p>æ·±å…¥ç ”ç©¶ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹æˆ–ç®€ç§°NeuralODEçš„é²æ£’æ€§ã€‚å°†å…¶ä½œä¸ºæ„å»ºæ›´å¼ºå¤§ç½‘ç»œçš„åŸºç¡€ã€‚</p>



<p><a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=B1e9Y2NYvS" target="_blank" rel="noreferrer noopener nofollow"> <strong>è®ºæ–‡</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img src="../Images/880c69551405fee39e9de219ec13bca0.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh4.googleusercontent.com/vbiuEw99cxQOr0nWJ5FT1WbMEuxMwk1lWZrMAsOcepnJfB8w52sk7Gr4IQkNxp1qliLtxRGmKla3u55oL6kDMn2D5ZHl2OV-68He06xAHlD8Z-pWCUPkwR3fUKKqJwacbvS5ph82?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/vbiuEw99cxQOr0nWJ5FT1WbMEuxMwk1lWZrMAsOcepnJfB8w52sk7Gr4IQkNxp1qliLtxRGmKla3u55oL6kDMn2D5ZHl2OV-68He06xAHlD8Z-pWCUPkwR3fUKKqJwacbvS5ph82"/><noscript><img data-lazy-fallback="1" src="../Images/880c69551405fee39e9de219ec13bca0.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/vbiuEw99cxQOr0nWJ5FT1WbMEuxMwk1lWZrMAsOcepnJfB8w52sk7Gr4IQkNxp1qliLtxRGmKla3u55oL6kDMn2D5ZHl2OV-68He06xAHlD8Z-pWCUPkwR3fUKKqJwacbvS5ph82"/></noscript><figcaption><br/><em>The architecture of an ODENet. The neural ODE block serves as a dimension-preserving nonlinear mapping.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 2ã€‚ä¸ºä»€ä¹ˆæ¢¯åº¦è£å‰ªåŠ é€Ÿè®­ç»ƒ:é€‚åº”æ€§çš„ç†è®ºè¯æ˜</strong></h3>



<p>æ¢¯åº¦è£å‰ªå¯è¯æ˜åœ°åŠ é€Ÿäº†éå…‰æ»‘éå‡¸å‡½æ•°çš„æ¢¯åº¦ä¸‹é™ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=BJgnXpVYwS" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://github.com/JingzhaoZhang/why-clipping-accelerates" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img src="../Images/e7e51ccb5e2002b222c1ecdd21cf01d4.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh6.googleusercontent.com/xtNtauoq4AY0zTCr0noq1YJPnt2ZC_OqPAhIQCh7IzwsFX-bx5eVw6R_hQXXbgrXNZrNC7z4X7g5CVsW7fsfCZ1hkPWzCd7q79I47V4qIvnkrJ9C6Skw527T6eIaSN8KMlIC4dYk?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh6.googleusercontent.com/xtNtauoq4AY0zTCr0noq1YJPnt2ZC_OqPAhIQCh7IzwsFX-bx5eVw6R_hQXXbgrXNZrNC7z4X7g5CVsW7fsfCZ1hkPWzCd7q79I47V4qIvnkrJ9C6Skw527T6eIaSN8KMlIC4dYk"/><noscript><img data-lazy-fallback="1" src="../Images/e7e51ccb5e2002b222c1ecdd21cf01d4.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh6.googleusercontent.com/xtNtauoq4AY0zTCr0noq1YJPnt2ZC_OqPAhIQCh7IzwsFX-bx5eVw6R_hQXXbgrXNZrNC7z4X7g5CVsW7fsfCZ1hkPWzCd7q79I47V4qIvnkrJ9C6Skw527T6eIaSN8KMlIC4dYk"/></noscript><figcaption><em>Gradient norm vs local gradient Lipschitz constant on a log-scale along the training trajectory for AWD-LSTM (Merity et al., 2018) on PTB dataset. The colorbar indicates the number of iterations during training.Â </em></figcaption></figure></div>



<h3 id="Art3">ç¬¬ä¸€ä½œè€…:å¼ äº¬å…†</h3>



<p><a href="https://web.archive.org/web/20220926095407/https://www.linkedin.com/in/jingzhao-zhang-6b0a09a4/" target="_blank" rel="noreferrer noopener nofollow"> LinkedIn </a> | <a href="https://web.archive.org/web/20220926095407/https://sites.google.com/view/jingzhao/home" target="_blank" rel="noreferrer noopener nofollow">ç½‘ç«™</a></p>



<hr class="wp-block-separator"/>



<h3><strong> 3ã€‚ç”¨äºç›‘ç£è¡¨ç¤ºå­¦ä¹ çš„ç›®æ ‡åµŒå…¥è‡ªåŠ¨ç¼–ç å™¨</strong></h3>



<p>ç”¨äºç›‘ç£é¢„æµ‹çš„æ–°çš„é€šç”¨ç›®æ ‡åµŒå…¥è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶ã€‚ä½œè€…ç»™å‡ºäº†ç†è®ºå’Œç»éªŒçš„è€ƒè™‘ã€‚</p>



<p><a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=BygXFkSYDH" target="_blank" rel="noreferrer noopener nofollow"> <strong>è®ºæ–‡</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/7e47b1f2c8712c69b96820e2b19b7a1d.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh4.googleusercontent.com/o4c1NohRc5U9V0-gFhVBjT_Ub8jK_nO6_eWkwy4l1O703H1iEJsvZEkWWRSO1zTvV0m7CTHZTAaG671eC_CVKgX87kMMsO1JCYXfLD5GiqtxUcFiVc-frPJZLjO5Gg7WnwPVy5SA?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/o4c1NohRc5U9V0-gFhVBjT_Ub8jK_nO6_eWkwy4l1O703H1iEJsvZEkWWRSO1zTvV0m7CTHZTAaG671eC_CVKgX87kMMsO1JCYXfLD5GiqtxUcFiVc-frPJZLjO5Gg7WnwPVy5SA"/><noscript><img data-lazy-fallback="1" src="../Images/7e47b1f2c8712c69b96820e2b19b7a1d.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/o4c1NohRc5U9V0-gFhVBjT_Ub8jK_nO6_eWkwy4l1O703H1iEJsvZEkWWRSO1zTvV0m7CTHZTAaG671eC_CVKgX87kMMsO1JCYXfLD5GiqtxUcFiVc-frPJZLjO5Gg7WnwPVy5SA"/></noscript><figcaption><em>(a) Feature-embedding and (b) Target-embedding autoencoders. Solid lines correspond to the (primary) prediction task; dashed lines to the (auxiliary) reconstruction task. Shared components are involved in both.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 4ã€‚ç†è§£å’ŒåŠ å¼ºå·®å¼‚åŒ–æ¶æ„æœç´¢</strong></h3>



<p>æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹æœ‰æ•ˆæ€§æŸå¤±çš„Hessiançš„ç‰¹å¾å€¼æ¥ç ”ç©¶DARTS(å¯åŒºåˆ†æ¶æ„æœç´¢)çš„å¤±æ•ˆæ¨¡å¼ï¼Œå¹¶åŸºäºæˆ‘ä»¬çš„åˆ†ææå‡ºç¨³å¥æ€§ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=H1gDNyrKDS" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://github.com/automl/RobustDARTS" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/25f96fea20a13c9e64dad2ffb0fcd899.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh4.googleusercontent.com/ztmByeDRTgimSFfh7ejcwogOjyReeKpC9CrgkAJrlLwc2RvHp2NOigpgLPVkO8xvxxvXHIx0TRnegCaOG5T-fhCGmvicoDL7TuiVdmvnrQiyv6TRPibvBiZaPmT1OwuSFSsJkLr3?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/ztmByeDRTgimSFfh7ejcwogOjyReeKpC9CrgkAJrlLwc2RvHp2NOigpgLPVkO8xvxxvXHIx0TRnegCaOG5T-fhCGmvicoDL7TuiVdmvnrQiyv6TRPibvBiZaPmT1OwuSFSsJkLr3"/><noscript><img data-lazy-fallback="1" src="../Images/25f96fea20a13c9e64dad2ffb0fcd899.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/ztmByeDRTgimSFfh7ejcwogOjyReeKpC9CrgkAJrlLwc2RvHp2NOigpgLPVkO8xvxxvXHIx0TRnegCaOG5T-fhCGmvicoDL7TuiVdmvnrQiyv6TRPibvBiZaPmT1OwuSFSsJkLr3"/></noscript><figcaption><em>The poor cells standard DARTS finds on spaces S1-S4. For all spaces, DARTS chooses mostly parameter-less operations (skip connection) or even the harmful Noise operation. Shown are the normal cells on CIFAR-10.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 5ã€‚æ¯”è¾ƒç¥ç»ç½‘ç»œä¿®å‰ªä¸­çš„å€’å›å’Œå¾®è°ƒ</strong></h3>



<p>åœ¨ä¿®å‰ªç¥ç»ç½‘ç»œæ—¶ï¼Œä¸æ˜¯åœ¨ä¿®å‰ªåè¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯å°†æƒé‡æˆ–å­¦ä¹ ç‡è®¡åˆ’å€’å›å®ƒä»¬åœ¨è®­ç»ƒæ—¶çš„å€¼ï¼Œå¹¶ä»é‚£é‡Œé‡æ–°è®­ç»ƒï¼Œä»¥å®ç°æ›´é«˜çš„ç²¾åº¦ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=S1gSj0NKvB" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://github.com/lottery-ticket/rewinding-iclr20-public" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/36d010599859bd171426b15353a0960d.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh4.googleusercontent.com/r54nt1nWjmM2wxQVx7nStNE--alnG_EAeIBW2oraIwzXGVKIweMcuLGTAKgJxq959m4QcmMv94xkN3o3flYDD1AOHUPJBtwXC0-3uK8n8HHhoB_e8SqHqEV_5_f1X82QPUr_xzZh?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/r54nt1nWjmM2wxQVx7nStNE--alnG_EAeIBW2oraIwzXGVKIweMcuLGTAKgJxq959m4QcmMv94xkN3o3flYDD1AOHUPJBtwXC0-3uK8n8HHhoB_e8SqHqEV_5_f1X82QPUr_xzZh"/><noscript><img data-lazy-fallback="1" src="../Images/36d010599859bd171426b15353a0960d.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/r54nt1nWjmM2wxQVx7nStNE--alnG_EAeIBW2oraIwzXGVKIweMcuLGTAKgJxq959m4QcmMv94xkN3o3flYDD1AOHUPJBtwXC0-3uK8n8HHhoB_e8SqHqEV_5_f1X82QPUr_xzZh"/></noscript><figcaption><em>The best achievable accuracy across retraining times by one-shot pruning.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 6ã€‚ç¥ç»è¿ç®—å•å…ƒ</strong></h3>



<p>ç¥ç»ç½‘ç»œè™½ç„¶èƒ½å¤Ÿé€¼è¿‘å¤æ‚çš„å‡½æ•°ï¼Œä½†åœ¨ç²¾ç¡®çš„ç®—æœ¯è¿ç®—æ–¹é¢ç›¸å½“å·®ã€‚è¿™é¡¹ä»»åŠ¡å¯¹æ·±åº¦å­¦ä¹ ç ”ç©¶äººå‘˜æ¥è¯´æ˜¯ä¸€ä¸ªé•¿æœŸçš„æŒ‘æˆ˜ã€‚è¿™é‡Œï¼Œæå‡ºäº†æ–°é¢–çš„ç¥ç»åŠ æ³•å•å…ƒ(NAU)å’Œç¥ç»ä¹˜æ³•å•å…ƒ(NMU)ï¼Œèƒ½å¤Ÿæ‰§è¡Œç²¾ç¡®çš„åŠ æ³•/å‡æ³•(NAU)å’Œå‘é‡çš„ä¹˜æ³•å­é›†(MNU)ã€‚è‘—åçš„ç¬¬ä¸€ä½œè€…æ˜¯ä¸€åç‹¬ç«‹çš„ç ”ç©¶å‘˜ğŸ™‚</p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=H1gNOeHKPS" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://github.com/AndreasMadsen/stable-nalu" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img src="../Images/c39110e868e6c1153e97730eb2710c1d.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh5.googleusercontent.com/V05cLldii8t2ELXPubol94UeJkYlaOAG7kqn_5HyJXB-TG2XFQ41PC71HOJitXXaKGypRN3VWWrNrLjza1lfJMfMkM6pyZq16OdvqO--_c5B_N4LOy9wwsoIG-XyasfnCWQQ4XTK?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh5.googleusercontent.com/V05cLldii8t2ELXPubol94UeJkYlaOAG7kqn_5HyJXB-TG2XFQ41PC71HOJitXXaKGypRN3VWWrNrLjza1lfJMfMkM6pyZq16OdvqO--_c5B_N4LOy9wwsoIG-XyasfnCWQQ4XTK"/><noscript><img data-lazy-fallback="1" src="../Images/c39110e868e6c1153e97730eb2710c1d.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh5.googleusercontent.com/V05cLldii8t2ELXPubol94UeJkYlaOAG7kqn_5HyJXB-TG2XFQ41PC71HOJitXXaKGypRN3VWWrNrLjza1lfJMfMkM6pyZq16OdvqO--_c5B_N4LOy9wwsoIG-XyasfnCWQQ4XTK"/></noscript><figcaption><em>Visualization of the NMU, where the weights (W<sub>i,j</sub> ) controls gating between 1 (identity) or x<sub>i</sub>, each intermediate result is then multiplied explicitly to form z<sub>j</sub>.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3>7 .<strong>ã€‚æ·±åº¦ç¥ç»ç½‘ç»œä¼˜åŒ–è½¨è¿¹ä¸Šçš„å¹³è¡¡ç‚¹</strong></h3>



<p>åœ¨æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼Œå­˜åœ¨ä¸€ä¸ªâ€œå¹³è¡¡ç‚¹â€,å®ƒå†³å®šäº†æ•´ä¸ªä¼˜åŒ–è½¨è¿¹çš„æ€§è´¨ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=r1g87C4KwB" target="_blank" rel="noreferrer noopener nofollow"> <strong>è®ºæ–‡</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img src="../Images/e0e66af325070416f71b39dac9a08426.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh4.googleusercontent.com/CD_4w-S9IT85OIfX2y4i_TgKsjsxmYU-T2H5k-c3nAbdgKWNJSdyiEB3_2t8e2tVxkdILidexgF_Dhdy2jE32cUKiSSh-3vNi5rHhrOEBe18ZY4DDB55NeL8vws-60CzTbAYSzJu?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/CD_4w-S9IT85OIfX2y4i_TgKsjsxmYU-T2H5k-c3nAbdgKWNJSdyiEB3_2t8e2tVxkdILidexgF_Dhdy2jE32cUKiSSh-3vNi5rHhrOEBe18ZY4DDB55NeL8vws-60CzTbAYSzJu"/><noscript><img data-lazy-fallback="1" src="../Images/e0e66af325070416f71b39dac9a08426.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/CD_4w-S9IT85OIfX2y4i_TgKsjsxmYU-T2H5k-c3nAbdgKWNJSdyiEB3_2t8e2tVxkdILidexgF_Dhdy2jE32cUKiSSh-3vNi5rHhrOEBe18ZY4DDB55NeL8vws-60CzTbAYSzJu"/></noscript><figcaption><em>Visualization of the early part of the training trajectories on CIFAR-10 (before reaching 65% training accuracy) of a simple CNN model optimized using SGD with learning rates Î· = 0.01 (red) and Î· = 0.001 (blue). Each model on the training trajectory, shown as a point, is represented by its test predictions embedded into a two-dimensional space using UMAP. The background color indicates the spectral norm of the covariance of gradients K (Î»<sup>1</sup><sub>K</sub>, left) and the training accuracy (right). For lower Î·, after reaching what we call the break-even point, the trajectory is steered towards a region characterized by larger Î»<sup>1</sup><sub>K</sub> (left) for the same training accuracy (right).</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 8ã€‚Hoppity:å­¦ä¹ å›¾å½¢è½¬æ¢æ¥æ£€æµ‹å’Œä¿®å¤ç¨‹åºä¸­çš„é”™è¯¯</strong></h3>



<p>ä¸€ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å’Œä¿®å¤Javascriptä¸­çš„é”™è¯¯ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=SJeqs6EFvB" target="_blank" rel="noreferrer noopener nofollow"> <strong>è®ºæ–‡</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/33b444bbffb20f355cc6be61871ade38.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh5.googleusercontent.com/fM4cJgv8F-mHvQdPBQCZ6QHu5G0bLOjwxIUJvphMyotOkVnDlyaIN9pPol5-TzF39FTKG2YY4a5imukFBsv6rshG5VlePvPM-Bb7ilemTCICaOxOBDNC0b6aKG5uxJzE509t78Ii?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh5.googleusercontent.com/fM4cJgv8F-mHvQdPBQCZ6QHu5G0bLOjwxIUJvphMyotOkVnDlyaIN9pPol5-TzF39FTKG2YY4a5imukFBsv6rshG5VlePvPM-Bb7ilemTCICaOxOBDNC0b6aKG5uxJzE509t78Ii"/><noscript><img data-lazy-fallback="1" src="../Images/33b444bbffb20f355cc6be61871ade38.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh5.googleusercontent.com/fM4cJgv8F-mHvQdPBQCZ6QHu5G0bLOjwxIUJvphMyotOkVnDlyaIN9pPol5-TzF39FTKG2YY4a5imukFBsv6rshG5VlePvPM-Bb7ilemTCICaOxOBDNC0b6aKG5uxJzE509t78Ii"/></noscript><figcaption><em>Example programs that illustrate limitations of existing approaches inculding both rulebased static analyzers and neural-based bug predictors.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 9ã€‚é€šè¿‡ä»£ç†é€‰æ‹©:æ·±åº¦å­¦ä¹ çš„é«˜æ•ˆæ•°æ®é€‰æ‹©</strong></h3>



<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨å°å¾—å¤šçš„ä»£ç†æ¨¡å‹æ¥æ‰§è¡Œæ•°æ®é€‰æ‹©ï¼Œä»è€Œæ˜¾è‘—æé«˜æ·±åº¦å­¦ä¹ ä¸­æ•°æ®é€‰æ‹©çš„è®¡ç®—æ•ˆç‡ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=HJg2b0VYDr" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://github.com/stanford-futuredata/selection-via-proxy" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/3bff50d92128a35f9a20c5dfe8f5dfee.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh4.googleusercontent.com/7yxFn6TwTZbOTAre73lNsFGl2vFrAW_Jaoy3uiG2s1yGSBdrgCBkE69DsRRs5Az8AUub9CMUqWvfgyvOZecu_x3NlcoT4RM4cs8S7nZQzfHP4ovpAiowstwY1kZ5e5IVrKDjjNmz?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/7yxFn6TwTZbOTAre73lNsFGl2vFrAW_Jaoy3uiG2s1yGSBdrgCBkE69DsRRs5Az8AUub9CMUqWvfgyvOZecu_x3NlcoT4RM4cs8S7nZQzfHP4ovpAiowstwY1kZ5e5IVrKDjjNmz"/><noscript><img data-lazy-fallback="1" src="../Images/3bff50d92128a35f9a20c5dfe8f5dfee.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/7yxFn6TwTZbOTAre73lNsFGl2vFrAW_Jaoy3uiG2s1yGSBdrgCBkE69DsRRs5Az8AUub9CMUqWvfgyvOZecu_x3NlcoT4RM4cs8S7nZQzfHP4ovpAiowstwY1kZ5e5IVrKDjjNmz"/></noscript><figcaption><em>SVP applied to active learning (left) and core-set selection (right). In active learning, we followed the same iterative procedure of training and selecting points to label as traditional approaches but replaced the target model with a cheaper-to-compute proxy model. For core-set selection, we learned a feature representation over the data using a proxy model and used it to select points to train a larger, more accurate model. In both cases, we found the proxy and target model have high rank-order correlation, leading to similar selections and downstream results.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3>10ã€‚æ¯”ç‰¹ä¸‹é™:é‡æ–°å®¡è§†ç¥ç»ç½‘ç»œçš„é‡å­åŒ–</h3>



<p>ä½¿ç”¨æ—¨åœ¨æ›´å¥½çš„åŸŸå†…é‡æ„çš„ç»“æ„åŒ–é‡åŒ–æŠ€æœ¯æ¥å‹ç¼©å·ç§¯ç¥ç»ç½‘ç»œã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=rJehVyrKwH" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://drive.google.com/file/d/12QK7onizf2ArpEBK706ly8bNfiM9cPzp/view?usp=sharing" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/ca3eb3e43d6f1eec920492b70222f3ef.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh6.googleusercontent.com/x2PzuRm78Ar1YqOVAEVv4qaTXITsEc2qTk_-zp-dnJnsqrwwvp1QXQW2yed7BF-Nc8w0MGq1tGmnZjKoEXUzZqpZK8LzvWZM5-3RygLzBlyGGhOTKylG-TA7I_1SThHJ21hvIC7a?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh6.googleusercontent.com/x2PzuRm78Ar1YqOVAEVv4qaTXITsEc2qTk_-zp-dnJnsqrwwvp1QXQW2yed7BF-Nc8w0MGq1tGmnZjKoEXUzZqpZK8LzvWZM5-3RygLzBlyGGhOTKylG-TA7I_1SThHJ21hvIC7a"/><noscript><img data-lazy-fallback="1" src="../Images/ca3eb3e43d6f1eec920492b70222f3ef.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh6.googleusercontent.com/x2PzuRm78Ar1YqOVAEVv4qaTXITsEc2qTk_-zp-dnJnsqrwwvp1QXQW2yed7BF-Nc8w0MGq1tGmnZjKoEXUzZqpZK8LzvWZM5-3RygLzBlyGGhOTKylG-TA7I_1SThHJ21hvIC7a"/></noscript><figcaption><em>Illustration of our method. We approximate a binary classifier Ï• that labels images as dogs or cats by quantizing its weights. Standard method: quantizing Ï• with the standard objective function (1) promotes a classifier Ï•b<sub>standard</sub> that tries to approximate Ï• over the entire input space and can thus perform badly for in-domain inputs. Our method: quantizing Ï• with our objective function (2) promotes a classifier Ï•b<sub>activations</sub> that performs well for in-domain inputs. Images lying in the hatched area of the input space are correctly classified by Ï•<sub>activations</sub> but incorrectly by Ï•<sub>standard</sub>.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 11ã€‚ç”¨äºåœ¨åˆå§‹åŒ–æ—¶ä¿®å‰ªç¥ç»ç½‘ç»œçš„ä¿¡å·ä¼ æ’­è§‚ç‚¹</strong></h3>



<p>æˆ‘ä»¬åœ¨åˆå§‹åŒ–æ—¶å½¢å¼åŒ–åœ°æè¿°äº†æœ‰æ•ˆå‰ªæçš„åˆå§‹åŒ–æ¡ä»¶ï¼Œå¹¶åˆ†æäº†ç”±æ­¤äº§ç”Ÿçš„å‰ªæç½‘ç»œçš„ä¿¡å·ä¼ æ’­ç‰¹æ€§ï¼Œè¿™å¯¼è‡´äº†ä¸€ç§å¢å¼ºå®ƒä»¬çš„å¯è®­ç»ƒæ€§å’Œå‰ªæç»“æœçš„æ–¹æ³•ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=HJeTo2VFwH" target="_blank" rel="noreferrer noopener nofollow"> <strong>è®ºæ–‡</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/6494a8538b66f0368ac1194df26e9675.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh5.googleusercontent.com/Fif7Oiy46j1I_s5I3h2aez5rtDDBOmUHYxICOhhiXKT_Q-FxoO7q5BAyEyVL0HHm7TUoJYboWTrC-bURAvV-ppMUQj4s8LQ3yRp5ItlMu6sJm_eCpNEFD0Jim2pt2tMjE1aNFOxy?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh5.googleusercontent.com/Fif7Oiy46j1I_s5I3h2aez5rtDDBOmUHYxICOhhiXKT_Q-FxoO7q5BAyEyVL0HHm7TUoJYboWTrC-bURAvV-ppMUQj4s8LQ3yRp5ItlMu6sJm_eCpNEFD0Jim2pt2tMjE1aNFOxy"/><noscript><img data-lazy-fallback="1" src="../Images/6494a8538b66f0368ac1194df26e9675.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh5.googleusercontent.com/Fif7Oiy46j1I_s5I3h2aez5rtDDBOmUHYxICOhhiXKT_Q-FxoO7q5BAyEyVL0HHm7TUoJYboWTrC-bURAvV-ppMUQj4s8LQ3yRp5ItlMu6sJm_eCpNEFD0Jim2pt2tMjE1aNFOxy"/></noscript><figcaption><em>(left) layerwise sparsity patterns c âˆˆ {0, 1} <sup>100Ã—100</sup> obtained as a result of pruning for the sparsity level ÎºÂ¯ = {10, .., 90}%. Here, black(0)/white(1) pixels refer to pruned/retained parameters; (right) connection sensitivities (CS) measured for the parameters in each layer. All networks are initialized with Î³ = 1.0. Unlike the linear case, the sparsity pattern for the tanh network is nonuniform over different layers. When pruning for a high sparsity level (e.g., ÎºÂ¯ = 90%), this becomes critical and leads to poor learning capability as there are only a few parameters left in later layers. This is explained by the connection sensitivity plot which shows that for the nonlinear network parameters in later layers have saturating, lower connection sensitivities than those in earlier layers.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 12ã€‚æ·±åº¦åŠç›‘ç£å¼‚å¸¸æ£€æµ‹</strong></h3>



<p>æˆ‘ä»¬ä»‹ç»äº†æ·±åº¦SADï¼Œä¸€ç§ç”¨äºä¸€èˆ¬åŠç›‘ç£å¼‚å¸¸æ£€æµ‹çš„æ·±åº¦æ–¹æ³•ï¼Œå®ƒç‰¹åˆ«åˆ©ç”¨äº†æ ‡è®°å¼‚å¸¸ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=HkgH0TEYwH" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://github.com/lukasruff/Deep-SAD-PyTorch" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/f8a04f019c4a8f5f318940ceb1ea257d.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh5.googleusercontent.com/S1VGAnaYbU4Nf1cssF3OA_I-fgY-g8B_JJPSiCUnDjooCtmaeg8dZJy6VKMG_h_UzKK4PrA0Np3XH-VXAnceWLWrIs9O3MvgYPT30dMHqRHPCG2DCKusHPV84jM8BuJGZG3k8bz5?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh5.googleusercontent.com/S1VGAnaYbU4Nf1cssF3OA_I-fgY-g8B_JJPSiCUnDjooCtmaeg8dZJy6VKMG_h_UzKK4PrA0Np3XH-VXAnceWLWrIs9O3MvgYPT30dMHqRHPCG2DCKusHPV84jM8BuJGZG3k8bz5"/><noscript><img data-lazy-fallback="1" src="../Images/f8a04f019c4a8f5f318940ceb1ea257d.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh5.googleusercontent.com/S1VGAnaYbU4Nf1cssF3OA_I-fgY-g8B_JJPSiCUnDjooCtmaeg8dZJy6VKMG_h_UzKK4PrA0Np3XH-VXAnceWLWrIs9O3MvgYPT30dMHqRHPCG2DCKusHPV84jM8BuJGZG3k8bz5"/></noscript><figcaption><em>The need for semi-supervised anomaly detection: The training data (shown in (a)) consists of (mostly normal) unlabeled data (gray) as well as a few labeled normal samples (blue) and labeled anomalies (orange). Figures (b)â€“(f) show the decision boundaries of the various learning paradigms at testing time along with novel anomalies that occur (bottom left in each plot). Our semi-supervised AD approach takes advantage of all training data: unlabeled samples, labeled normal samples, as well as labeled anomalies. This strikes a balance between one-class learning and classification.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3>13ã€‚ä½¿ç”¨ç½‘æ ¼å•å…ƒçš„ç©ºé—´ç‰¹å¾åˆ†å¸ƒçš„å¤šå°ºåº¦è¡¨ç¤ºå­¦ä¹ </h3>



<p>æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºSpace2vecçš„è¡¨å¾å­¦ä¹ æ¨¡å‹æ¥ç¼–ç åœ°ç‚¹çš„ç»å¯¹ä½ç½®å’Œç©ºé—´å…³ç³»ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=rJljdh4KDH" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://github.com/gengchenmai/space2vec" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/e4e7a2c2607f599329ff94c594187945.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh4.googleusercontent.com/wM3-jD8jKm8o2TunSD8-p7zNL9V3S5xBihsqVCN5h3hN3uVAIHKqaQ5GImq8hR3TUw5kXYenM2Inc6Rxj_gOYHE5FRSLiJHIi5Zg0VDCbsyNu-OJdCmSHlPABiZps0Y4FcPjKk6u?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/wM3-jD8jKm8o2TunSD8-p7zNL9V3S5xBihsqVCN5h3hN3uVAIHKqaQ5GImq8hR3TUw5kXYenM2Inc6Rxj_gOYHE5FRSLiJHIi5Zg0VDCbsyNu-OJdCmSHlPABiZps0Y4FcPjKk6u"/><noscript><img data-lazy-fallback="1" src="../Images/e4e7a2c2607f599329ff94c594187945.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/wM3-jD8jKm8o2TunSD8-p7zNL9V3S5xBihsqVCN5h3hN3uVAIHKqaQ5GImq8hR3TUw5kXYenM2Inc6Rxj_gOYHE5FRSLiJHIi5Zg0VDCbsyNu-OJdCmSHlPABiZps0Y4FcPjKk6u"/></noscript><figcaption><em>The challenge of joint modeling distributions with very different characteristics. (a)(b) The POI locations (red dots) in Las Vegas and Space2Vec predicted conditional likelihood of Womenâ€™s Clothing (with a clustered distribution) and Education (with an even distribution). The dark area in (b) indicates that the downtown area has more POIs of other types than education. (c) Ripleyâ€™s K curves of POI types for which Space2Vec has the largest and smallest improvement over wrap (Mac Aodha et al., 2019). Each curve represents the number of POIs of a certain type inside certain radios centered at every POI of that type; (d) Ripleyâ€™s K curves renormalized by POI densities and shown in log-scale. To efficiently achieve multi-scale representation Space2Vec concatenates the grid cell encoding of 64 scales (with wave lengths ranging from 50 meters to 40k meters) as the first layer of a deep model, and trains with POI data in an unsupervised fashion.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3><strong> 14ã€‚åŒ¹é…å¹³å‡çš„è”åˆå­¦ä¹ </strong></h3>



<p>å…·æœ‰é€å±‚åŒ¹é…çš„é€šä¿¡é«˜æ•ˆè”é‚¦å­¦ä¹ ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=BkluqlSFDS" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://github.com/IBM/FedMA" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/ea5e434af0d509bb4af865e0dedab621.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh3.googleusercontent.com/wuu2MJuzu2lfl-vJ4lkbUN0cShVqSTGTumV5BiZ1XLfJuXXiVL7eKr-1PXd2ADKvjYz2p3hPA5FuEvjEstLDbCtpItITWKFoDUEnOMiMb2_YaekjXeEXWTDvkOXTNlLR2v5w663x?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh3.googleusercontent.com/wuu2MJuzu2lfl-vJ4lkbUN0cShVqSTGTumV5BiZ1XLfJuXXiVL7eKr-1PXd2ADKvjYz2p3hPA5FuEvjEstLDbCtpItITWKFoDUEnOMiMb2_YaekjXeEXWTDvkOXTNlLR2v5w663x"/><noscript><img data-lazy-fallback="1" src="../Images/ea5e434af0d509bb4af865e0dedab621.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh3.googleusercontent.com/wuu2MJuzu2lfl-vJ4lkbUN0cShVqSTGTumV5BiZ1XLfJuXXiVL7eKr-1PXd2ADKvjYz2p3hPA5FuEvjEstLDbCtpItITWKFoDUEnOMiMb2_YaekjXeEXWTDvkOXTNlLR2v5w663x"/></noscript><figcaption><em>Comparison among various federated learning methods with limited number of communications on LeNet trained on MNIST; VGG-9 trained on CIFAR-10 dataset; LSTM trained on Shakespeare dataset over: (a) homogeneous data partition (b) heterogeneous data partition.Â </em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3>15ã€‚å˜è‰²é¾™:åŠ é€Ÿæ·±åº¦ç¥ç»ç½‘ç»œç¼–è¯‘çš„è‡ªé€‚åº”ä»£ç ä¼˜åŒ–</h3>



<p>æ·±åº¦ç¥ç»ç½‘ç»œä¼˜åŒ–ç¼–è¯‘çš„å¼ºåŒ–å­¦ä¹ å’Œè‡ªé€‚åº”é‡‡æ ·ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª<a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow">OpenReview.net</a>)</em></p>



<p><a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=rygG4AVFvH" target="_blank" rel="noreferrer noopener nofollow"> <strong>è®ºæ–‡</strong> </a></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/bd2a688f16efb653a3335161579d9e17.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh4.googleusercontent.com/5mY957vN1ZjmYgSnDTRnRqkAHD2E36WbixCN_NxX5ZlNxNrczSRADQE-bGDs873QkjEPHeG1wbzBGD1tdnKYVA7N2ZJ5KCP3Fmj1wXuT0rEqz6UozD00JdgSeM6Ps1oAVTOv1gBx?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/5mY957vN1ZjmYgSnDTRnRqkAHD2E36WbixCN_NxX5ZlNxNrczSRADQE-bGDs873QkjEPHeG1wbzBGD1tdnKYVA7N2ZJ5KCP3Fmj1wXuT0rEqz6UozD00JdgSeM6Ps1oAVTOv1gBx"/><noscript><img data-lazy-fallback="1" src="../Images/bd2a688f16efb653a3335161579d9e17.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh4.googleusercontent.com/5mY957vN1ZjmYgSnDTRnRqkAHD2E36WbixCN_NxX5ZlNxNrczSRADQE-bGDs873QkjEPHeG1wbzBGD1tdnKYVA7N2ZJ5KCP3Fmj1wXuT0rEqz6UozD00JdgSeM6Ps1oAVTOv1gBx"/></noscript><figcaption><em>Overview of our model compilation workflow, and highlighted is the scope of this work.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h3>16ã€‚ç½‘ç»œå»å·ç§¯</h3>



<p>ä¸ºäº†æ›´å¥½åœ°è®­ç»ƒå·ç§¯ç½‘ç»œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç±»ä¼¼äºåŠ¨ç‰©è§†è§‰ç³»ç»Ÿçš„ç½‘ç»œå»å·ç§¯æ–¹æ³•ã€‚</p>



<p><em>(TLï¼›åšå£«ï¼Œæ¥è‡ª</em><a href="https://web.archive.org/web/20220926095407/http://openreview.net/" target="_blank" rel="noreferrer noopener nofollow"><em/></a><em>)</em></p>



<p><strong> <a href="https://web.archive.org/web/20220926095407/https://openreview.net/forum?id=rkeu30EtvS" target="_blank" rel="noreferrer noopener nofollow">è®ºæ–‡</a> | <a href="https://web.archive.org/web/20220926095407/https://github.com/yechengxi/deconvolution" target="_blank" rel="noreferrer noopener nofollow">ä»£ç </a> </strong></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="../Images/83469e3d258dfd2f7f85df364551218f.png" alt="" data-lazy-src="https://web.archive.org/web/20220926095407/https://lh6.googleusercontent.com/jGO6N61hYDt_yxrDMSWyWn_z2Pe0274O8a4vtbX48YsMxgy9lXX6q6BkGyhnK1CEkqxNJswKJdQGwwXZkivHY13M6SreKRaRPJvnczaSPJu7LI9w_9F0WFsVqFK5ZI2ltw_57Npf?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh6.googleusercontent.com/jGO6N61hYDt_yxrDMSWyWn_z2Pe0274O8a4vtbX48YsMxgy9lXX6q6BkGyhnK1CEkqxNJswKJdQGwwXZkivHY13M6SreKRaRPJvnczaSPJu7LI9w_9F0WFsVqFK5ZI2ltw_57Npf"/><noscript><img data-lazy-fallback="1" src="../Images/83469e3d258dfd2f7f85df364551218f.png" alt="" data-original-src="https://web.archive.org/web/20220926095407im_/https://lh6.googleusercontent.com/jGO6N61hYDt_yxrDMSWyWn_z2Pe0274O8a4vtbX48YsMxgy9lXX6q6BkGyhnK1CEkqxNJswKJdQGwwXZkivHY13M6SreKRaRPJvnczaSPJu7LI9w_9F0WFsVqFK5ZI2ltw_57Npf"/></noscript><figcaption><em>Performing convolution on this real world image using a correlative filter, such as a Gaussian kernel, adds correlations to the resulting image, which makes object recognition more difficult. The process of removing this blur is called deconvolution. What if, however, what we saw as the real world image was itself the result of some unknown correlative filter, which has made recognition more difficult? Our proposed network deconvolution operation can decorrelate underlying image features which allows neural networks to perform better.</em></figcaption></figure></div>







<hr class="wp-block-separator"/>



<h1>æ‘˜è¦</h1>



<p>ICLRå‡ºç‰ˆç‰©çš„æ·±åº¦å’Œå¹¿åº¦ç›¸å½“é¼“èˆäººå¿ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘åªæ˜¯å±•ç¤ºäº†ä¸“æ³¨äºâ€œæ·±åº¦å­¦ä¹ â€ä¸»é¢˜çš„å†°å±±ä¸€è§’ã€‚ç„¶è€Œï¼Œ<a href="https://web.archive.org/web/20220926095407/https://www.analyticsvidhya.com/blog/2020/05/key-takeaways-iclr-2020/" target="_blank" rel="noreferrer noopener nofollow">è¿™ä¸€åˆ†æ</a>è¡¨æ˜ï¼Œå¾ˆå°‘æœ‰å—æ¬¢è¿çš„åœ°åŒºï¼Œç‰¹åˆ«æ˜¯:</p>


<div class="custom-point-list">
<ol><li>æ·±åº¦å­¦ä¹ (åœ¨è¿™ç¯‡æ–‡ç« ä¸­è®¨è®º)</li><li>å¼ºåŒ–å­¦ä¹ (<a href="/web/20220926095407/https://neptune.ai/blog/iclr-2020-reinforcement-learning" target="_blank" rel="noreferrer noopener">æ­¤å¤„</a>)</li><li>ç”Ÿæˆæ¨¡å‹(<a href="/web/20220926095407/https://neptune.ai/blog/iclr-2020-generative-models" target="_blank" rel="noreferrer noopener">æ­¤å¤„</a>)</li><li>è‡ªç„¶è¯­è¨€å¤„ç†/ç†è§£(<a href="/web/20220926095407/https://neptune.ai/blog/iclr-2020-nlp-nlu" target="_blank" rel="noreferrer noopener">æ­¤å¤„</a></li></ol>
</div>


<p>ä¸ºäº†å¯¹ICLRå¤§å­¦çš„é¡¶çº§è®ºæ–‡æœ‰ä¸€ä¸ªæ›´å®Œæ•´çš„æ¦‚è¿°ï¼Œæˆ‘ä»¬æ­£åœ¨å»ºç«‹ä¸€ç³»åˆ—çš„å¸–å­ï¼Œæ¯ä¸ªå¸–å­éƒ½ä¸“æ³¨äºä¸Šé¢æåˆ°çš„ä¸€ä¸ªä¸»é¢˜ã€‚ä½ å¯èƒ½æƒ³è¦<strong>æŸ¥çœ‹</strong>ä»¥è·å¾—æ›´å®Œæ•´çš„æ¦‚è¿°ã€‚</p>



<p>å¿«ä¹é˜…è¯»ï¼</p>




<div id="author-box-new-format-block_605afcb8e8a39" class="article__footer article__author">
  

  <div class="article__authorContent">
          <h3 class="article__authorContent-name">å¡ç±³å°”Â·å¡ä»€é©¬é›·å…‹</h3>
    
          <p class="article__authorContent-text">äººå·¥æ™ºèƒ½ç ”ç©¶å€¡å¯¼è€…ï¼Œåœ¨MLOpsé¢†åŸŸå·¥ä½œã€‚æ€»æ˜¯åœ¨å¯»æ‰¾æ–°çš„MLå·¥å…·ã€è¿‡ç¨‹è‡ªåŠ¨åŒ–æŠ€å·§å’Œæœ‰è¶£çš„MLè®ºæ–‡ã€‚å¶å°”ä¼šæœ‰åšå®¢ä½œè€…å’Œä¼šè®®å‘è¨€äººã€‚</p>
    
          
    
  </div>
</div>


<div class="wp-container-1 wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>



<p class="has-text-color"><strong>é˜…è¯»ä¸‹ä¸€ç¯‡</strong></p>



<h2>å¦‚ä½•ç»„ç»‡æ·±åº¦å­¦ä¹ é¡¹ç›®â€”â€”æœ€ä½³å®è·µèŒƒä¾‹</h2>



<p class="has-small-font-size">13åˆ†é’Ÿé˜…è¯»|ä½œè€…Nilesh Barla |å¹´5æœˆ31æ—¥æ›´æ–°</p>


<p id="block_5ffc75def9f8e" class="separator separator-10"/>



<p>ä¸€ä¸ªæˆåŠŸçš„æ·±åº¦å­¦ä¹ é¡¹ç›®ï¼Œä½ éœ€è¦å¾ˆå¤šè¿­ä»£ï¼Œå¾ˆå¤šæ—¶é—´ï¼Œå¾ˆå¤šåŠªåŠ›ã€‚ä¸ºäº†è®©è¿™ä¸ªè¿‡ç¨‹ä¸é‚£ä¹ˆç—›è‹¦ï¼Œä½ åº”è¯¥å°½é‡åˆ©ç”¨ä½ çš„èµ„æºã€‚</p>



<p>ä¸€ä¸ªå¥½çš„å¾ªåºæ¸è¿›çš„å·¥ä½œæµç¨‹å°†å¸®åŠ©ä½ åšåˆ°è¿™ä¸€ç‚¹ã€‚æœ‰äº†å®ƒï¼Œä½ çš„é¡¹ç›®å˜å¾—<strong>é«˜æ•ˆã€å¯å¤åˆ¶ã€</strong>å’Œ<strong>å¯ç†è§£</strong>ã€‚</p>



<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä½•æ„å»ºæ·±åº¦å­¦ä¹ é¡¹ç›®çš„å·¥ä½œâ€”â€”ä»å¼€å§‹åˆ°éƒ¨ç½²ï¼Œç›‘æ§éƒ¨ç½²çš„æ¨¡å‹ï¼Œä»¥åŠä¸­é—´çš„ä¸€åˆ‡ã€‚</p>



<p>åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Neptuneæ¥è¿è¡Œã€ç›‘æ§å’Œåˆ†ææ‚¨çš„å®éªŒã€‚Neptuneæ˜¯æé«˜MLé¡¹ç›®ç”Ÿäº§ç‡çš„ä¸€ä¸ªå¾ˆé…·çš„å·¥å…·ã€‚</p>



<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†äº†è§£åˆ°:</p>


<div class="custom-point-list">
<ol><li>å…³äºé¡¹ç›®çš„ç”Ÿå‘½å‘¨æœŸã€‚</li><li>å®šä¹‰é¡¹ç›®ç›®æ ‡çš„é‡è¦æ€§ã€‚</li><li>æ ¹æ®é¡¹ç›®éœ€æ±‚æ”¶é›†æ•°æ®ã€‚</li><li>æ¨¡å‹è®­ç»ƒå’Œç»“æœæ¢ç´¢ï¼ŒåŒ…æ‹¬:<ol><li>ä¸ºæ›´å¥½çš„ç»“æœå»ºç«‹åŸºçº¿ã€‚</li><li>é‡‡ç”¨ç°æœ‰çš„å¼€æºæœ€æ–°æ¨¡å‹ç ”ç©¶è®ºæ–‡å’Œä»£ç åº“ä¸­çš„æŠ€æœ¯å’Œæ–¹æ³•ã€‚</li><li>å®éªŒè·Ÿè¸ªå’Œç®¡ç†</li></ol></li><li>é¿å…æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆçš„æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯ï¼Œä¾‹å¦‚:<ol><li>æ§åˆ¶è¶…å‚æ•°</li><li>è§„èŒƒåŒ–</li><li>ä¿®å‰ª</li></ol></li><li>åœ¨éƒ¨ç½²ä¹‹å‰æµ‹è¯•å’Œè¯„ä¼°æ‚¨çš„é¡¹ç›®ã€‚</li><li>æ¨¡å‹éƒ¨ç½²</li><li>é¡¹ç›®ç»´æŠ¤</li></ol>
</div>

<a class="button continous-post blue-filled" href="/web/20220926095407/https://neptune.ai/blog/how-to-organize-deep-learning-projects-best-practices" target="_blank">
    Continue reading -&gt;</a>



<hr class="wp-block-separator"/>
</div></div>
</div>
      </div>    
</body>
</html>