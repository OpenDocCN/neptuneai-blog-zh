# 推荐系统:构建和部署的经验

> 原文：<https://web.archive.org/web/https://neptune.ai/blog/recommender-systems-lessons-from-building-and-deployment>

如果你看一下推荐系统的论文，你会发现大量的论文来自于行业而不是学术界。这是因为 RecSys 实际上是一个实际问题。电子商务的 RecSys 可能与社交媒体的 RecSys 有很大不同，因为它们的业务目标不同。此外，每一个新颖的想法都需要在现实世界中接受检验，才能获得可信度。因此，了解 RecSys 的实用性就像了解新颖的架构一样重要。

![Structure of a recommender system](img/ff54c2b5ab8c91a045b48889849efbfa.png)

*Structure of a recommender system | [Source](https://web.archive.org/web/20221230103328/https://www.researchgate.net/figure/Structure-of-a-recommender-system_fig2_220827211)*

本文讨论了构建推荐系统时的实际考虑事项。具体来说，我们将从以下几个方面谈谈我对推荐系统的认识:

*   数据集创建
*   目标设计
*   模特培训
*   模型评估
    *   离线评估
    *   检测和减轻偏见
*   检查模型正确性的清单
*   RecSys 架构
*   在线营销
*   A/B 测试

*注:文中所有观点均为作者个人观点，不代表作者现任或前任雇主。*

## 推荐系统:数据集创建

RecSys 的这一步不像文本或图像分类那样简单。例如，假设我们正在创建一个 RecSys，它可以预测电子商务网站的点击量。如果我们有少量的用户和项目，我们可以在所有数据上训练我们的模型。

然而，如果我们在亚马逊或沃尔玛的规模上工作，我们有数百万的每日活跃用户和目录中的商品。在整个历史交互上训练一个简单的协作过滤模型将花费我们很多成本——从数据仓库读取数据(如果不是 PBs，则以 TB 为单位),旋转一个高容量的 VM(将运行数周)。我们必须质疑这样做是否值得，以及正确的做法是什么。

如果我们的数据库中有十亿用户，每天有几百万活跃用户，那么我们必须只为这些活跃用户训练，因为不活跃的用户出现的机会更少。用户可以通过设置最近 N 天的活动阈值来选择这个用户子集，例如选择在最近 10 天内点击了> =10 个项目的用户。如果我们没有包括在培训中的一些用户出现，我们可以退回到一个定制的逻辑，比如基于内容或基于流行度的检索。由于 RecSys 模型会定期接受训练，因此这部分用户会不断变化。一旦我们选择了这个用户子集，我们就可以在与这些用户的交互上训练我们的模型。

**下一个问题是，多少数据才够？**如果我们有五年的数据，我们不需要全部。是的，模型受益于更多的数据。但在 RecSys 中，主要思想是最好地抓住用户的兴趣，这种兴趣会随着时间而变化。所以有新鲜的训练数据更有意义。此外，简单的协同过滤模型不能捕获太多的复杂性。人们可以通过绘制一个度量与训练步骤数的关系图来验证这一点，这很可能会显示收益递减。

接下来，**在你的数据集中检测重复是有帮助的**，就像相同的视频/物品用不同的 id 发布两次。此外， [NLP](/web/20221230103328/https://neptune.ai/blog/category/natural-language-processing) 和 [CV](/web/20221230103328/https://neptune.ai/blog/category/computer-vision) 模型可以帮助删除数据集的 NSFW、有害和非法内容。

遵循这些步骤可以大大减少数据集的大小。这将帮助我们以最小的质量损失节省成本。

## 推荐系统:设计最优目标

RecSys 的最终目标是给人们想要的东西。尽管这是一个宽泛且颇具哲理性的问题，但我们必须将其缩小到模型必须优化的特定信号——预测点击、喜欢、分享等。当我们训练一个模型来预测点击并使用它来提供推荐时，我们的基本假设是，如果你点击了一个项目，它就与你相关。通常情况下，这并不完全正确。

为了更好地理解这一点，让我们用一个不同的例子。假设你正在为 YouTube 构建一个 RecSys，它可以预测用户是否会点击某个特定的视频。该模型用于基于点击概率提供推荐。然而，这种模式减少了用户在平台上花费的时间。**原因是点击不等同于关联**。大多数 clickbait 视频的点击率都很高，但观众在几秒钟后就会停止观看。一个 100%准确的模型将提供大量被点击但未被观看的视频。

从上面的学习中，您决定训练一个模型来预测用户是否会观看至少 75%的视频。因此，训练示例将包括(用户、视频、标签)三元组，其中如果> =75%的视频被观看，则标签=1，否则为 0。这比点击模型更好，因为现在我们认为用户所做的不仅仅是点击一个视频。然而，即使这样也有一个大问题。

考虑两个视频，A 和 B，A 是娱乐性的 20 秒长的视频，B 是 60 分钟的教程视频。要看 75%，需要看 15 秒的 A，45 秒的 b。

自然地，A 将比 B 具有更高的该标签的肯定率。然而，观看 15 秒的 A 可能意味着用户不喜欢 A(因为 15 秒对于决定你是否更喜欢该内容来说太短了)，观看 30 分钟(50%)的 B 很可能意味着 B 与用户相关。即使是高度精确的模型最终也会提供不成比例的大量较短持续时间的视频，这不是最优的。

关键是一个信号很少定义完全的相关性。每个信号都有自己的偏差。在多个信号上训练多个模型，结合它们各自的分数(例如，加权相加)，并创建最终分数，这是一个很好的做法。

## 推荐系统:模型训练

大型 NLP 或视觉模型有数十亿个分布在线性、卷积、递归或注意力层中的参数。这些参数中的每一个都涉及到输出的计算。然而，在推荐模型中，模型大小比大多数 NLP 或 CV 模型大得多。

考虑矩阵分解，其中模型学习一个用户和一个项目嵌入(在[协同过滤](https://web.archive.org/web/20221230103328/https://developers.google.com/machine-learning/recommendation/collaborative/basics)的情况下)。如果嵌入维度是 100，你有 1 亿个用户，1000 万个商品。总嵌入数为 1.1 亿。每个嵌入有 100 个可学习的参数。因此，该模型有 110*1 亿或约 110 亿个参数。然而，要计算一个用户的分数，你一次只需要访问 1 亿个用户嵌入中的一个。这个特定的用户嵌入与所有的项目嵌入一起使用来对所有的项目评分。因此，推荐模型占用大量内存，但计算量较小。

这是一个不同的挑战，因为现在你不能也不需要为一批数据在 GPU/TPU 上加载整个嵌入表。然而，在传统框架如 [TensorFlow](https://web.archive.org/web/20221230103328/https://www.tensorflow.org/) 或 [PyTorch](https://web.archive.org/web/20221230103328/https://pytorch.org/) 上编写这样的模型很难，因为它们的默认行为是在 GPU/TPUs 上加载整个模型。幸运的是，许多框架都为此构建了功能。

Tensorflow 构建了一个名为[**tensor flow _ re commenders**](https://web.archive.org/web/20221230103328/https://www.tensorflow.org/recommenders)**的框架，并带有一个名为[tpuemleding](https://web.archive.org/web/20221230103328/https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/embedding/TPUEmbedding)的特殊嵌入表。此外，它还实现了 RecSys 中许多常见任务的版本，如[检索](https://web.archive.org/web/20221230103328/https://www.tensorflow.org/recommenders/api_docs/python/tfrs/tasks/Retrieval)和[排序](https://web.archive.org/web/20221230103328/https://www.tensorflow.org/recommenders/api_docs/python/tfrs/tasks/Ranking)以及流行的架构，如 [DCN](https://web.archive.org/web/20221230103328/https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/dcn) 。**

 **近日，PyTorch 公布了 [**torchrec**](https://web.archive.org/web/20221230103328/https://pytorch.org/torchrec/) 。据该小组称:

“TorchRec 是一个 PyTorch 域库，旨在提供大规模推荐系统(RecSys)所需的通用稀疏度&并行原语。它允许作者使用分布在许多 GPU 上的大型嵌入表来训练模型。”

NVIDIA 也有 [**梅林**](https://web.archive.org/web/20221230103328/https://developer.nvidia.com/nvidia-merlin) ，它可以自动化 RecSys 中的常见流程，以实现更快的生产级系统。它支持 Tensorflow 和 PyTorch，构建在 cuDF(相当于熊猫的 GPU)、RAPIDS(基于 GPU 的分析和数据操作库)和 Triton(高性能推理服务器)之上。

## 推荐系统:模型评估

### 离线评估

典型的分类任务针对准确性、精确度、召回率或 F1 分数等指标进行优化。使用这些指标评估 RecSys 具有欺骗性。在 RecSys，我们对客观概率不感兴趣。我们对排名更感兴趣。例如，如果视频 A 和 B 的预测得分是 0.9 和 0.8，我们将在服务时首先显示视频 A，然后显示视频 B。即使 A 和 B 的概率是 0.5，0.4，或者 0.3，0.2，结果仍然是一样的。重要的是顺序，而不是绝对数字。因此，ROC-AUC、PR-AUC、NDCG、recall@K 和 precision@K 等指标更适合。

然而，即使这样，这种评价也是可遇不可求的。推荐系统因对某些主题、人口统计或受欢迎程度的复合偏见而臭名昭著。推荐系统在自己生成的日志上进行训练。如果受欢迎的内容被系统提升得更多，那么所生成的增量日志将具有更多针对该受欢迎的内容的三元组。在这些新日志上训练的模型的下一个版本将会看到一个偏斜的分布，并且将会知道推荐流行的项目是一个安全的选择。这就是所谓的流行偏见。

建议计算不同级别的指标，如年龄、性别、位置等用户属性。这有助于我们了解该模型对于特定的一组用户是否表现更好，而对于其他用户是否表现不佳。像 [reclist](https://web.archive.org/web/20221230103328/https://github.com/jacopotagliabue/reclist) 这样的工具提供了一个简单的界面来深入你的推荐模型。

另一个有用的工具可能是 [Neptune](/web/20221230103328/https://neptune.ai/experiment-tracking) ，因为它提供了简单的日志 API 来进行更有组织性、协作性和综合性的分析。人们可以[创建定制的仪表板](https://web.archive.org/web/20221230103328/https://docs.neptune.ai/you-should-know/displaying-metadata#creating-dashboards)通过交互式可视化来可视化日志。如上所述，我们感兴趣的是基于人口统计和位置等属性的多重切割的度量。我们可以在这里绘制 ROC/PR AUC、损失曲线和对数排名指标，并轻松比较和确定该模型是否真正稳健。

![Example dashboard in Neptune with  visualize logs through interactive visualizations](img/8ee4a5f7285c1e6c8644c352f2714049.png)

*Example dashboard in Neptune | [Source](/web/20221230103328/https://neptune.ai/experiment-tracking)*

### 检测和减轻偏见

如前所述，如果不加以注意，像受欢迎程度偏差这样的偏差很容易在系统中传播。但是我们如何在减轻偏见之前衡量它呢？

![Mitigating bias](img/ff9b85541ad9cb050a3dfb22fd81ad6d.png)

*A loop of detecting and mitigating bias| [Source](https://web.archive.org/web/20221230103328/https://people.engr.tamu.edu/caverlee/pubs/Ziwei_KDD_2021.pdf)*

一个简单的测量受欢迎程度偏差的方法是检查有多少独特的项目占 10%，20%，50%，..百分之百的推荐。在理想的情况下，项目的数量应该随着推荐量的增加而增加。但是，对于有偏差的模型，项目数量会在某个百分比(通常在较低端)后饱和。这是因为该模型仅依赖可推荐项目的某个子集来进行预测。

但是这种方法没有考虑用户的偏好。例如，如果用户 U1 与三个项目 A、B 和 C 交互；并且喜欢项目 A 和 B 但不喜欢项目 C。类似地，用户 U2 与 A、B 和 C 交互；只喜欢 A。我们知道 A 是受欢迎的商品，而 B 和 C 不是。

|  | 答(受欢迎) | b(不流行) | c(不流行) |
| --- | --- | --- | --- |
|  |  |  | 0 |
|  |  | 0 | 0 |

*简单有偏模型的例子*

对 U1 来说，如果模型对 A 的评分高于 B，那么它可能是有偏见的。因为用户对它们的反应都是积极的。即使模型始终支持更受欢迎的项目，我们也有一个有偏见的模型。然而，对于 U2 来说，将受欢迎的项目排名更高是有意义的，因为 U2 不喜欢另外两个不受欢迎的项目。虽然我们使用的例子非常简单，但有统计奇偶校验这样的方法可以帮助你衡量这一点。

有一些简单的方法可以减轻偏见。**一种方法是引入阴性样本。**考虑一个电子商务平台，用户在这个平台上与数百件商品中的几件互动。我们只知道用户交互了哪些项目(正面的例子)。然而，我们不知道其他物品发生了什么。为了平衡这个数据集，我们通过为一个用户随机抽样一个项目并给它分配一个负标签(=0)来引入负样本。假设用户不会喜欢随机挑选的商品。由于这种假设很可能是正确的，所以添加负样本实际上会将缺失的信息添加到数据集中。

## 测试推荐系统模型正确性的清单

像任何软件一样，人们应该通过编写单元测试来确保模型的正确性。不幸的是，编写 ML 代码单元测试并不常见，也很棘手。但是，对于 RecSys 来说，让我们关注一个简单的 CF(协同过滤)模型。众所周知，该模型本质上是用户嵌入和项目嵌入的集合。您可以对该模型进行以下测试:

1.  **正确评分**–消耗用户和项目嵌入的评分操作应产生介于 0 和 1 之间的分数。
2.  **正确的版本控制**–由于嵌入会定期重新训练，因此正确地对它们进行版本控制以保持分数一致是非常重要的。
3.  **正确的特征**–一些模型，如双塔模型，使用诸如最近 X 小时的用户活动的特征。我们需要确保模型使用的特性管道不会产生泄漏的特性。
4.  **正确的训练数据集**–数据集不应有重复的用户-项目对，标签应正确，训练-测试-分割应是随机的。

## RecSys 架构

推荐系统必须从数百万个项目中为用户挑选最好的一组。然而，这必须在严格的延迟要求内完成。因此，我们训练的模型越复杂，处理一个请求所需的时间就越多。因此，RecSys 采用多级架构。你可以把它想象成一个漏斗，从一百万个项目开始，到少数几个推荐结束。

这个想法是在这个漏斗的顶端使用一个简单、轻量级的模型，就像一个简单的协作过滤模型。该模型应该能够挑选出几千个最相关的项目，可能没有最好的排名，即相关项目应该出现在这几千个项目中，如果它们不在顶部也没关系。因此，该模型优化了召回率和速度。这个模型也被称为候选生成器。即使在简单的协同过滤模型中，也要确保嵌入维度不要太大。使用 100 维可能会使您的回忆略有增加，但会影响您的延迟。

然后，这数以千计的物品被送到另一个名为 light ranker 的模型中。顾名思义，这个模型的任务就是寻找最佳排名。模型经过高精度训练，比候选发电机更复杂(例如双塔模型)。它还使用了更多基于用户活动、项目元数据等的功能。这个模型的结果是一个包含数百个项目的排序列表。

最后，这数百件物品被送到重 ranker。这个等级与轻等级有着相似的目标，除了它比轻等级更重并且使用更多的特性。因为它只对数百个项目进行操作，所以这种复杂体系结构所涉及的延迟是可管理的。

![RecSys architecture](img/05a6acf680122c307ea927159cd40490.png)

*Recommender systems architecture | [Source](https://web.archive.org/web/20221230103328/https://milvus.io/blog/2021-11-26-accelerating-candidate-generation-in-recommender-systems-using-milvus-paired-with-paddlepaddle.md)*

## 用于推荐系统的在线 MLOps

与分类或回归模型相比，推荐模型的一个好处是我们可以获得实时反馈或“标签”。因此，我们可以建立一个全面的 ML Ops 管道来密切监控您的模型性能。

我们可以监控许多指标。

## 

*   1 在平台上花费的时间

*   2 订婚
*   3 次点击
*   4 采购

*   5 用户流失

模型在参与度等指标上的表现很容易在离线实验中衡量。然而，你不能在一个离线实验中测量像流失这样的东西。在现实世界的 RecSys 中发现这种差异是很常见的。通常，我们会分析哪些在线指标(如花费的时间、参与度、点击量)与客户流失正相关。这减少了在离线实验中改进一组可预测指标的问题。

**除了模型质量和性能，我们还应该监控平均值、第 95 个百分点、**和第 99 个百分点的延迟、CPU、非 200 状态代码率、**和内存使用情况。**这并不奇怪，但是提高这些指标也可以减少花费的时间并减少流失。像 [Grafana](https://web.archive.org/web/20221230103328/https://grafana.com/) 这样的工具有助于建立全面的观察仪表板。

再培训管道也可能因为与代码错误无关的问题而中断，比如 Kubernetes 集群中没有足够的 pods 可用，或者没有足够的 GPU 资源可用。**如果您在 Airflow 上使用 DAGs，它可以选择在 Slack 上设置故障警报。**或者，调整重试次数和超时参数，以便提高自动恢复的机会。

## 推荐系统:A/B 测试

改进推荐系统是一个持续的过程。然而，这种改进不应该恶化用户体验。如果你的团队提出了一个新颖的模型，在离线评估中显示出惊人的收益，那么向所有用户推广这个模型并不明显。这就是 A/B 测试发挥作用的地方。

任何新的目标模型都必须对照控制(现有生产)模型进行评估。在 A/B 测试中，您将随机选择一小部分用户，并使用目标模型为他们服务，而其余的用户像以前一样从控制模型接收建议。几天/几周后，看看哪个模型表现更好，并使用假设检验对其进行量化。如果测试的结论是新的模型比控制的模型更好，那么您就向所有用户推出新的模型。

然而，一个好的做法是只向 98-99%的用户推广新模型，而让其余 1-2%的用户使用控制模型。这 1-2%的用户被称为维持组。**这里的想法是看看，在某个时候，新模型是否开始退化，是由于影响所有模型的一些变化，还是仅仅这个新模型有问题？**在 RecSys 中，当服务于一小组用户时，目标模型仍然在主要由控制模型生成的日志上被训练。然而，有可能当新模型成为控制时，它开始从主要由它自己生成的日志中学习并退化。

## 结论

RecSys 有许多移动部件，这些部件中的每一个都是一个旋钮，可以通过调节来使系统变得更好。就我个人而言，这是 RecSys 让我真正感兴趣的地方。我希望这篇文章能够提供新的思考方向。每个主题都有不同数量的文献供你探索。下面我链接了一些参考。一定要检查他们！

### 参考

[1] [TwHIN:嵌入 Twitter 异构信息网络进行个性化推荐](https://web.archive.org/web/20221230103328/https://arxiv.org/abs/2202.05387)

[2] [协同过滤中的流行度-机会偏差](https://web.archive.org/web/20221230103328/https://dl.acm.org/doi/pdf/10.1145/3437963.3441820)

[3] [在 Twitter 上解决基于模型的候选人生成中数据集偏差的经验教训](https://web.archive.org/web/20221230103328/https://arxiv.org/pdf/2105.09293.pdf)**