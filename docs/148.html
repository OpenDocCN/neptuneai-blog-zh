<html>
<head>
<title>Segmenting and Colorizing Images in IOS App Using Deoldify and Django API </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>使用Deoldify和Django API在IOS应用程序中分割和着色图像</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/segmenting-and-colorizing-images-in-ios-app#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/segmenting-and-colorizing-images-in-ios-app#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p><a href="https://web.archive.org/web/20221206013957/https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/" target="_blank" rel="noreferrer noopener nofollow">图像分割</a>属于涉及<a href="https://web.archive.org/web/20221206013957/https://machinelearningmastery.com/object-recognition-with-deep-learning/" target="_blank" rel="noreferrer noopener nofollow">深度物体检测</a>和识别的成像领域。如果我们通过按像素分离的方式将一幅图像分成多个区域，场景中的每个对象都允许我们为需要高标准图像分析和上下文解释的任务训练复杂的深度学习模型。以这种方式训练的模型可以确定检测到的物体的形状，预测检测到的物体将进入的方向，并产生许多其他见解。</p>







<p>我们将通过动手开发一个处理模型服务的后端API和一个使用服务的小型IOS应用程序来学习图像分割的工作原理。</p>



<p>API将由在不同阶段处理输入图像的多个视图组成。后端服务的处理逻辑就像每个视图都是负责单个功能的嵌套微服务一样运行，从背景定制到背景灰度和旧图像着色。</p>



<p>图像分割可以用各种技术完成，每种技术都有其优点和缺点:</p>



<p id="separator-block_61af563e86c86" class="block-separator block-separator--10"> </p>



<div id="medium-table-block_61af564686c87" class="block-medium-table c-table__outer-wrapper ">

    <table class="c-table">
                    <thead class="c-table__head">
            <tr>
                                    <td class="c-item">
                        <p class="c-item__inner">分割技术</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">描述</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">优势</p>
                    </td>
                                    <td class="c-item">
                        <p class="c-item__inner">不足之处</p>
                    </td>
                            </tr>
            </thead>
        
        <tbody class="c-table__body">

                    
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>根据图像直方图峰值寻找特定阈值</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>最简单的方法。不需要以前的信息</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>空间细节考虑不周全，很大程度上取决于颜色变化和峰值</p> </div></td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>应用间断检测技术</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>对物体间对比度好的图像表现很好</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>图像中有太多边缘时不适合</p> </div></td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>同质图像分割以找到特定区域</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>当相似性标准可以很容易地定义时有用</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>在时间和空间复杂度方面相当昂贵</p> </div></td>

                    
                </tr>

            
                <tr class="c-row">

                    
                        <td class="c-ceil">                                                      </td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>基于模拟的学习过程进行决策</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>神经网络架构，无需编写复杂程序</p> </div></td>

                    
                        <td class="c-ceil"><div class="c-ceil__inner"> <p>要求大量的训练数据</p> </div></td>

                    
                </tr>

                    
        </tbody>
    </table>

</div>



<p id="separator-block_60adfc3d8e14d" class="block-separator block-separator--20"> </p>



<p>有三种主要类型的图像分割:</p>



<ul><li><strong>语义分割</strong>:识别可训练对象类，<em>相应地分离</em>。</li><li><strong>实例分割</strong>:检测每个对象类的实例数量。因此，它可以更准确地分离组件，并有助于将整个图像分解为多个标记区域，这些区域引用模型被训练的类别。</li><li><strong>全景分割</strong>:语义和实例分割的统一版本。</li></ul>







<p>我们还将看看最近的图像技术，这些技术可以准确地给旧的黑白照片上色。用于执行这种艺术任务的算法是特定的<a href="/web/20221206013957/https://neptune.ai/blog/generative-adversarial-networks-gan-applications" target="_blank" rel="noreferrer noopener"> <strong>生成对抗</strong> <strong>网络</strong>【GANs】</a>的组合，这些网络生成与图像中存在的对象相匹配的精确颜色颜料。模型分割图像，并根据它们被训练的类别对每个像素进行着色。</p>



<p>我们将使用的库是<a href="https://web.archive.org/web/20221206013957/https://github.com/jantic/DeOldify#about-deoldify">去文件夹</a>。他们有一个配备齐全的Github repo，里面有很多例子和教程，可以帮助你快速入门。</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/699fe2ae36fb4f77d6fcd90a94eb62ba.png" alt="Deoldify example" class="wp-image-46660" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013957im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Deoldify-example.jpeg?ssl=1"/><figcaption><em>Deoldify example from their github repo | <a href="https://web.archive.org/web/20221206013957/https://github.com/jantic/DeOldify#about-deoldify" target="_blank" rel="noreferrer noopener nofollow">Source</a></em></figcaption></figure></div>



<p>对于本文，我们将讨论以下内容:</p>



<ul><li>关于图像分割的DeepLab V3+的一些技术背景，</li><li>使用DeepLab-ResNet101的Pytorch实现，</li><li>测试Deoldify处理黑白图像并提出彩色版本，</li><li>将所有模型包装在一个API中为它们服务，</li><li>创建一个小的IOS应用程序来获取图像结果，</li><li>结论。</li></ul>



<p>你可以在我的<a href="https://web.archive.org/web/20221206013957/https://github.com/aymanehachcham/Background-Customization" target="_blank" rel="noreferrer noopener nofollow"> Github repo </a>中查看这个项目的全部代码。</p>



<h2 id="h-technical-background-for-deeplab-v3">DeepLab V3+的技术背景</h2>



<h3>阿特鲁空间金字塔汇集卷积</h3>



<p>大多数分割模型使用FCNN作为第一处理阶段，以在对象检测阶段之前正确地放置所需的遮罩和边界。DeepLab V3+是谷歌DeepLab细分模型的最新和最复杂的迭代。</p>



<p>开发DeepLab的众多原因之一是它能够实现众多应用，例如在Pixel 2智能手机的人像模式功能中使用的<a href="https://web.archive.org/web/20221206013957/https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html" target="_blank" rel="noreferrer noopener nofollow">合成浅景深效果</a>。</p>



<p>DeepLab V3+版本包括建立在CNN架构主干之上的模型，但该模型主要依赖于新引入的<a href="https://web.archive.org/web/20221206013957/https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noreferrer noopener nofollow">阿特鲁空间金字塔池卷积</a> (ASPP)。总体结构呈现以下阶段:</p>



<ul><li>使用CNN主干提取图像特征。在我们的例子中，主干是ResNet-101，它将识别和检测将被馈送到进一步阶段的掩模特征图。</li><li>控制输出的大小，使语义信息不会丢失太多。</li><li>在最后阶段，ASPP对输出图像的不同像素进行分类，并通过1×1卷积层进行处理，以恢复正确的原始大小。</li></ul>







<h2 id="h-pytorch-implementation-of-deeplab-resnet101">DeepLab-ResNet101的PyTorch实现</h2>



<p>为了快速体验DeepLab，我们将使用PyTorch实现，该实现提出了一个以ResNet101为基础的deeplab v3版本，该版本在COCO数据集上进行了预先训练，并且可以从torchvision包中轻松加载。</p>







<p>我将尽可能详细地描述编写调用模型的Python模块所需的不同步骤。我们将从头开始。</p>



<h3>启用您的Python虚拟环境</h3>



<p>使用<em> virtualenv </em>或<em> anaconda </em>为项目创建一个虚拟环境，在其中您将安装所有需要的依赖项。请记住，我们将要测试的预训练版本是基于GPU的。</p>



<p>1.下载安装Anaconda: <a href="https://web.archive.org/web/20221206013957/https://docs.anaconda.com/anaconda/install/" target="_blank" rel="noreferrer noopener nofollow">网站</a></p>



<p>2.创建您的虚拟环境:</p>



<pre class="hljs">conda create --name seg_env python=<span class="hljs-number">3.6</span>
</pre>



<p>3.激活虚拟环境:</p>



<pre class="hljs">conda activate seg_env</pre>



<p>4.安装所需的库:</p>



<pre class="hljs">conda install pytorch torchvision cudatoolkit=<span class="hljs-number">10.2</span> -c pytorch

pip install opencv-python

pip install numpy

pip install Pillow=<span class="hljs-number">2.2</span><span class="hljs-number">.1</span></pre>



<h3>安装验证要求</h3>



<ul><li>克隆Deoldify Github repo并安装requirements.txt</li></ul>



<p>https://github.com/jantic/DeOldify.git</p>



<h3>实现数据加载器类</h3>



<p>在开始编码Python模块来包装模型行为之前，我们需要编码一个数据加载器来处理输入图像文件。数据加载器的主要目的是预处理所有图像输入文件，将它们转换为具有特定属性和特性的高级对象，这将有助于在我们想要针对一批原始输入训练或评估模型时简化工作。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SegmentationSample</span><span class="hljs-params">(Dataset)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, root_dir, image_file, device)</span>:</span>
        
        self.image_file = os.path.join(root_dir, image_file)
        self.image = Image.open(self.image_file)

        
        <span class="hljs-keyword">if</span> device == <span class="hljs-string">'cuda'</span> <span class="hljs-keyword">and</span> torch.cuda.is_available():
            self.device = <span class="hljs-string">'cuda'</span>
        <span class="hljs-keyword">if</span> device == <span class="hljs-string">'cpu'</span>:
            self.device = <span class="hljs-string">'cpu'</span>

        
        self.preprocessing = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])
        ])
        self.unload_tensor = transforms.ToPILImage()

        
        self.processed_image = self.preprocessing(self.image)
        self.processed_image = self.processed_image.unsqueeze(<span class="hljs-number">0</span>).to(self.device)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, item)</span>:</span>
        <span class="hljs-keyword">return</span> self.processed_image

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">print_image</span><span class="hljs-params">(self, title=None)</span>:</span>
        image = self.image
        <span class="hljs-keyword">if</span> title <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            plt.title = title
        plt.imshow(image)
        plt.pause(<span class="hljs-number">5</span>)
        plt.figure()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">print_processed</span><span class="hljs-params">(self, title=<span class="hljs-string">'After processing'</span>)</span>:</span>
        image = self.processed_image.squeeze(<span class="hljs-number">0</span>).detach().cpu()
        image = self.unload_tensor(image)
        plt.title = title
        plt.imshow(image)
        plt.pause(<span class="hljs-number">5</span>)
        plt.figure()</pre>



<ul><li>init方法:获取root_dir和image文件，并将其转换为Pillow image对象，然后转换为torch张量。原始输入图像的像素值根据特定的平均值和标准值进行归一化。一旦所有的变换发生，我们就得到一个形状良好的量纲张量。因此，我们保证输入和模型维度完全匹配。</li></ul>



<h3>创建Python包装器来服务DeepLab模型</h3>



<p>该模块必须初始化deeplab-resnet101型号版本的预训练权重。它还要求用户指定在推理期间是使用CPU还是GPU加速。</p>



<p>此外，该模型还将实现自定义背景的方法。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SemanticSeg</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, pretrained: bool, device)</span>:</span>
        super(SemanticSeg, self).__init__()
        <span class="hljs-keyword">if</span> device == <span class="hljs-string">'cuda'</span> <span class="hljs-keyword">and</span> torch.cuda.is_available():
            self.device = <span class="hljs-string">'cuda'</span>
        <span class="hljs-keyword">if</span> device == <span class="hljs-string">'cpu'</span>:
            self.device = <span class="hljs-string">'cpu'</span>

        self.model = self.load_model(pretrained)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, input: SegmentationSample)</span>:</span>
        
        <span class="hljs-keyword">with</span> torch.no_grad():
            output = self.model(input.processed_image)[<span class="hljs-string">'out'</span>]

        reshaped_output = torch.argmax(output.squeeze(), dim=<span class="hljs-number">0</span>).detach().cpu()
        <span class="hljs-keyword">return</span> reshaped_output

    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_model</span><span class="hljs-params">(self, pretrained=False)</span>:</span>
        <span class="hljs-keyword">if</span> pretrained:
            model = models.deeplabv3_resnet101(pretrained=<span class="hljs-keyword">True</span>)
        <span class="hljs-keyword">else</span>:
            model = models.deeplabv3_resnet101()

        model.to(self.device)
        model.eval()
        <span class="hljs-keyword">return</span> model</pre>



<ul><li><code>forward(self, input: SegmentationSample)</code>:对采样图像输入进行推理，并返回张量预测。</li><li><code>load_model(self, pretrained=<strong>False</strong>)</code>:加载<strong> <em> Pytorch cloud </em> </strong>中的deeplabv3_resnet101预训练版本。将模型检查点的<strong> <em> eval </em> </strong>模式保存到相应的设备。</li></ul>



<p>之后，我们将添加后处理方法来帮助在模型预测的顶部重新映射定制的背景。请记住，输出张量有21个通道与模型训练的每个目标类的预测结果相匹配。因此，我们需要解码张量形状以输出正确的图像结果。</p>



<pre class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">background_custom</span><span class="hljs-params">(self, input_image, source, background_source,number_channels=<span class="hljs-number">21</span>)</span>:</span>

        label_colors = np.array([(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>),  
                                 
                                 (<span class="hljs-number">128</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">128</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">128</span>, <span class="hljs-number">0</span>, <span class="hljs-number">128</span>),
                                 
                                 (<span class="hljs-number">0</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">64</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">192</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">0</span>),
                                 
                                 (<span class="hljs-number">192</span>, <span class="hljs-number">128</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">64</span>, <span class="hljs-number">0</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">192</span>, <span class="hljs-number">0</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">192</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>),
                                 
                                 (<span class="hljs-number">0</span>, <span class="hljs-number">64</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">128</span>, <span class="hljs-number">64</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">192</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">64</span>, <span class="hljs-number">128</span>)])

        
        r = np.zeros_like(input_image).astype(np.uint8)
        g = np.zeros_like(input_image).astype(np.uint8)
        b = np.zeros_like(input_image).astype(np.uint8)

        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, number_channels):
            <span class="hljs-keyword">if</span> l == <span class="hljs-number">15</span>:
                idx = input_image == l
                r[idx] = label_colors[l, <span class="hljs-number">0</span>]
                g[idx] = label_colors[l, <span class="hljs-number">1</span>]
                b[idx] = label_colors[l, <span class="hljs-number">2</span>]

        rgb = np.stack([r, g, b], axis=<span class="hljs-number">2</span>)
        

        
        foreground = cv2.imread(source)
        foreground = cv2.resize(foreground, (r.shape[<span class="hljs-number">1</span>], r.shape[<span class="hljs-number">0</span>]))
        foreground = cv2.cvtColor(foreground, cv2.COLOR_BGR2RGB)

        background = cv2.imread(background_source, cv2.IMREAD_COLOR)
        background = cv2.resize(background, (rgb.shape[<span class="hljs-number">1</span>], rgb.shape[<span class="hljs-number">0</span>]), interpolation=cv2.INTER_AREA)
        background = cv2.cvtColor(background, cv2.COLOR_BGR2RGB)

        
        th, alpha = cv2.threshold(np.array(rgb), <span class="hljs-number">0</span>, <span class="hljs-number">255</span>, cv2.THRESH_BINARY)

        
        foreground = foreground.astype(float)
        background = background.astype(float)
        
        alpha = alpha.astype(float) / <span class="hljs-number">255</span>
        
        foreground = cv2.multiply(alpha, foreground)
        
        background = cv2.multiply(<span class="hljs-number">1.0</span> - alpha, background)
        
        outImage = cv2.add(foreground, background)

        <span class="hljs-keyword">return</span> outImage / <span class="hljs-number">255</span></pre>



<ul><li><code>background_custom(self, input_image, source, background_source, channels=21)</code>:该方法采用具有高度、宽度和21个特征图预测[1，21，H，W]的输出张量、图像源文件的路径和背景图像文件的路径。该逻辑包括从剩余特征中仅提取人物特征图(特征15 ),并将所有剩余特征标记为属于背景。最后，将先前标记的特征作为背景与新的图像源文件合并。</li></ul>



<h3>将Deoldify添加到模块中</h3>



<pre class="hljs"><span class="hljs-keyword">from</span> deoldify <span class="hljs-keyword">import</span> device
<span class="hljs-keyword">from</span> deoldify.device_id <span class="hljs-keyword">import</span> DeviceId
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> fastai
<span class="hljs-keyword">from</span> deoldify.visualize <span class="hljs-keyword">import</span> *

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">colorize_image</span><span class="hljs-params">(self, input_image, output_image, render_factor=<span class="hljs-number">35</span>)</span>:</span>
    torch.backends.cudnn.benchmark = <span class="hljs-keyword">True</span>
    
    colorizer = get_image_colorizer(artistic=<span class="hljs-keyword">False</span>)
    colorized_image = colorizer.get_transformed_image(input_image, render_factor, watermarked=<span class="hljs-keyword">False</span>)
    colorized_image.save(output_image)</pre>



<ul><li>colorize_image(self，input_image，output_image):获取输入图像并调用colorizer . get _ transformed _ image(input _ image)，后者运行推理并返回输出彩色图像。</li></ul>



<h2 id="h-wrap-the-models-in-an-api">将模型包装在一个API中</h2>



<p>正如我们通常所做的那样，我们将使用Django来创建一个小型的Restful API，它定义了本地托管的端点，以通过forward POST和GET调用来测试我们的模型。</p>



<p>通常，API是数据库的一个窗口。API后端处理数据库查询和响应格式化。您收到的是一个静态响应，通常是JSON格式的，是您请求的任何资源的静态响应。</p>



<h3>让我们设置姜戈部分</h3>



<p>安装Django和Django Rest框架:</p>



<pre class="hljs">pip install django djangorestframework</pre>



<p>一旦正确安装了依赖项，转到根文件夹并初始化Django应用程序:</p>



<pre class="hljs">django-admin startproject semantic-seg</pre>



<p>现在你的Django项目已经准备好了。剩下唯一要做的就是实例化<strong> <em> Django rest框架</em> </strong>，并在初始项目文件夹中为它创建一个特定的文件夹。</p>



<ul><li>启动您的api应用程序:python manage.py startapp api</li><li>将新创建的api文件夹的路径添加到general settings.py文件中:</li></ul>



<pre class="hljs">INSTALLED_APPS = [
<span class="hljs-string">'api.apps.ApiConfig'</span>,
<span class="hljs-string">'django.contrib.admin'</span>,
<span class="hljs-string">'django.contrib.auth'</span>,
...
]</pre>



<p>API文件夹的树结构应该如下所示:</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img decoding="async" src="../Images/6e431016261a01b896fd6df55ddcef95.png" alt="Semantic-segmentation-API-folder-structure" class="wp-image-47434" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013957im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Semantic-segmentation-API-folder-structure.png?ssl=1"/><figcaption><em>Tree print of the project folder structure</em></figcaption></figure></div>



<p>一旦所有配置就绪，我们将继续对模型和序列化程序进行编码，这些模型和序列化程序将最终处理所有涉及来回请求的图像数据的事务性流程。</p>



<p>由于API将负责检索得到的修改图像，您可以利用<a href="https://web.archive.org/web/20221206013957/https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#images" target="_blank" rel="noreferrer noopener"> Neptune的图像记录</a>系统来跟踪和记录整个模型迭代中产生的不同图像版本。</p>



<p>基本上，每个输出图像都可以保存在您的Neptune平台中，并告知模型的性能和准确性。每一次迭代都会给出更好的结果，因此你可以用一种结构化的、组织良好的方式来比较所有的结果。</p>



<p>关于如何在Neptune中记录内容的更多信息，无论是表格、图表还是图像，我强烈建议您看看我以前的文章:</p>







<section id="blog-intext-cta-block_60ae13808e14e" class="block-blog-intext-cta  c-box c-box--default c-box--dark c-box--no-hover c-box--standard ">

            
    
            <p>检查您可以<a href="https://web.archive.org/web/20221206013957/https://docs.neptune.ai/you-should-know/what-can-you-log-and-display" target="_blank" rel="noopener">在Neptune </a>中记录和显示哪些元数据。</p>
    
    </section>



<h3>姜戈模具模块</h3>



<p>为了简化Django，我们正在构建一个ML后端的简化版本，我们可以依赖Django提供的ORM类。它们的重要性在于，我们需要第三方软件来管理和存储从API调用中生成的所有数据。对于我们的特殊情况，我们需要发布图像，应用模型推理来获得语义过滤器，然后恢复它们。</p>



<p>因此，我们需要两个主要组件:</p>



<ol><li><strong> <em>模型</em> </strong>表示图像对象互换的类，</li><li><strong> <em>输入</em> </strong>和<strong> <em>输出</em> </strong>图像串行化器帮助将图像存储在数据库中。</li></ol>



<h4>模特班</h4>



<p>继承自django.db.models.Model类的Python类，定义了一组与图像对象相关的属性和特征。</p>



<ul><li><code>models.FileField</code>:存储图像文件的路径</li><li><code>models.UUIDField</code>:为每个图像实例生成一个特定的id</li><li><code>models.CharField</code>:命名每个对象实例的方法</li><li><code>models.DateTimeField</code>:保存它们被存储或更新的准确时间</li></ul>



<pre class="hljs"><span class="hljs-keyword">from</span> django.db <span class="hljs-keyword">import</span> models
<span class="hljs-keyword">from</span> API.utils <span class="hljs-keyword">import</span> get_input_image_path, get_output_image_path

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ImageSegmentation</span><span class="hljs-params">(models.Model)</span>:</span>
    uuid = models.UUIDField(primary_key=<span class="hljs-keyword">True</span>, default=uuid.uuid4, editable=<span class="hljs-keyword">False</span>)
    name = models.CharField(max_length=<span class="hljs-number">255</span>, null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
    input_image = models.FileField(upload_to=get_input_image_path, null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
    output_image = models.FileField(upload_to=get_output_image_path, null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
    verified = models.BooleanField(default=<span class="hljs-keyword">False</span>)
    created_at = models.DateTimeField(auto_now_add=<span class="hljs-keyword">True</span>)
    updated_at = models.DateTimeField(auto_now=<span class="hljs-keyword">True</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> <span class="hljs-string">"%s"</span> % self.name</pre>



<p>编写完类的代码后，将您的更改迁移到SQL数据库:</p>



<pre class="hljs">python manage.py makemigrations</pre>



<pre class="hljs">python manage.py migrate</pre>



<h4>输入和输出图像串行器</h4>



<p>用image对象的相应属性定义Django序列化程序。我们将制作两个序列化器来处理传入和传出的图像对象。</p>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">InputImageSerializer</span><span class="hljs-params">(serializers.ModelSerializer)</span>:</span>
    <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Meta</span>:</span>
        model = ImageSegmentation
        fields = (<span class="hljs-string">'uuid'</span>, <span class="hljs-string">'name'</span>, )

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">OutputImageSerializer</span><span class="hljs-params">(serializers.ModelSerializer)</span>:</span>
    <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Meta</span>:</span>
        model = ImageSegmentation
        fields = (<span class="hljs-string">'uuid'</span>, <span class="hljs-string">'name'</span>, <span class="hljs-string">'input_image'</span>, <span class="hljs-string">'output_image'</span>, <span class="hljs-string">'created_at'</span>, <span class="hljs-string">'updated_at'</span>)</pre>



<p>最后，在完成所有更改后，您需要在管理门户中注册您的新模型。只需转到admin.py文件并添加以下行即可:</p>



<pre class="hljs">admin.site.register(ImageSegmentation)</pre>



<h3>构建API端点</h3>



<p>对于POST请求，将有两个部分。一种方法处理背景定制，另一种方法用于着色部分。</p>



<ul><li><strong> <em>贴背景定制</em> </strong>:发送两张文件图片，原始照片，匹配背景。它处理它们并把它们保存到相应的文件夹中。</li></ul>



<pre class="hljs"><span class="hljs-meta">@api_view(['POST'])</span>
<span class="hljs-meta">@never_cache</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_inference</span><span class="hljs-params">(request)</span>:</span>
    property_id = request.POST.get(<span class="hljs-string">'property_id'</span>)

    
    images = dict((request.data).lists())[<span class="hljs-string">'image'</span>]
    flag = <span class="hljs-number">1</span>
    arr = []
    <span class="hljs-keyword">for</span> img_name <span class="hljs-keyword">in</span> images:
        modified_data = modify_input_for_multiple_files(property_id,
                                                        img_name)
        file_serializer = ImageSerializer(data=modified_data)
        <span class="hljs-keyword">if</span> file_serializer.is_valid():
            file_serializer.save()
            arr.append(file_serializer.data)
        <span class="hljs-keyword">else</span>:
            flag = <span class="hljs-number">0</span>

    <span class="hljs-keyword">if</span> flag == <span class="hljs-number">1</span>:
        image_path = os.path.relpath(arr[<span class="hljs-number">0</span>][<span class="hljs-string">'image'</span>], <span class="hljs-string">'/'</span>)
        bg_path = os.path.relpath(arr[<span class="hljs-number">1</span>][<span class="hljs-string">'image'</span>], <span class="hljs-string">'/'</span>)
        input_image = ImageSegmentation.objects.create(input_image=image_path, name=<span class="hljs-string">'image_%02d'</span> % uuid.uuid1())
        bg_image = ImageSegmentation.objects.create(input_image=bg_path, name=<span class="hljs-string">'image_%02d'</span> % uuid.uuid1())
        RunDeepLabInference(input_image, bg_image).save_bg_custom_output()
        serializer = OutputImageSerializer(input_image)
        <span class="hljs-keyword">return</span> Response(serializer.data)</pre>



<pre class="hljs"><span class="hljs-meta">@api_view(['POST'])</span>
<span class="hljs-meta">@never_cache</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_grayscale_inference</span><span class="hljs-params">(request)</span>:</span>
    file_ = request.FILES[<span class="hljs-string">'image'</span>]
    image = ImageSegmentation.objects.create(input_image=file_, name=<span class="hljs-string">'image_%02d'</span> % uuid.uuid1())
    RunDeepLabInference(image).save_grayscale_output()
    serializer = OutputImageSerializer(image)
    <span class="hljs-keyword">return</span> Response(serializer.data)</pre>



<ul><li><strong><em>POST for the Colorizing de oldify model</em></strong>:解析请求，提取base64图像字符串。对base64字符串进行解码，并在将其保存到输出图像文件夹之前执行彩色化滤镜。</li></ul>



<pre class="hljs"><span class="hljs-meta">@api_view(['POST'])</span>
<span class="hljs-meta">@never_cache</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">colorize_image</span><span class="hljs-params">(request)</span>:</span>
    file_image = request.FILES[<span class="hljs-string">'image'</span>]
    image = ImageSegmentation.objects.create(input_image=file_image, name=<span class="hljs-string">'image_%02d'</span> % uuid.uuid1())
    image_string = base64.b64decode(image)
    image_data = BytesIO(image_string)
    img = Image.open(image_data)
    img.save(INPUT_IMAGE)
    colorized_image = colorizer.get_transformed_image(file_image, render_factor=<span class="hljs-number">35</span>, watermarked=<span class="hljs-keyword">False</span>)
    colorized_image.save()
    serializer = OutputImageSerializer(image)
    <span class="hljs-keyword">return</span> Response(serializer.data)</pre>



<p><strong><em>GET方法</em> </strong>将简单地检索我们存储在数据库中的转换后的图像，并将它们作为静态文件提供。</p>



<pre class="hljs"><span class="hljs-meta">@api_view(['GET'])</span>
<span class="hljs-meta">@never_cache</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_images</span><span class="hljs-params">(request)</span>:</span>
    property_id = request.POST.get(<span class="hljs-string">'property_id'</span>)

    
    images = dict((request.data).lists())[<span class="hljs-string">'image'</span>]
    flag = <span class="hljs-number">1</span>
    arr = []
    <span class="hljs-keyword">for</span> img_name <span class="hljs-keyword">in</span> images:
        modified_data = modify_input_for_multiple_files(property_id,
                                                        img_name)
        file_serializer = ImageSerializer(data=modified_data)
        <span class="hljs-keyword">if</span> file_serializer.is_valid():
            file_serializer.save()
            arr.append(file_serializer.data)
        <span class="hljs-keyword">else</span>:
            flag = <span class="hljs-number">0</span>

    <span class="hljs-keyword">if</span> flag == <span class="hljs-number">1</span>:
        <span class="hljs-keyword">return</span> Response(arr, status=status.HTTP_201_CREATED)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> Response(arr, status=status.HTTP_400_BAD_REQUEST)</pre>



<h4>配置API路由</h4>



<p>1.在urls.py文件中设置URL模式的路径:</p>



<pre class="hljs">app_name = <span class="hljs-string">'api'</span>

urlpatterns = [
    path(<span class="hljs-string">r'test/'</span>, views.test_api, name=<span class="hljs-string">'test_api_communication'</span>),
    path(<span class="hljs-string">r'images/'</span>, views.get_images, name=<span class="hljs-string">'get_images'</span>),
    path(<span class="hljs-string">r'inference/'</span>, views.run_inference, name=<span class="hljs-string">'run_inference_on_images'</span>),
    path(<span class="hljs-string">r'grayscale/'</span>, views.run_grayscale_inference, name=<span class="hljs-string">'run_grayscale_inference_on_images'</span>),
    path(<span class="hljs-string">r'colorize/'</span>, views.colorize_image, name=<span class="hljs-string">'run_deoldify_colorize_filter_on_images'</span>),
    path(<span class="hljs-string">r'clean/'</span>, views.clean_folders, name=<span class="hljs-string">'clean_output_folder'</span>)
]</pre>



<p>2.在api.urls.py文件中创建API端点的地址:</p>



<pre class="hljs">urlpatterns = [
 path(<span class="hljs-string">r'test/'</span>, views.test_api, name=<span class="hljs-string">'test_api_communication'</span>),
 path(<span class="hljs-string">r'images/'</span>, views.get_images, name=<span class="hljs-string">'get_images'</span>),
 path(<span class="hljs-string">r'api/'</span>, views.run_inference, name=<span class="hljs-string">'run_inference_on_images'</span>),
]</pre>



<h2 id="h-build-the-ios-application">构建IOS应用程序</h2>



<p>总结一下，现在我们的API已经完美地运行了，我们需要做的就是构建一个小的IOS应用程序，它有两个viewcontrollers来上传图片并取回它们漂亮的转换版本，带有背景定制和颜色过滤。</p>



<p>我们最终将在应用程序界面中获得的结果类似于API中的这些示例:</p>







<p>我喜欢完全以编程方式在Swift中编写代码，我承认我对故事板和任何与XML相关的UI开发有一种厌恶。因此，让我们通过移除主故事板并设置SceneDelegate.swift文件来保持事情的简单和娱乐性。</p>



<ol><li>删除文件中的故事板名称</li><li>相应更改SceneDelegate文件:</li></ol>



<pre class="hljs">func scene(_ scene: UIScene, willConnectTo session: UISceneSession, options connectionOptions: UIScene.ConnectionOptions) {
    guard let windowScene = (scene <span class="hljs-keyword">as</span>? UIWindowScene) <span class="hljs-keyword">else</span> { <span class="hljs-keyword">return</span> }

    window = UIWindow(frame: windowScene.coordinateSpace.bounds)
    window?.windowScene = windowScene
    window?.rootViewController = ViewController()
    window?.makeKeyAndVisible()
}</pre>



<h3>创建入口点ViewController</h3>



<p>第一个ViewController将充当我们的应用程序的入口点。它将使用两个按钮定义基本布局，这两个按钮可以让用户拍照或从库中上传照片。</p>



<p>手动约束布局，避免自动布局自动元素定位。</p>



<p>该布局包含两个垂直对齐的按钮，顶部有一个UIImageView徽标。</p>



<h4>徽标图像</h4>



<ul><li>小UIImageView作为应用程序的徽标</li></ul>



<pre class="hljs">let logo: UIImageView = {
    let image = UIImageView(image: 
    image.translatesAutoresizingMaskIntoConstraints = false
   <span class="hljs-keyword">return</span> image
}()</pre>



<h4>小跟班</h4>







<pre class="hljs">lazy var openCameraBtn : CustomButton = {
   let btn = CustomButton()
    btn.translatesAutoresizingMaskIntoConstraints = false
    btn.setTitle(<span class="hljs-string">"Camera"</span>, <span class="hljs-keyword">for</span>: .normal)
    let icon = UIImage(named: <span class="hljs-string">"camera"</span>)?.resized(newSize: CGSize(width: <span class="hljs-number">45</span>, height: <span class="hljs-number">45</span>))
    let tintedImage = icon?.withRenderingMode(.alwaysTemplate)
    btn.setImage(tintedImage, <span class="hljs-keyword">for</span>: .normal)
    btn.tintColor = 
    btn.addTarget(self, action: 
    <span class="hljs-keyword">return</span> btn
}()</pre>



<ul><li>图片上传按钮:</li></ul>



<pre class="hljs">lazy var openToUploadBtn : CustomButton = {
   let btn = CustomButton()
    btn.addTarget(self, action: 
    btn.translatesAutoresizingMaskIntoConstraints = false
    <span class="hljs-keyword">return</span> btn
}()</pre>



<h4>为每个UI元素设置常规布局和约束</h4>



<pre class="hljs">fileprivate func addButtonsToSubview() {
    view.addSubview(logo)
    view.addSubview(openCameraBtn)
    view.addSubview(openToUploadBtn)
}
fileprivate func setupView() {

    logo.centerXAnchor.constraint(equalTo: self.view.centerXAnchor).isActive = true
    logo.topAnchor.constraint(equalTo: self.view.safeAreaLayoutGuide.topAnchor, constant: <span class="hljs-number">20</span>).isActive = true

    openCameraBtn.centerXAnchor.constraint(equalTo: view.centerXAnchor).isActive = true
    openCameraBtn.widthAnchor.constraint(equalToConstant: view.frame.width - <span class="hljs-number">40</span>).isActive = true
    openCameraBtn.heightAnchor.constraint(equalToConstant: <span class="hljs-number">60</span>).isActive = true
    openCameraBtn.bottomAnchor.constraint(equalTo: openToUploadBtn.topAnchor, constant: <span class="hljs-number">-40</span>).isActive = true

    openToUploadBtn.centerXAnchor.constraint(equalTo: view.centerXAnchor).isActive = true
    openToUploadBtn.widthAnchor.constraint(equalToConstant: view.frame.width - <span class="hljs-number">40</span>).isActive = true
    openToUploadBtn.heightAnchor.constraint(equalToConstant: <span class="hljs-number">60</span>).isActive = true
    openToUploadBtn.bottomAnchor.constraint(equalTo: view.bottomAnchor, constant: <span class="hljs-number">-120</span>).isActive = true

}</pre>



<ul><li>处理打开相机动作:</li></ul>



<pre class="hljs"><span class="hljs-meta">@objc func openCamera() {</span>
        <span class="hljs-keyword">if</span> UIImagePickerController.isSourceTypeAvailable(.camera) {
            let imagePicker = UIImagePickerController()
            imagePicker.delegate = self
            imagePicker.sourceType = .camera
            imagePicker.allowsEditing = true
            self.present(imagePicker, animated: true, completion: nil)
        }
    }</pre>



<ul><li>处理从库上传操作:</li></ul>



<pre class="hljs"><span class="hljs-meta">@objc func uploadLibrary() {</span>
        <span class="hljs-keyword">if</span> UIImagePickerController.isSourceTypeAvailable(.photoLibrary) {
            let imagePicker = UIImagePickerController()
            imagePicker.delegate = self
            imagePicker.sourceType = .photoLibrary
            imagePicker.allowsEditing = false
            self.present(imagePicker, animated: true, completion: nil)
        }
    }</pre>



<ul><li>从<strong><em>UIImagePickerControllerDelegate</em></strong>中覆盖<strong><em>imagePickerController</em></strong>:</li></ul>



<pre class="hljs">func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey : Any]) {
        <span class="hljs-keyword">if</span> let image = info[.originalImage] <span class="hljs-keyword">as</span>? UIImage {
            let segmentationController = ImageSegmentationViewController()
            segmentationController.modalPresentationStyle = .fullScreen
            segmentationController.inputImage.image = image
            dismiss(animated: true, completion: nil)
            self.present(segmentationController, animated: true, completion: nil)

        }
    }</pre>



<h3>处理API回调</h3>



<p>为了管理分段控制器中的HTTP API调用，我们将使用Alamofire，这是一个广泛使用的Swift包，用于处理与Swift的优雅HTTP联网。用你喜欢的方法安装软件包，我用的是CocoaPod。</p>



<p>POST方法需要一个[String: String]类型的字典，键是图像，值是原始图像的base64格式。</p>



<p>实现回调的步骤如下:</p>



<ol><li>将UIImage转换为无压缩比的base64编码，</li><li>创建将用于发送要编码的POST请求值的参数，</li><li>用Alamofire请求方法执行请求，</li><li>处理API结果，</li><li>用筛选后的图像更新UIImageView。</li></ol>



<pre class="hljs">func colorizeImages() {
    let imageDataBase64 = inputImage.image!.jpegData(compressionQuality: <span class="hljs-number">1</span>)!.base64EncodedString(options: .lineLength64Characters)
    let parameters: Parameters = [<span class="hljs-string">"image"</span>: imageDataBase64]

    AF.request(URL.init(string: self.apiEntryPoint)!, method: .post, parameters: parameters, encoding: JSONEncoding.default, headers: .none).responseJSON { (response) <span class="hljs-keyword">in</span>

    switch response.result {
        case .success(let value):
                <span class="hljs-keyword">if</span> let JSON = value <span class="hljs-keyword">as</span>? [String: Any] {
                    let base64StringOutput = JSON[<span class="hljs-string">"output_image"</span>] <span class="hljs-keyword">as</span>! String
                    let newImageData = Data(base64Encoded: base64StringOutput)
                    <span class="hljs-keyword">if</span> let newImageData = newImageData {
                       let outputImage = UIImage(data: newImageData)
                        let finalOutputImage = outputImage
                        self.inputImage.image = finalOutputImage
                        self.colorizedImage = finalOutputImage
                    }
                }
            <span class="hljs-keyword">break</span>
        case .failure(let error):
            print(error)
            <span class="hljs-keyword">break</span>
        }
    }
}</pre>



<h3>结果</h3>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/b97e68d3d09c8e52d9f104b317cfee35.png" alt="Semantic segmentation results" class="wp-image-46664" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013957im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Semantic-segmentation-results.png?resize=577%2C768&amp;ssl=1"/><figcaption><em>Results obtained applying the background customization and gray filtering</em></figcaption></figure></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img decoding="async" loading="lazy" src="../Images/df228751c380df6fe886ddf3319d75a7.png" alt="Image colorizer results" class="wp-image-46665" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206013957im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Image-colorizer-results.png?resize=768%2C607&amp;ssl=1"/><figcaption><em>Results obtained from colorize filters, transforming old black and white photographs into fully colorized ones.</em><br/><em>Top left corner original photo | Source: <a href="https://web.archive.org/web/20221206013957/http://www.skyscrapercity.com/showthread.php?t=440044" target="_blank" rel="noreferrer noopener nofollow">Old pictures Casablanca</a>, bottom right original photograph | Source: <a href="https://web.archive.org/web/20221206013957/http://www.darnna.com/phorum/read.php?3,15599,page=7" target="_blank" rel="noreferrer noopener nofollow">Souvenirs, Souvenirs</a></em></figcaption></figure></div>



<h2 id="h-conclusion">结论</h2>



<p>我们已经通过图像分割进行了一次全面的旅行，一些应用程序被证明是易于实现和相当有趣的。通过我提出的这个小应用程序，我希望我已经为你的创造力增加了一点活力。</p>



<p>我鼓励你用同样的画布测试其他应用程序。DeepLab V3还可以实现其他一些很酷的特性。</p>



<p>最后，我推荐你查阅以下参考资料:</p>




        </div>
        
    </div>    
</body>
</html>