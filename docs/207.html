<html>
<head>
<title>Conversational AI Architectures Powered by Nvidia: Tools Guide </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>英伟达支持的对话式人工智能架构:工具指南</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://neptune.ai/blog/conversational-ai-nvidia-tools-guide#0001-01-01">https://web.archive.org/web/https://neptune.ai/blog/conversational-ai-nvidia-tools-guide#0001-01-01</a></blockquote><div><div class="l-content content-wrapper js-content">
            
<p>随着自然语音合成和语音识别等深度学习领域的最新进展，AI和深度学习模型越来越多地进入我们的日常生活。事实上，许多无害的应用程序，与我们的日常生活无缝集成，正慢慢变得不可或缺。</p>



<p>大型数据驱动的服务公司严重依赖复杂的网络架构，其管道使用对话式深度学习模型，并遵守各种各样的语音任务，以尽可能最好的方式为客户服务。</p>



<p>更广泛地说，“对话式人工智能”一词意味着所有能够自然模仿人类声音、理解对话、开发个人口语意图识别配置文件(如Alexa或谷歌助手)的智能系统。简而言之，对话式AI是为了类似人类的对话。</p>



<p>深度学习在改进现有的语音合成方法方面发挥了巨大的作用，它用纯数据训练的神经网络取代了整个管道过程。根据这个观点，我想探索两个在他们的领域中非常有名的新模型:</p>



<ul><li>用于文本到语音转换的Tacotron 2，</li><li>用于自动语音识别的Quartznet。</li></ul>



<p>我们将讨论的模型版本基于Nvidia最近推出的神经模块<strong> NeMo </strong>技术。</p>



<p>我们将探索它们的架构，并深入研究Github上的Pytorch。此外，我们将实现一个Django REST API，通过公共端点为模型提供服务，最后，我们将创建一个小型IOS应用程序，通过客户端的HTTP请求来使用后端。</p>



<h2 id="ASS-TTS">深入研究ASR和TTS架构</h2>



<h3>石英网</h3>



<p>正如他们的<a href="https://web.archive.org/web/20221206024929/https://arxiv.org/pdf/1904.03288.pdf" target="_blank" rel="noreferrer noopener nofollow">论文</a>所述，Jasper是一个用于自动语音识别的端到端神经声学模型。所有Nvidia的语音识别模型，像<strong>石英网</strong>，都来自Jasper。</p>



<p>由于它是端到端的，所以整体架构支持从输入音频处理到文本转录的所有必要阶段。基础设施背后的管道涉及三个主要部分:</p>



<ul><li>编码器和解码器，用于将音频输入转换为Mel频谱图；</li><li>统计语言模型，即强化的<a href="https://web.archive.org/web/20221206024929/https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9" target="_blank" rel="noreferrer noopener nofollow"> <em> n-gram语言模型</em> </a>，从声学输入中寻找最可能产生的词序列，并生成与声谱图采样率紧密匹配的特定嵌入；</li><li>产生对应于音频输入的文本输出。</li></ul>







<p>符合Jasper架构的主要层是<strong>卷积神经网络</strong>。它们旨在通过允许整个子块融合到单个GPU内核中来促进快速GPU推理。这对于部署阶段的严格实时场景极其重要。</p>



<p>使用密集残差连接，每个块输入都与所有后续块的最后一个子块紧密连接(要了解更多关于残差网络的信息，请查看本文<a href="https://web.archive.org/web/20221206024929/https://towardsdatascience.com/introduction-to-resnets-c0a830a288a4" target="_blank" rel="noreferrer noopener nofollow">)。每个块的内核大小和过滤器数量都不相同，越深的层其大小越大。</a></p>







<h3>塔克特龙二号</h3>



<p>就编码器-解码器管道而言，Tacotron的整体架构遵循与Quartznet相似的模式。Tacotron也可以被视为一个序列到序列模型，它将字符嵌入映射到可扩展的Mel频谱图，然后是一个修改的声码器(WaveNet)，以帮助合成时域波形并生成人类听觉输出。</p>



<p><strong> <em>注</em> </strong> <em> : </em> <a href="https://web.archive.org/web/20221206024929/https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53" target="_blank" rel="noreferrer noopener nofollow"> <em>梅尔频谱图</em> </a> <em>是基于梅尔曲线的时频分布图，反映了人耳蜗的特征。</em></p>



<p>该体系结构的关键阶段是:</p>



<ul><li>处理字符嵌入的第一递归的基于序列间注意力的特征提取器。它产生Mel谱图帧作为第二级的前向输入；</li><li>一个改进的WaveNet声码器，它产生先前根据<em> Mel频谱图</em>调节的时域波形样本。</li></ul>







<p>解码器基于自回归递归神经网络。它试图从编码的输入序列中一次一帧地预测Mel光谱图。来自前面步骤的预测通过一个小的预网络，具有256个隐藏ReLU单元的2个完全连接的层。主要思想是使用完全连接的层作为信息瓶颈，因此它可以有效地学习注意力。关于注意力机制的更多见解，你可以查看以下文章:</p>







<p><em>注:这两个模型在流行的Librispeech和WSJ数据集上都取得了平均意见得分</em><strong><em>4.53</em></strong><em>的MOS，几乎可以与专业录制的演讲相媲美。</em></p>



<h2 id="NeMo">神经模块工具包，<strong> NeMo </strong></h2>



<p>NeMo是一个编程库，它利用可重用神经组件的能力来帮助您轻松安全地构建复杂的架构。神经模块是为速度而设计的，可以在并行GPU节点上横向扩展训练。</p>



<p>通过神经模块，他们想要创建通用Pytorch类，每个模型架构都是从这些类中派生出来的。该库非常强大，并提供了对话式人工智能所需的不同深度学习模型的整体旅行。语音识别、语音合成、文本到语音到自然语言处理，等等。</p>



<p>通常，神经模块是对应于神经网络的概念部分的软件抽象，例如编码器、解码器、专用损耗、语言和声学模型、或者音频和频谱图数据处理器。</p>



<p>该库构建在CUDA和cuDNN低级软件之上，利用Nvidia GPUs进行并行训练和速度推理。</p>







<p>看看他们的<a href="https://web.archive.org/web/20221206024929/https://github.com/NVIDIA/NeMo" target="_blank" rel="noreferrer noopener nofollow"> Github repo </a>，很有见地。</p>



<h2 id="Pytorch">通过Pytorch处理程序服务模型</h2>



<p>为了构建和公开访问模型推理的API端点，我们需要创建一个管理中间所需步骤的类；从预处理原始输入数据，用配置和检查点文件初始化模型实例，到运行推理并产生合适的结果——这需要很好地管理。</p>



<p>此外，如果我们想要组合多个模型来构建一个更复杂的管道，组织我们的工作是分离每个部分的关注点的关键，并使我们的代码易于维护。</p>



<p>例如，ASR输出通常不会被打断。如果我们在一个敏感的场景中使用模型，我们必须用标点符号将ASR模型的文本原始输出链接起来，以帮助澄清上下文并增强可读性。</p>



<p>在这方面，以下代码说明了不同的模型处理程序:</p>



<h3>Tacotron处理器</h3>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TacotronHandler</span><span class="hljs-params">(nn.Module)</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
       super().__init__()
       self.tacotron_model = <span class="hljs-keyword">None</span>
       self.waveglow = <span class="hljs-keyword">None</span>
       self.device = <span class="hljs-keyword">None</span>
       self.initialized = <span class="hljs-keyword">None</span>

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_load_tacotron2</span><span class="hljs-params">(self, checkpoint_file, hparams_config: Hparams)</span>:</span>
       tacotron2_checkpoint = torch.load(os.path.join(_WORK_DIR, _MODEL_DIR, checkpoint_file))
       self.tacotron_model = Tacotron2(hparams= hparams_config)
       self.tacotron_model.load_state_dict(tacotron2_checkpoint[<span class="hljs-string">'state_dict'</span>])
       self.tacotron_model.to(self.device)
       self.tacotron_model.eval()

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_load_waveglow</span><span class="hljs-params">(self, checkpoint_file, is_fp16: bool)</span>:</span>
       waveglow_checkpoint = torch.load(os.path.join(_WORK_DIR, _MODEL_DIR, checkpoint_file))
       waveglow_model = WaveGlow(
           n_mel_channels=waveglow_params.n_mel_channels,
           n_flows=waveglow_params.n_flows,
           n_group=waveglow_params.n_group,
           n_early_every=waveglow_params.n_early_every,
           n_early_size=waveglow_params.n_early_size,
           WN_config=WN_config
       )
       self.waveglow = waveglow_model
       self.waveglow.load_state_dict(waveglow_checkpoint)
       self.waveglow = waveglow_model.remove_weightnorm(waveglow_model)
       self.waveglow.to(self.device)
       self.waveglow.eval()
       <span class="hljs-keyword">if</span> is_fp16:
           <span class="hljs-keyword">from</span> apex <span class="hljs-keyword">import</span> amp
           self.waveglow, _ = amp.initialize(waveglow_model, [], opt_level=<span class="hljs-string">"3"</span>)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span><span class="hljs-params">(self)</span>:</span>
       <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.cuda.is_available():
           <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">"This model is not supported on CPU machines."</span>)
       self.device = torch.device(<span class="hljs-string">'cuda'</span>)

       self._load_tacotron2(
           checkpoint_file=<span class="hljs-string">'tacotron2.pt'</span>,
           hparams_config=tacotron_hparams)

       self._load_waveglow(
           is_fp16=<span class="hljs-keyword">False</span>,
           checkpoint_file=<span class="hljs-string">'waveglow_weights.pt'</span>)

       self.initialized = <span class="hljs-keyword">True</span>

       logger.debug(<span class="hljs-string">'Tacotron and Waveglow models successfully loaded!'</span>)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess</span><span class="hljs-params">(self, text_seq)</span>:</span>
       text = text_seq
       <span class="hljs-keyword">if</span> text_seq[<span class="hljs-number">-1</span>].isalpha() <span class="hljs-keyword">or</span> text_seq[<span class="hljs-number">-1</span>].isspace():
           text = text_seq + <span class="hljs-string">'.'</span>
       sequence = np.array(text_to_sequence(text, [<span class="hljs-string">'english_cleaners'</span>]))[<span class="hljs-keyword">None</span>, :]
       sequence = torch.from_numpy(sequence).to(device=self.device, dtype=torch.int64)
       <span class="hljs-keyword">return</span> sequence

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inference</span><span class="hljs-params">(self, data)</span>:</span>
       start_inference_time = time.time()
       _, mel_output_postnet, _, _ = self.tacotron_model.inference(data)
       <span class="hljs-keyword">with</span> torch.no_grad():
           audio = self.waveglow.infer(mel_output_postnet, sigma=<span class="hljs-number">0.666</span>)
       <span class="hljs-keyword">return</span> audio, time.time() - start_inference_time

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">postprocess</span><span class="hljs-params">(self, inference_output)</span>:</span>
       audio_numpy = inference_output[<span class="hljs-number">0</span>].data.cpu().numpy()
       output_name = <span class="hljs-string">'tts_output_{}.wav'</span>.format(uuid.uuid1())
       path = os.path.join(_AUDIO_DIR, output_name)
       print(path)
       write(path, tacotron_hparams.sampling_rate, audio_numpy)
       <span class="hljs-keyword">return</span> <span class="hljs-string">'API/audio/'</span>+ output_name</pre>



<ul><li>initialize():用各自的检查点加载<strong> Tacontron </strong>和<strong> Wave Glow </strong>。</li></ul>



<ul><li>预处理(text_seq):将原始文本转换成适合模型的输入。转换它</li><li>一组特定的字符序列。</li></ul>



<ul><li>推断(数据):对之前处理的输入进行推断，并返回与输入文本匹配的相应合成音频。</li></ul>



<ul><li>postprocess(inference_output):将wav音频文件保存到容器文件系统下的一个目录中。</li></ul>



<h3>石英网装载机</h3>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Quartznet_loader</span><span class="hljs-params">()</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, torch_device=None)</span>:</span>
       <span class="hljs-keyword">if</span> torch_device <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
           <span class="hljs-keyword">if</span> torch.cuda.is_available():
               torch_device = torch.device(<span class="hljs-string">'cuda'</span>)
           <span class="hljs-keyword">else</span>:
               torch_device = torch.device(<span class="hljs-string">'cpu'</span>)

       self.file_config = path.join(WORK_DIR, _MODEL_CONFIG)
       self.file_checkpoints = path.join(WORK_DIR, _MODEL_WEIGHTS)

       model_config = OmegaConf.load(self.file_config)
       OmegaConf.set_struct(model_config, <span class="hljs-keyword">True</span>)

       <span class="hljs-keyword">if</span> isinstance(model_config, DictConfig):
           self.config = OmegaConf.to_container(model_config, resolve=<span class="hljs-keyword">True</span>)
           self.config = OmegaConf.create(self.config)
           OmegaConf.set_struct(self.config, <span class="hljs-keyword">True</span>)

       
       instance = EncDecCTCModel(cfg=self.config)

       self.model_instance = instance
       self.model_instance.to(torch_device)
       self.model_instance.load_state_dict(torch.load(self.file_checkpoints, torch_device), <span class="hljs-keyword">False</span>)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">covert_to_text</span><span class="hljs-params">(self, audio_files)</span>:</span>
       <span class="hljs-keyword">return</span> self.model_instance.transcribe(paths2audio_files=audio_files)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_output_manifest</span><span class="hljs-params">(self, file_path)</span>:</span>
       
       manifest = dict()
       manifest[<span class="hljs-string">'audio_filepath'</span>] = file_path
       manifest[<span class="hljs-string">'duration'</span>] = <span class="hljs-number">18000</span>
       manifest[<span class="hljs-string">'text'</span>] = <span class="hljs-string">'todo'</span>

       <span class="hljs-keyword">with</span> open(file_path + <span class="hljs-string">".json"</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> fout:
           fout.write(json.dumps(manifest))

       <span class="hljs-keyword">return</span> file_path + <span class="hljs-string">".json"</span></pre>



<ul><li>init(torch_device):初始化将要运行模型检查点的硬件设备，无论它是在CPU上还是在支持Cuda的GPU上。</li><li>model _ config = omegaconf . load(self . file _ config):用编码器、解码器、优化器、预处理器和声谱图增强器的相应结构加载模型的配置文件。</li><li>instance = EncDecCTCModel(CFG = self . config):调用神经模块EncDecCTCModel从各自的Quartz-Net配置文件中实例化。该模块将创建一个特定的NeMo实例，通过它可以访问一些非常方便的驱动模型行为的函数。convert_to_text(audio_files):调用EncDecCTCModel.instance将音频wav格式文件转录为文本输出。</li><li>create _ output _ manifest(file _ path):将最终结果保存到一个json文件中，文件中包含输入音频文件路径、音频时长、音频采样率以及相应的转录文本。</li></ul>



<h3>Bert Loader</h3>



<pre class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Bert_loader</span><span class="hljs-params">()</span>:</span>
   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, torch_device=None)</span>:</span>
       <span class="hljs-keyword">if</span> torch_device <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
           <span class="hljs-keyword">if</span> torch.cuda.is_available():
               torch_device = torch.device(<span class="hljs-string">'cuda'</span>)
           <span class="hljs-keyword">else</span>:
               torch_device = torch.device(<span class="hljs-string">'cpu'</span>)

       self.file_config = path.join(WORK_DIR, _MODEL_CONFIG)
       self.file_checkpoints = path.join(WORK_DIR, _MODEL_WEIGHTS)

       model_config = OmegaConf.load(self.file_config)
       OmegaConf.set_struct(model_config, <span class="hljs-keyword">True</span>)

       <span class="hljs-keyword">if</span> isinstance(model_config, DictConfig):
           self.config = OmegaConf.to_container(model_config, resolve=<span class="hljs-keyword">True</span>)
           self.config = OmegaConf.create(self.config)
           OmegaConf.set_struct(self.config, <span class="hljs-keyword">True</span>)

       
       instance = PunctuationCapitalizationModel(cfg=self.config)

       self.model_instance = instance
       self.model_instance.to(torch_device)
       self.model_instance.load_state_dict(torch.load(self.file_checkpoints, torch_device), <span class="hljs-keyword">False</span>)

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">punctuate</span><span class="hljs-params">(self, query)</span>:</span>
       <span class="hljs-keyword">return</span> self.model_instance.add_punctuation_capitalization(query)[<span class="hljs-number">0</span>]</pre>



<ul><li>init(torch_device):在CPU或支持Cuda的GPU上初始化将运行模型检查点的硬件设备。</li><li>model _ config = omegaconf . load(self . file _ config):用编码器、解码器、优化器、预处理器和声谱图增强器的相应结构加载模型的配置文件。</li><li>instance = PunctuationCapitalizationModel(CFG = self . config):调用神经模块，该模块实现了轻量级Bert语言模型行为的内部代码。实际的实例将能够标点出原始输入文本。</li><li>标点(查询):以查询的形式获取原始输入文本，并输出一个整洁的文本版本，该文本带有完整的标点符号，可读性很好。</li></ul>



<p><em>注:关于代码的更多细节，请参考我的github repo，我在那里获得了整个项目的资源——</em><a href="https://web.archive.org/web/20221206024929/https://github.com/aymanehachcham/Conversational-API" target="_blank" rel="noreferrer noopener nofollow"><em>对话式API </em> </a></p>



<h2 id="Django">构建Django API</h2>



<p>我们将使用Django REST框架构建一个简单的API来服务我们的模型。我们的想法是配置所有需要的文件，包括模型、路由管道和视图，这样我们就可以通过forward POST和GET请求轻松地测试推理。</p>



<p>如果你需要做一个Django Rest项目的完整旅程，可以看看我以前关于这个主题的文章:<a href="https://web.archive.org/web/20221206024929/https://towardsdatascience.com/semantic-segmentation-using-a-django-api-deeplabv3-7b7904ddfed9" target="_blank" rel="noreferrer noopener nofollow">使用Django API进行语义分割</a></p>



<p>对于项目需求，我们将依靠第三方服务来通过我们的端点存储和检索语音生成的数据。所以，Django ORM助手和序列化器将会派上用场。正如他们的文档所述，Django ORM是“<em>一种创建SQL来查询和操作数据库并以Pythonic方式获得结果的Pythonic方式</em>”</p>



<p>现在:</p>



<ul><li>为TTS、ASR输出创建ORM模型；</li><li>创建相应的序列化程序；</li><li>构建您的视图(发布、删除)和路由。</li></ul>



<pre class="hljs">
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ASRText</span><span class="hljs-params">(models.Model)</span>:</span>
   uuid = models.UUIDField(primary_key=<span class="hljs-keyword">True</span>, default=uuid.uuid4, editable=<span class="hljs-keyword">False</span>)
   name = models.CharField(max_length=<span class="hljs-number">255</span>,null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
   Output = models.TextField(null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
   inference_time = models.CharField(max_length=<span class="hljs-number">255</span>,null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
   audio_join_Transformed = models.FileField(upload_to=get_asr_media, null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
   created_at = models.DateTimeField(auto_now_add=<span class="hljs-keyword">True</span>)
   <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Meta</span>:</span>
       verbose_name = <span class="hljs-string">"ASRText"</span>
       verbose_name_plural = <span class="hljs-string">"ASRTexts"</span>
       

   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>
       <span class="hljs-keyword">return</span> <span class="hljs-string">"%s"</span> % self.name

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TTSSound</span><span class="hljs-params">(models.Model)</span>:</span>
   uuid = models.UUIDField(primary_key=<span class="hljs-keyword">True</span>, default=uuid.uuid4, editable=<span class="hljs-keyword">False</span>)
   name = models.CharField(max_length=<span class="hljs-number">255</span>,null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
   text_content = models.TextField(null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
   audio_join = models.FileField(upload_to=get_tts_media, null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
   inference_time = models.CharField(max_length=<span class="hljs-number">255</span>,null=<span class="hljs-keyword">True</span>, blank=<span class="hljs-keyword">True</span>)
   created_at = models.DateTimeField(auto_now_add=<span class="hljs-keyword">True</span>)</pre>



<p>连续地创建你的视图。出于示例的目的，我将展示两个简单的POST请求，它们在ASR和TTS这两种场景中都完成了工作:</p>



<ul><li>ASR文本实例的ASRText</li><li>TTS音频实例的TTSound</li></ul>



<p>但是在实际实现API视图之前，我们需要在项目的全局范围内实例化模型处理程序，以便可以将繁重的配置文件和检查点加载到内存中并准备使用。</p>



<pre class="hljs">
bert_punctuator = Bert_loader()
quartznet_asr =Quartznet_loader()
tacotron2_tts = TTS_loader()
ASR_SAMPLING_RATE = <span class="hljs-number">22050</span></pre>



<h3>ASR发布请求</h3>



<pre class="hljs"><span class="hljs-meta">@api_view(['POST'])</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">asr_conversion</span><span class="hljs-params">(request)</span>:</span>
   data = request.FILES[<span class="hljs-string">'audio'</span>]

   audio = ASRInputSound.objects.create(audio_file=data)

   file = [audio.audio_file.path]
   start_time = time.time()
   transcription = quartznet_asr.covert_to_text(file)
   well_formatted = bert_punctuator.punctuate(transcription)

   text = ASRText.objects.create(
       name=<span class="hljs-string">'ASR_Text_%02d'</span> % uuid.uuid1(),
       Output=well_formatted,
       inference_time=str((time.time() - start_time)),
       audio_join_Transformed=audio.audio_file.path
   )

   serializer = ASROutputSeralizer(text)
   <span class="hljs-keyword">return</span> Response(serializer.data, status=status.HTTP_200_OK)</pre>



<ul><li>它获取从POST api调用file = [audio.audio_file.path]发送的音频文件的存储路径。在获得文件后，quartznet实例立即对后者进行推理，提取一个原始转录，调用bert _ punctuator.punctuate(转录)对其进行标点。</li><li>接下来，调用ASRText序列化程序将实例对象保存到数据库中，如果运行顺利，它会输出一个HTTP成功状态200，并带有包含实际转录输出的格式化JSON响应。</li></ul>



<h3>TTS发布请求</h3>



<pre class="hljs"><span class="hljs-meta">@api_view(['POST'])</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tts_transcription</span><span class="hljs-params">(request)</span>:</span>
   text = request.data.get(<span class="hljs-string">'text'</span>)
   tts_id = uuid.uuid1()
   path = <span class="hljs-string">"Audio/tts_output_%02d.wav"</span> % tts_id

   start_time = time.time()
   output_audio = tacotron2_tts.tts_inference(text)

   write(path, int(ASR_SAMPLING_RATE), output_audio)
   audio = TTSSound.objects.create(
       audio_join=path,
       name=<span class="hljs-string">'Sound_%02d'</span> % tts_id,
       text_content=text,
       inference_time=str((time.time() - start_time))
   )

   audio.save()
   serializer = TTSOutputSerializer(audio)
   <span class="hljs-keyword">return</span> Response(serializer.data, status=status.HTTP_200_OK)</pre>



<p>tts_transcription post方法也是如此，我们对输入文本进行推理，生成采样率为22050的输出音频文件，并使用write(path)方法将其保存在文件系统的本地。</p>



<p>类似地，如果请求成功，带有包含音频路径的格式化JSON响应，我们返回HTTP 200状态。</p>



<h2 id="IOS">创建iOS应用程序来使用服务</h2>



<p>在本节中，我们将创建一个简单的IOS应用程序，它带有两个ViewControllers，以一种有趣的方式使用API。</p>



<p>我们将以编程方式构建应用程序，不使用故事板，这意味着没有要切换的框或按钮——只有纯代码。</p>







<p>要从项目设置中完全删除故事板，您需要遵循以下步骤:</p>



<ul><li>删除项目结构中的<em> main.storyboard </em>文件。</li><li>对于Xcode 11或更高版本，转到i <em> nfo.plist </em>文件，删除<em>故事板名称</em>属性。</li></ul>



<figure class="wp-block-image size-large"><img decoding="async" src="../Images/25c75b980726ac920cfe4f4198d5bf86.png" alt="Storyboard Name " class="wp-image-40603" data-recalc-dims="1" data-original-src="https://web.archive.org/web/20221206024929im_/https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Storyboard-Name.png?ssl=1"/></figure>



<ul><li>如下修改SceneDelegate.swift文件中的代码，使<strong> AudioViewController </strong>成为主<strong> UINavigationController </strong></li></ul>



<pre class="hljs">func scene(_ scene: UIScene, willConnectTo session: UISceneSession, options connectionOptions: UIScene.ConnectionOptions) {
       guard let windowScence = (scene <span class="hljs-keyword">as</span>? UIWindowScene) <span class="hljs-keyword">else</span> { <span class="hljs-keyword">return</span> }

       window = UIWindow(frame: windowScence.coordinateSpace.bounds)
       window?.windowScene = windowScence
       let recordingController = AudioRecordingViewController() <span class="hljs-keyword">as</span> AudioRecordingViewController
       let navigationController = UINavigationController(rootViewController: recordingController)
       navigationController.navigationBar.isTranslucent = false
       window?.rootViewController = navigationController
       window?.makeKeyAndVisible()
   }

</pre>



<p>现在，您已经准备好开始编写主视图控制器了。</p>



<p>对于这个简单的应用程序，我们将呈现两个屏幕，每个屏幕关注一个特定的部分:</p>



<ul><li><strong> AudioViewController </strong>将负责录制您的语音，并使用录音按钮设置通用UI。</li><li><strong> ASRViewController </strong>将发送一个包含您录制的音频文件的HTTP Post请求，并接收转录的文本。将解析JSON响应，并显示文本内容。</li></ul>



<h3>为您的AudioViewContoller构造布局和UI配置</h3>



<p>AudioViewController有两个按钮:一个用于录音，一个用于停止录音。在录音按钮下面有一个最后的标签，显示一条消息通知用户他的声音正在被录音。</p>



<p>要构建没有AutoLayout的视图，我们需要在每个UI元素上设置自定义约束。</p>



<p>下面的代码可以做到这一点:</p>



<pre class="hljs">func constrainstInit(){
   NSLayoutConstraint.activate([

       // Constraints <span class="hljs-keyword">for</span> the Recording Button:
       recordingButton.widthAnchor.constraint(equalToConstant: view.frame.width - <span class="hljs-number">60</span>),
       recordingButton.heightAnchor.constraint(equalToConstant: <span class="hljs-number">60</span>),
       recordingButton.centerXAnchor.constraint(equalTo: view.centerXAnchor),
       recordingButton.topAnchor.constraint(equalTo: logo.bottomAnchor, constant: view.frame.height/<span class="hljs-number">6</span>),

       // Constraints <span class="hljs-keyword">for</span> the Label:
       recordingLabel.centerXAnchor.constraint(equalTo: view.centerXAnchor),
       recordingLabel.topAnchor.constraint(equalTo: recordingButton.bottomAnchor, constant: <span class="hljs-number">10</span>),

       // Constraints <span class="hljs-keyword">for</span> the Stop Recording Button:
       stopRecordingButton.widthAnchor.constraint(equalToConstant: view.frame.width - <span class="hljs-number">60</span>),
       stopRecordingButton.heightAnchor.constraint(equalToConstant: <span class="hljs-number">60</span>),
       stopRecordingButton.centerXAnchor.constraint(equalTo: view.centerXAnchor),
       stopRecordingButton.topAnchor.constraint(equalTo: recordingButton.bottomAnchor, constant: <span class="hljs-number">40</span>)
   ])
}</pre>



<p>将布局添加到主视图:</p>



<pre class="hljs">override func viewDidLoad() {
   super.viewDidLoad()
   view.backgroundColor = .white

   view.addSubview(recordingButton)
   view.addSubview(recordingLabel)
   view.addSubview(stopRecordingButton)
   view.addSubview(logo)

   constrainstInit()
}</pre>



<h3>音频记录逻辑</h3>



<p>对于记录部分，我们将使用来自UIKit的广为人知的AVFoundation库，它完全符合我们的目的。</p>



<p>三个主要功能将实现核心逻辑:</p>



<h4>1.录音音频()</h4>



<p>使用AVAudioRecorder共享实例处理与语音录制相关的所有逻辑，并设置内部目录路径以保存生成的音频文件。</p>



<pre class="hljs"><span class="hljs-meta">@objc</span>
func recordAudio(){
   recordingButton.isEnabled = false
   recordingLabel.isHidden = false
   stopRecordingButton.isEnabled = true

   // Code <span class="hljs-keyword">for</span> audio record:
   let dirPath = NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true)[<span class="hljs-number">0</span>] <span class="hljs-keyword">as</span> String
   let recordedFileName = <span class="hljs-string">"recording.wav"</span>
   let pathArray = [dirPath, recordedFileName]
   let filePath = URL(string: pathArray.joined(separator: <span class="hljs-string">"/"</span>))

   let recordingSession = AVAudioSession.sharedInstance()
   <span class="hljs-keyword">try</span>! recordingSession.setCategory(AVAudioSession.Category.playAndRecord, options: .defaultToSpeaker)

   <span class="hljs-keyword">try</span>! audioRecorder = AVAudioRecorder(url: filePath!, settings:  [:])
   audioRecorder.delegate = self
   audioRecorder.isMeteringEnabled = true
   audioRecorder.prepareToRecord()
   audioRecorder.record()

}</pre>



<h4>2.停止录制()</h4>



<p>完成录音会话并使默认iphone扬声器静音，以避免原始录音上出现任何不必要的噪音。</p>



<pre class="hljs"><span class="hljs-meta">@objc</span>
func stopRecording() {
   stopRecordingButton.isEnabled = false
   recordingButton.isEnabled = true

   // Stop the recording <span class="hljs-keyword">and</span> AVAudioRecord session previously active:
   audioRecorder.stop()
   let audioSession = AVAudioSession.sharedInstance()
   <span class="hljs-keyword">try</span>! audioSession.setActive(false)
}

</pre>



<h4>3.audioRecorderDidFinishRecording()委托</h4>



<p>当AVAudioRecorder会话成功结束且未引发任何硬件相关错误时触发。它将记录器文件的路径发送给ASRViewController。</p>



<pre class="hljs">func audioRecorderDidFinishRecording(_ recorder: AVAudioRecorder, successfully flag: Bool) {
   <span class="hljs-keyword">if</span> flag {
       asrController.recorderAudioURL = audioRecorder.url
       self.navigationController?.pushViewController(asrController, animated: true)
   }
   <span class="hljs-keyword">else</span> {
       print(<span class="hljs-string">"Your audio was not saved"</span>)
   }
}</pre>



<h3>ASRViewController，处理API回调</h3>



<p>API期望一个[<strong>String</strong>:<strong>String</strong>]类型的字典——键是“audio”，值是音频文件路径，实际上是一个字符串。</p>



<p>我们将使用<a href="https://web.archive.org/web/20221206024929/https://github.com/Alamofire/Alamofire" target="_blank" rel="noreferrer noopener nofollow"> Alamofire </a>，这是一个广泛使用的Swift软件包，用于处理与Swift的<em>优雅的HTTP联网</em>。用你喜欢的方法安装软件包，我用的是CocoaPod。</p>



<ul><li>创建将用于发送要编码到URLRequest中的POST请求值的参数；</li></ul>



<ul><li>使用Alamofire请求方法执行请求。传递API入口点、方法类型(在我们的例子中是POST)和参数；</li></ul>



<ul><li>处理API响应结果。如果成功，我们将把API响应解析为JSON对象，并提取输出文本来显示它。</li></ul>



<pre class="hljs">func transcribeAudio() {
   let audioFilePath = audioRecorder.url! <span class="hljs-keyword">as</span> String

   AF.request(URL.init(string: self.apiEntryPoint)!, method: .post, parameters: parameters, encoding: JSONEncoding.default, headers: .none).responseJSON { (response) <span class="hljs-keyword">in</span>

   switch response.result {
       case .success(let value):
               <span class="hljs-keyword">if</span> let JSON = value <span class="hljs-keyword">as</span>? [String: Any] {
                   let trasncribedText = JSON[<span class="hljs-string">"Output"</span>] <span class="hljs-keyword">as</span>! String
               }
           <span class="hljs-keyword">break</span>
       case .failure(let error):
           print(error)
           <span class="hljs-keyword">break</span>
       }
   }
}</pre>



<h3>结果</h3>



<p>对于下面的音频文件:<a href="https://web.archive.org/web/20221206024929/https://soundcloud.com/user-77268239/sample-for-asr" target="_blank" rel="noreferrer noopener nofollow"> Sample ASR </a>，我们从API调用:<br/> <img decoding="async" loading="lazy" src="../Images/f79b940f3f6daa7c97232c786ebcd14f.png" data-original-src="https://web.archive.org/web/20221206024929im_/https://lh4.googleusercontent.com/jC1ng_EUtLlDOxxdFjFfAvHpvJAJ_HUe4Cr2nyRLaU1PSwdg8pEePp2kc3Wa6oLdRDBy0mrq0leV3LxbBrY2lNX7DZd1yRUac0wrfMGY9F_78wQnGvkLUlcvA2ViSP8g_dyzyulu"/>获得结果JSON响应</p>



<h2 id="Conclusion">结论</h2>



<p>好的，这是整个项目的概述。从构建后端API来服务于我们的深度学习模型，到以有趣而简单的方式消费服务的小应用程序。我强烈建议您查看Github repos以获得更深入的见解:</p>







<p>正如你所看到的，语音合成和语音识别是非常有前途的，它们将不断改进，直到我们达到令人惊叹的结果。</p>



<p>对话式人工智能越来越接近无缝地讨论智能系统，甚至没有注意到与人类语音的任何实质性差异。</p>



<p>如果您想深入了解，我将为您提供更多资源:</p>







<p>感谢阅读！</p>
        </div>
        
    </div>    
</body>
</html>